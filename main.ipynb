{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config ignore table when extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curves_to_edges(cs):\n",
    "    edges = []\n",
    "    for c in cs:\n",
    "        edges += pdfplumber.utils.rect_to_edges(c)\n",
    "    return edges\n",
    "\n",
    "def not_within_bboxes(obj, bboxes):\n",
    "    def obj_in_bbox(_bbox):\n",
    "        v_mid = (obj[\"top\"] + obj[\"bottom\"]) / 2\n",
    "        h_mid = (obj[\"x0\"] + obj[\"x1\"]) / 2\n",
    "        x0, top, x1, bottom = _bbox\n",
    "        return (h_mid >= x0) and (h_mid < x1) and (v_mid >= top) and (v_mid < bottom)\n",
    "    return not any(obj_in_bbox(__bbox) for __bbox in bboxes)\n",
    "  \n",
    "def get_table_config(page):\n",
    "  config = {\n",
    "      \"vertical_strategy\": \"explicit\",\n",
    "      \"horizontal_strategy\": \"explicit\",\n",
    "      \"explicit_vertical_lines\": curves_to_edges(page.curves + page.edges),\n",
    "      \"explicit_horizontal_lines\": curves_to_edges(page.curves + page.edges),\n",
    "      \"intersection_y_tolerance\": 10,\n",
    "  }\n",
    "  return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create raw data after read from pdf (ignore table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pdfplumber.open('s3-userguide.pdf') as pdf:\n",
    "    # init raw data array\n",
    "    lines_data = []\n",
    "    # loop each page with \n",
    "    for page_num, page in enumerate(pdf.pages, start=1):\n",
    "        lines = page.extract_text(layout=True).split('\\n')\n",
    "        for line_num, line in enumerate(lines, start=1):\n",
    "            lines_data.append({\n",
    "                \"page\": page_num,\n",
    "                \"line\": line_num,\n",
    "                \"content\": line,\n",
    "                \"title\": lines[2],\n",
    "                \"section_title\": lines[-4]\n",
    "            })\n",
    "\n",
    "    with open('raw.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(lines_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean, add rule and group raw data to data which can import to ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw.json', 'r', encoding='utf-8') as f:\n",
    "    lines_data = json.load(f)\n",
    "\n",
    "filtered_lines_data = []\n",
    "current_content = ''\n",
    "for line_data in lines_data:\n",
    "    content = line_data['content'].strip()\n",
    "    if content:\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        if \"API Version 2006-03-01\" not in content and \"Amazon Simple Storage Service\" not in content:\n",
    "            if (current_content != '' and current_content[-1] != '.') or content[0].islower() or content[0] == '-' or content[0] == 'â€¢':\n",
    "              if content[0] == '-' or content[0] == 'â€¢':\n",
    "                current_content += '\\n' +  content\n",
    "              else:        \n",
    "                current_content += ' ' + content\n",
    "            else:\n",
    "                if current_content:\n",
    "                    line_data['title'] = re.sub(r'\\s+', ' ', line_data['title'].strip())\n",
    "                    line_data['content'] = current_content\n",
    "                    line_data['section_title'] = line_data['section_title'].strip().split(\"API Version\")[0].strip()\n",
    "                    if line_data['section_title'] and \"Table of Contents\" not in line_data['content']:\n",
    "                        filtered_lines_data.append(line_data)\n",
    "                current_content = content\n",
    "if current_content:\n",
    "    line_data['content'] = current_content\n",
    "    filtered_lines_data.append(line_data)\n",
    "\n",
    "with open('output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(filtered_lines_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put data to ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/tvpwz_9d6c56fms92tltys800000gn/T/ipykernel_55600/2731284087.py:8: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(index='s3_guide', body=doc)\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'scheme': 'http'}])\n",
    "\n",
    "with open('output.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for doc in data:\n",
    "    es.index(index='s3_guide', body=doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
