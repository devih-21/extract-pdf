{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import json\n",
    "import re\n",
    "from elasticsearch import Elasticsearch\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config ignore table when extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_within_bboxes(obj, bboxes):\n",
    "    \"\"\"Check if the object is in any of the table's bbox.\"\"\"\n",
    "\n",
    "    def obj_in_bbox(_bbox):\n",
    "        \"\"\"Define objects in box.\n",
    "\n",
    "        See https://github.com/jsvine/pdfplumber/blob/stable/pdfplumber/table.py#L404\n",
    "        \"\"\"\n",
    "        v_mid = (obj[\"top\"] + obj[\"bottom\"]) / 2\n",
    "        h_mid = (obj[\"x0\"] + obj[\"x1\"]) / 2\n",
    "        x0, top, x1, bottom = _bbox\n",
    "        return (h_mid >= x0) and (h_mid < x1) and (v_mid >= top) and (v_mid < bottom)\n",
    "\n",
    "    return not any(obj_in_bbox(__bbox) for __bbox in bboxes)\n",
    "\n",
    "\n",
    "def filter_tables(page: pdfplumber.page.Page) -> pdfplumber.page.Page:\n",
    "    if page.find_tables() != []:\n",
    "        bboxes = [table.bbox for table in page.find_tables()]\n",
    "        bbox_not_within_bboxes = partial(not_within_bboxes, bboxes=bboxes)\n",
    "        page = page.filter(bbox_not_within_bboxes)\n",
    "\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create raw data after read from pdf (ignore table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nYou can access Amazon S3 and its features only in the AWS Regions that are enabled for\\nyour account. For more information about enabling a Region to create and manage AWS\\nresources, see Managing AWS Regions in the AWS General Reference.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• Amazon S3 does not support object locking for concurrent writers. If two PUT requests\\nare simultaneously made to the same key, the request with the latest timestamp wins. If\\nthis is an issue, you must build an object-locking mechanism into your application.\\n• Updates are key-based. There is no way to make atomic updates across keys. For\\nexample, you cannot make the update of one key dependent on the update of another\\nkey unless you design this functionality into your application.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nSOAP API support over HTTP is deprecated, but it is still available over HTTPS. Newer\\nAmazon S3 features are not supported for SOAP. We recommend that you use either the\\nREST API or the AWS SDKs.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou are not charged for creating a bucket. You are charged only for storing objects in\\nthe bucket and for transferring objects in and out of the bucket. The charges that you\\nincur through following the examples in this guide are minimal (less than $1). For more\\ninformation about storage charges, see Amazon S3 pricing.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo minimize latency and costs and address regulatory requirements, choose a Region\\nclose to you. Objects stored in a Region never leave that Region unless you explicitly\\ntransfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS\\nservice endpoints in the Amazon Web Services General Reference.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nAvoid including sensitive information, such as account numbers, in the bucket name.\\nThe bucket name is visible in the URLs that point to the objects in the bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThis option:\\n• Is not available in the AWS CLI and is only available in console\\n• Is not available for directory buckets\\n• Does not copy the bucket policy from the existing bucket to the new bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe default setting is Bucket owner enforced. To apply the default setting and keep\\nACLs disabled, only the s3:CreateBucket permission is needed. To enable ACLs, you\\nmust have the s3:PutBucketOwnershipControls permission.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo enable all Block Public Access settings, only the s3:CreateBucket permission\\nis required. To turn off any Block Public Access settings, you must have the\\ns3:PutBucketPublicAccessBlock permission.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf you use the SSE-KMS option for your default encryption configuration, you are\\nsubject to the requests per second (RPS) quota of AWS KMS. For more information\\nabout AWS KMS quotas and how to request a quota increase, see Quotas in the AWS\\nKey Management Service Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nYou can use only KMS keys that are available in the same AWS Region as the\\nbucket. The Amazon S3 console lists only the first 100 KMS keys in the same\\nRegion as the bucket. To use a KMS key that is not listed, you must enter your\\nKMS key ARN. If you want to use a KMS key that is owned by a different account,\\nyou must first have permission to use the key and then you must enter the KMS\\nkey ARN. For more information on cross account permissions for KMS keys, see\\nCreating KMS keys that other accounts can use in the AWS Key Management\\nService Developer Guide. For more information on SSE-KMS, see Specifying server-\\nside encryption with AWS KMS (SSE-KMS).\\nWhen you use an AWS KMS key for server-side encryption in Amazon S3, you must\\nchoose a symmetric encryption KMS key. Amazon S3 supports only symmetric\\nencryption KMS keys and not asymmetric KMS keys. For more information, see\\nIdentifying symmetric and asymmetric KMS keys in the AWS Key Management\\nService Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nEnabling Object Lock also enables versioning for the bucket. After enabling\\nyou must configure the Object Lock default retention and legal hold settings to\\nprotect new objects from being deleted or overwritten.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo create an Object Lock enabled bucket, you must have the following\\npermissions: s3:CreateBucket, s3:PutBucketVersioning and\\ns3:PutBucketObjectLockConfiguration.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\n• You can download only one object at a time.\\n• If you use the Amazon S3 console to download an object whose key name ends with a\\nperiod (.), the period is removed from the key name of the downloaded object. To retain', '']]]\n",
      "[[['', 'the period at the end of the name of the downloaded object, you must use the AWS\\nCommand Line Interface (AWS CLI), AWS SDKs, or Amazon S3 REST API.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nEmptying the bucket cannot be undone. Objects added to the bucket while the empty\\nbucket action is in progress will be deleted.', ''], ['', '', '']], [['', '', ''], ['', \"Important\\nDeleting a bucket cannot be undone. Bucket names are unique. If you delete your\\nbucket, another AWS user can use the name. If you want to continue to use the same\\nbucket name, don't delete your bucket. Instead, empty and keep the bucket.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor simplicity, this tutorial creates and uses an IAM user. After completing this tutorial,\\nremember to Delete the IAM user. For production use, we recommend that you follow\\nthe Security best practices in IAM in the IAM User Guide. A best practice requires human\\nusers to use federation with an identity provider to access AWS with temporary credentials.\\nAnother best practice is to require workloads to use temporary credentials with IAM\\nroles to access AWS. To learn about using AWS IAM Identity Center to create users with\\ntemporary credentials, see Getting started in the AWS IAM Identity Center User Guide.\\nThis tutorial also uses full-access AWS managed policies. For production use, we\\nrecommend that you instead grant only the minimum permissions necessary for your use\\ncase, in accordance with security best practices.', ''], ['', '', '']], [['', '', ''], ['', 'python --version', ''], ['', '', '']], [['', '', ''], ['', 'python3 --version', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'python --version', ''], ['', '', '']], [['', '', ''], ['', 'pip --version', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Amazon S3 Object Lambda Tutorial:\\nYou can add your own code to process data retrieved from S3 before\\nreturning it to an application.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'import boto3\\nimport requests\\nfrom botocore.config import Config\\n# This function capitalizes all text in the original object\\ndef lambda_handler(event, context):\\nobject_context = event[\"getObjectContext\"]\\n# Get the presigned URL to fetch the requested original object\\n# from S3\\ns3_url = object_context[\"inputS3Url\"]\\n# Extract the route and request token from the input context\\nrequest_route = object_context[\"outputRoute\"]\\nrequest_token = object_context[\"outputToken\"]\\n# Get the original S3 object using the presigned URL\\nresponse = requests.get(s3_url)\\noriginal_object = response.content.decode(\"utf-8\")\\n# Transform all text in the original object to uppercase', '']]]\n",
      "[[['', \"# You can replace it with your custom code based on your use case\\ntransformed_object = original_object.upper()\\n# Write object back to S3 Object Lambda\\ns3 = boto3.client('s3', config=Config(signature_version='s3v4'))\\n# The WriteGetObjectResponse API sends the transformed data\\n# back to S3 Object Lambda and then to the user\\ns3.write_get_object_response(\\nBody=transformed_object,\\nRequestRoute=request_route,\\nRequestToken=request_token)\\n# Exit the Lambda function: return the status code\\nreturn {'status_code': 200}\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe preceding example Lambda function loads the entire requested object into\\nmemory before transforming it and returning it to the client. Alternatively, you\\ncan stream the object from S3 to avoid loading the entire object into memory. This\\napproach can be useful when working with large objects. For more information about\\nstreaming responses with Object Lambda Access Points, see the streaming examples in\\nWorking with GetObject requests in Lambda.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'python -m pip install virtualenv', ''], ['', '', '']], [['', '', ''], ['', 'python -m virtualenv venv', ''], ['', '', '']], [['', '', ''], ['', 'source venv/bin/activate', ''], ['', '', '']], [['', '', ''], ['', '.\\\\venv\\\\Scripts\\\\activate', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'pip3 install boto3', ''], ['', '', '']], [['', '', ''], ['', 'pip3 install requests', ''], ['', '', '']], [['', '', ''], ['', 'deactivate', ''], ['', '', '']], [['', '', ''], ['', 'Tip\\nThe following commands might need to be adjusted to work in your particular\\nenvironment. For example, a library might appear in site-packages or dist-\\npackages, and the first folder might be lib or lib64. Also, the python folder might\\nbe named with a different Python version. To locate a specific package, use the pip\\nshow command.', ''], ['', '', '']], [['', '', ''], ['', 'cd venv/lib/python3.8/site-packages', ''], ['', '', '']], [['', '', ''], ['', 'zip -r ../../../../lambda.zip .', ''], ['', '', '']], [['', '', ''], ['', 'cd .\\\\venv\\\\Lib\\\\site-packages\\\\', ''], ['', '', '']], [['', '', ''], ['', 'powershell Compress-Archive * ../../../lambda.zip', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'cd ../../../../', ''], ['', '', '']], [['', '', ''], ['', 'zip -g lambda.zip transform.py', ''], ['', '', '']], [['', '', ''], ['', 'cd ..\\\\..\\\\..\\\\', ''], ['', '', '']], [['', '', ''], ['', 'powershell Compress-Archive -update transform.py lambda.zip', ''], ['', '', '']], [['', '', ''], ['', 'lambda.zip$\\n# transform.py\\n# __pycache__\\n| boto3/\\n# certifi/\\n# pip/\\n# requests/\\n...', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'import boto3\\nfrom botocore.config import Config\\ns3 = boto3.client(\\'s3\\', config=Config(signature_version=\\'s3v4\\'))\\ndef getObject(bucket, key):\\nobjectBody = s3.get_object(Bucket = bucket, Key = key)\\nprint(objectBody[\"Body\"].read().decode(\"utf-8\"))', '']]]\n",
      "[[['', 'print(\"\\\\n\")\\nprint(\\'Original object from the S3 bucket:\\')\\n# Replace the two input parameters of getObject() below with\\n# the S3 bucket name that you created in Step 1 and\\n# the name of the file that you uploaded to the S3 bucket in Step 2\\ngetObject(\"tutorial-bucket\",\\n\"tutorial.txt\")\\nprint(\\'Object transformed by S3 Object Lambda:\\')\\n# Replace the two input parameters of getObject() below with\\n# the ARN of your S3 Object Lambda Access Point that you saved earlier and\\n# the name of the file with the transformed data (which in this case is\\n# the same as the name of the file that you uploaded to the S3 bucket\\n# in Step 2)\\ngetObject(\"arn:aws:s3-object-lambda:us-west-2:111122223333:accesspoint/tutorial-\\nobject-lambda-accesspoint\",\\n\"tutorial.txt\")', ''], ['', '', '']], [['', '', ''], ['', 'python3 tutorial_print.py', ''], ['', '', '']], [['', '', ''], ['', 'Original object from the S3 bucket:\\nAmazon S3 Object Lambda Tutorial:\\nYou can add your own code to process data retrieved from S3 before\\nreturning it to an application.\\nObject transformed by S3 Object Lambda:\\nAMAZON S3 OBJECT LAMBDA TUTORIAL:\\nYOU CAN ADD YOUR OWN CODE TO PROCESS DATA RETRIEVED FROM S3 BEFORE\\nRETURNING IT TO AN APPLICATION.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor simplicity, this tutorial creates and uses an IAM user. After completing this tutorial,\\nremember to Delete the IAM user. For production use, we recommend that you follow\\nthe Security best practices in IAM in the IAM User Guide. A best practice requires human\\nusers to use federation with an identity provider to access AWS with temporary credentials.\\nAnother best practice is to require workloads to use temporary credentials with IAM\\nroles to access AWS. To learn about using AWS IAM Identity Center to create users with\\ntemporary credentials, see Getting started in the AWS IAM Identity Center User Guide.\\nThis tutorial also uses full-access policies. For production use, we recommend that you\\ninstead grant only the minimum permissions necessary for your use case, in accordance\\nwith security best practices.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Hello Zhang Wei, I am John. Your AnyCompany Financial Services,\\nLLC credit card account 1111-0000-1111-0008 has a minimum payment\\nof $24.53 that is due by July 31st. Based on your autopay settings,\\nwe will withdraw your payment on the due date from your\\nbank account number XXXXXX1111 with the routing number XXXXX0000.\\nYour latest statement was mailed to 100 Main Street, Any City,\\nWA 98121.\\nAfter your payment is received, you will receive a confirmation\\ntext message at 206-555-0100.\\nIf you have questions about your bill, AnyCompany Customer Service\\nis available by phone at 206-555-0199 or\\nemail at support@anycompany.com.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Hello *********. Your AnyCompany Financial Services,\\nLLC credit card account ******************* has a minimum payment\\nof $24.53 that is due by *********. Based on your autopay settings,\\nwe will withdraw your payment on the due date from your\\nbank account ********** with the routing number *********.\\nYour latest statement was mailed to **********************************.\\nAfter your payment is received, you will receive a confirmation\\ntext message at ************.\\nIf you have questions about your bill, AnyCompany Customer Service', '']]]\n",
      "[[['', 'is available by phone at ************ or\\nemail at **********************.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'https://CloudFront distribution domain name/Path to an S3 video', ''], ['', '', '']], [['', '', ''], ['', 'https://CloudFront distribution alternate domain name/Path to an S3 video', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nWhen you register a domain, it costs money immediately and it's irreversible. You can\\nchoose not to auto-renew the domain, but you pay up front and own it for the year. For\\nmore information, see Registering a new domain in the Amazon Route 53 Developer Guide.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2008-10-17\",\\n\"Id\": \"PolicyForCloudFrontPrivateContent\",\\n\"Statement\": [\\n{\\n\"Sid\": \"1\",', '']]]\n",
      "[[['', '\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access\\nIdentity EH1HDMB1FH2TC\"\\n},\\n\"Action\": \"s3:GetObject\",\\n\"Resource\": \"arn:aws:s3:::tutorial-bucket/*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'https://CloudFront distribution domain name/Path to the S3 video', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nThe alternate domain name (CNAME) that you add must be covered by the SSL\\ncertificate that you previously attached to your CloudFront distribution.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf you don't see the SSL certificate immediately after you request it, wait 30 minutes,\\nand then refresh the list until the SSL certificate is available for you to select.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'https://CloudFront distribution alternate domain name/Path to the S3 video', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nWhen you register a domain, it costs money immediately and it's irreversible. You can\\nchoose not to auto-renew the domain, but you pay up front and own it for the year. For\\nmore information, see Registering a new domain in the Amazon Route 53 Developer Guide.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't delete records that have a Type value of NS or SOA.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Warning\\nIf you want to keep your domain registration but stop routing internet traffic to your\\nwebsite or web application, we recommend that you delete records in the hosted zone (as\\ndescribed in the prior section) instead of deleting the hosted zone.\\nIf you delete a hosted zone, someone else can use the domain and route traffic to their own\\nresources using your domain name.\\nIn addition, if you delete a hosted zone, you can't undelete it. You must create a new\\nhosted zone and update the name servers for your domain registration, which can take up\\nto 48 hours to take effect.\\nIf you want to make the domain unavailable on the internet, you can first transfer your\\nDNS service to a free DNS service and then delete the Route 53 hosted zone. This prevents\\nfuture DNS queries from possibly being misrouted.\\n1. If the domain is registered with Route 53, see Adding or changing name servers and glue\\nrecords for a domain in the Amazon Route 53 Developer Guide for information about how\\nto replace Route 53 name servers with name servers for the new DNS service.\\n2. If the domain is registered with another registrar, use the method provided by the\\nregistrar to change name servers for the domain.\\nNote\\nIf you're deleting a hosted zone for a subdomain (www.example.com), you don't\\nneed to change name servers for the domain (example.com).\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf you're deleting a hosted zone for a subdomain (www.example.com), you don't\\nneed to change name servers for the domain (example.com).\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nIf you want to transfer the domain between AWS accounts or transfer the domain to\\nanother registrar, don't delete the domain and expect to immediately reregister it. Instead,\\nsee the applicable documentation in the Amazon Route 53 Developer Guide:\\n• Transferring a domain to a different AWS account\\n• Transferring a domain from Amazon Route 53 to another registrar\", ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Warning\\nBefore you complete this step, review Blocking public access to your Amazon S3\\nstorage to ensure that you understand and accept the risks involved with allowing\\npublic access. When you turn off Block Public Access settings to make your bucket\\npublic, anyone on the internet can access your bucket. We recommend that you block\\nall public access to your buckets.\\nIf you don’t want to clear the Block Public Access settings, you can use Amazon\\nCloudFront to deliver the transcoded media files to viewers (end users). For more', '']]]\n",
      "[[['', 'information, see Tutorial: Hosting on-demand streaming video with Amazon S3,\\nAmazon CloudFront, and Amazon Route 53.', ''], ['', '', '']], [['', '', ''], ['', '[\\n{\\n\"AllowedOrigins\": [\\n\"*\"\\n],\\n\"AllowedMethods\": [\\n\"GET\"\\n],\\n\"AllowedHeaders\": [\\n\"*\"', '']]]\n",
      "[[['', '],\\n\"ExposeHeaders\": []\\n}\\n]', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Action\": [\\n\"logs:CreateLogGroup\",\\n\"logs:CreateLogStream\",\\n\"logs:PutLogEvents\"\\n],\\n\"Resource\": \"*\",\\n\"Effect\": \"Allow\",\\n\"Sid\": \"Logging\"\\n},\\n{\\n\"Action\": [\\n\"iam:PassRole\"\\n],\\n\"Resource\": [\\n\"arn:aws:iam::111122223333:role/tutorial-mediaconvert-role\"\\n],\\n\"Effect\": \"Allow\",\\n\"Sid\": \"PassRole\"\\n},\\n{\\n\"Action\": [\\n\"mediaconvert:*\"\\n],\\n\"Resource\": [\\n\"*\"\\n],\\n\"Effect\": \"Allow\",\\n\"Sid\": \"MediaConvertService\"\\n},\\n{\\n\"Action\": [\\n\"s3:*\"\\n],\\n\"Resource\": [\\n\"*\"\\n],\\n\"Effect\": \"Allow\",\\n\"Sid\": \"S3Service\"\\n}', '']]]\n",
      "[[['', ']\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"OutputGroups\": [\\n{\\n\"CustomName\": \"HLS\",\\n\"Name\": \"Apple HLS\",\\n\"Outputs\": [\\n{\\n\"ContainerSettings\": {\\n\"Container\": \"M3U8\",\\n\"M3u8Settings\": {\\n\"AudioFramesPerPes\": 4,\\n\"PcrControl\": \"PCR_EVERY_PES_PACKET\",\\n\"PmtPid\": 480,\\n\"PrivateMetadataPid\": 503,\\n\"ProgramNumber\": 1,\\n\"PatInterval\": 0,\\n\"PmtInterval\": 0,\\n\"TimedMetadata\": \"NONE\",', '']]]\n",
      "[[['', '\"VideoPid\": 481,\\n\"AudioPids\": [\\n482,\\n483,\\n484,\\n485,\\n486,\\n487,\\n488,\\n489,\\n490,\\n491,\\n492\\n]\\n}\\n},\\n\"VideoDescription\": {\\n\"Width\": 640,\\n\"ScalingBehavior\": \"DEFAULT\",\\n\"Height\": 360,\\n\"TimecodeInsertion\": \"DISABLED\",\\n\"AntiAlias\": \"ENABLED\",\\n\"Sharpness\": 50,\\n\"CodecSettings\": {\\n\"Codec\": \"H_264\",\\n\"H264Settings\": {\\n\"InterlaceMode\": \"PROGRESSIVE\",\\n\"NumberReferenceFrames\": 3,\\n\"Syntax\": \"DEFAULT\",\\n\"Softness\": 0,\\n\"GopClosedCadence\": 1,\\n\"GopSize\": 2,\\n\"Slices\": 1,\\n\"GopBReference\": \"DISABLED\",\\n\"MaxBitrate\": 1200000,\\n\"SlowPal\": \"DISABLED\",\\n\"SpatialAdaptiveQuantization\": \"ENABLED\",\\n\"TemporalAdaptiveQuantization\": \"ENABLED\",\\n\"FlickerAdaptiveQuantization\": \"DISABLED\",\\n\"EntropyEncoding\": \"CABAC\",\\n\"FramerateControl\": \"INITIALIZE_FROM_SOURCE\",\\n\"RateControlMode\": \"QVBR\",\\n\"CodecProfile\": \"MAIN\",\\n\"Telecine\": \"NONE\",', '']]]\n",
      "[[['', '\"MinIInterval\": 0,\\n\"AdaptiveQuantization\": \"HIGH\",\\n\"CodecLevel\": \"AUTO\",\\n\"FieldEncoding\": \"PAFF\",\\n\"SceneChangeDetect\": \"TRANSITION_DETECTION\",\\n\"QualityTuningLevel\": \"SINGLE_PASS_HQ\",\\n\"FramerateConversionAlgorithm\": \"DUPLICATE_DROP\",\\n\"UnregisteredSeiTimecode\": \"DISABLED\",\\n\"GopSizeUnits\": \"SECONDS\",\\n\"ParControl\": \"INITIALIZE_FROM_SOURCE\",\\n\"NumberBFramesBetweenReferenceFrames\": 2,\\n\"RepeatPps\": \"DISABLED\"\\n}\\n},\\n\"AfdSignaling\": \"NONE\",\\n\"DropFrameTimecode\": \"ENABLED\",\\n\"RespondToAfd\": \"NONE\",\\n\"ColorMetadata\": \"INSERT\"\\n},\\n\"OutputSettings\": {\\n\"HlsSettings\": {\\n\"AudioGroupId\": \"program_audio\",\\n\"AudioRenditionSets\": \"program_audio\",\\n\"SegmentModifier\": \"$dt$\",\\n\"IFrameOnlyManifest\": \"EXCLUDE\"\\n}\\n},\\n\"NameModifier\": \"_360\"\\n},\\n{\\n\"ContainerSettings\": {\\n\"Container\": \"M3U8\",\\n\"M3u8Settings\": {\\n\"AudioFramesPerPes\": 4,\\n\"PcrControl\": \"PCR_EVERY_PES_PACKET\",\\n\"PmtPid\": 480,\\n\"PrivateMetadataPid\": 503,\\n\"ProgramNumber\": 1,\\n\"PatInterval\": 0,\\n\"PmtInterval\": 0,\\n\"TimedMetadata\": \"NONE\",\\n\"TimedMetadataPid\": 502,\\n\"VideoPid\": 481,\\n\"AudioPids\": [', '']]]\n",
      "[[['', '482,\\n483,\\n484,\\n485,\\n486,\\n487,\\n488,\\n489,\\n490,\\n491,\\n492\\n]\\n}\\n},\\n\"VideoDescription\": {\\n\"Width\": 960,\\n\"ScalingBehavior\": \"DEFAULT\",\\n\"Height\": 540,\\n\"TimecodeInsertion\": \"DISABLED\",\\n\"AntiAlias\": \"ENABLED\",\\n\"Sharpness\": 50,\\n\"CodecSettings\": {\\n\"Codec\": \"H_264\",\\n\"H264Settings\": {\\n\"InterlaceMode\": \"PROGRESSIVE\",\\n\"NumberReferenceFrames\": 3,\\n\"Syntax\": \"DEFAULT\",\\n\"Softness\": 0,\\n\"GopClosedCadence\": 1,\\n\"GopSize\": 2,\\n\"Slices\": 1,\\n\"GopBReference\": \"DISABLED\",\\n\"MaxBitrate\": 3500000,\\n\"SlowPal\": \"DISABLED\",\\n\"SpatialAdaptiveQuantization\": \"ENABLED\",\\n\"TemporalAdaptiveQuantization\": \"ENABLED\",\\n\"FlickerAdaptiveQuantization\": \"DISABLED\",\\n\"EntropyEncoding\": \"CABAC\",\\n\"FramerateControl\": \"INITIALIZE_FROM_SOURCE\",\\n\"RateControlMode\": \"QVBR\",\\n\"CodecProfile\": \"MAIN\",\\n\"Telecine\": \"NONE\",\\n\"MinIInterval\": 0,\\n\"AdaptiveQuantization\": \"HIGH\",', '']]]\n",
      "[[['', '\"CodecLevel\": \"AUTO\",\\n\"FieldEncoding\": \"PAFF\",\\n\"SceneChangeDetect\": \"TRANSITION_DETECTION\",\\n\"QualityTuningLevel\": \"SINGLE_PASS_HQ\",\\n\"FramerateConversionAlgorithm\": \"DUPLICATE_DROP\",\\n\"UnregisteredSeiTimecode\": \"DISABLED\",\\n\"GopSizeUnits\": \"SECONDS\",\\n\"ParControl\": \"INITIALIZE_FROM_SOURCE\",\\n\"NumberBFramesBetweenReferenceFrames\": 2,\\n\"RepeatPps\": \"DISABLED\"\\n}\\n},\\n\"AfdSignaling\": \"NONE\",\\n\"DropFrameTimecode\": \"ENABLED\",\\n\"RespondToAfd\": \"NONE\",\\n\"ColorMetadata\": \"INSERT\"\\n},\\n\"OutputSettings\": {\\n\"HlsSettings\": {\\n\"AudioGroupId\": \"program_audio\",\\n\"AudioRenditionSets\": \"program_audio\",\\n\"SegmentModifier\": \"$dt$\",\\n\"IFrameOnlyManifest\": \"EXCLUDE\"\\n}\\n},\\n\"NameModifier\": \"_540\"\\n},\\n{\\n\"ContainerSettings\": {\\n\"Container\": \"M3U8\",\\n\"M3u8Settings\": {\\n\"AudioFramesPerPes\": 4,\\n\"PcrControl\": \"PCR_EVERY_PES_PACKET\",\\n\"PmtPid\": 480,\\n\"PrivateMetadataPid\": 503,\\n\"ProgramNumber\": 1,\\n\"PatInterval\": 0,\\n\"PmtInterval\": 0,\\n\"TimedMetadata\": \"NONE\",\\n\"VideoPid\": 481,\\n\"AudioPids\": [\\n482,\\n483,\\n484,', '']]]\n",
      "[[['', '485,\\n486,\\n487,\\n488,\\n489,\\n490,\\n491,\\n492\\n]\\n}\\n},\\n\"VideoDescription\": {\\n\"Width\": 1280,\\n\"ScalingBehavior\": \"DEFAULT\",\\n\"Height\": 720,\\n\"TimecodeInsertion\": \"DISABLED\",\\n\"AntiAlias\": \"ENABLED\",\\n\"Sharpness\": 50,\\n\"CodecSettings\": {\\n\"Codec\": \"H_264\",\\n\"H264Settings\": {\\n\"InterlaceMode\": \"PROGRESSIVE\",\\n\"NumberReferenceFrames\": 3,\\n\"Syntax\": \"DEFAULT\",\\n\"Softness\": 0,\\n\"GopClosedCadence\": 1,\\n\"GopSize\": 2,\\n\"Slices\": 1,\\n\"GopBReference\": \"DISABLED\",\\n\"MaxBitrate\": 5000000,\\n\"SlowPal\": \"DISABLED\",\\n\"SpatialAdaptiveQuantization\": \"ENABLED\",\\n\"TemporalAdaptiveQuantization\": \"ENABLED\",\\n\"FlickerAdaptiveQuantization\": \"DISABLED\",\\n\"EntropyEncoding\": \"CABAC\",\\n\"FramerateControl\": \"INITIALIZE_FROM_SOURCE\",\\n\"RateControlMode\": \"QVBR\",\\n\"CodecProfile\": \"MAIN\",\\n\"Telecine\": \"NONE\",\\n\"MinIInterval\": 0,\\n\"AdaptiveQuantization\": \"HIGH\",\\n\"CodecLevel\": \"AUTO\",\\n\"FieldEncoding\": \"PAFF\",\\n\"SceneChangeDetect\": \"TRANSITION_DETECTION\",', '']]]\n",
      "[[['', '\"QualityTuningLevel\": \"SINGLE_PASS_HQ\",\\n\"FramerateConversionAlgorithm\": \"DUPLICATE_DROP\",\\n\"UnregisteredSeiTimecode\": \"DISABLED\",\\n\"GopSizeUnits\": \"SECONDS\",\\n\"ParControl\": \"INITIALIZE_FROM_SOURCE\",\\n\"NumberBFramesBetweenReferenceFrames\": 2,\\n\"RepeatPps\": \"DISABLED\"\\n}\\n},\\n\"AfdSignaling\": \"NONE\",\\n\"DropFrameTimecode\": \"ENABLED\",\\n\"RespondToAfd\": \"NONE\",\\n\"ColorMetadata\": \"INSERT\"\\n},\\n\"OutputSettings\": {\\n\"HlsSettings\": {\\n\"AudioGroupId\": \"program_audio\",\\n\"AudioRenditionSets\": \"program_audio\",\\n\"SegmentModifier\": \"$dt$\",\\n\"IFrameOnlyManifest\": \"EXCLUDE\"\\n}\\n},\\n\"NameModifier\": \"_720\"\\n},\\n{\\n\"ContainerSettings\": {\\n\"Container\": \"M3U8\",\\n\"M3u8Settings\": {}\\n},\\n\"AudioDescriptions\": [\\n{\\n\"AudioSourceName\": \"Audio Selector 1\",\\n\"CodecSettings\": {\\n\"Codec\": \"AAC\",\\n\"AacSettings\": {\\n\"Bitrate\": 96000,\\n\"CodingMode\": \"CODING_MODE_2_0\",\\n\"SampleRate\": 48000\\n}\\n}\\n}\\n],\\n\"OutputSettings\": {\\n\"HlsSettings\": {', '']]]\n",
      "[[['', '\"AudioGroupId\": \"program_audio\",\\n\"AudioTrackType\": \"ALTERNATE_AUDIO_AUTO_SELECT_DEFAULT\"\\n}\\n},\\n\"NameModifier\": \"_audio\"\\n}\\n],\\n\"OutputGroupSettings\": {\\n\"Type\": \"HLS_GROUP_SETTINGS\",\\n\"HlsGroupSettings\": {\\n\"ManifestDurationFormat\": \"INTEGER\",\\n\"SegmentLength\": 6,\\n\"TimedMetadataId3Period\": 10,\\n\"CaptionLanguageSetting\": \"OMIT\",\\n\"Destination\": \"s3://EXAMPLE-BUCKET/HLS/\",\\n\"DestinationSettings\": {\\n\"S3Settings\": {\\n\"AccessControl\": {\\n\"CannedAcl\": \"PUBLIC_READ\"\\n}\\n}\\n},\\n\"TimedMetadataId3Frame\": \"PRIV\",\\n\"CodecSpecification\": \"RFC_4281\",\\n\"OutputSelection\": \"MANIFESTS_AND_SEGMENTS\",\\n\"ProgramDateTimePeriod\": 600,\\n\"MinSegmentLength\": 0,\\n\"DirectoryStructure\": \"SINGLE_DIRECTORY\",\\n\"ProgramDateTime\": \"EXCLUDE\",\\n\"SegmentControl\": \"SEGMENTED_FILES\",\\n\"ManifestCompression\": \"NONE\",\\n\"ClientCache\": \"ENABLED\",\\n\"StreamInfResolution\": \"INCLUDE\"\\n}\\n}\\n},\\n{\\n\"CustomName\": \"MP4\",\\n\"Name\": \"File Group\",\\n\"Outputs\": [\\n{\\n\"ContainerSettings\": {\\n\"Container\": \"MP4\",\\n\"Mp4Settings\": {', '']]]\n",
      "[[['', '\"CslgAtom\": \"INCLUDE\",\\n\"FreeSpaceBox\": \"EXCLUDE\",\\n\"MoovPlacement\": \"PROGRESSIVE_DOWNLOAD\"\\n}\\n},\\n\"VideoDescription\": {\\n\"Width\": 1280,\\n\"ScalingBehavior\": \"DEFAULT\",\\n\"Height\": 720,\\n\"TimecodeInsertion\": \"DISABLED\",\\n\"AntiAlias\": \"ENABLED\",\\n\"Sharpness\": 100,\\n\"CodecSettings\": {\\n\"Codec\": \"H_264\",\\n\"H264Settings\": {\\n\"InterlaceMode\": \"PROGRESSIVE\",\\n\"ParNumerator\": 1,\\n\"NumberReferenceFrames\": 3,\\n\"Syntax\": \"DEFAULT\",\\n\"Softness\": 0,\\n\"GopClosedCadence\": 1,\\n\"HrdBufferInitialFillPercentage\": 90,\\n\"GopSize\": 2,\\n\"Slices\": 2,\\n\"GopBReference\": \"ENABLED\",\\n\"HrdBufferSize\": 10000000,\\n\"MaxBitrate\": 5000000,\\n\"ParDenominator\": 1,\\n\"EntropyEncoding\": \"CABAC\",\\n\"RateControlMode\": \"QVBR\",\\n\"CodecProfile\": \"HIGH\",\\n\"MinIInterval\": 0,\\n\"AdaptiveQuantization\": \"AUTO\",\\n\"CodecLevel\": \"AUTO\",\\n\"FieldEncoding\": \"PAFF\",\\n\"SceneChangeDetect\": \"ENABLED\",\\n\"QualityTuningLevel\": \"SINGLE_PASS_HQ\",\\n\"UnregisteredSeiTimecode\": \"DISABLED\",\\n\"GopSizeUnits\": \"SECONDS\",\\n\"ParControl\": \"SPECIFIED\",\\n\"NumberBFramesBetweenReferenceFrames\": 3,\\n\"RepeatPps\": \"DISABLED\",\\n\"DynamicSubGop\": \"ADAPTIVE\"\\n}', '']]]\n",
      "[[['', '},\\n\"AfdSignaling\": \"NONE\",\\n\"DropFrameTimecode\": \"ENABLED\",\\n\"RespondToAfd\": \"NONE\",\\n\"ColorMetadata\": \"INSERT\"\\n},\\n\"AudioDescriptions\": [\\n{\\n\"AudioTypeControl\": \"FOLLOW_INPUT\",\\n\"AudioSourceName\": \"Audio Selector 1\",\\n\"CodecSettings\": {\\n\"Codec\": \"AAC\",\\n\"AacSettings\": {\\n\"AudioDescriptionBroadcasterMix\": \"NORMAL\",\\n\"Bitrate\": 160000,\\n\"RateControlMode\": \"CBR\",\\n\"CodecProfile\": \"LC\",\\n\"CodingMode\": \"CODING_MODE_2_0\",\\n\"RawFormat\": \"NONE\",\\n\"SampleRate\": 48000,\\n\"Specification\": \"MPEG4\"\\n}\\n},\\n\"LanguageCodeControl\": \"FOLLOW_INPUT\",\\n\"AudioType\": 0\\n}\\n]\\n}\\n],\\n\"OutputGroupSettings\": {\\n\"Type\": \"FILE_GROUP_SETTINGS\",\\n\"FileGroupSettings\": {\\n\"Destination\": \"s3://EXAMPLE-BUCKET/MP4/\",\\n\"DestinationSettings\": {\\n\"S3Settings\": {\\n\"AccessControl\": {\\n\"CannedAcl\": \"PUBLIC_READ\"\\n}\\n}\\n}\\n}\\n}\\n},\\n{', '']]]\n",
      "[[['', '\"CustomName\": \"Thumbnails\",\\n\"Name\": \"File Group\",\\n\"Outputs\": [\\n{\\n\"ContainerSettings\": {\\n\"Container\": \"RAW\"\\n},\\n\"VideoDescription\": {\\n\"Width\": 1280,\\n\"ScalingBehavior\": \"DEFAULT\",\\n\"Height\": 720,\\n\"TimecodeInsertion\": \"DISABLED\",\\n\"AntiAlias\": \"ENABLED\",\\n\"Sharpness\": 50,\\n\"CodecSettings\": {\\n\"Codec\": \"FRAME_CAPTURE\",\\n\"FrameCaptureSettings\": {\\n\"FramerateNumerator\": 1,\\n\"FramerateDenominator\": 5,\\n\"MaxCaptures\": 500,\\n\"Quality\": 80\\n}\\n},\\n\"AfdSignaling\": \"NONE\",\\n\"DropFrameTimecode\": \"ENABLED\",\\n\"RespondToAfd\": \"NONE\",\\n\"ColorMetadata\": \"INSERT\"\\n}\\n}\\n],\\n\"OutputGroupSettings\": {\\n\"Type\": \"FILE_GROUP_SETTINGS\",\\n\"FileGroupSettings\": {\\n\"Destination\": \"s3://EXAMPLE-BUCKET/Thumbnails/\",\\n\"DestinationSettings\": {\\n\"S3Settings\": {\\n\"AccessControl\": {\\n\"CannedAcl\": \"PUBLIC_READ\"\\n}\\n}\\n}\\n}\\n}\\n}', '']]]\n",
      "[[['', '],\\n\"AdAvailOffset\": 0,\\n\"Inputs\": [\\n{\\n\"AudioSelectors\": {\\n\"Audio Selector 1\": {\\n\"Offset\": 0,\\n\"DefaultSelection\": \"DEFAULT\",\\n\"ProgramSelection\": 1\\n}\\n},\\n\"VideoSelector\": {\\n\"ColorSpace\": \"FOLLOW\"\\n},\\n\"FilterEnable\": \"AUTO\",\\n\"PsiControl\": \"USE_PSI\",\\n\"FilterStrength\": 0,\\n\"DeblockFilter\": \"DISABLED\",\\n\"DenoiseFilter\": \"DISABLED\",\\n\"TimecodeSource\": \"EMBEDDED\",\\n\"FileInput\": \"s3://EXAMPLE-INPUT-BUCKET/input.mp4\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import json\\nimport os\\nfrom urllib.parse import urlparse\\nimport uuid\\nimport boto3\\n\"\"\"\\nWhen you run an S3 Batch Operations job, your job\\ninvokes this Lambda function. Specifically, the Lambda function is\\ninvoked on each video object listed in the manifest that you specify', '']]]\n",
      "[[['', 'for the S3 Batch Operations job in Step 5.\\nInput parameter \"event\": The S3 Batch Operations event as a request\\nfor the Lambda function.\\nInput parameter \"context\": Context about the event.\\nOutput: A result structure that Amazon S3 uses to interpret the result\\nof the operation. It is a job response returned back to S3 Batch\\nOperations.\\n\"\"\"\\ndef handler(event, context):\\ninvocation_schema_version = event[\\'invocationSchemaVersion\\']\\ninvocation_id = event[\\'invocationId\\']\\ntask_id = event[\\'tasks\\'][0][\\'taskId\\']\\nsource_s3_key = event[\\'tasks\\'][0][\\'s3Key\\']\\nsource_s3_bucket = event[\\'tasks\\'][0][\\'s3BucketArn\\'].split(\\':::\\')[-1]\\nsource_s3 = \\'s3://\\' + source_s3_bucket + \\'/\\' + source_s3_key\\nresult_list = []\\nresult_code = \\'Succeeded\\'\\nresult_string = \\'The input video object was converted successfully.\\'\\n# The type of output group determines which media players can play\\n# the files transcoded by MediaConvert.\\n# For more information, see Creating outputs with AWS Elemental MediaConvert.\\noutput_group_type_dict = {\\n\\'HLS_GROUP_SETTINGS\\': \\'HlsGroupSettings\\',\\n\\'FILE_GROUP_SETTINGS\\': \\'FileGroupSettings\\',\\n\\'CMAF_GROUP_SETTINGS\\': \\'CmafGroupSettings\\',\\n\\'DASH_ISO_GROUP_SETTINGS\\': \\'DashIsoGroupSettings\\',\\n\\'MS_SMOOTH_GROUP_SETTINGS\\': \\'MsSmoothGroupSettings\\'\\n}\\ntry:\\njob_name = \\'Default\\'\\nwith open(\\'job.json\\') as file:\\njob_settings = json.load(file)\\njob_settings[\\'Inputs\\'][0][\\'FileInput\\'] = source_s3\\n# The path of each output video is constructed based on the values of', '']]]\n",
      "[[['', '# the attributes in each object of OutputGroups in the job.json file.\\ndestination_s3 = \\'s3://{0}/{1}/{2}\\' \\\\\\n.format(os.environ[\\'DestinationBucket\\'],\\nos.path.splitext(os.path.basename(source_s3_key))[0],\\nos.path.splitext(os.path.basename(job_name))[0])\\nfor output_group in job_settings[\\'OutputGroups\\']:\\noutput_group_type = output_group[\\'OutputGroupSettings\\'][\\'Type\\']\\nif output_group_type in output_group_type_dict.keys():\\noutput_group_type = output_group_type_dict[output_group_type]\\noutput_group[\\'OutputGroupSettings\\'][output_group_type]\\n[\\'Destination\\'] = \\\\\\n\"{0}{1}\".format(destination_s3,\\nurlparse(output_group[\\'OutputGroupSettings\\']\\n[output_group_type][\\'Destination\\']).path)\\nelse:\\nraise ValueError(\"Exception: Unknown Output Group Type {}.\"\\n.format(output_group_type))\\njob_metadata_dict = {\\n\\'assetID\\': str(uuid.uuid4()),\\n\\'application\\': os.environ[\\'Application\\'],\\n\\'input\\': source_s3,\\n\\'settings\\': job_name\\n}\\nregion = os.environ[\\'AWS_DEFAULT_REGION\\']\\nendpoints = boto3.client(\\'mediaconvert\\', region_name=region) \\\\\\n.describe_endpoints()\\nclient = boto3.client(\\'mediaconvert\\', region_name=region,\\nendpoint_url=endpoints[\\'Endpoints\\'][0][\\'Url\\'],\\nverify=False)\\ntry:\\nclient.create_job(Role=os.environ[\\'MediaConvertRole\\'],\\nUserMetadata=job_metadata_dict,\\nSettings=job_settings)\\n# You can customize error handling based on different error codes that\\n# MediaConvert can return.\\n# For more information, see MediaConvert error codes.\\n# When the result_code is TemporaryFailure, S3 Batch Operations retries\\n# the task before the job is completed. If this is the final retry,\\n# the error message is included in the final report.\\nexcept Exception as error:', '']]]\n",
      "[[['', \"result_code = 'TemporaryFailure'\\nraise\\nexcept Exception as error:\\nif result_code != 'TemporaryFailure':\\nresult_code = 'PermanentFailure'\\nresult_string = str(error)\\nfinally:\\nresult_list.append({\\n'taskId': task_id,\\n'resultCode': result_code,\\n'resultString': result_string,\\n})\\nreturn {\\n'invocationSchemaVersion': invocation_schema_version,\\n'treatMissingKeyAs': 'PermanentFailure',\\n'invocationId': invocation_id,\\n'results': result_list\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'zip -r lambda.zip convert.py job.json', ''], ['', '', '']], [['', '', ''], ['', 'powershell Compress-Archive convert.py lambda.zip', ''], ['', '', '']], [['', '', ''], ['', 'powershell Compress-Archive -update job.json lambda.zip', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{', '']]]\n",
      "[[['', '\"Sid\":\"InventoryAndAnalyticsExamplePolicy\",\\n\"Effect\":\"Allow\",\\n\"Principal\": {\"Service\": \"s3.amazonaws.com\"},\\n\"Action\":\"s3:PutObject\",\\n\"Resource\":[\"arn:aws:s3:::tutorial-bucket-3/*\"],\\n\"Condition\": {\\n\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:::tutorial-bucket-1\"\\n},\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"111122223333\",\\n\"s3:x-amz-acl\": \"bucket-owner-full-control\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIt can take up to 48 hours to deliver the first inventory report. If the Create job from\\nmanifest button is disabled, the first inventory report has not been delivered. Wait\\nuntil the first inventory report is delivered and the Create job from manifest button is\\nenabled before you create an S3 Batch Operations job in Step 7.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"S3Get\",\\n\"Effect\": \"Allow\",\\n\"Action\": [', '']]]\n",
      "[[['', '\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::tutorial-bucket-1/*\",\\n\"arn:aws:s3:::tutorial-bucket-3/*\"\\n]\\n},\\n{\\n\"Sid\": \"S3PutJobCompletionReport\",\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::tutorial-bucket-2/*\"\\n},\\n{\\n\"Sid\": \"S3BatchOperationsInvokeLambda\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"lambda:InvokeFunction\"\\n],\\n\"Resource\": [\\n\"arn:aws:lambda:us-west-2:111122223333:function:tutorial-lambda-\\nconvert\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\":\"batchoperations.s3.amazonaws.com\"\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nBefore you start creating an S3 Batch Operations job, make sure that the Create job from\\nmanifest button is enabled. For more information, see Check the inventory report for\\nyour S3 video source bucket. If the Create job from manifest button is disabled, the first\\ninventory report has not been delivered and you must wait until the button is enabled.', '']]]\n",
      "[[['', 'After you configure Amazon S3 Inventory for your S3 source bucket in Step 5, it can take up\\nto 48 hours to deliver the first inventory report.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nWhen you use S3 Batch Operations with a Lambda function, the Lambda function is\\ninvoked on each object. If your S3 Batch Operations job is large, it can invoke multiple\\nLambda functions at the same time, causing a spike in Lambda concurrency.\\nEach AWS account has a Lambda concurrency quota per Region. For more information,\\nsee AWS Lambda Function Scaling in the AWS Lambda Developer Guide. A best practice\\nfor using Lambda functions with S3 Batch Operations is to set a concurrency limit on\\nthe Lambda function itself. Setting a concurrency limit keeps your job from consuming\\nmost of your Lambda concurrency and potentially throttling other functions in your\\naccount. For more information, see Managing Lambda reserved concurrency in the\\nAWS Lambda Developer Guide.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nThe following tutorial requires disabling Block Public Access. We recommend keeping Block\\nPublic Access enabled. If you want to keep all four Block Public Access settings enabled\\nand host a static website, you can use Amazon CloudFront origin access control (OAC).\\nAmazon CloudFront provides the capabilities required to set up a secure static website.\\nAmazon S3 static websites support only HTTP endpoints. Amazon CloudFront uses the\\ndurable storage of Amazon S3 while providing additional security headers, such as HTTPS.', '']]]\n",
      "[[['', 'HTTPS adds security by encrypting a normal HTTP request and protecting against common\\ncyberattacks. For more information, see Getting started with a secure static website in the\\nAmazon CloudFront Developer Guide.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Warning\\nBefore you complete this step, review Blocking public access to your Amazon S3 storage to\\nensure that you understand and accept the risks involved with allowing public access. When\\nyou turn off block public access settings to make your bucket public, anyone on the internet\\ncan access your bucket. We recommend that you block all public access to your buckets.', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nBefore you complete this step, review Blocking public access to your Amazon S3\\nstorage to ensure you understand and accept the risks involved with allowing public\\naccess. When you turn off block public access settings to make your bucket public,\\nanyone on the internet can access your bucket. We recommend that you block all\\npublic access to your buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nThe following policy is an example only and allows full access to the contents of your\\nbucket. Before you proceed with this step, review How can I secure the files in my Amazon\\nS3 bucket? to ensure that you understand the best practices for securing the files in your S3\\nbucket and risks involved in granting public access.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"PublicReadGetObject\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": [\\n\"s3:GetObject\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::Bucket-Name/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<html xmlns=\"http://www.w3.org/1999/xhtml\" >\\n<head>\\n<title>My Website Home Page</title>\\n</head>\\n<body>\\n<h1>Welcome to my website</h1>\\n<p>Now hosted on Amazon S3!</p>\\n</body>\\n</html>', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nAmazon S3 does not support HTTPS access to the website. If you want to use HTTPS, you\\ncan use Amazon CloudFront to serve a static website hosted on Amazon S3.\\nFor more information, see How do I use CloudFront to serve a static website hosted on\\nAmazon S3? and Requiring HTTPS for communication between viewers and CloudFront.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAmazon S3 website endpoints do not support HTTPS or access points. If you want to use\\nHTTPS, you can use Amazon CloudFront to serve a static website hosted on Amazon S3.\\nFor more information, see How do I use CloudFront to serve a static website hosted on\\nAmazon S3? and Requiring HTTPS for communication between viewers and CloudFront.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nWhen you create or update a distribution and enable CloudFront logging, CloudFront\\nupdates the bucket access control list (ACL) to give the awslogsdelivery account\\nFULL_CONTROL permissions to write logs to your bucket. For more information, see\\nPermissions required to configure standard logging and to access your log files in the\\nAmazon CloudFront Developer Guide. If the bucket that stores the logs uses the Bucket\\nowner enforced setting for S3 Object Ownership to disable ACLs, CloudFront cannot\\nwrite logs to the bucket. For more information, see Controlling ownership of objects\\nand disabling ACLs for your bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<html xmlns=\"http://www.w3.org/1999/xhtml\" >\\n<head>\\n<title>My Website Home Page</title>\\n</head>\\n<body>\\n<h1>Welcome to my website</h1>\\n<p>Now hosted on Amazon S3!</p>\\n</body>\\n</html>', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Warning\\nBefore you complete this step, review Blocking public access to your Amazon S3 storage to\\nensure that you understand and accept the risks involved with allowing public access. When\\nyou turn off block public access settings to make your bucket public, anyone on the internet\\ncan access your bucket. We recommend that you block all public access to your buckets.', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nBefore you complete this step, review Blocking public access to your Amazon S3\\nstorage to ensure you understand and accept the risks involved with allowing public\\naccess. When you turn off block public access settings to make your bucket public,\\nanyone on the internet can access your bucket. We recommend that you block all\\npublic access to your buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nThe following policy is an example only and allows full access to the contents of your\\nbucket. Before you proceed with this step, review How can I secure the files in my Amazon\\nS3 bucket? to ensure that you understand the best practices for securing the files in your S3\\nbucket and risks involved in granting public access.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"PublicReadGetObject\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": [\\n\"s3:GetObject\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::Bucket-Name/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAmazon S3 does not support HTTPS access to the website. If you want to use HTTPS, you\\ncan use Amazon CloudFront to serve a static website hosted on Amazon S3.\\nFor more information, see How do I use CloudFront to serve a static website hosted on\\nAmazon S3? and Requiring HTTPS for communication between viewers and CloudFront.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf you don't already use Route 53, see Step 1: Register a domain in the Amazon\\nRoute 53 Developer Guide. After completing your setup, you can resume the\\ninstructions.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you want to use quick create to create your alias records, see Configuring Route 53 to\\nroute traffic to an S3 Bucket.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nChanges generally propagate to all Route 53 servers within 60 seconds. When propagation\\nis done, you can route traffic to your Amazon S3 bucket by using the names of the alias\\nrecords that you created in this procedure.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf you don't already use Route 53, see Step 1: Register a domain in the Amazon\\nRoute 53 Developer Guide. After completing your setup, you can resume the\\ninstructions.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nChanges generally propagate to all Route 53 servers within 60 seconds. When propagation\\nis done, you can route traffic to your Amazon S3 bucket by using the names of the alias\\nrecords that you created in this procedure.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nBefore you perform this step, note the requirements for using alternate domain\\nnames, in particular the need for a valid SSL/TLS certificate.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you create or update a distribution and enable CloudFront logging,\\nCloudFront updates the bucket access control list (ACL) to give the\\nawslogsdelivery account FULL_CONTROL permissions to write logs to your\\nbucket. For more information, see Permissions required to configure standard\\nlogging and to access your log files in the Amazon CloudFront Developer Guide.\\nIf the bucket that stores the logs uses the Bucket owner enforced setting for S3\\nObject Ownership to disable ACLs, CloudFront cannot write logs to the bucket.\\nFor more information, see Controlling ownership of objects and disabling ACLs for\\nyour bucket.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Tip\\nBrowsers can cache redirect settings. If you think the new A record settings should have\\ntaken effect, but your browser still redirects http://www.example.com to http://\\nexample.com, try clearing your browser history and cache, closing and reopening your\\nbrowser application, or using a different web browser.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWith Amazon S3, you pay only for what you use. For more information about Amazon S3\\nfeatures and pricing, see Amazon S3. If you are a new Amazon S3 customer, you can get\\nstarted with Amazon S3 for free. For more information, see AWS Free Tier.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nObjects that belong to a bucket that you create in a specific AWS Region never leave that\\nRegion, unless you explicitly transfer them to another Region. For example, objects that are\\nstored in the Europe (Ireland) Region never leave it.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf you see an Error when you list your buckets and their public access settings, you might\\nnot have the required permissions. Make sure that you have the following permissions\\nadded to your user or role policy:\\ns3:GetAccountPublicAccessBlock\\ns3:GetBucketPublicAccessBlock\\ns3:GetBucketPolicyStatus\\ns3:GetBucketLocation\\ns3:GetBucketAcl\\ns3:ListAccessPoints\\ns3:ListAllMyBuckets\\nIn some rare cases, requests can also fail because of an AWS Region outage.', ''], ['', '', '']], [['', '', ''], ['', 's3:GetAccountPublicAccessBlock\\ns3:GetBucketPublicAccessBlock\\ns3:GetBucketPolicyStatus\\ns3:GetBucketLocation\\ns3:GetBucketAcl\\ns3:ListAccessPoints\\ns3:ListAllMyBuckets', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThere are also object-level configurations. For example, you can configure object-level\\npermissions by configuring an access control list (ACL) specific to that object.', ''], ['', '', '']]]\n",
      "[[['Subresource', 'Description'], ['cors (cross-origin\\nresource sharing)', 'You can configure your bucket to allow cross-origin requests.\\nFor more information, see Using cross-origin resource sharing (CORS).'], ['event notification', 'You can enable your bucket to send you notifications of specified bucket\\nevents.\\nFor more information, see Amazon S3 Event Notifications.'], ['lifecycle', 'You can define lifecycle rules for objects in your bucket that have a well-\\ndefined lifecycle. For example, you can define a rule to archive objects one\\nyear after creation, or delete an object 10 years after creation.\\nFor more information, see Managing your storage lifecycle.'], ['location', 'When you create a bucket, you specify the AWS Region where you want\\nAmazon S3 to create the bucket. Amazon S3 stores this information in the\\nlocation subresource and provides an API for you to retrieve this infor\\nmation.'], ['logging', 'Logging enables you to track requests for access to your bucket. Each\\naccess log record provides details about a single access request, such as\\nthe requester, bucket name, request time, request action, response status,\\nand error code, if any. Access log information can be useful in security and\\naccess audits. It can also help you learn about your customer base and\\nunderstand your Amazon S3 bill.\\nFor more information, see Logging requests with server access logging.'], ['object locking', 'To use S3 Object Lock, you must enable it for a bucket. You can also\\noptionally configure a default retention mode and period that applies to\\nnew objects that are placed in the bucket.\\nFor more information, see Using S3 Object Lock.'], ['policy and ACL\\n(access control\\nlist)', 'All your resources (such as buckets and objects) are private by default\\n. Amazon S3 supports both bucket policy and access control list (ACL)']]]\n",
      "[[['Subresource', 'Description'], ['', 'options for you to grant and manage bucket-level permissions. Amazon S3\\nstores the permission information in the p olicy and acl subresources.\\nFor more information, see Identity and Access Management for Amazon S3.'], ['replication', 'Replication is the automatic, asynchronous copying of objects across\\nbuckets in different or the same AWS Regions. For more information, see\\nReplicating objects overview.'], ['requestPayment', 'By default, the AWS account that creates the bucket (the bucket own\\ner) pays for downloads from the bucket. Using this subresource, the\\nbucket owner can specify that the person requesting the download will be\\ncharged for the download. Amazon S3 provides an API for you to manage\\nthis subresource.\\nFor more information, see Using Requester Pays buckets for storage\\ntransfers and usage.'], ['tagging', 'You can add cost allocation tags to your bucket to categorize and track\\nyour AWS costs. Amazon S3 provides the tagging subresource to store\\nand manage tags on a bucket. Using tags you apply to your bucket, AWS\\ngenerates a cost allocation report with usage and costs aggregated by your\\ntags.\\nFor more information, see Billing and usage reporting for Amazon S3.'], ['transfer accelerat\\nion', 'Transfer Acceleration enables fast, easy, and secure transfers of files over\\nlong distances between your client and an S3 bucket. Transfer Accelerat\\nion takes advantage of the globally distributed edge locations of Amazon\\nCloudFront.\\nFor more information, see Configuring fast, secure file transfers using\\nAmazon S3 Transfer Acceleration.']]]\n",
      "[[['Subresource', 'Description'], ['versioning', 'Versioning helps you recover accidental overwrites and deletes.\\nWe recommend versioning as a best practice to recover objects from being\\ndeleted or overwritten by mistake.\\nFor more information, see Using versioning in S3 buckets.'], ['website', 'You can configure your bucket for static website hosting. Amazon S3 stores\\nthis configuration by creating a website subresource.\\nFor more information, see Hosting a static website using Amazon S3.']]]\n",
      "[[['', '', ''], ['', 'Note\\nBefore March 1, 2018, buckets created in the US East (N. Virginia) Region could have names\\nthat were up to 255 characters long and included uppercase letters and underscores.\\nBeginning March 1, 2018, new buckets in US East (N. Virginia) must conform to the same\\nrules applied in all other Regions.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen you create a directory bucket using the console a suffix is automatically added to the\\nbase name that you provide. This suffix includes the Availability Zone ID of the Availability\\nZone that you chose.', '']]]\n",
      "[[['', 'When you create a directory bucket using an API you must provide the full suffix, including\\nthe Availability Zone ID, in your request. For a list of Availability Zone IDs, see S3 Express\\nOne Zone Availability Zones and Regions.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAmazon S3 supports both virtual-hosted–style and path-style URLs for static website\\naccess. Because buckets can be accessed using path-style and virtual-hosted–style URLs,\\nwe recommend that you create buckets with DNS-compliant bucket names. For more\\ninformation, see Bucket restrictions and limitations.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe General purpose buckets list includes buckets that are located in all AWS Regions.', ''], ['', '', '']], [['', '', ''], ['', '$ aws s3 ls s3://DOC-EXAMPLE-BUCKET1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '$ aws s3 ls', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo minimize latency and costs and address regulatory requirements, choose a Region\\nclose to you. Objects stored in a Region never leave that Region unless you explicitly\\ntransfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS\\nservice endpoints in the Amazon Web Services General Reference.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAvoid including sensitive information, such as account numbers, in the bucket name.\\nThe bucket name is visible in the URLs that point to the objects in the bucket.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis option:\\n• Is not available in the AWS CLI and is only available in console\\n• Is not available for directory buckets\\n• Does not copy the bucket policy from the existing bucket to the new bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe default setting is Bucket owner enforced. To apply the default setting and keep\\nACLs disabled, only the s3:CreateBucket permission is needed. To enable ACLs, you\\nmust have the s3:PutBucketOwnershipControls permission.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo enable all Block Public Access settings, only the s3:CreateBucket permission\\nis required. To turn off any Block Public Access settings, you must have the\\ns3:PutBucketPublicAccessBlock permission.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nIf you use the SSE-KMS option for your default encryption configuration, you are\\nsubject to the requests per second (RPS) quota of AWS KMS. For more information\\nabout AWS KMS quotas and how to request a quota increase, see Quotas in the AWS\\nKey Management Service Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nYou can use only KMS keys that are available in the same AWS Region as the\\nbucket. The Amazon S3 console lists only the first 100 KMS keys in the same\\nRegion as the bucket. To use a KMS key that is not listed, you must enter your\\nKMS key ARN. If you want to use a KMS key that is owned by a different account,\\nyou must first have permission to use the key and then you must enter the KMS\\nkey ARN. For more information on cross account permissions for KMS keys, see\\nCreating KMS keys that other accounts can use in the AWS Key Management\\nService Developer Guide. For more information on SSE-KMS, see Specifying server-\\nside encryption with AWS KMS (SSE-KMS).\\nWhen you use an AWS KMS key for server-side encryption in Amazon S3, you must\\nchoose a symmetric encryption KMS key. Amazon S3 supports only symmetric\\nencryption KMS keys and not asymmetric KMS keys. For more information, see\\nIdentifying symmetric and asymmetric KMS keys in the AWS Key Management\\nService Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nEnabling Object Lock also enables versioning for the bucket. After enabling\\nyou must configure the Object Lock default retention and legal hold settings to\\nprotect new objects from being deleted or overwritten.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo create an Object Lock enabled bucket, you must have the following\\npermissions: s3:CreateBucket, s3:PutBucketVersioning and\\ns3:PutBucketObjectLockConfiguration.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.CreateBucketRequest;', '']]]\n",
      "[[['', 'import com.amazonaws.services.s3.model.GetBucketLocationRequest;\\nimport java.io.IOException;\\npublic class CreateBucket2 {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\nif (!s3Client.doesBucketExistV2(bucketName)) {\\n// Because the CreateBucketRequest object doesn\\'t specify a region,\\nthe\\n// bucket is created in the region specified in the client.\\ns3Client.createBucket(new CreateBucketRequest(bucketName));\\n// Verify that the bucket was created by retrieving it and checking\\nits\\n// location.\\nString bucketLocation = s3Client.getBucketLocation(new\\nGetBucketLocationRequest(bucketName));\\nSystem.out.println(\"Bucket location: \" + bucketLocation);\\n}\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing Amazon.S3.Util;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass CreateBucketTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\npublic static void Main()\\n{\\ns3Client = new AmazonS3Client(bucketRegion);\\nCreateBucketAsync().Wait();\\n}\\nstatic async Task CreateBucketAsync()\\n{\\ntry\\n{\\nif (!(await AmazonS3Util.DoesS3BucketExistAsync(s3Client,\\nbucketName)))\\n{\\nvar putBucketRequest = new PutBucketRequest\\n{\\nBucketName = bucketName,\\nUseClientRegion = true\\n};\\nPutBucketResponse putBucketResponse = await\\ns3Client.PutBucketAsync(putBucketRequest);', '']]]\n",
      "[[['', '}\\n// Retrieve the bucket location.\\nstring bucketLocation = await FindBucketLocationAsync(s3Client);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\nstatic async Task<string> FindBucketLocationAsync(IAmazonS3 client)\\n{\\nstring bucketLocation;\\nvar request = new GetBucketLocationRequest()\\n{\\nBucketName = bucketName\\n};\\nGetBucketLocationResponse response = await\\nclient.GetBucketLocationAsync(request);\\nbucketLocation = response.Location.ToString();\\nreturn bucketLocation;\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 bucket actions.\\nclass BucketCreateWrapper\\nattr_reader :bucket', '']]]\n",
      "[[['', '# @param bucket [Aws::S3::Bucket] An Amazon S3 bucket initialized with a name.\\nThis is a client-side object until\\n# create is called.\\ndef initialize(bucket)\\n@bucket = bucket\\nend\\n# Creates an Amazon S3 bucket in the specified AWS Region.\\n#\\n# @param region [String] The Region where the bucket is created.\\n# @return [Boolean] True when the bucket is created; otherwise, false.\\ndef create?(region)\\n@bucket.create(create_bucket_configuration: { location_constraint: region })\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t create bucket. Here\\'s why: #{e.message}\"\\nfalse\\nend\\n# Gets the Region where the bucket is located.\\n#\\n# @return [String] The location of the bucket.\\ndef location\\nif @bucket.nil?\\n\"None. You must create a bucket before you can get its location!\"\\nelse\\n@bucket.client.get_bucket_location(bucket: @bucket.name).location_constraint\\nend\\nrescue Aws::Errors::ServiceError => e\\n\"Couldn\\'t get the location of #{@bucket.name}. Here\\'s why: #{e.message}\"\\nend\\nend\\n# Example usage:\\ndef run_demo\\nregion = \"us-west-2\"\\nwrapper = BucketCreateWrapper.new(Aws::S3::Bucket.new(\"doc-example-bucket-\\n#{Random.uuid}\"))\\nreturn unless wrapper.create?(region)\\nputs \"Created bucket #{wrapper.bucket.name}.\"\\nputs \"Your bucket\\'s region is: #{wrapper.location}\"\\nend', '']]]\n",
      "[[['', 'run_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-tagging --bucket example-s3-bucket1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-versioning --bucket example-s3-bucket1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-encryption --bucket example-s3-bucket1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-notification-configuration --bucket example-s3-bucket1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-logging --bucket example-s3-bucket1', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '$ aws s3 rm s3://bucket-name/doc --recursive', ''], ['', '', '']], [['', '', ''], ['', '$ aws s3 rm s3://bucket-name --recursive', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nYou can't remove objects from a bucket that has versioning enabled. Amazon S3 adds a\\ndelete marker when you delete an object, which is what this command does. For more\\ninformation about S3 Bucket Versioning, see Using versioning in S3 buckets.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\nBucket names are unique. If you delete a bucket, another AWS user can use the name.\\nIf you want to continue to use the same bucket name, don't delete the bucket. We\\nrecommend that you empty the bucket and keep it.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf the bucket contains any objects, empty the bucket before deleting it by selecting\\nthe empty bucket configuration link in the This bucket is not empty error alert and\\nfollowing the instructions on the Empty bucket page. Then return to the Delete\\nbucket page and delete the bucket.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor buckets without versioning enabled, you can delete all objects directly and then\\ndelete the bucket. For buckets with versioning enabled, you must delete all object\\nversions before deleting the bucket.', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;', '']]]\n",
      "[[['', 'import com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.*;\\nimport java.util.Iterator;\\npublic class DeleteBucket2 {\\npublic static void main(String[] args) {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\n// Delete all objects from the bucket. This is sufficient\\n// for unversioned buckets. For versioned buckets, when you attempt to\\ndelete\\n// objects, Amazon S3 inserts\\n// delete markers for all objects, but doesn\\'t delete the object\\nversions.\\n// To delete objects from versioned buckets, delete all of the object\\nversions\\n// before deleting\\n// the bucket (see below for an example).\\nObjectListing objectListing = s3Client.listObjects(bucketName);\\nwhile (true) {\\nIterator<S3ObjectSummary> objIter =\\nobjectListing.getObjectSummaries().iterator();\\nwhile (objIter.hasNext()) {\\ns3Client.deleteObject(bucketName, objIter.next().getKey());\\n}\\n// If the bucket contains many objects, the listObjects() call\\n// might not return all of the objects in the first listing. Check\\nto\\n// see whether the listing was truncated. If so, retrieve the next\\npage of\\n// objects', '']]]\n",
      "[[['', \"// and delete them.\\nif (objectListing.isTruncated()) {\\nobjectListing = s3Client.listNextBatchOfObjects(objectListing);\\n} else {\\nbreak;\\n}\\n}\\n// Delete all object versions (required for versioned buckets).\\nVersionListing versionList = s3Client.listVersions(new\\nListVersionsRequest().withBucketName(bucketName));\\nwhile (true) {\\nIterator<S3VersionSummary> versionIter =\\nversionList.getVersionSummaries().iterator();\\nwhile (versionIter.hasNext()) {\\nS3VersionSummary vs = versionIter.next();\\ns3Client.deleteVersion(bucketName, vs.getKey(),\\nvs.getVersionId());\\n}\\nif (versionList.isTruncated()) {\\nversionList = s3Client.listNextBatchOfVersions(versionList);\\n} else {\\nbreak;\\n}\\n}\\n// After all objects and object versions are deleted, delete the bucket.\\ns3Client.deleteBucket(bucketName);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\ncouldn't\\n// parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '$ aws s3 rb s3://bucket-name --force', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nWe've changed buckets to encrypt new object uploads automatically. If you previously\\ncreated a bucket without default encryption, Amazon S3 will enable encryption by\\ndefault for the bucket using SSE-S3. There will be no changes to the default encryption\\nconfiguration for an existing bucket that already has SSE-S3 or SSE-KMS configured. If you\\nwant to encrypt your objects with SSE-KMS, you must change the encryption type in your\\nbucket settings. For more information, see Using server-side encryption with AWS KMS\\nkeys (SSE-KMS).\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAmazon S3 buckets with default bucket encryption set to SSE-KMS cannot be used as\\ndestination buckets for the section called “Logging server access”. Only SSE-S3 default\\nencryption is supported for server access log destination buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you use PutBucketEncryption to set your default bucket encryption to SSE-KMS, you\\nshould verify that your KMS key ID is correct. Amazon S3 does not validate the KMS key ID\\nprovided in PutBucketEncryption requests.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nObjects uploaded before default encryption was enabled will not be encrypted. For\\ninformation about encrypting existing objects, see the section called “Setting default\\nbucket encryption”.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf you use the SSE-KMS or DSSE-KMS options for your default encryption\\nconfiguration, you are subject to the requests per second (RPS) quotas of AWS KMS.\\nFor more information about AWS KMS quotas and how to request a quota increase,\\nsee Quotas in the AWS Key Management Service Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nYou can only use KMS keys that are enabled in the same AWS Region as the\\nbucket. When you choose Choose from your KMS keys, the S3 console only\\nlists 100 KMS keys per Region. If you have more than 100 KMS keys in the same\\nRegion, you can only see the first 100 KMS keys in the S3 console. To use a KMS\\nkey that is not listed in the console, choose Enter AWS KMS key ARN, and enter\\nthe KMS key ARN.\\nWhen you use an AWS KMS key for server-side encryption in Amazon S3, you must\\nchoose a symmetric encryption KMS key. Amazon S3 only supports symmetric\\nencryption KMS keys. For more information about these keys, see Symmetric\\nencryption KMS keys in the AWS Key Management Service Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nS3 Bucket Keys aren't supported for DSSE-KMS.\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-encryption --bucket example-s3-bucket --server-side-encryption-\\nconfiguration \\'{\\n\"Rules\": [\\n{\\n\"ApplyServerSideEncryptionByDefault\": {\\n\"SSEAlgorithm\": \"AES256\"\\n}\\n}\\n]\\n}\\'', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-encryption --bucket example-s3-bucket --server-side-encryption-\\nconfiguration \\'{\\n\"Rules\": [\\n{', '']]]\n",
      "[[['', '\"ApplyServerSideEncryptionByDefault\": {\\n\"SSEAlgorithm\": \"aws:kms\",\\n\"KMSMasterKeyID\": \"KMS-Key-ARN\"\\n},\\n\"BucketKeyEnabled\": true\\n}\\n]\\n}\\'', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm', ''], ['', '', '']], [['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/arm64/mount-s3.rpm', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'wget download-link', ''], ['', '', '']], [['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.rpm.asc', ''], ['', '', '']], [['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/arm64/mount-s3.rpm.asc', ''], ['', '', '']], [['', '', ''], ['', 'sudo yum install ./mount-s3.rpm', ''], ['', '', '']], [['', '', ''], ['', 'mount-s3 --version', ''], ['', '', '']], [['', '', ''], ['', 'mount-s3 1.3.1', ''], ['', '', '']], [['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.deb', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/arm64/mount-s3.deb', ''], ['', '', '']], [['', '', ''], ['', 'wget download-link', ''], ['', '', '']], [['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.deb.asc', ''], ['', '', '']], [['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/arm64/mount-s3.deb.asc', ''], ['', '', '']], [['', '', ''], ['', 'sudo apt-get install ./mount-s3.deb', ''], ['', '', '']], [['', '', ''], ['', 'mount-s3 --version', ''], ['', '', '']], [['', '', ''], ['', 'mount-s3 1.3.1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.tar.gz', ''], ['', '', '']], [['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/arm64/mount-s3.tar.gz', ''], ['', '', '']], [['', '', ''], ['', 'wget download-link', ''], ['', '', '']], [['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.tar.gz.asc', ''], ['', '', '']], [['', '', ''], ['', 'https://s3.amazonaws.com/mountpoint-s3-release/latest/arm64/mount-s3.tar.gz.asc', ''], ['', '', '']], [['', '', ''], ['', 'sudo mkdir -p /opt/aws/mountpoint-s3 && sudo tar -C /opt/aws/mountpoint-s3 -xzf ./\\nmount-s3.tar.gz', ''], ['', '', '']], [['', '', ''], ['', 'export PATH=$PATH:/opt/aws/mountpoint-s3/bin', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'source $HOME/.profile', ''], ['', '', '']], [['', '', ''], ['', 'mount-s3 --version', ''], ['', '', '']], [['', '', ''], ['', 'mount-s3 1.3.1', ''], ['', '', '']], [['', '', ''], ['', 'wget https://s3.amazonaws.com/mountpoint-s3-release/public_keys/KEYS', ''], ['', '', '']], [['', '', ''], ['', 'gpg --import KEYS', ''], ['', '', '']], [['', '', ''], ['', 'gpg --fingerprint mountpoint-s3@amazon.com', ''], ['', '', '']], [['', '', ''], ['', '673F E406 1506 BB46 9A0E F857 BE39 7A52 B086 DA5A', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'wget signature-link', ''], ['', '', '']], [['', '', ''], ['', 'gpg --verify signature-filename', ''], ['', '', '']], [['', '', ''], ['', 'gpg --verify mount-s3.rpm.asc', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'mkdir ~/mnt\\nmount-s3 DOC-EXAMPLE-BUCKET ~/mnt', ''], ['', '', '']], [['', '', ''], ['', 'umount ~/mnt', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo get a list of options for this command, run umount --help.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'mkdir ~/mnt\\nmount-s3 --cache CACHE_PATH DOC-EXAMPLE-BUCKET ~/mnt', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nIf you enable caching, Mountpoint will persist unencrypted object content from your S3\\nbucket at the caching location configured at mount. In order to protect your data, we\\nrecommend that you restrict access to the data cache location.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'journalctl -e SYSLOG_IDENTIFIER=mount-s3', ''], ['', '', '']], [['', '', ''], ['', '[WARN] open{req=12 ino=2}: mountpoint_s3::fuse: open failed: inode error: inode 2 (full\\nkey \"README.md\") is not writable', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTransfer Acceleration is currently supported for buckets located in the following Regions:\\n• Asia Pacific (Tokyo) (ap-northeast-1)\\n• Asia Pacific (Seoul) (ap-northeast-2)\\n• Asia Pacific (Mumbai) (ap-south-1)\\n• Asia Pacific (Singapore) (ap-southeast-1)\\n• Asia Pacific (Sydney) (ap-southeast-2)\\n• Canada (Central) (ca-central-1)\\n• Europe (Frankfurt) (eu-central-1)\\n• Europe (Ireland) (eu-west-1)\\n• Europe (London) (eu-west-2)', '']]]\n",
      "[[['', '• Europe (Paris) (eu-west-3)\\n• South America (São Paulo) (sa-east-1)\\n• US East (N. Virginia) (us-east-1)\\n• US East (Ohio) (us-east-2)\\n• US West (N. California) (us-west-1)\\n• US West (Oregon) (us-west-2)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nFor your bucket to work with transfer acceleration, the bucket name must conform to\\nDNS naming requirements and must not contain periods (\".\").', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYour data transfer application must use one of the following two types of endpoints to\\naccess the bucket for faster data transfer: .s3-accelerate.amazonaws.com or .s3-\\naccelerate.dualstack.amazonaws.com for the dual-stack endpoint. If you want to\\nuse standard data transfer, you can continue to use the regular endpoints.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf you want to compare accelerated and non-accelerated upload speeds, open the Amazon\\nS3 Transfer Acceleration Speed Comparison tool.\\nThe Speed Comparison tool uses multipart upload to transfer a file from your browser to\\nvarious AWS Regions with and without Amazon S3 transfer acceleration. You can compare\\nthe upload speed for direct uploads and transfer accelerated uploads by Region.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '$ aws s3api put-bucket-accelerate-configuration --bucket bucketname --accelerate-\\nconfiguration Status=Enabled', ''], ['', '', '']], [['', '', ''], ['', '$ aws configure set default.s3.use_accelerate_endpoint true', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '$ aws s3 cp file.txt s3://bucketname/keyname --region region', ''], ['', '', '']], [['', '', ''], ['', '$ aws configure set s3.addressing_style virtual\\n$ aws s3 cp file.txt s3://bucketname/keyname --region region --endpoint-url https://s3-\\naccelerate.amazonaws.com', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.BucketAccelerateConfiguration;\\nimport com.amazonaws.services.s3.model.BucketAccelerateStatus;\\nimport com.amazonaws.services.s3.model.GetBucketAccelerateConfigurationRequest;\\nimport com.amazonaws.services.s3.model.SetBucketAccelerateConfigurationRequest;\\npublic class TransferAcceleration {\\npublic static void main(String[] args) {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString keyName = \"*** Key name ***\";\\ntry {\\n// Create an Amazon S3 client that is configured to use the accelerate\\nendpoint.\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withRegion(clientRegion)\\n.withCredentials(new ProfileCredentialsProvider())\\n.enableAccelerateMode()\\n.build();\\n// Enable Transfer Acceleration for the specified bucket.\\ns3Client.setBucketAccelerateConfiguration(\\nnew SetBucketAccelerateConfigurationRequest(bucketName,\\nnew BucketAccelerateConfiguration(\\nBucketAccelerateStatus.Enabled)));\\n// Verify that transfer acceleration is enabled for the bucket.\\nString accelerateStatus = s3Client.getBucketAccelerateConfiguration(', '']]]\n",
      "[[['', 'new GetBucketAccelerateConfigurationRequest(bucketName))\\n.getStatus();\\nSystem.out.println(\"Bucket accelerate status: \" + accelerateStatus);\\n// Upload a new object using the accelerate endpoint.\\ns3Client.putObject(bucketName, keyName, \"Test object for transfer\\nacceleration\");\\nSystem.out.println(\"Object \\\\\"\" + keyName + \"\\\\\" uploaded with transfer\\nacceleration.\");\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass TransferAccelerationTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;', '']]]\n",
      "[[['', 'private static IAmazonS3 s3Client;\\npublic static void Main()\\n{\\ns3Client = new AmazonS3Client(bucketRegion);\\nEnableAccelerationAsync().Wait();\\n}\\nstatic async Task EnableAccelerationAsync()\\n{\\ntry\\n{\\nvar putRequest = new PutBucketAccelerateConfigurationRequest\\n{\\nBucketName = bucketName,\\nAccelerateConfiguration = new AccelerateConfiguration\\n{\\nStatus = BucketAccelerateStatus.Enabled\\n}\\n};\\nawait\\ns3Client.PutBucketAccelerateConfigurationAsync(putRequest);\\nvar getRequest = new GetBucketAccelerateConfigurationRequest\\n{\\nBucketName = bucketName\\n};\\nvar response = await\\ns3Client.GetBucketAccelerateConfigurationAsync(getRequest);\\nConsole.WriteLine(\"Acceleration state = \\'{0}\\' \",\\nresponse.Status);\\n}\\ncatch (AmazonS3Exception amazonS3Exception)\\n{\\nConsole.WriteLine(\\n\"Error occurred. Message:\\'{0}\\' when setting transfer\\nacceleration\",\\namazonS3Exception.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'var client = new AmazonS3Client(new AmazonS3Config\\n{\\nRegionEndpoint = TestRegionEndpoint,\\nUseAccelerateEndpoint = true\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf you enable Requester Pays on a bucket, anonymous access to that bucket is not allowed.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'DELETE /Key+?versionId=VersionId HTTP/1.1\\nHost: Bucket.s3.amazonaws.com\\nx-amz-mfa: MFA\\nx-amz-request-payer: RequestPayer\\nx-amz-bypass-governance-retention: BypassGovernanceRetention\\nx-amz-expected-bucket-owner: ExpectedBucketOwner', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'PUT ?requestPayment HTTP/1.1\\nHost: [BucketName].s3.amazonaws.com\\nContent-Length: 173\\nDate: Wed, 01 Mar 2009 12:00:00 GMT\\nAuthorization: AWS [Signature]\\n<RequestPaymentConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Payer>Requester</Payer>\\n</RequestPaymentConfiguration>', ''], ['', '', '']], [['', '', ''], ['', 'HTTP/1.1 200 OK\\nx-amz-id-2: [id]\\nx-amz-request-id: [request_id]\\nDate: Wed, 01 Mar 2009 12:00:00 GMT\\nContent-Length: 0\\nConnection: close\\nServer: AmazonS3\\nx-amz-request-charged:requester', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nBucket owners who give out presigned URLs should consider carefully before configuring\\na bucket to be Requester Pays, especially if the URL has a long lifetime. The bucket owner', '']]]\n",
      "[[['', \"is charged each time the requester uses a presigned URL that uses the bucket owner's\\ncredentials.\", ''], ['', '', '']], [['', '', ''], ['', 'GET ?requestPayment HTTP/1.1\\nHost: [BucketName].s3.amazonaws.com\\nDate: Wed, 01 Mar 2009 12:00:00 GMT\\nAuthorization: AWS [Signature]', ''], ['', '', '']], [['', '', ''], ['', 'HTTP/1.1 200 OK\\nx-amz-id-2: [id]\\nx-amz-request-id: [request_id]\\nDate: Wed, 01 Mar 2009 12:00:00 GMT\\nContent-Type: [type]\\nContent-Length: [length]\\nConnection: close\\nServer: AmazonS3\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<RequestPaymentConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Payer>Requester</Payer>\\n</RequestPaymentConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nBucket owners do not need to add x-amz-request-payer to their requests.\\nEnsure that you have included x-amz-request-payer and its value in your signature\\ncalculation. For more information, see Constructing the CanonicalizedAmzHeaders\\nElement.', ''], ['', '', '']], [['', '', ''], ['', 'GET / [destinationObject] HTTP/1.1\\nHost: [BucketName].s3.amazonaws.com\\nx-amz-request-payer : requester\\nDate: Wed, 01 Mar 2009 12:00:00 GMT\\nAuthorization: AWS [Signature]', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou do not need to submit multiple quota increase requests for each AWS Region. Your\\nbucket quota is applied to your AWS account.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nIn the Amazon S3 console, when you choose Open or Download As for an object, these\\noperations create presigned URLs. For the duration of five minutes, your object will be\\naccessible to anyone who has access to these presigned URLs. For more information about\\npresigned URLs, see Using presigned URLS.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['Subresour\\nce', 'Description'], ['acl', 'Contains a list of grants identifying the grantees and the permissions granted.\\nWhen you create an object, the acl identifies the object owner as having full\\ncontrol over the object. You can retrieve an object ACL or replace it with an']]]\n",
      "[[['Subresour\\nce', 'Description'], ['', 'updated list of grants. Any update to an ACL requires you to replace the existing\\nACL. For more information about ACLs, see Access control list (ACL) overview.']], [['', '', ''], ['', 'Note\\nObject key names with the value \"soap\" aren\\'t supported for virtual-hosted-style requests.\\nFor object key name values where \"soap\" is used, a path-style URL must be used instead.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nObjects with key names ending with period(s) \".\" downloaded using the Amazon S3 console\\nwill have the period(s) \".\" removed from the key name of the downloaded object. To\\ndownload an object with the key name ending in period(s) \".\" retained in the downloaded\\nobject, you must use the AWS Command Line Interface (AWS CLI), AWS SDKs, or REST API.\\nIn addition, be aware of the following prefix limitations:\\n• Objects with a prefix of \"./\" must be uploaded or downloaded with the AWS Command\\nLine Interface (AWS CLI), AWS SDKs, or REST API. You cannot use the Amazon S3 console.\\n• Objects with a prefix of \"../\" cannot be uploaded using the AWS Command Line Interface\\n(AWS CLI) or Amazon S3 console.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '<Delete xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Object>\\n<Key>/some/prefix/objectwith&#13;carriagereturn</Key>\\n</Object>\\n</Delete>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe PUT request header is limited to 8 KB in size. Within the PUT request header, the\\nsystem-defined metadata is limited to 2 KB in size. The size of system-defined metadata is\\nmeasured by taking the sum of the number of bytes in the US-ASCII encoding of each key\\nand value.', ''], ['', '', '']]]\n",
      "[[['Name', 'Description', 'Can user\\nmodify the\\nvalue?'], ['Date', 'The current date and time.', 'No'], ['Cache-Control', 'A general header field used to specify caching policies.', 'Yes'], ['Content-D\\nisposition', 'Object presentational information.', 'Yes'], ['Content-Length', 'The object size in bytes.', 'No'], ['Content-Type', 'The object type.', 'Yes'], ['Last-Modified', 'The object creation date or the last modified date,\\nwhichever is the latest. For multipart uploads, the object\\ncreation date is the date of initiation of the multipart\\nupload.', 'No'], ['ETag', 'An entity tag (ETag) that represents a specific version\\nof an object. For objects that are not uploaded as\\na multipart upload and are either unencrypted or\\nencrypted by server-side encryption with Amazon S3\\nmanaged keys (SSE-S3), the ETag is an MD5 digest of\\nthe data.', 'No'], ['x-amz-server-\\nside-encryptio\\nn', 'A header that indicates whether server-side encryption\\nis enabled for the object, and whether that encryption\\nis using the AWS Key Management Service (AWS KMS)\\nkeys (SSE-KMS) or using Amazon S3 managed encryptio\\nn keys (SSE-S3). For more information, see Protecting\\ndata with server-side encryption.', 'Yes'], ['x-amz-che\\ncksum-crc32 , x-\\namz-checksum-\\ncrc32c , x-amz-', 'Headers that contain the checksum or digest of the\\nobject. At most, one of these headers will be set at a\\ntime, depending on the checksum algorithm that you\\ninstruct Amazon S3 to use. For more information about', 'No']]]\n",
      "[[['Name', 'Description', 'Can user\\nmodify the\\nvalue?'], ['checksum-sha\\n1 , x-amz-che\\ncksum-sha256', 'choosing the checksum algorithm, see Checking object\\nintegrity.', ''], ['x-amz-version-\\nid', 'The object version. When you enable versioning on a\\nbucket, Amazon S3 assigns a version ID to objects added\\nto the bucket. For more information, see Using versionin\\ng in S3 buckets.', 'No'], ['x-amz-delete-\\nmarker', 'A Boolean marker that indicates whether the object is a\\ndelete marker. This marker is used only in buckets that\\nhave versioning enabled,', 'No'], ['x-amz-storage-\\nclass', 'The storage class used for storing the object. For more\\ninformation, see Using Amazon S3 storage classes.', 'Yes'], ['x-amz-website-\\nredirect-loca\\ntion', 'A header that redirects requests for the associated\\nobject to another object in the same bucket or to an\\nexternal URL. For more information, see (Optional)\\nConfiguring a webpage redirect.', 'Yes'], ['x-amz-server-\\nside-encryptio\\nn-aws-kms-\\nkey-id', 'A header that indicates the ID of the AWS KMS\\nsymmetric encryption KMS key that was used to encrypt\\nthe object. This header is used only when the x-amz-\\nserver-side-encryption header is present and\\nhas the value of aws:kms.', 'Yes'], ['x-amz-server-\\nside-encryptio\\nn-customer-\\nalgorithm', 'A header that indicates whether server-side encryptio\\nn with customer-provided encryption keys (SSE-C) is\\nenabled. For more information, see Using server-side\\nencryption with customer-provided keys (SSE-C).', 'Yes'], ['x-amz-tagging', 'The tag-set for the object. The tag-set must be encoded\\nas URL Query parameters.', 'Yes']]]\n",
      "[[['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'PUT /Key HTTP/1.1\\nHost: DOC-EXAMPLE-BUCKET1.s3.amazonaws.com\\nx-amz-meta-nonascii: ÄMÄZÕÑ S3\\nHEAD /Key HTTP/1.1\\nHost: DOC-EXAMPLE-BUCKET1.s3.amazonaws.com\\nx-amz-meta-nonascii: =?UTF-8?B?w4PChE3Dg8KEWsODwpXDg8KRIFMz?=\\nPUT /Key HTTP/1.1\\nHost: DOC-EXAMPLE-BUCKET1.s3.amazonaws.com\\nx-amz-meta-ascii: AMAZONS3\\nHEAD /Key HTTP/1.1\\nHost: DOC-EXAMPLE-BUCKET1.s3.amazonaws.com\\nx-amz-meta-ascii: AMAZONS3', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe PUT request header is limited to 8 KB in size. Within the PUT request header, the user-\\ndefined metadata is limited to 2 KB in size. The size of user-defined metadata is measured\\nby taking the sum of the number of bytes in the UTF-8 encoding of each key and value.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nConsider the following issues when you are editing object metadata in Amazon S3:\\n• This action creates a copy of the object with updated settings and the last-modified\\ndate. If S3 Versioning is enabled, a new version of the object is created, and the existing\\nobject becomes an older version. If S3 Versioning is not enabled, a new copy of the object\\nreplaces the original object. The AWS account associated with the IAM role that changes\\nthe property also becomes the owner of the new object or (object version).\\n• To use the Amazon S3 console to edit metadata for an object that has user-defined tags,\\nyou must also have the s3:GetObjectTagging permission. If you are using the Amazon\\nS3 console to edit the metadata for an object that doesn't have user-defined tags but is\\nover 16 MB in size, you must also have the s3:GetObjectTagging permission.\\nIf the destination bucket policy denies the s3:GetObjectTagging action, the metadata\\nfor the object will be updated, but the user-defined tags will be removed from the object,\\nand you will receive an error.\\n• Editing metadata updates values for existing key names.\\n• Objects that are encrypted with customer-provided encryption keys (SSE-C) cannot be\\ncopied using the console. You must use the AWS CLI, AWS SDK, or the Amazon S3 REST\\nAPI.\", ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nWhen editing metadata of folders, wait for the Edit metadata operation to finish before\\nadding new objects to the folder. Otherwise, new objects might also be edited.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf you rename an object or change any of the properties in the Amazon S3 console, for\\nexample Storage Class, Encryption, or Metadata, a new object is created to replace the\\nold one. If S3 Versioning is enabled, a new version of the object is created, and the existing\\nobject becomes an older version. The role that changes the property also becomes the\\nowner of the new object (or object version).', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nYou can use only KMS keys that are available in the same AWS Region as the\\nbucket. The Amazon S3 console lists only the first 100 KMS keys in the same\\nRegion as the bucket. To use a KMS key that is not listed, you must enter your\\nKMS key ARN. If you want to use a KMS key that is owned by a different account,\\nyou must first have permission to use the key and then you must enter the KMS\\nkey ARN.\\nAmazon S3 supports only symmetric encryption KMS keys, and not asymmetric\\nKMS keys. For more information, see Identifying symmetric and asymmetric\\nKMS keys in the AWS Key Management Service Developer Guide.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass UploadObjectTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\n// For simplicity the example creates two objects from the same file.\\n// You specify key names for these objects.\\nprivate const string keyName1 = \"*** key name for first object created ***\";\\nprivate const string keyName2 = \"*** key name for second object created\\n***\";\\nprivate const string filePath = @\"*** file path ***\";\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.EUWest1;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nclient = new AmazonS3Client(bucketRegion);\\nWritingAnObjectAsync().Wait();\\n}\\nstatic async Task WritingAnObjectAsync()\\n{\\ntry\\n{\\n// 1. Put object-specify only key name for the new object.\\nvar putRequest1 = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName1,\\nContentBody = \"sample text\"\\n};', '']]]\n",
      "[[['', 'PutObjectResponse response1 = await\\nclient.PutObjectAsync(putRequest1);\\n// 2. Put the object-set ContentType and add metadata.\\nvar putRequest2 = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName2,\\nFilePath = filePath,\\nContentType = \"text/plain\"\\n};\\nputRequest2.Metadata.Add(\"x-amz-meta-title\", \"someTitle\");\\nPutObjectResponse response2 = await\\nclient.PutObjectAsync(putRequest2);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\\n\"Error encountered ***. Message:\\'{0}\\' when writing an\\nobject\"\\n, e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\\n\"Unknown encountered on server. Message:\\'{0}\\' when writing an\\nobject\"\\n, e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.ObjectMetadata;\\nimport com.amazonaws.services.s3.model.PutObjectRequest;\\nimport java.io.File;\\nimport java.io.IOException;\\npublic class UploadObject {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString stringObjKeyName = \"*** String object key name ***\";\\nString fileObjKeyName = \"*** File object key name ***\";\\nString fileName = \"*** Path to file to upload ***\";\\ntry {\\n// This code expects that you have AWS credentials set up per:\\n// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withRegion(clientRegion)\\n.build();\\n// Upload a text string as a new object.\\ns3Client.putObject(bucketName, stringObjKeyName, \"Uploaded String\\nObject\");\\n// Upload a file as a new object with ContentType and title specified.\\nPutObjectRequest request = new PutObjectRequest(bucketName,\\nfileObjKeyName, new File(fileName));\\nObjectMetadata metadata = new ObjectMetadata();\\nmetadata.setContentType(\"plain/text\");\\nmetadata.addUserMetadata(\"title\", \"someTitle\");\\nrequest.setMetadata(metadata);', '']]]\n",
      "[[['', \"s3Client.putObject(request);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'import { PutObjectCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new PutObjectCommand({\\nBucket: \"test-bucket\",\\nKey: \"hello-s3.txt\",\\nBody: \"Hello S3!\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"require 'vendor/autoload.php';\\nuse Aws\\\\S3\\\\Exception\\\\S3Exception;\\nuse Aws\\\\S3\\\\S3Client;\\n$bucket = '*** Your Bucket Name ***';\\n$keyname = '*** Your Object Key ***';\\n$s3 = new S3Client([\\n'version' => 'latest',\\n'region' => 'us-east-1'\\n]);\\ntry {\\n// Upload data.\\n$result = $s3->putObject([\\n'Bucket' => $bucket,\\n'Key' => $keyname,\\n'Body' => 'Hello, world!',\\n'ACL' => 'public-read'\\n]);\\n// Print the URL to the object.\\necho $result['ObjectURL'] . PHP_EOL;\\n} catch (S3Exception $e) {\\necho $e->getMessage() . PHP_EOL;\\n}\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectUploadFileWrapper\\nattr_reader :object\\n# @param object [Aws::S3::Object] An existing Amazon S3 object.\\ndef initialize(object)\\n@object = object\\nend\\n# Uploads a file to an Amazon S3 object by using a managed uploader.\\n#\\n# @param file_path [String] The path to the file to upload.\\n# @return [Boolean] True when the file is uploaded; otherwise false.\\ndef upload_file(file_path)\\n@object.upload_file(file_path)\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t upload file #{file_path} to #{@object.key}. Here\\'s why:\\n#{e.message}\"\\nfalse\\nend\\nend\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nobject_key = \"my-uploaded-file\"\\nfile_path = \"object_upload_file.rb\"\\nwrapper = ObjectUploadFileWrapper.new(Aws::S3::Object.new(bucket_name,\\nobject_key))\\nreturn unless wrapper.upload_file(file_path)\\nputs \"File #{file_path} successfully uploaded to #{bucket_name}:#{object_key}.\"', '']]]\n",
      "[[['', 'end\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectPutWrapper\\nattr_reader :object\\n# @param object [Aws::S3::Object] An existing Amazon S3 object.\\ndef initialize(object)\\n@object = object\\nend\\ndef put_object(source_file_path)\\nFile.open(source_file_path, \"rb\") do |file|\\n@object.put(body: file)\\nend\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t put #{source_file_path} to #{object.key}. Here\\'s why:\\n#{e.message}\"\\nfalse\\nend\\nend\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nobject_key = \"my-object-key\"', '']]]\n",
      "[[['', 'file_path = \"my-local-file.txt\"\\nwrapper = ObjectPutWrapper.new(Aws::S3::Object.new(bucket_name, object_key))\\nsuccess = wrapper.put_object(file_path)\\nreturn unless success\\nputs \"Put file #{file_path} into #{object_key} in #{bucket_name}.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets. For more\\ninformation about using multipart upload with S3 Express One Zone and directory buckets,\\nsee Using multipart uploads with directory buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAfter you initiate a multipart upload and upload one or more parts, you must either\\ncomplete or stop the multipart upload to stop getting charged for storage of the uploaded\\nparts. Only after you either complete or stop a multipart upload will Amazon S3 free up the\\nparts storage and stop charging you for the parts storage.\\nAfter stopping a multipart upload, you cannot upload any part using that upload ID again.\\nIf any part uploads were in-progress, they can still succeed or fail even after you stop the\\nupload. To make sure you free all storage consumed by all parts, you must stop a multipart\\nupload only after all part uploads have been completed.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nIf you are using a multipart upload with additional checksums, the multipart part numbers\\nmust use consecutive part numbers. When using additional checksums, if you try to\\ncomplete a multipart upload request with nonconsecutive part numbers, Amazon S3\\ngenerates HTTP 500 Internal Server Error error.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIt is possible for some other request received between the time you initiated a multipart\\nupload and completed it to take precedence. For example, if another operation deletes\\na key after you initiate a multipart upload with that key, but before you complete it, the\\ncomplete multipart upload response might indicate a successful object creation without\\nyou ever seeing the object.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo minimize your storage costs, we recommend that you configure a lifecycle rule to\\ndelete incomplete multipart uploads after a specified number of days by using the\\nAbortIncompleteMultipartUpload action. For more information about creating a\\nlifecycle rule to delete incomplete multipart uploads, see Configuring a bucket lifecycle\\nconfiguration to delete incomplete multipart uploads.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['Action', 'Required permissions'], ['Create\\nMultipart\\nUpload', 'You must be allowed to perform the s3:PutObject action on an object to\\ncreate multipart upload.\\nThe bucket owner can allow other principals to perform the s3:PutObject\\naction.'], ['Initiate\\nMultipart\\nUpload', 'You must be allowed to perform the s3:PutObject action on an object to\\ninitiate multipart upload.\\nThe bucket owner can allow other principals to perform the s3:PutObject\\naction.'], ['Initiator', 'Container element that identifies who initiated the multipart upload. If the\\ninitiator is an AWS account, this element provides the same information as the\\nOwner element. If the initiator is an IAM user, this element provides the user\\nARN and display name.'], ['Upload Part', 'You must be allowed to perform the s3:PutObject action on an object to\\nupload a part.\\nThe bucket owner must allow the initiator to perform the s3:PutObject\\naction on an object in order for the initiator to upload a part for that object.'], ['Upload Part\\n(Copy)', 'You must be allowed to perform the s3:PutObject action on an object to\\nupload a part. Because you are uploading a part from an existing object, you\\nmust be allowed s3:GetObject on the source object.\\nFor the initiator to upload a part for an object, the owner of the bucket must\\nallow the initiator to perform the s3:PutObject action on the object.'], ['Complete\\nMultipart\\nUpload', 'You must be allowed to perform the s3:PutObject action on an object to\\ncomplete a multipart upload.\\nThe bucket owner must allow the initiator to perform the s3:PutObject\\naction on an object in order for the initiator to complete a multipart upload for\\nthat object.']]]\n",
      "[[['Action', 'Required permissions'], ['Stop\\nMultipart\\nUpload', \"You must be allowed to perform the s3:AbortMultipartUpload action to\\nstop a multipart upload.\\nBy default, the bucket owner and the initiator of the multipart upload are\\nallowed to perform this action as a part of IAM and bucket polices. If the\\ninitiator is an IAM user, that user's AWS account is also allowed to stop that\\nmultipart upload. With VPC endpoint policies, the initiator of the multipart\\nupload does not automatically gain the permission to perform the s3:AbortM\\nultipartUpload action.\\nIn addition to these defaults, the bucket owner can allow other principals\\nto perform the s3:AbortMultipartUpload action on an object. The\\nbucket owner can deny any principal the ability to perform the s3:AbortM\\nultipartUpload action.\"], ['List Parts', 'You must be allowed to perform the s3:ListMultipartUploadParts\\naction to list parts in a multipart upload.\\nBy default, the bucket owner has permission to list parts for any multipart\\nupload to the bucket. The initiator of the multipart upload has the permission\\nto list parts of the specific multipart upload. If the multipart upload initiator is\\nan IAM user, the AWS account controlling that IAM user also has permission to\\nlist parts of that upload.\\nIn addition to these defaults, the bucket owner can allow other principals to\\nperform the s3:ListMultipartUploadParts action on an object. The\\nbucket owner can also deny any principal the ability to perform the s3:ListMu\\nltipartUploadParts action.'], ['List Multipart\\nUploads', 'You must be allowed to perform the s3:ListBucketMultipartUploads\\naction on a bucket to list multipart uploads in progress to that bucket.\\nIn addition to the default, the bucket owner can allow other principals to\\nperform the s3:ListBucketMultipartUploads action on the bucket.']]]\n",
      "[[['Action', 'Required permissions'], ['AWS KMS\\nEncrypt and\\nDecrypt\\nrelated\\npermissions', 'To perform a multipart upload with encryption using an AWS Key Managemen\\nt Service (AWS KMS) KMS key, the requester must have permission to the\\nkms:Decrypt and kms:GenerateDataKey actions on the key. These\\npermissions are required because Amazon S3 must decrypt and read data from\\nthe encrypted file parts before it completes the multipart upload.\\nFor more information, see Uploading a large file to Amazon S3 with encryption\\nusing an AWS KMS key in the AWS Knowledge Center.\\nIf your IAM user or role is in the same AWS account as the KMS key, then you\\nmust have these permissions on the key policy. If your IAM user or role belongs\\nto a different account than the KMS key, then you must have the permissions on\\nboth the key policy and your IAM user or role.']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>sample-rule</ID>\\n<Prefix></Prefix>\\n<Status>Enabled</Status>\\n<AbortIncompleteMultipartUpload>\\n<DaysAfterInitiation>7</DaysAfterInitiation>\\n</AbortIncompleteMultipartUpload>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf the multipart upload is completed within the number of days specified in the rule, the\\nAbortIncompleteMultipartUpload lifecycle action does not apply (that is, Amazon S3', '']]]\n",
      "[[['', \"doesn't take any action). Also, this action doesn't apply to objects. No objects are deleted\\nby this lifecycle action. Additionally, you will not incur early delete charges for S3 Lifecycle\\nwhen you remove any incomplete multipart upload parts.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-lifecycle-configuration \\\\\\n--bucket example-s3-bucket1 \\\\\\n--lifecycle-configuration filename-containing-lifecycle-configuration', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Rules\": [\\n{\\n\"ID\": \"Test Rule\",\\n\"Status\": \"Enabled\",\\n\"Filter\": {\\n\"Prefix\": \"\"\\n},\\n\"AbortIncompleteMultipartUpload\": {\\n\"DaysAfterInitiation\": 7\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-lifecycle-configuration \\\\\\n--bucket example-s3-bucket1 \\\\\\n--lifecycle-configuration file://lifecycle.json', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-lifecycle \\\\\\n--bucket example-s3-bucket1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-bucket-lifecycle \\\\\\n--bucket example-s3-bucket1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nWhen you're using a stream for the source of data, the TransferManager class does\\nnot do concurrent uploads.\", ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.transfer.TransferManager;\\nimport com.amazonaws.services.s3.transfer.TransferManagerBuilder;\\nimport com.amazonaws.services.s3.transfer.Upload;\\nimport java.io.File;\\npublic class HighLevelMultipartUpload {\\npublic static void main(String[] args) throws Exception {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString keyName = \"*** Object key ***\";', '']]]\n",
      "[[['', 'String filePath = \"*** Path for file to upload ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withRegion(clientRegion)\\n.withCredentials(new ProfileCredentialsProvider())\\n.build();\\nTransferManager tm = TransferManagerBuilder.standard()\\n.withS3Client(s3Client)\\n.build();\\n// TransferManager processes all transfers asynchronously,\\n// so this call returns immediately.\\nUpload upload = tm.upload(bucketName, keyName, new File(filePath));\\nSystem.out.println(\"Object upload started\");\\n// Optionally, wait for the upload to finish before continuing.\\nupload.waitForCompletion();\\nSystem.out.println(\"Object upload complete\");\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nWhen you're using a stream for the source of data, the TransferUtility class does\\nnot do concurrent uploads.\", ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Transfer;\\nusing System;\\nusing System.IO;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass UploadFileMPUHighLevelAPITest\\n{\\nprivate const string bucketName = \"*** provide bucket name ***\";\\nprivate const string keyName = \"*** provide a name for the uploaded object\\n***\";\\nprivate const string filePath = \"*** provide the full path name of the file\\nto upload ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\npublic static void Main()\\n{\\ns3Client = new AmazonS3Client(bucketRegion);\\nUploadFileAsync().Wait();\\n}\\nprivate static async Task UploadFileAsync()\\n{\\ntry', '']]]\n",
      "[[['', '{\\nvar fileTransferUtility =\\nnew TransferUtility(s3Client);\\n// Option 1. Upload a file. The file name is used as the object key\\nname.\\nawait fileTransferUtility.UploadAsync(filePath, bucketName);\\nConsole.WriteLine(\"Upload 1 completed\");\\n// Option 2. Specify object key name explicitly.\\nawait fileTransferUtility.UploadAsync(filePath, bucketName,\\nkeyName);\\nConsole.WriteLine(\"Upload 2 completed\");\\n// Option 3. Upload data from a type of System.IO.Stream.\\nusing (var fileToUpload =\\nnew FileStream(filePath, FileMode.Open, FileAccess.Read))\\n{\\nawait fileTransferUtility.UploadAsync(fileToUpload,\\nbucketName, keyName);\\n}\\nConsole.WriteLine(\"Upload 3 completed\");\\n// Option 4. Specify advanced settings.\\nvar fileTransferUtilityRequest = new TransferUtilityUploadRequest\\n{\\nBucketName = bucketName,\\nFilePath = filePath,\\nStorageClass = S3StorageClass.StandardInfrequentAccess,\\nPartSize = 6291456, // 6 MB.\\nKey = keyName,\\nCannedACL = S3CannedACL.PublicRead\\n};\\nfileTransferUtilityRequest.Metadata.Add(\"param1\", \"Value1\");\\nfileTransferUtilityRequest.Metadata.Add(\"param2\", \"Value2\");\\nawait fileTransferUtility.UploadAsync(fileTransferUtilityRequest);\\nConsole.WriteLine(\"Upload 4 completed\");\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}', '']]]\n",
      "[[['', 'catch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import {\\nCreateMultipartUploadCommand,\\nUploadPartCommand,\\nCompleteMultipartUploadCommand,\\nAbortMultipartUploadCommand,\\nS3Client,\\n} from \"@aws-sdk/client-s3\";\\nconst twentyFiveMB = 25 * 1024 * 1024;\\nexport const createString = (size = twentyFiveMB) => {\\nreturn \"x\".repeat(size);\\n};\\nexport const main = async () => {\\nconst s3Client = new S3Client({});\\nconst bucketName = \"test-bucket\";\\nconst key = \"multipart.txt\";\\nconst str = createString();\\nconst buffer = Buffer.from(str, \"utf8\");\\nlet uploadId;\\ntry {\\nconst multipartUpload = await s3Client.send(\\nnew CreateMultipartUploadCommand({\\nBucket: bucketName,\\nKey: key,', '']]]\n",
      "[[['', '}),\\n);\\nuploadId = multipartUpload.UploadId;\\nconst uploadPromises = [];\\n// Multipart uploads require a minimum size of 5 MB per part.\\nconst partSize = Math.ceil(buffer.length / 5);\\n// Upload each part.\\nfor (let i = 0; i < 5; i++) {\\nconst start = i * partSize;\\nconst end = start + partSize;\\nuploadPromises.push(\\ns3Client\\n.send(\\nnew UploadPartCommand({\\nBucket: bucketName,\\nKey: key,\\nUploadId: uploadId,\\nBody: buffer.subarray(start, end),\\nPartNumber: i + 1,\\n}),\\n)\\n.then((d) => {\\nconsole.log(\"Part\", i + 1, \"uploaded\");\\nreturn d;\\n}),\\n);\\n}\\nconst uploadResults = await Promise.all(uploadPromises);\\nreturn await s3Client.send(\\nnew CompleteMultipartUploadCommand({\\nBucket: bucketName,\\nKey: key,\\nUploadId: uploadId,\\nMultipartUpload: {\\nParts: uploadResults.map(({ ETag }, i) => ({\\nETag,\\nPartNumber: i + 1,\\n})),\\n},', '']]]\n",
      "[[['', '}),\\n);\\n// Verify the output by downloading the file from the Amazon Simple Storage\\nService (Amazon S3) console.\\n// Because the output is a 25 MB string, text editors might struggle to open the\\nfile.\\n} catch (err) {\\nconsole.error(err);\\nif (uploadId) {\\nconst abortCommand = new AbortMultipartUploadCommand({\\nBucket: bucketName,\\nKey: key,\\nUploadId: uploadId,\\n});\\nawait s3Client.send(abortCommand);\\n}\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', 'import { GetObjectCommand, S3Client } from \"@aws-sdk/client-s3\";\\nimport { createWriteStream } from \"fs\";\\nconst s3Client = new S3Client({});\\nconst oneMB = 1024 * 1024;\\nexport const getObjectRange = ({ bucket, key, start, end }) => {\\nconst command = new GetObjectCommand({\\nBucket: bucket,\\nKey: key,\\nRange: `bytes=${start}-${end}`,\\n});\\nreturn s3Client.send(command);\\n};\\n/**\\n* @param {string | undefined} contentRange', '']]]\n",
      "[[['', '*/\\nexport const getRangeAndLength = (contentRange) => {\\nconst [range, length] = contentRange.split(\"/\");\\nconst [start, end] = range.split(\"-\");\\nreturn {\\nstart: parseInt(start),\\nend: parseInt(end),\\nlength: parseInt(length),\\n};\\n};\\nexport const isComplete = ({ end, length }) => end === length - 1;\\n// When downloading a large file, you might want to break it down into\\n// smaller pieces. Amazon S3 accepts a Range header to specify the start\\n// and end of the byte range to be downloaded.\\nconst downloadInChunks = async ({ bucket, key }) => {\\nconst writeStream = createWriteStream(\\nfileURLToPath(new URL(`./${key}`, import.meta.url)),\\n).on(\"error\", (err) => console.error(err));\\nlet rangeAndLength = { start: -1, end: -1, length: -1 };\\nwhile (!isComplete(rangeAndLength)) {\\nconst { end } = rangeAndLength;\\nconst nextRange = { start: end + 1, end: end + oneMB };\\nconsole.log(`Downloading bytes ${nextRange.start} to ${nextRange.end}`);\\nconst { ContentRange, Body } = await getObjectRange({\\nbucket,\\nkey,\\n...nextRange,\\n});\\nwriteStream.write(await Body.transformToByteArray());\\nrangeAndLength = getRangeAndLength(ContentRange);\\n}\\n};\\nexport const main = async () => {\\nawait downloadInChunks({\\nbucket: \"my-cool-bucket\",\\nkey: \"my-cool-object.txt\",', '']]]\n",
      "[[['', '});\\n};', ''], ['', '', '']], [['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3) actions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform bucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}', ''], ['', '', '']], [['', '', ''], ['', '// UploadLargeObject uses an upload manager to upload data to an object in a bucket.\\n// The upload manager breaks large data into parts and uploads the parts\\nconcurrently.\\nfunc (basics BucketBasics) UploadLargeObject(bucketName string, objectKey string,\\nlargeObject []byte) error {\\nlargeBuffer := bytes.NewReader(largeObject)\\nvar partMiBs int64 = 10\\nuploader := manager.NewUploader(basics.S3Client, func(u *manager.Uploader) {\\nu.PartSize = partMiBs * 1024 * 1024\\n})\\n_, err := uploader.Upload(context.TODO(), &s3.PutObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\nBody: largeBuffer,\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t upload large object to %v:%v. Here\\'s why: %v\\\\n\",\\nbucketName, objectKey, err)\\n}\\nreturn err\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', '// DownloadLargeObject uses a download manager to download an object from a bucket.\\n// The download manager gets the data in parts and writes them to a buffer until all\\nof\\n// the data has been downloaded.\\nfunc (basics BucketBasics) DownloadLargeObject(bucketName string, objectKey string)\\n([]byte, error) {\\nvar partMiBs int64 = 10\\ndownloader := manager.NewDownloader(basics.S3Client, func(d *manager.Downloader) {\\nd.PartSize = partMiBs * 1024 * 1024\\n})\\nbuffer := manager.NewWriteAtBuffer([]byte{})\\n_, err := downloader.Download(context.TODO(), buffer, &s3.GetObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t download large object from %v:%v. Here\\'s why: %v\\\\n\",\\nbucketName, objectKey, err)\\n}\\nreturn buffer.Bytes(), err\\n}', ''], ['', '', '']], [['', '', ''], ['', \"require 'vendor/autoload.php';\", '']]]\n",
      "[[['', 'use Aws\\\\Exception\\\\MultipartUploadException;\\nuse Aws\\\\S3\\\\MultipartUploader;\\nuse Aws\\\\S3\\\\S3Client;\\n$bucket = \\'*** Your Bucket Name ***\\';\\n$keyname = \\'*** Your Object Key ***\\';\\n$s3 = new S3Client([\\n\\'version\\' => \\'latest\\',\\n\\'region\\' => \\'us-east-1\\'\\n]);\\n// Prepare the upload parameters.\\n$uploader = new MultipartUploader($s3, \\'/path/to/large/file.zip\\', [\\n\\'bucket\\' => $bucket,\\n\\'key\\' => $keyname\\n]);\\n// Perform the upload.\\ntry {\\n$result = $uploader->upload();\\necho \"Upload complete: {$result[\\'ObjectURL\\']}\" . PHP_EOL;\\n} catch (MultipartUploadException $e) {\\necho $e->getMessage() . PHP_EOL;\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import sys\\nimport threading\\nimport boto3\\nfrom boto3.s3.transfer import TransferConfig\\nMB = 1024 * 1024\\ns3 = boto3.resource(\"s3\")', '']]]\n",
      "[[['', 'class TransferCallback:\\n\"\"\"\\nHandle callbacks from the transfer manager.\\nThe transfer manager periodically calls the __call__ method throughout\\nthe upload and download process so that it can take action, such as\\ndisplaying progress to the user and collecting data about the transfer.\\n\"\"\"\\ndef __init__(self, target_size):\\nself._target_size = target_size\\nself._total_transferred = 0\\nself._lock = threading.Lock()\\nself.thread_info = {}\\ndef __call__(self, bytes_transferred):\\n\"\"\"\\nThe callback method that is called by the transfer manager.\\nDisplay progress during file transfer and collect per-thread transfer\\ndata. This method can be called by multiple threads, so shared instance\\ndata is protected by a thread lock.\\n\"\"\"\\nthread = threading.current_thread()\\nwith self._lock:\\nself._total_transferred += bytes_transferred\\nif thread.ident not in self.thread_info.keys():\\nself.thread_info[thread.ident] = bytes_transferred\\nelse:\\nself.thread_info[thread.ident] += bytes_transferred\\ntarget = self._target_size * MB\\nsys.stdout.write(\\nf\"\\\\r{self._total_transferred} of {target} transferred \"\\nf\"({(self._total_transferred / target) * 100:.2f}%).\"\\n)\\nsys.stdout.flush()\\ndef upload_with_default_configuration(\\nlocal_file_path, bucket_name, object_key, file_size_mb\\n):\\n\"\"\"\\nUpload a file from a local folder to an Amazon S3 bucket, using the default', '']]]\n",
      "[[['', 'configuration.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\ns3.Bucket(bucket_name).upload_file(\\nlocal_file_path, object_key, Callback=transfer_callback\\n)\\nreturn transfer_callback.thread_info\\ndef upload_with_chunksize_and_meta(\\nlocal_file_path, bucket_name, object_key, file_size_mb, metadata=None\\n):\\n\"\"\"\\nUpload a file from a local folder to an Amazon S3 bucket, setting a\\nmultipart chunk size and adding metadata to the Amazon S3 object.\\nThe multipart chunk size controls the size of the chunks of data that are\\nsent in the request. A smaller chunk size typically results in the transfer\\nmanager using more threads for the upload.\\nThe metadata is a set of key-value pairs that are stored with the object\\nin Amazon S3.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\nconfig = TransferConfig(multipart_chunksize=1 * MB)\\nextra_args = {\"Metadata\": metadata} if metadata else None\\ns3.Bucket(bucket_name).upload_file(\\nlocal_file_path,\\nobject_key,\\nConfig=config,\\nExtraArgs=extra_args,\\nCallback=transfer_callback,\\n)\\nreturn transfer_callback.thread_info\\ndef upload_with_high_threshold(local_file_path, bucket_name, object_key,\\nfile_size_mb):\\n\"\"\"\\nUpload a file from a local folder to an Amazon S3 bucket, setting a\\nmultipart threshold larger than the size of the file.\\nSetting a multipart threshold larger than the size of the file results', '']]]\n",
      "[[['', 'in the transfer manager sending the file as a standard upload instead of\\na multipart upload.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\nconfig = TransferConfig(multipart_threshold=file_size_mb * 2 * MB)\\ns3.Bucket(bucket_name).upload_file(\\nlocal_file_path, object_key, Config=config, Callback=transfer_callback\\n)\\nreturn transfer_callback.thread_info\\ndef upload_with_sse(\\nlocal_file_path, bucket_name, object_key, file_size_mb, sse_key=None\\n):\\n\"\"\"\\nUpload a file from a local folder to an Amazon S3 bucket, adding server-side\\nencryption with customer-provided encryption keys to the object.\\nWhen this kind of encryption is specified, Amazon S3 encrypts the object\\nat rest and allows downloads only when the expected encryption key is\\nprovided in the download request.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\nif sse_key:\\nextra_args = {\"SSECustomerAlgorithm\": \"AES256\", \"SSECustomerKey\": sse_key}\\nelse:\\nextra_args = None\\ns3.Bucket(bucket_name).upload_file(\\nlocal_file_path, object_key, ExtraArgs=extra_args,\\nCallback=transfer_callback\\n)\\nreturn transfer_callback.thread_info\\ndef download_with_default_configuration(\\nbucket_name, object_key, download_file_path, file_size_mb\\n):\\n\"\"\"\\nDownload a file from an Amazon S3 bucket to a local folder, using the\\ndefault configuration.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\ns3.Bucket(bucket_name).Object(object_key).download_file(\\ndownload_file_path, Callback=transfer_callback', '']]]\n",
      "[[['', ')\\nreturn transfer_callback.thread_info\\ndef download_with_single_thread(\\nbucket_name, object_key, download_file_path, file_size_mb\\n):\\n\"\"\"\\nDownload a file from an Amazon S3 bucket to a local folder, using a\\nsingle thread.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\nconfig = TransferConfig(use_threads=False)\\ns3.Bucket(bucket_name).Object(object_key).download_file(\\ndownload_file_path, Config=config, Callback=transfer_callback\\n)\\nreturn transfer_callback.thread_info\\ndef download_with_high_threshold(\\nbucket_name, object_key, download_file_path, file_size_mb\\n):\\n\"\"\"\\nDownload a file from an Amazon S3 bucket to a local folder, setting a\\nmultipart threshold larger than the size of the file.\\nSetting a multipart threshold larger than the size of the file results\\nin the transfer manager sending the file as a standard download instead\\nof a multipart download.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\nconfig = TransferConfig(multipart_threshold=file_size_mb * 2 * MB)\\ns3.Bucket(bucket_name).Object(object_key).download_file(\\ndownload_file_path, Config=config, Callback=transfer_callback\\n)\\nreturn transfer_callback.thread_info\\ndef download_with_sse(\\nbucket_name, object_key, download_file_path, file_size_mb, sse_key\\n):\\n\"\"\"\\nDownload a file from an Amazon S3 bucket to a local folder, adding a\\ncustomer-provided encryption key to the request.', '']]]\n",
      "[[['', 'When this kind of encryption is specified, Amazon S3 encrypts the object\\nat rest and allows downloads only when the expected encryption key is\\nprovided in the download request.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\nif sse_key:\\nextra_args = {\"SSECustomerAlgorithm\": \"AES256\", \"SSECustomerKey\": sse_key}\\nelse:\\nextra_args = None\\ns3.Bucket(bucket_name).Object(object_key).download_file(\\ndownload_file_path, ExtraArgs=extra_args, Callback=transfer_callback\\n)\\nreturn transfer_callback.thread_info', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.*;\\nimport java.io.File;\\nimport java.io.IOException;\\nimport java.util.ArrayList;\\nimport java.util.List;\\npublic class LowLevelMultipartUpload {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString keyName = \"*** Key name ***\";\\nString filePath = \"*** Path to file to upload ***\";\\nFile file = new File(filePath);\\nlong contentLength = file.length();\\nlong partSize = 5 * 1024 * 1024; // Set part size to 5 MB.\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withRegion(clientRegion)\\n.withCredentials(new ProfileCredentialsProvider())\\n.build();', '']]]\n",
      "[[['', \"// Create a list of ETag objects. You retrieve ETags for each object\\npart\\n// uploaded,\\n// then, after each individual part has been uploaded, pass the list of\\nETags to\\n// the request to complete the upload.\\nList<PartETag> partETags = new ArrayList<PartETag>();\\n// Initiate the multipart upload.\\nInitiateMultipartUploadRequest initRequest = new\\nInitiateMultipartUploadRequest(bucketName, keyName);\\nInitiateMultipartUploadResult initResponse =\\ns3Client.initiateMultipartUpload(initRequest);\\n// Upload the file parts.\\nlong filePosition = 0;\\nfor (int i = 1; filePosition < contentLength; i++) {\\n// Because the last part could be less than 5 MB, adjust the part\\nsize as\\n// needed.\\npartSize = Math.min(partSize, (contentLength - filePosition));\\n// Create the request to upload a part.\\nUploadPartRequest uploadRequest = new UploadPartRequest()\\n.withBucketName(bucketName)\\n.withKey(keyName)\\n.withUploadId(initResponse.getUploadId())\\n.withPartNumber(i)\\n.withFileOffset(filePosition)\\n.withFile(file)\\n.withPartSize(partSize);\\n// Upload the part and add the response's ETag to our list.\\nUploadPartResult uploadResult = s3Client.uploadPart(uploadRequest);\\npartETags.add(uploadResult.getPartETag());\\nfilePosition += partSize;\\n}\\n// Complete the multipart upload.\\nCompleteMultipartUploadRequest compRequest = new\\nCompleteMultipartUploadRequest(bucketName, keyName,\\ninitResponse.getUploadId(), partETags);\\ns3Client.completeMultipartUpload(compRequest);\", '']]]\n",
      "[[['', \"} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen you use the AWS SDK for .NET API to upload large objects, a timeout might occur\\nwhile data is being written to the request stream. You can set an explicit timeout using\\nthe UploadPartRequest.', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.Runtime;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.IO;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass UploadFileMPULowLevelAPITest', '']]]\n",
      "[[['', '{\\nprivate const string bucketName = \"*** provide bucket name ***\";\\nprivate const string keyName = \"*** provide a name for the uploaded object\\n***\";\\nprivate const string filePath = \"*** provide the full path name of the file\\nto upload ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\npublic static void Main()\\n{\\ns3Client = new AmazonS3Client(bucketRegion);\\nConsole.WriteLine(\"Uploading an object\");\\nUploadObjectAsync().Wait();\\n}\\nprivate static async Task UploadObjectAsync()\\n{\\n// Create list to store upload part responses.\\nList<UploadPartResponse> uploadResponses = new\\nList<UploadPartResponse>();\\n// Setup information required to initiate the multipart upload.\\nInitiateMultipartUploadRequest initiateRequest = new\\nInitiateMultipartUploadRequest\\n{\\nBucketName = bucketName,\\nKey = keyName\\n};\\n// Initiate the upload.\\nInitiateMultipartUploadResponse initResponse =\\nawait s3Client.InitiateMultipartUploadAsync(initiateRequest);\\n// Upload parts.\\nlong contentLength = new FileInfo(filePath).Length;\\nlong partSize = 5 * (long)Math.Pow(2, 20); // 5 MB\\ntry\\n{\\nConsole.WriteLine(\"Uploading parts\");', '']]]\n",
      "[[['', 'long filePosition = 0;\\nfor (int i = 1; filePosition < contentLength; i++)\\n{\\nUploadPartRequest uploadRequest = new UploadPartRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nUploadId = initResponse.UploadId,\\nPartNumber = i,\\nPartSize = partSize,\\nFilePosition = filePosition,\\nFilePath = filePath\\n};\\n// Track upload progress.\\nuploadRequest.StreamTransferProgress +=\\nnew\\nEventHandler<StreamTransferProgressArgs>(UploadPartProgressEventCallback);\\n// Upload a part and add the response to our list.\\nuploadResponses.Add(await\\ns3Client.UploadPartAsync(uploadRequest));\\nfilePosition += partSize;\\n}\\n// Setup to complete the upload.\\nCompleteMultipartUploadRequest completeRequest = new\\nCompleteMultipartUploadRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nUploadId = initResponse.UploadId\\n};\\ncompleteRequest.AddPartETags(uploadResponses);\\n// Complete the upload.\\nCompleteMultipartUploadResponse completeUploadResponse =\\nawait s3Client.CompleteMultipartUploadAsync(completeRequest);\\n}\\ncatch (Exception exception)\\n{\\nConsole.WriteLine(\"An AmazonS3Exception was thrown: { 0}\",\\nexception.Message);', '']]]\n",
      "[[['', '// Abort the upload.\\nAbortMultipartUploadRequest abortMPURequest = new\\nAbortMultipartUploadRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nUploadId = initResponse.UploadId\\n};\\nawait s3Client.AbortMultipartUploadAsync(abortMPURequest);\\n}\\n}\\npublic static void UploadPartProgressEventCallback(object sender,\\nStreamTransferProgressArgs e)\\n{\\n// Process event.\\nConsole.WriteLine(\"{0}/{1}\", e.TransferredBytes, e.TotalBytes);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"require 'vendor/autoload.php';\\nuse Aws\\\\S3\\\\Exception\\\\S3Exception;\\nuse Aws\\\\S3\\\\S3Client;\\n$bucket = '*** Your Bucket Name ***';\\n$keyname = '*** Your Object Key ***';\\n$filename = '*** Path to and Name of the File to Upload ***';\\n$s3 = new S3Client([\\n'version' => 'latest',\\n'region' => 'us-east-1'\\n]);\", '']]]\n",
      "[[['', '$result = $s3->createMultipartUpload([\\n\\'Bucket\\' => $bucket,\\n\\'Key\\' => $keyname,\\n\\'StorageClass\\' => \\'REDUCED_REDUNDANCY\\',\\n\\'Metadata\\' => [\\n\\'param1\\' => \\'value 1\\',\\n\\'param2\\' => \\'value 2\\',\\n\\'param3\\' => \\'value 3\\'\\n]\\n]);\\n$uploadId = $result[\\'UploadId\\'];\\n// Upload the file in parts.\\ntry {\\n$file = fopen($filename, \\'r\\');\\n$partNumber = 1;\\nwhile (!feof($file)) {\\n$result = $s3->uploadPart([\\n\\'Bucket\\' => $bucket,\\n\\'Key\\' => $keyname,\\n\\'UploadId\\' => $uploadId,\\n\\'PartNumber\\' => $partNumber,\\n\\'Body\\' => fread($file, 5 * 1024 * 1024),\\n]);\\n$parts[\\'Parts\\'][$partNumber] = [\\n\\'PartNumber\\' => $partNumber,\\n\\'ETag\\' => $result[\\'ETag\\'],\\n];\\n$partNumber++;\\necho \"Uploading part $partNumber of $filename.\" . PHP_EOL;\\n}\\nfclose($file);\\n} catch (S3Exception $e) {\\n$result = $s3->abortMultipartUpload([\\n\\'Bucket\\' => $bucket,\\n\\'Key\\' => $keyname,\\n\\'UploadId\\' => $uploadId\\n]);\\necho \"Upload of $filename failed.\" . PHP_EOL;\\n}', '']]]\n",
      "[[['', '// Complete the multipart upload.\\n$result = $s3->completeMultipartUpload([\\n\\'Bucket\\' => $bucket,\\n\\'Key\\' => $keyname,\\n\\'UploadId\\' => $uploadId,\\n\\'MultipartUpload\\' => $parts,\\n]);\\n$url = $result[\\'Location\\'];\\necho \"Uploaded $filename to $url.\" . PHP_EOL;', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'C:\\\\myfolder\\n\\\\a.txt\\n\\\\b.pdf\\n\\\\media\\\\\\nAn.mp3', ''], ['', '', '']], [['', '', ''], ['', 'a.txt\\nb.pdf\\nmedia/An.mp3', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Transfer;\\nusing System;\\nusing System.IO;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass UploadDirMPUHighLevelAPITest\\n{\\nprivate const string existingBucketName = \"*** bucket name ***\";\\nprivate const string directoryPath = @\"*** directory path ***\";\\n// The example uploads only .txt files.\\nprivate const string wildCard = \"*.txt\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\nstatic void Main()', '']]]\n",
      "[[['', '{\\ns3Client = new AmazonS3Client(bucketRegion);\\nUploadDirAsync().Wait();\\n}\\nprivate static async Task UploadDirAsync()\\n{\\ntry\\n{\\nvar directoryTransferUtility =\\nnew TransferUtility(s3Client);\\n// 1. Upload a directory.\\nawait directoryTransferUtility.UploadDirectoryAsync(directoryPath,\\nexistingBucketName);\\nConsole.WriteLine(\"Upload statement 1 completed\");\\n// 2. Upload only the .txt files from a directory\\n// and search recursively.\\nawait directoryTransferUtility.UploadDirectoryAsync(\\ndirectoryPath,\\nexistingBucketName,\\nwildCard,\\nSearchOption.AllDirectories);\\nConsole.WriteLine(\"Upload statement 2 completed\");\\n// 3. The same as Step 2 and some optional configuration.\\n// Search recursively for .txt files to upload.\\nvar request = new TransferUtilityUploadDirectoryRequest\\n{\\nBucketName = existingBucketName,\\nDirectory = directoryPath,\\nSearchOption = SearchOption.AllDirectories,\\nSearchPattern = wildCard\\n};\\nawait directoryTransferUtility.UploadDirectoryAsync(request);\\nConsole.WriteLine(\"Upload statement 3 completed\");\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\\n\"Error encountered ***. Message:\\'{0}\\' when writing an object\",\\ne.Message);', '']]]\n",
      "[[['', '}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\\n\"Unknown encountered on server. Message:\\'{0}\\' when writing an\\nobject\", e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['1', 'Create an instance of the ListMultipartUploadsRequest class and\\nprovide the bucket name.'], ['2', 'Run the AmazonS3Client.listMultipartUploads method. The method\\nreturns an instance of the MultipartUploadListing class that gives you\\ninformation about the multipart uploads in progress.']], [['', '', ''], ['', 'ListMultipartUploadsRequest allMultpartUploadsRequest =\\nnew ListMultipartUploadsRequest(existingBucketName);\\nMultipartUploadListing multipartUploadListing =\\ns3Client.listMultipartUploads(allMultpartUploadsRequest);', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'ListMultipartUploadsRequest request = new ListMultipartUploadsRequest\\n{\\nBucketName = bucketName // Bucket receiving the uploads.\\n};\\nListMultipartUploadsResponse response = await\\nAmazonS3Client.ListMultipartUploadsAsync(request);', ''], ['', '', '']], [['', '', ''], ['', \"require 'vendor/autoload.php';\\nuse Aws\\\\S3\\\\S3Client;\\n$bucket = '*** Your Bucket Name ***';\\n$s3 = new S3Client([\\n'version' => 'latest',\", '']]]\n",
      "[[['', \"'region' => 'us-east-1'\\n]);\\n// Retrieve a list of the current multipart uploads.\\n$result = $s3->listMultipartUploads([\\n'Bucket' => $bucket\\n]);\\n// Write the list of uploads to the page.\\nprint_r($result->toArray());\", ''], ['', '', '']], [['', '', ''], ['', 'TransferManager tm = new TransferManager(new ProfileCredentialsProvider());', '']]]\n",
      "[[['', 'PutObjectRequest request = new PutObjectRequest(\\nexistingBucketName, keyName, new File(filePath));\\n// Subscribe to the event and provide event handler.\\nrequest.setProgressListener(new ProgressListener() {\\npublic void progressChanged(ProgressEvent event) {\\nSystem.out.println(\"Transferred bytes: \" +\\nevent.getBytesTransfered());\\n}\\n});', ''], ['', '', '']], [['', '', ''], ['', 'import java.io.File;\\nimport com.amazonaws.AmazonClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.event.ProgressEvent;\\nimport com.amazonaws.event.ProgressListener;\\nimport com.amazonaws.services.s3.model.PutObjectRequest;\\nimport com.amazonaws.services.s3.transfer.TransferManager;\\nimport com.amazonaws.services.s3.transfer.Upload;\\npublic class TrackMPUProgressUsingHighLevelAPI {\\npublic static void main(String[] args) throws Exception {\\nString existingBucketName = \"*** Provide bucket name ***\";\\nString keyName = \"*** Provide object key ***\";\\nString filePath = \"*** file to upload ***\";\\nTransferManager tm = new TransferManager(new ProfileCredentialsProvider());\\n// For more advanced uploads, you can create a request object\\n// and supply additional request parameters (ex: progress listeners,\\n// canned ACLs, etc.)\\nPutObjectRequest request = new PutObjectRequest(\\nexistingBucketName, keyName, new File(filePath));', '']]]\n",
      "[[['', '// You can ask the upload for its progress, or you can\\n// add a ProgressListener to your request to receive notifications\\n// when bytes are transferred.\\nrequest.setGeneralProgressListener(new ProgressListener() {\\n@Override\\npublic void progressChanged(ProgressEvent progressEvent) {\\nSystem.out.println(\"Transferred bytes: \" +\\nprogressEvent.getBytesTransferred());\\n}\\n});\\n// TransferManager processes all transfers asynchronously,\\n// so this call will return immediately.\\nUpload upload = tm.upload(request);\\ntry {\\n// You can block and wait for the upload to finish\\nupload.waitForCompletion();\\n} catch (AmazonClientException amazonClientException) {\\nSystem.out.println(\"Unable to upload file, upload aborted.\");\\namazonClientException.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Transfer;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass TrackMPUUsingHighLevelAPITest\\n{', '']]]\n",
      "[[['', 'private const string bucketName = \"*** provide the bucket name ***\";\\nprivate const string keyName = \"*** provide the name for the uploaded object\\n***\";\\nprivate const string filePath = \" *** provide the full path name of the file\\nto upload **\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\npublic static void Main()\\n{\\ns3Client = new AmazonS3Client(bucketRegion);\\nTrackMPUAsync().Wait();\\n}\\nprivate static async Task TrackMPUAsync()\\n{\\ntry\\n{\\nvar fileTransferUtility = new TransferUtility(s3Client);\\n// Use TransferUtilityUploadRequest to configure options.\\n// In this example we subscribe to an event.\\nvar uploadRequest =\\nnew TransferUtilityUploadRequest\\n{\\nBucketName = bucketName,\\nFilePath = filePath,\\nKey = keyName\\n};\\nuploadRequest.UploadProgressEvent +=\\nnew EventHandler<UploadProgressArgs>\\n(uploadRequest_UploadPartProgressEvent);\\nawait fileTransferUtility.UploadAsync(uploadRequest);\\nConsole.WriteLine(\"Upload completed\");\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);', '']]]\n",
      "[[['', '}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\nstatic void uploadRequest_UploadPartProgressEvent(object sender,\\nUploadProgressArgs e)\\n{\\n// Process event.\\nConsole.WriteLine(\"{0}/{1}\", e.TransferredBytes, e.TotalBytes);\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['1', 'Create an instance of the TransferManager class.'], ['2', 'Run the TransferManager.abortMultipartUploads method by\\npassing the bucket name and a Date value.']], [['', '', ''], ['', 'import java.util.Date;\\nimport com.amazonaws.AmazonClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3.transfer.TransferManager;\\npublic class AbortMPUUsingHighLevelAPI {\\npublic static void main(String[] args) throws Exception {\\nString existingBucketName = \"*** Provide existing bucket name ***\";\\nTransferManager tm = new TransferManager(new ProfileCredentialsProvider());\\nint sevenDays = 1000 * 60 * 60 * 24 * 7;\\nDate oneWeekAgo = new Date(System.currentTimeMillis() - sevenDays);\\ntry {\\ntm.abortMultipartUploads(existingBucketName, oneWeekAgo);\\n} catch (AmazonClientException amazonClientException) {\\nSystem.out.println(\"Unable to upload file, upload was aborted.\");\\namazonClientException.printStackTrace();\\n}\\n}', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can also stop a specific multipart upload. For more information, see Using the AWS\\nSDKs (low-level API).', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Transfer;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass AbortMPUUsingHighLevelAPITest\\n{\\nprivate const string bucketName = \"*** provide bucket name ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\npublic static void Main()\\n{\\ns3Client = new AmazonS3Client(bucketRegion);\\nAbortMPUAsync().Wait();\\n}\\nprivate static async Task AbortMPUAsync()\\n{\\ntry\\n{\\nvar transferUtility = new TransferUtility(s3Client);', '']]]\n",
      "[[['', '// Abort all in-progress uploads initiated before the specified\\ndate.\\nawait transferUtility.AbortMultipartUploadsAsync(\\nbucketName, DateTime.Now.AddDays(-7));\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can also stop a specific multipart upload. For more information, see Using the AWS\\nSDKs (low-level API).', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'InitiateMultipartUploadRequest initRequest =\\nnew InitiateMultipartUploadRequest(existingBucketName, keyName);\\nInitiateMultipartUploadResult initResponse =\\ns3Client.initiateMultipartUpload(initRequest);\\nAmazonS3 s3Client = new AmazonS3Client(new ProfileCredentialsProvider());\\ns3Client.abortMultipartUpload(new AbortMultipartUploadRequest(\\nexistingBucketName, keyName, initResponse.getUploadId()));', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nInstead of a specific multipart upload, you can stop all your multipart uploads initiated\\nbefore a specific time that are still in progress. This clean-up operation is useful to\\nstop old multipart uploads that you initiated but did not complete or stop. For more\\ninformation, see Using the AWS SDKs (high-level API).', ''], ['', '', '']], [['', '', ''], ['', 'AbortMultipartUploadRequest abortMPURequest = new AbortMultipartUploadRequest\\n{\\nBucketName = existingBucketName,\\nKey = keyName,\\nUploadId = initResponse.UploadId\\n};\\nawait AmazonS3Client.AbortMultipartUploadAsync(abortMPURequest);', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"require 'vendor/autoload.php';\\nuse Aws\\\\S3\\\\S3Client;\\n$bucket = '*** Your Bucket Name ***';\\n$keyname = '*** Your Object Key ***';\\n$uploadId = '*** Upload ID of upload to Abort ***';\\n$s3 = new S3Client([\\n'version' => 'latest',\\n'region' => 'us-east-1'\\n]);\\n// Abort the multipart upload.\\n$s3->abortMultipartUpload([\\n'Bucket' => $bucket,\\n'Key' => $keyname,\\n'UploadId' => $uploadId,\\n]);\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.*;\\nimport java.io.IOException;\\nimport java.util.ArrayList;\\nimport java.util.List;\\npublic class LowLevelMultipartCopy {\\npublic static void main(String[] args) throws IOException {', '']]]\n",
      "[[['', 'Regions clientRegion = Regions.DEFAULT_REGION;\\nString sourceBucketName = \"*** Source bucket name ***\";\\nString sourceObjectKey = \"*** Source object key ***\";\\nString destBucketName = \"*** Target bucket name ***\";\\nString destObjectKey = \"*** Target object key ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\n// Initiate the multipart upload.\\nInitiateMultipartUploadRequest initRequest = new\\nInitiateMultipartUploadRequest(destBucketName,\\ndestObjectKey);\\nInitiateMultipartUploadResult initResult =\\ns3Client.initiateMultipartUpload(initRequest);\\n// Get the object size to track the end of the copy operation.\\nGetObjectMetadataRequest metadataRequest = new\\nGetObjectMetadataRequest(sourceBucketName, sourceObjectKey);\\nObjectMetadata metadataResult =\\ns3Client.getObjectMetadata(metadataRequest);\\nlong objectSize = metadataResult.getContentLength();\\n// Copy the object using 5 MB parts.\\nlong partSize = 5 * 1024 * 1024;\\nlong bytePosition = 0;\\nint partNum = 1;\\nList<CopyPartResult> copyResponses = new ArrayList<CopyPartResult>();\\nwhile (bytePosition < objectSize) {\\n// The last part might be smaller than partSize, so check to make\\nsure\\n// that lastByte isn\\'t beyond the end of the object.\\nlong lastByte = Math.min(bytePosition + partSize - 1, objectSize -\\n1);\\n// Copy this part.\\nCopyPartRequest copyRequest = new CopyPartRequest()\\n.withSourceBucketName(sourceBucketName)\\n.withSourceKey(sourceObjectKey)\\n.withDestinationBucketName(destBucketName)\\n.withDestinationKey(destObjectKey)', '']]]\n",
      "[[['', '.withUploadId(initResult.getUploadId())\\n.withFirstByte(bytePosition)\\n.withLastByte(lastByte)\\n.withPartNumber(partNum++);\\ncopyResponses.add(s3Client.copyPart(copyRequest));\\nbytePosition += partSize;\\n}\\n// Complete the upload request to concatenate all uploaded parts and\\nmake the\\n// copied object available.\\nCompleteMultipartUploadRequest completeRequest = new\\nCompleteMultipartUploadRequest(\\ndestBucketName,\\ndestObjectKey,\\ninitResult.getUploadId(),\\ngetETags(copyResponses));\\ns3Client.completeMultipartUpload(completeRequest);\\nSystem.out.println(\"Multipart copy complete.\");\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n// This is a helper function to construct a list of ETags.\\nprivate static List<PartETag> getETags(List<CopyPartResult> responses) {\\nList<PartETag> etags = new ArrayList<PartETag>();\\nfor (CopyPartResult response : responses) {\\netags.add(new PartETag(response.getPartNumber(), response.getETag()));\\n}\\nreturn etags;\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass CopyObjectUsingMPUapiTest\\n{\\nprivate const string sourceBucket = \"*** provide the name of the bucket with\\nsource object ***\";\\nprivate const string targetBucket = \"*** provide the name of the bucket to\\ncopy the object to ***\";\\nprivate const string sourceObjectKey = \"*** provide the name of object to\\ncopy ***\";\\nprivate const string targetObjectKey = \"*** provide the name of the object\\ncopy ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\npublic static void Main()\\n{\\ns3Client = new AmazonS3Client(bucketRegion);\\nConsole.WriteLine(\"Copying an object\");\\nMPUCopyObjectAsync().Wait();\\n}\\nprivate static async Task MPUCopyObjectAsync()\\n{', '']]]\n",
      "[[['', '// Create a list to store the upload part responses.\\nList<UploadPartResponse> uploadResponses = new\\nList<UploadPartResponse>();\\nList<CopyPartResponse> copyResponses = new List<CopyPartResponse>();\\n// Setup information required to initiate the multipart upload.\\nInitiateMultipartUploadRequest initiateRequest =\\nnew InitiateMultipartUploadRequest\\n{\\nBucketName = targetBucket,\\nKey = targetObjectKey\\n};\\n// Initiate the upload.\\nInitiateMultipartUploadResponse initResponse =\\nawait s3Client.InitiateMultipartUploadAsync(initiateRequest);\\n// Save the upload ID.\\nString uploadId = initResponse.UploadId;\\ntry\\n{\\n// Get the size of the object.\\nGetObjectMetadataRequest metadataRequest = new\\nGetObjectMetadataRequest\\n{\\nBucketName = sourceBucket,\\nKey = sourceObjectKey\\n};\\nGetObjectMetadataResponse metadataResponse =\\nawait s3Client.GetObjectMetadataAsync(metadataRequest);\\nlong objectSize = metadataResponse.ContentLength; // Length in\\nbytes.\\n// Copy the parts.\\nlong partSize = 5 * (long)Math.Pow(2, 20); // Part size is 5 MB.\\nlong bytePosition = 0;\\nfor (int i = 1; bytePosition < objectSize; i++)\\n{\\nCopyPartRequest copyRequest = new CopyPartRequest\\n{\\nDestinationBucket = targetBucket,', '']]]\n",
      "[[['', 'DestinationKey = targetObjectKey,\\nSourceBucket = sourceBucket,\\nSourceKey = sourceObjectKey,\\nUploadId = uploadId,\\nFirstByte = bytePosition,\\nLastByte = bytePosition + partSize - 1 >= objectSize ?\\nobjectSize - 1 : bytePosition + partSize - 1,\\nPartNumber = i\\n};\\ncopyResponses.Add(await s3Client.CopyPartAsync(copyRequest));\\nbytePosition += partSize;\\n}\\n// Set up to complete the copy.\\nCompleteMultipartUploadRequest completeRequest =\\nnew CompleteMultipartUploadRequest\\n{\\nBucketName = targetBucket,\\nKey = targetObjectKey,\\nUploadId = initResponse.UploadId\\n};\\ncompleteRequest.AddPartETags(copyResponses);\\n// Complete the copy.\\nCompleteMultipartUploadResponse completeUploadResponse =\\nawait s3Client.CompleteMultipartUploadAsync(completeRequest);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['Item', 'Specification'], ['Maximum object size', '5 TiB'], ['Maximum number of parts per\\nupload', '10,000'], ['Part numbers', '1 to 10,000 (inclusive)'], ['Part size', '5 MiB to 5 GiB. There is no minimum size limit on the last\\npart of your multipart upload.'], ['Maximum number of parts returned\\nfor a list parts request', '1000']]]\n",
      "[[['Item', 'Specification'], ['Maximum number of multipart\\nuploads returned in a list multipart\\nuploads request', '1000']], [['', '', ''], ['', 'Note\\nCopying or moving objects across AWS Regions incurs bandwidth charges. For more\\ninformation, see Amazon S3 Pricing.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\n• When copying an object by using the Amazon S3 console, you must have the\\ns3:ListAllMyBuckets permission. The console needs this permission to validate the\\nCopy operation. For example policies that grant this permission, see the section called\\n“Identity-based policy examples”.\\nIf you're copying an object that has user-defined tags, you must also have the\\ns3:GetObjectTagging permission. If you're copying an object that doesn't have user-\\ndefined tags but is over 16 MB in size, you must also have the s3:GetObjectTagging\\npermission.\\nIf the destination bucket policy denies the s3:GetObjectTagging action, the object\\nwill be copied without the user-defined tags, and you will receive an error.\\n• Objects encrypted with customer-provided encryption keys (SSE-C) cannot be copied by\\nusing the S3 console. To copy objects encrypted with SSE-C, use the AWS CLI, AWS SDK,\\nor the Amazon S3 REST API.\\n• Cross-Region copying of objects encrypted with SSE-KMS is not supported by the\\nAmazon S3 console. To copy objects encrypted with SSE-KMS across Regions, use the\\nAWS CLI, AWS SDK, or the Amazon S3 REST API.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nEven if you opt to use the same checksum function, your checksum value might change\\nif you copy the object and it is over 16 MB in size. The checksum value might change\\nbecause of how checksums are calculated for multipart uploads. For more information\\nabout how the checksum might change when copying the object, see Using part-level\\nchecksums for multipart uploads.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.CopyObjectRequest;\\nimport java.io.IOException;\\npublic class CopyObjectSingleOperation {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString sourceKey = \"*** Source object key *** \";\\nString destinationKey = \"*** Destination object key ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\n// Copy the object into a new object in the same bucket.\\nCopyObjectRequest copyObjRequest = new CopyObjectRequest(bucketName,\\nsourceKey, bucketName, destinationKey);\\ns3Client.copyObject(copyObjRequest);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client', '']]]\n",
      "[[['', \"// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass CopyObjectTest\\n{\\nprivate const string sourceBucket = \"*** provide the name of the bucket with\\nsource object ***\";\\nprivate const string destinationBucket = \"*** provide the name of the bucket\\nto copy the object to ***\";\\nprivate const string objectKey = \"*** provide the name of object to copy\\n***\";\\nprivate const string destObjectKey = \"*** provide the destination object key\\nname ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\npublic static void Main()\\n{\\ns3Client = new AmazonS3Client(bucketRegion);', '']]]\n",
      "[[['', 'Console.WriteLine(\"Copying an object\");\\nCopyingObjectAsync().Wait();\\n}\\nprivate static async Task CopyingObjectAsync()\\n{\\ntry\\n{\\nCopyObjectRequest request = new CopyObjectRequest\\n{\\nSourceBucket = sourceBucket,\\nSourceKey = objectKey,\\nDestinationBucket = destinationBucket,\\nDestinationKey = destObjectKey\\n};\\nCopyObjectResponse response = await\\ns3Client.CopyObjectAsync(request);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['1', 'Create an instance of an Amazon S3 client by using the Aws\\\\S3\\\\S3Client class\\nconstructor.'], ['2', 'To make multiple copies of an object, you run a batch of calls to the Amazon S3\\nclient getCommand() method, which is inherited from the Aws\\\\CommandInterfa\\nce class. You provide the CopyObject command as the first argument and an\\narray containing the source bucket, source key name, target bucket, and target\\nkey name as the second argument.']], [['', '', ''], ['', 'require \\'vendor/autoload.php\\';\\nuse Aws\\\\CommandPool;\\nuse Aws\\\\Exception\\\\AwsException;\\nuse Aws\\\\ResultInterface;\\nuse Aws\\\\S3\\\\S3Client;\\n$sourceBucket = \\'*** Your Source Bucket Name ***\\';\\n$sourceKeyname = \\'*** Your Source Object Key ***\\';\\n$targetBucket = \\'*** Your Target Bucket Name ***\\';\\n$s3 = new S3Client([\\n\\'version\\' => \\'latest\\',\\n\\'region\\' => \\'us-east-1\\'\\n]);\\n// Copy an object.\\n$s3->copyObject([\\n\\'Bucket\\' => $targetBucket,\\n\\'Key\\' => \"$sourceKeyname-copy\",\\n\\'CopySource\\' => \"$sourceBucket/$sourceKeyname\",\\n]);\\n// Perform a batch of CopyObject operations.\\n$batch = array();\\nfor ($i = 1; $i <= 3; $i++) {\\n$batch[] = $s3->getCommand(\\'CopyObject\\', [\\n\\'Bucket\\' => $targetBucket,\\n\\'Key\\' => \"{targetKeyname}-$i\",\\n\\'CopySource\\' => \"$sourceBucket/$sourceKeyname\",', '']]]\n",
      "[[['', ']);\\n}\\ntry {\\n$results = CommandPool::batch($s3, $batch);\\nforeach ($results as $result) {\\nif ($result instanceof ResultInterface) {\\n// Result handling here\\n}\\nif ($result instanceof AwsException) {\\n// AwsException handling here\\n}\\n}\\n} catch (Exception $e) {\\n// General error handling here\\n}', ''], ['', '', '']], [['', '', ''], ['', 'class ObjectWrapper:\\n\"\"\"Encapsulates S3 object actions.\"\"\"\\ndef __init__(self, s3_object):\\n\"\"\"\\n:param s3_object: A Boto3 Object resource. This is a high-level resource in\\nBoto3\\nthat wraps object actions in a class-like structure.\\n\"\"\"\\nself.object = s3_object\\nself.key = self.object.key', ''], ['', '', '']], [['', '', ''], ['', 'def copy(self, dest_object):\\n\"\"\"\\nCopies the object to another bucket.\\n:param dest_object: The destination object initialized with a bucket and\\nkey.\\nThis is a Boto3 Object resource.\\n\"\"\"\\ntry:\\ndest_object.copy_from(', '']]]\n",
      "[[['', 'CopySource={\"Bucket\": self.object.bucket_name, \"Key\":\\nself.object.key}\\n)\\ndest_object.wait_until_exists()\\nlogger.info(\\n\"Copied object from %s:%s to %s:%s.\",\\nself.object.bucket_name,\\nself.object.key,\\ndest_object.bucket_name,\\ndest_object.key,\\n)\\nexcept ClientError:\\nlogger.exception(\\n\"Couldn\\'t copy object from %s/%s to %s/%s.\",\\nself.object.bucket_name,\\nself.object.key,\\ndest_object.bucket_name,\\ndest_object.key,\\n)\\nraise', ''], ['', '', '']], [['1', 'Use the Amazon S3 modularized gem for version 3 of the AWS SDK for Ruby,\\nrequire aws-sdk-s3 , and provide your AWS credentials. For more information\\nabout how to provide your credentials, see Making requests using AWS account or\\nIAM user credentials.'], ['2', 'Provide the request information, such as the source bucket name, source key\\nname, destination bucket name, and destination key.']], [['', '', ''], ['', 'require \"aws-sdk-s3\"', '']]]\n",
      "[[['', '# Wraps Amazon S3 object actions.\\nclass ObjectCopyWrapper\\nattr_reader :source_object\\n# @param source_object [Aws::S3::Object] An existing Amazon S3 object. This is\\nused as the source object for\\n# copy actions.\\ndef initialize(source_object)\\n@source_object = source_object\\nend\\n# Copy the source object to the specified target bucket and rename it with the\\ntarget key.\\n#\\n# @param target_bucket [Aws::S3::Bucket] An existing Amazon S3 bucket where the\\nobject is copied.\\n# @param target_object_key [String] The key to give the copy of the object.\\n# @return [Aws::S3::Object, nil] The copied object when successful; otherwise,\\nnil.\\ndef copy_object(target_bucket, target_object_key)\\n@source_object.copy_to(bucket: target_bucket.name, key: target_object_key)\\ntarget_bucket.object(target_object_key)\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t copy #{@source_object.key} to #{target_object_key}. Here\\'s why:\\n#{e.message}\"\\nend\\nend\\n# Example usage:\\ndef run_demo\\nsource_bucket_name = \"doc-example-bucket1\"\\nsource_key = \"my-source-file.txt\"\\ntarget_bucket_name = \"doc-example-bucket2\"\\ntarget_key = \"my-target-file.txt\"\\nsource_bucket = Aws::S3::Bucket.new(source_bucket_name)\\nwrapper = ObjectCopyWrapper.new(source_bucket.object(source_key))\\ntarget_bucket = Aws::S3::Bucket.new(target_bucket_name)\\ntarget_object = wrapper.copy_object(target_bucket, target_key)\\nreturn unless target_object\\nputs \"Copied #{source_key} from #{source_bucket_name} to\\n#{target_object.bucket_name}:#{target_object.key}.\"', '']]]\n",
      "[[['', 'end\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', 'PUT /jetsam HTTP/1.1\\nHost: example-s3-bucket2.s3.amazonaws.com\\nx-amz-copy-source: /example-s3-bucket1/flotsam\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:ENoSbxYByFA0UGLZUqJN5EUnLDg=\\nDate: Wed, 20 Feb 2008 22:12:21 +0000', ''], ['', '', '']], [['', '', ''], ['', 'PUT\\\\r\\\\n\\n\\\\r\\\\n\\n\\\\r\\\\n\\nWed, 20 Feb 2008 22:12:21 +0000\\\\r\\\\n\\nx-amz-copy-source:/example-s3-bucket1/flotsam\\\\r\\\\n\\n/example-s3-bucket2/jetsam', ''], ['', '', '']], [['', '', ''], ['', 'HTTP/1.1 200 OK\\nx-amz-id-2: Vyaxt7qEbzv34BnSu5hctyyNSlHTYZFMWK4FtzO+iX8JQNyaLdTshL0KxatbaOZt\\nx-amz-request-id: 6B13C3C5B34AF333\\nDate: Wed, 20 Feb 2008 22:13:01 +0000\\nContent-Type: application/xml\\nTransfer-Encoding: chunked\\nConnection: close\\nServer: AmazonS3', '']]]\n",
      "[[['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<CopyObjectResult>\\n<LastModified>2008-02-20T22:13:01</LastModified>\\n<ETag>\"7e9c608af58950deeb370c98608ed097\"</ETag>\\n</CopyObjectResult>', ''], ['', '', '']], [['', '', ''], ['', \"Note\\n• If you're moving an object that has user-defined tags, you must have the\\ns3:GetObjectTagging permission. If you're moving an object that doesn't have user-\\ndefined tags but is over 16 MB in size, you must also have the s3:GetObjectTagging\\npermission.\\nIf the destination bucket policy denies the s3:GetObjectTagging action, the object\\nwill be moved without the user-defined tags, and you will receive an error.\\n• Objects encrypted with customer-provided encryption keys (SSE-C) cannot be moved by\\nusing the Amazon S3 console. To move objects encrypted with SSE-C, use the AWS CLI,\\nAWS SDKs, or the Amazon S3 REST API.\\n• When moving folders, wait for the Move operation to finish before making additional\\nchanges in the folders.\\n• You can't use S3 access point aliases as the source or destination for Move operations in\\nthe Amazon S3 console.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• This action creates a copy of all specified objects with updated settings, updates the last-\\nmodified date in the specified location, and adds a delete marker to the original object.\\n• This action updates metadata for bucket versioning, encryption, Object Lock features,\\nand archived objects.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\n• Renaming an object creates a copy of the object with a new last-modified date, and then\\nadds a delete marker to the original object.\\n• Bucket settings for default encryption are automatically applied to any specified object\\nthat is unencrypted.\\n• You can't use the Amazon S3 console to rename objects with customer-provided\\nencryption keys (SSE-C) . To rename objects encrypted with SSE-C, use the AWS CLI, AWS\\nSDKs, or the Amazon S3 REST API to copy those objects with new names.\\n• If this bucket uses the bucket owner enforced setting for S3 Object Ownership, object\\naccess control lists (ACLs) won't be copied.\\n• If you're renaming an object that has user-defined tags, you must have the\\ns3:GetObjectTagging permission. If you're renaming an object that doesn't have user-\\ndefined tags but is over 16 MB in size, you must also have the s3:GetObjectTagging\\npermission.\\nIf the destination bucket policy denies the s3:GetObjectTagging action, the object\\nwill be renamed, but the user-defined tags will be removed from the object, and you will\\nreceive an error.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\n• You can download only one object at a time.\\n• If you use the Amazon S3 console to download an object whose key name ends with a\\nperiod (.), the period is removed from the key name of the downloaded object. To retain\\nthe period at the end of the name of the downloaded object, you must use the AWS\\nCommand Line Interface (AWS CLI), AWS SDKs, or Amazon S3 REST API.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-object --bucket example-s3-bucket1 --key folder/\\nmy_image my_downloaded_image', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWith AWS CloudShell, your home directory has storage up to 1GB per AWS Region.\\nTherefore you cannot sync buckets with objects totaling over this amount. For more\\nlimitations, see Service quotas and restrictions in the AWS CloudShell User Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3 sync s3://example-s3-bucket1 ./temp', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo perform pattern matching to either exclude or include particular objects, you can\\nuse the --exclude \"value\" and --include \"value\" parameters with the sync\\ncommand.', ''], ['', '', '']], [['', '', ''], ['', 'zip temp.zip -r temp/', ''], ['', '', '']], [['', '', ''], ['', 'rm temp.zip && rm -rf temp/', ''], ['', '', '']], [['', '', ''], ['', 'aws s3 cp s3://example-s3-bucket1 . --recursive', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3 cp s3://example-s3-bucket1/logs/ . --recursive --exclude \"*\" --include \"*.log\"', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object --bucket example-s3-bucket1 --key folder/my_data --range\\nbytes=0-500 my_data_range', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nAmazon S3 doesn't support retrieving multiple ranges of data in a single GET request.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\nIf you're using a multipart upload with additional checksums, the multipart part numbers\\nmust use consecutive part numbers. When using additional checksums, if you try to\", '']]]\n",
      "[[['', 'complete a multipart upload request with nonconsecutive part numbers, Amazon S3\\ngenerates an HTTP 500 Internal Server Error error.', ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.auth.credentials.AwsCredentials;\\nimport software.amazon.awssdk.auth.credentials.AwsCredentialsProvider;\\nimport software.amazon.awssdk.core.ResponseInputStream;\\nimport software.amazon.awssdk.core.sync.RequestBody;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.AbortMultipartUploadRequest;\\nimport software.amazon.awssdk.services.s3.model.ChecksumAlgorithm;\\nimport software.amazon.awssdk.services.s3.model.ChecksumMode;\\nimport software.amazon.awssdk.services.s3.model.CompleteMultipartUploadRequest;\\nimport software.amazon.awssdk.services.s3.model.CompleteMultipartUploadResponse;\\nimport software.amazon.awssdk.services.s3.model.CompletedMultipartUpload;\\nimport software.amazon.awssdk.services.s3.model.CompletedPart;\\nimport software.amazon.awssdk.services.s3.model.CreateMultipartUploadRequest;\\nimport software.amazon.awssdk.services.s3.model.CreateMultipartUploadResponse;\\nimport software.amazon.awssdk.services.s3.model.GetObjectAttributesRequest;\\nimport software.amazon.awssdk.services.s3.model.GetObjectAttributesResponse;', '']]]\n",
      "[[['', 'import software.amazon.awssdk.services.s3.model.GetObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.GetObjectResponse;\\nimport software.amazon.awssdk.services.s3.model.GetObjectTaggingRequest;\\nimport software.amazon.awssdk.services.s3.model.ObjectAttributes;\\nimport software.amazon.awssdk.services.s3.model.PutObjectTaggingRequest;\\nimport software.amazon.awssdk.services.s3.model.Tag;\\nimport software.amazon.awssdk.services.s3.model.Tagging;\\nimport software.amazon.awssdk.services.s3.model.UploadPartRequest;\\nimport software.amazon.awssdk.services.s3.model.UploadPartResponse;\\nimport java.io.File;\\nimport java.io.FileInputStream;\\nimport java.io.FileOutputStream;\\nimport java.io.IOException;\\nimport java.io.InputStream;\\nimport java.io.OutputStream;\\nimport java.nio.ByteBuffer;\\nimport java.security.MessageDigest;\\nimport java.security.NoSuchAlgorithmException;\\nimport java.util.ArrayList;\\nimport java.util.Base64;\\nimport java.util.List;\\npublic class LargeObjectValidation {\\nprivate static String FILE_NAME = \"sample.file\";\\nprivate static String BUCKET = \"sample-bucket\";\\n//Optional, if you want a method of storing the full multipart object\\nchecksum in S3.\\nprivate static String CHECKSUM_TAG_KEYNAME = \"fullObjectChecksum\";\\n//If you have existing full-object checksums that you need to validate\\nagainst, you can do the full object validation on a sequential upload.\\nprivate static String SHA256_FILE_BYTES = \"htCM5g7ZNdoSw8bN/\\nmkgiAhXt5MFoVowVg+LE9aIQmI=\";\\n//Example Chunk Size - this must be greater than or equal to 5MB.\\nprivate static int CHUNK_SIZE = 5 * 1024 * 1024;\\npublic static void main(String[] args) {\\nS3Client s3Client = S3Client.builder()\\n.region(Region.US_EAST_1)\\n.credentialsProvider(new AwsCredentialsProvider() {\\n@Override\\npublic AwsCredentials resolveCredentials() {\\nreturn new AwsCredentials() {\\n@Override', '']]]\n",
      "[[['', 'public String accessKeyId() {\\nreturn Constants.ACCESS_KEY;\\n}\\n@Override\\npublic String secretAccessKey() {\\nreturn Constants.SECRET;\\n}\\n};\\n}\\n})\\n.build();\\nuploadLargeFileBracketedByChecksum(s3Client);\\ndownloadLargeFileBracketedByChecksum(s3Client);\\nvalidateExistingFileAgainstS3Checksum(s3Client);\\n}\\npublic static void uploadLargeFileBracketedByChecksum(S3Client s3Client) {\\nSystem.out.println(\"Starting uploading file validation\");\\nFile file = new File(FILE_NAME);\\ntry (InputStream in = new FileInputStream(file)) {\\nMessageDigest sha256 = MessageDigest.getInstance(\"SHA-256\");\\nCreateMultipartUploadRequest createMultipartUploadRequest =\\nCreateMultipartUploadRequest.builder()\\n.bucket(BUCKET)\\n.key(FILE_NAME)\\n.checksumAlgorithm(ChecksumAlgorithm.SHA256)\\n.build();\\nCreateMultipartUploadResponse createdUpload =\\ns3Client.createMultipartUpload(createMultipartUploadRequest);\\nList<CompletedPart> completedParts = new ArrayList<CompletedPart>();\\nint partNumber = 1;\\nbyte[] buffer = new byte[CHUNK_SIZE];\\nint read = in.read(buffer);\\nwhile (read != -1) {\\nUploadPartRequest uploadPartRequest =\\nUploadPartRequest.builder()\\n.partNumber(partNumber).uploadId(createdUpload.uploadId()).key(FILE_NAME).bucket(BUC\\nUploadPartResponse uploadedPart =\\ns3Client.uploadPart(uploadPartRequest,\\nRequestBody.fromByteBuffer(ByteBuffer.wrap(buffer, 0, read)));\\nCompletedPart part =\\nCompletedPart.builder().partNumber(partNumber).checksumSHA256(uploadedPart.checksumS', 'K\\nH']]]\n",
      "[[['', 'completedParts.add(part);\\nsha256.update(buffer, 0, read);\\nread = in.read(buffer);\\npartNumber++;\\n}\\nString fullObjectChecksum =\\nBase64.getEncoder().encodeToString(sha256.digest());\\nif (!fullObjectChecksum.equals(SHA256_FILE_BYTES)) {\\n//Because the SHA256 is uploaded after the part is uploaded; the\\nupload is bracketed and the full object can be fully validated.\\ns3Client.abortMultipartUpload(AbortMultipartUploadRequest.builder().bucket(BUCKET).k\\nthrow new IOException(\"Byte mismatch between stored checksum and\\nupload, do not proceed with upload and cleanup\");\\n}\\nCompletedMultipartUpload completedMultipartUpload =\\nCompletedMultipartUpload.builder().parts(completedParts).build();\\nCompleteMultipartUploadResponse completedUploadResponse =\\ns3Client.completeMultipartUpload(\\nCompleteMultipartUploadRequest.builder().bucket(BUCKET).key(FILE_NAME).uploadId(crea\\nTag checksumTag =\\nTag.builder().key(CHECKSUM_TAG_KEYNAME).value(fullObjectChecksum).build();\\n//Optionally, if you need the full object checksum stored with the\\nfile; you could add it as a tag after completion.\\ns3Client.putObjectTagging(PutObjectTaggingRequest.builder().bucket(BUCKET).key(FILE_\\n} catch (IOException | NoSuchAlgorithmException e) {\\ne.printStackTrace();\\n}\\nGetObjectAttributesResponse\\nobjectAttributes =\\ns3Client.getObjectAttributes(GetObjectAttributesRequest.builder().bucket(BUCKET).key\\n.objectAttributes(ObjectAttributes.OBJECT_PARTS,\\nObjectAttributes.CHECKSUM).build());\\nSystem.out.println(objectAttributes.objectParts().parts());\\nSystem.out.println(objectAttributes.checksum().checksumSHA256());\\n}\\npublic static void downloadLargeFileBracketedByChecksum(S3Client s3Client) {\\nSystem.out.println(\"Starting downloading file validation\");\\nFile file = new File(\"DOWNLOADED_\" + FILE_NAME);\\ntry (OutputStream out = new FileOutputStream(file)) {\\nGetObjectAttributesResponse', 'e\\nt\\nN\\n(']]]\n",
      "[[['', 'objectAttributes =\\ns3Client.getObjectAttributes(GetObjectAttributesRequest.builder().bucket(BUCKET).key\\n.objectAttributes(ObjectAttributes.OBJECT_PARTS,\\nObjectAttributes.CHECKSUM).build());\\n//Optionally if you need the full object checksum, you can grab a\\ntag you added on the upload\\nList<Tag> objectTags =\\ns3Client.getObjectTagging(GetObjectTaggingRequest.builder().bucket(BUCKET).key(FILE_\\nString fullObjectChecksum = null;\\nfor (Tag objectTag : objectTags) {\\nif (objectTag.key().equals(CHECKSUM_TAG_KEYNAME)) {\\nfullObjectChecksum = objectTag.value();\\nbreak;\\n}\\n}\\nMessageDigest sha256FullObject =\\nMessageDigest.getInstance(\"SHA-256\");\\nMessageDigest sha256ChecksumOfChecksums =\\nMessageDigest.getInstance(\"SHA-256\");\\n//If you retrieve the object in parts, and set the ChecksumMode to\\nenabled, the SDK will automatically validate the part checksum\\nfor (int partNumber = 1; partNumber <=\\nobjectAttributes.objectParts().totalPartsCount(); partNumber++) {\\nMessageDigest sha256Part = MessageDigest.getInstance(\"SHA-256\");\\nResponseInputStream<GetObjectResponse> response =\\ns3Client.getObject(GetObjectRequest.builder().bucket(BUCKET).key(FILE_NAME).partNumb\\nGetObjectResponse getObjectResponse = response.response();\\nbyte[] buffer = new byte[CHUNK_SIZE];\\nint read = response.read(buffer);\\nwhile (read != -1) {\\nout.write(buffer, 0, read);\\nsha256FullObject.update(buffer, 0, read);\\nsha256Part.update(buffer, 0, read);\\nread = response.read(buffer);\\n}\\nbyte[] sha256PartBytes = sha256Part.digest();\\nsha256ChecksumOfChecksums.update(sha256PartBytes);\\n//Optionally, you can do an additional manual validation again\\nthe part checksum if needed in addition to the SDK check\\nString base64PartChecksum =\\nBase64.getEncoder().encodeToString(sha256PartBytes);\\nString base64PartChecksumFromObjectAttributes =\\nobjectAttributes.objectParts().parts().get(partNumber - 1).checksumSHA256();', '(\\nN\\ne']]]\n",
      "[[['', 'if (!\\nbase64PartChecksum.equals(getObjectResponse.checksumSHA256()) || !\\nbase64PartChecksum.equals(base64PartChecksumFromObjectAttributes)) {\\nthrow new IOException(\"Part checksum didn\\'t match for the\\npart\");\\n}\\nSystem.out.println(partNumber + \" \" + base64PartChecksum);\\n}\\n//Before finalizing, do the final checksum validation.\\nString base64FullObject =\\nBase64.getEncoder().encodeToString(sha256FullObject.digest());\\nString base64ChecksumOfChecksums =\\nBase64.getEncoder().encodeToString(sha256ChecksumOfChecksums.digest());\\nif (fullObjectChecksum != null && !\\nfullObjectChecksum.equals(base64FullObject)) {\\nthrow new IOException(\"Failed checksum validation for full\\nobject\");\\n}\\nSystem.out.println(fullObjectChecksum);\\nString base64ChecksumOfChecksumFromAttributes =\\nobjectAttributes.checksum().checksumSHA256();\\nif (base64ChecksumOfChecksumFromAttributes != null && !\\nbase64ChecksumOfChecksums.equals(base64ChecksumOfChecksumFromAttributes)) {\\nthrow new IOException(\"Failed checksum validation for full\\nobject checksum of checksums\");\\n}\\nSystem.out.println(base64ChecksumOfChecksumFromAttributes);\\nout.flush();\\n} catch (IOException | NoSuchAlgorithmException e) {\\n//Cleanup bad file\\nfile.delete();\\ne.printStackTrace();\\n}\\n}\\npublic static void validateExistingFileAgainstS3Checksum(S3Client s3Client)\\n{\\nSystem.out.println(\"Starting existing file validation\");\\nFile file = new File(\"DOWNLOADED_\" + FILE_NAME);\\nGetObjectAttributesResponse\\nobjectAttributes =\\ns3Client.getObjectAttributes(GetObjectAttributesRequest.builder().bucket(BUCKET).key\\n.objectAttributes(ObjectAttributes.OBJECT_PARTS,\\nObjectAttributes.CHECKSUM).build());', '(']]]\n",
      "[[['', 'try (InputStream in = new FileInputStream(file)) {\\nMessageDigest sha256ChecksumOfChecksums =\\nMessageDigest.getInstance(\"SHA-256\");\\nMessageDigest sha256Part = MessageDigest.getInstance(\"SHA-256\");\\nbyte[] buffer = new byte[CHUNK_SIZE];\\nint currentPart = 0;\\nint partBreak =\\nobjectAttributes.objectParts().parts().get(currentPart).size();\\nint totalRead = 0;\\nint read = in.read(buffer);\\nwhile (read != -1) {\\ntotalRead += read;\\nif (totalRead >= partBreak) {\\nint difference = totalRead - partBreak;\\nbyte[] partChecksum;\\nif (totalRead != partBreak) {\\nsha256Part.update(buffer, 0, read - difference);\\npartChecksum = sha256Part.digest();\\nsha256ChecksumOfChecksums.update(partChecksum);\\nsha256Part.reset();\\nsha256Part.update(buffer, read - difference,\\ndifference);\\n} else {\\nsha256Part.update(buffer, 0, read);\\npartChecksum = sha256Part.digest();\\nsha256ChecksumOfChecksums.update(partChecksum);\\nsha256Part.reset();\\n}\\nString base64PartChecksum =\\nBase64.getEncoder().encodeToString(partChecksum);\\nif (!\\nbase64PartChecksum.equals(objectAttributes.objectParts().parts().get(currentPart).che\\n{\\nthrow new IOException(\"Part checksum didn\\'t match S3\");\\n}\\ncurrentPart++;\\nSystem.out.println(currentPart + \" \" + base64PartChecksum);\\nif (currentPart <\\nobjectAttributes.objectParts().totalPartsCount()) {\\npartBreak +=\\nobjectAttributes.objectParts().parts().get(currentPart - 1).size();\\n}\\n} else {\\nsha256Part.update(buffer, 0, read);', 'c']]]\n",
      "[[['', '}\\nread = in.read(buffer);\\n}\\nif (currentPart != objectAttributes.objectParts().totalPartsCount())\\n{\\ncurrentPart++;\\nbyte[] partChecksum = sha256Part.digest();\\nsha256ChecksumOfChecksums.update(partChecksum);\\nString base64PartChecksum =\\nBase64.getEncoder().encodeToString(partChecksum);\\nSystem.out.println(currentPart + \" \" + base64PartChecksum);\\n}\\nString base64CalculatedChecksumOfChecksums =\\nBase64.getEncoder().encodeToString(sha256ChecksumOfChecksums.digest());\\nSystem.out.println(base64CalculatedChecksumOfChecksums);\\nSystem.out.println(objectAttributes.checksum().checksumSHA256());\\nif (!\\nbase64CalculatedChecksumOfChecksums.equals(objectAttributes.checksum().checksumSHA256\\n{\\nthrow new IOException(\"Full object checksum of checksums don\\'t\\nmatch S3\");\\n}\\n} catch (IOException | NoSuchAlgorithmException e) {\\ne.printStackTrace();\\n}\\n}\\n}', '('], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\nIf you're using S3 Object Lambda, all requests to S3 Object Lambda are signed using s3-\\nobject-lambda instead of s3. This behavior affects the signature of trailing checksum\\nvalues. For more information about S3 Object Lambda, see Transforming objects with S3\\nObject Lambda.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nObjects that are larger than the size limitations of the CopyObject API operation must use\\nmultipart copy commands.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you perform some operations using the AWS Management Console, Amazon S3 uses\\na multipart upload if the object is greater than 16 MB in size. In this case, the checksum is\\nnot a direct checksum of the full object, but rather a calculation based on the checksum\\nvalues of each individual part.\\nFor example, consider an object 100 MB in size that you uploaded as a single-part direct\\nupload using the REST API. The checksum in this case is a checksum of the entire object. If\\nyou later use the console to rename that object, copy it, change the storage class, or edit\\nthe metadata, Amazon S3 uses the multipart upload functionality to update the object. As', '']]]\n",
      "[[['', 'a result, Amazon S3 creates a new checksum value for the object that is calculated based\\non the checksum values of the individual parts.\\nThe preceding list of console operations is not a complete list of all the possible actions\\nthat you can take in the AWS Management Console that result in Amazon S3 updating\\nthe object using the multipart upload functionality. Keep in mind that whenever you\\nuse the console to act on objects over 16 MB in size, the checksum value might not be the\\nchecksum of the entire object.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Warning\\nWhen you permanently delete an object or specified object version in the Amazon S3\\nconsole, the deletion can't be undone.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf the version ID for an object in a versioning-suspended bucket is marked as NULL, S3\\npermanently deletes the object since no previous versions exist. However, if a valid version\\nID is listed for the object in a versioning-suspended bucket, then S3 creates a delete marker\\nfor the deleted object, while retaining the previous versions of the object.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Warning\\nWhen you permanently delete a specific object version in Amazon S3, the deletion can't be\\nundone.\", ''], ['', '', '']], [['', '', ''], ['', \"Warning\\nWhen you permanently delete an object in Amazon S3, the deletion can't be undone. Also,\\nfor any buckets without versioning enabled, deletions are permanent.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf you're experiencing any issues with deleting your object, see I want to permanently\\ndelete versioned objects.\", ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;', '']]]\n",
      "[[['', 'import com.amazonaws.services.s3.model.DeleteObjectRequest;\\nimport java.io.IOException;\\npublic class DeleteObjectNonVersionedBucket {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString keyName = \"*** Key name ****\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\ns3Client.deleteObject(new DeleteObjectRequest(bucketName, keyName));\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou can get the version IDs of an object by sending a ListVersions request.', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.BucketVersioningConfiguration;\\nimport com.amazonaws.services.s3.model.DeleteVersionRequest;\\nimport com.amazonaws.services.s3.model.PutObjectResult;\\nimport java.io.IOException;\\npublic class DeleteObjectVersionEnabledBucket {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString keyName = \"*** Key name ****\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\n// Check to ensure that the bucket is versioning-enabled.\\nString bucketVersionStatus =\\ns3Client.getBucketVersioningConfiguration(bucketName).getStatus();\\nif (!bucketVersionStatus.equals(BucketVersioningConfiguration.ENABLED))\\n{\\nSystem.out.printf(\"Bucket %s is not versioning-enabled.\",\\nbucketName);\\n} else {\\n// Add an object.\\nPutObjectResult putResult = s3Client.putObject(bucketName, keyName,', '']]]\n",
      "[[['', '\"Sample content for deletion example.\");\\nSystem.out.printf(\"Object %s added to bucket %s\\\\n\", keyName,\\nbucketName);\\n// Delete the version of the object that we just created.\\nSystem.out.println(\"Deleting versioned object \" + keyName);\\ns3Client.deleteVersion(new DeleteVersionRequest(bucketName, keyName,\\nputResult.getVersionId()));\\nSystem.out.printf(\"Object %s, version %s deleted\\\\n\", keyName,\\nputResult.getVersionId());\\n}\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;', '']]]\n",
      "[[['', 'namespace Amazon.DocSamples.S3\\n{\\nclass DeleteObjectNonVersionedBucketTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\nprivate const string keyName = \"*** object key ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nclient = new AmazonS3Client(bucketRegion);\\nDeleteObjectNonVersionedBucketAsync().Wait();\\n}\\nprivate static async Task DeleteObjectNonVersionedBucketAsync()\\n{\\ntry\\n{\\nvar deleteObjectRequest = new DeleteObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName\\n};\\nConsole.WriteLine(\"Deleting an object\");\\nawait client.DeleteObjectAsync(deleteObjectRequest);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\ndeleting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\ndeleting an object\", e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou can also get the version ID of an object by sending a ListVersions request.\\nvar listResponse = client.ListVersions(new ListVersionsRequest { BucketName\\n= bucketName, Prefix = keyName });', ''], ['', '', '']], [['', '', ''], ['', 'var listResponse = client.ListVersions(new ListVersionsRequest { BucketName\\n= bucketName, Prefix = keyName });', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass DeleteObjectVersion\\n{\\nprivate const string bucketName = \"*** versioning-enabled bucket name ***\";\\nprivate const string keyName = \"*** Object Key Name ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;', '']]]\n",
      "[[['', 'public static void Main()\\n{\\nclient = new AmazonS3Client(bucketRegion);\\nCreateAndDeleteObjectVersionAsync().Wait();\\n}\\nprivate static async Task CreateAndDeleteObjectVersionAsync()\\n{\\ntry\\n{\\n// Add a sample object.\\nstring versionID = await PutAnObject(keyName);\\n// Delete the object by specifying an object key and a version ID.\\nDeleteObjectRequest request = new DeleteObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nVersionId = versionID\\n};\\nConsole.WriteLine(\"Deleting an object\");\\nawait client.DeleteObjectAsync(request);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\ndeleting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\ndeleting an object\", e.Message);\\n}\\n}\\nstatic async Task<string> PutAnObject(string objectKey)\\n{\\nPutObjectRequest request = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = objectKey,\\nContentBody = \"This is the content body!\"\\n};', '']]]\n",
      "[[['', 'PutObjectResponse response = await client.PutObjectAsync(request);\\nreturn response.VersionId;\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"<?php\\nrequire 'vendor/autoload.php';\\nuse Aws\\\\S3\\\\S3Client;\\nuse Aws\\\\S3\\\\Exception\\\\S3Exception;\\n$bucket = '*** Your Bucket Name ***';\\n$keyname = '*** Your Object Key ***';\\n$s3 = new S3Client([\\n'version' => 'latest',\\n'region' => 'us-east-1'\\n]);\\n// 1. Delete the object from the bucket.\\ntry\\n{\\necho 'Attempting to delete ' . $keyname . '...' . PHP_EOL;\\n$result = $s3->deleteObject([\\n'Bucket' => $bucket,\\n'Key' => $keyname\\n]);\", '']]]\n",
      "[[['', \"if ($result['DeleteMarker'])\\n{\\necho $keyname . ' was deleted or does not exist.' . PHP_EOL;\\n} else {\\nexit('Error: ' . $keyname . ' was not deleted.' . PHP_EOL);\\n}\\n}\\ncatch (S3Exception $e) {\\nexit('Error: ' . $e->getAwsErrorMessage() . PHP_EOL);\\n}\\n// 2. Check to see if the object was deleted.\\ntry\\n{\\necho 'Checking to see if ' . $keyname . ' still exists...' . PHP_EOL;\\n$result = $s3->getObject([\\n'Bucket' => $bucket,\\n'Key' => $keyname\\n]);\\necho 'Error: ' . $keyname . ' still exists.';\\n}\\ncatch (S3Exception $e) {\\nexit($e->getAwsErrorMessage());\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'import { DeleteObjectCommand } from \"@aws-sdk/client-s3\";\\nimport { s3Client } from \"./libs/s3Client.js\" // Helper function that creates Amazon\\nS3 service client module.\\nexport const bucketParams = { Bucket: \"BUCKET_NAME\", Key: \"KEY\" };\\nexport const run = async () => {\\ntry {\\nconst data = await s3Client.send(new DeleteObjectCommand(bucketParams));\\nconsole.log(\"Success. Object deleted.\", data);\\nreturn data; // For unit tests.\\n} catch (err) {\\nconsole.log(\"Error\", err);\\n}', '']]]\n",
      "[[['', '};\\nrun();', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\n• Deleting a specified object cannot be undone.\\n• This action deletes all specified objects. When deleting folders, wait for the delete action\\nto finish before adding new objects to the folder. Otherwise, new objects might be\\ndeleted as well.\\n• When deleting objects in a bucket without versioning enabled, Amazon S3 will\\npermanently delete the objects.', '']]]\n",
      "[[['', '• When deleting objects in a bucket with bucket versioning enabled or suspended,\\nAmazon S3 creates delete markers. For more information, see Working with delete\\nmarkers.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf the version IDs for the object in a versioning-suspended bucket are marked as NULL, S3\\npermanently deletes the objects since no previous versions exist. However, if a valid version\\nID is listed for the objects in a versioning-suspended bucket, then S3 creates delete markers\\nfor the deleted objects, while retaining the previous versions of the objects.', ''], ['', '', '']], [['', '', ''], ['', \"Warning\\nWhen you permanently delete specific object versions in Amazon S3, the deletion can't be\\nundone.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Warning\\nWhen you permanently delete an object in Amazon S3, the deletion can't be undone. Also,\\nfor any buckets without versioning enabled, deletions are permanent.\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf you're experiencing any issues with deleting your objects, see I want to permanently\\ndelete versioned objects.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', '<ListBucketResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Name>DOC-EXAMPLE-BUCKET</Name>\\n<Prefix></Prefix>\\n<Marker></Marker>\\n<MaxKeys>1000</MaxKeys>\\n<Delimiter>/</Delimiter>\\n<IsTruncated>false</IsTruncated>\\n<Contents>\\n<Key>sample.jpg</Key>\\n<LastModified>2011-07-24T19:39:30.000Z</LastModified>\\n<ETag>&quot;d1a7fb5eab1c16cb4f7cf341cf188c3d&quot;</ETag>\\n<Size>6</Size>\\n<Owner>\\n<ID>75cc57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a</ID>\\n<DisplayName>displayname</DisplayName>\\n</Owner>\\n<StorageClass>STANDARD</StorageClass>\\n</Contents>\\n<CommonPrefixes>\\n<Prefix>photos/</Prefix>\\n</CommonPrefixes>\\n</ListBucketResult>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"aws s3api list-objects --bucket text-content --query 'Contents[].{Key: Key, Size:\\nSize}'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Get-S3Object -BucketName test-files', ''], ['', '', '']], [['', '', ''], ['', 'Get-S3Object -BucketName test-files -Key sample.txt', ''], ['', '', '']], [['', '', ''], ['', 'Get-S3Object -BucketName test-files -KeyPrefix sample', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nWhen you create a folder in Amazon S3, S3 creates a 0-byte object with a key that's set to\\nthe folder name that you provided. For example, if you create a folder named photos in\\nyour bucket, the Amazon S3 console creates a 0-byte object with the key photos/. The\\nconsole creates this object to support the idea of folders.\\nThe Amazon S3 console treats all objects that have a forward slash (/) character as the last\\n(trailing) character in the key name as a folder (for example, examplekeyname/). You can't\\nupload an object that has a key name with a trailing / character by using the Amazon S3\\nconsole. However, you can upload objects that are named with a trailing / with the Amazon\\nS3 API by using the AWS Command Line Interface (AWS CLI), AWS SDKs, or REST API.\\nAn object that is named with a trailing / appears as a folder in the Amazon S3 console. The\\nAmazon S3 console does not display the content and metadata for such an object. When\\nyou use the console to copy an object named with a trailing /, a new folder is created in the\\ndestination location, but the object's data and metadata are not copied.\", ''], ['', '', '']], [['', '', ''], ['', \"Important\\nIf your bucket policy prevents uploading objects to this bucket without tags, metadata,\\nor access control list (ACL) grantees, you can't create a folder by using the following\", '']]]\n",
      "[[['', 'procedure. Instead, upload an empty folder and specify the following settings in the upload\\nconfiguration.', ''], ['', '', '']], [['', '', ''], ['', \"Warning\\nAfter you make a folder public in the Amazon S3 console, you can't make it private again.\\nInstead, you must set permissions on each individual object in the public folder so that the\\nobjects have no public access. For more information, see Configuring ACLs.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen you navigate away from the page, the folder information (including the total size)\\nwill no longer be available. You must calculate the total size again if you want to see it\\nagain.', ''], ['', '', '']], [['', '', ''], ['', \"Important\\n• When you use the Calculate total size action on specified objects or folders within\\nyour bucket, Amazon S3 calculates the total number of objects and the total storage\\nsize. However, incomplete or in-progress multipart uploads and previous or noncurrent\\nversions aren't calculated in the total number of objects or the total size. This action\\ncalculates only the total number of objects and the total size for the current or newest\\nversion of each object that is stored in the bucket.\\nFor example, if there are two versions of an object in your bucket, then the storage\\ncalculator in Amazon S3 counts them as only one object. As a result, the total number\\nof objects that is calculated in the Amazon S3 console can differ from the Object\\nCount metric shown in S3 Storage Lens and from the number reported by the\\nAmazon CloudWatch metric, NumberOfObjects. Likewise, the total storage size can\\nalso differ from the Total Storage metric shown in S3 Storage Lens and from the\\nBucketSizeBytes metric shown in CloudWatch.\", '']]]\n",
      "[[['', '• If the time to calculate the total size of a large folder is taking too long, consider using\\nAmazon S3 Inventory and Amazon S3 Select as an alternative. First, create an S3\\nInventory configuration to include the Size metadata for each object of the large folder\\nin an inventory report. It might take up to 48 hours to deliver the first S3 Inventory\\nreport. When the inventory report is published, query the inventory report with an\\nS3 Select SUM expression to aggregate the sizes of the objects in the folder. For more\\ninformation, see Configuring inventory by using the S3 console and SUM example.', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nThis action deletes all specified objects. When deleting folders, wait for the delete action to\\nfinish before adding new objects to the folder. Otherwise, new objects might be deleted as\\nwell.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nYou can undelete an object only if it was deleted as the latest (current) version. You\\ncan't undelete a previous version of an object that was deleted.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\n• If you change the Storage Class, Encryption, or Metadata properties, a new object\\nis created to replace the old one. If S3 Versioning is enabled, a new version of the\\nobject is created, and the existing object becomes an older version. The role that\\nchanges the property also becomes the owner of the new object or (object version).\\n• If you change the Storage Class, Encryption, or Metadata properties for an object\\nthat has user-defined tags, you must have the s3:GetObjectTagging permission.\\nIf you're changing these properties for an object that doesn't have user-defined\\ntags but is over 16 MB in size, you must also have the s3:GetObjectTagging\\npermission.\\nIf the destination bucket policy denies the s3:GetObjectTagging action, these\\nproperties for the object will be updated, but the user-defined tags will be removed\\nfrom the object, and you will receive an error.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf you created a presigned URL using a temporary credential, the URL expires when the\\ncredential expires. In general, a presigned URL expires when the credential you used to\\ncreate it is revoked, deleted, or deactivated. This is true even if the URL was created with a\\nlater expiration time. For temporary security credentials lifetimes, see Comparing AWS STS\\nAPI operations in the IAM User Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Deny a presigned URL request if the signature is more than 10 min\\nold\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {\"AWS\":\"*\"},\\n\"Action\": \"s3:*\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket1/*\",\\n\"Condition\": {\\n\"NumericGreaterThan\": {', '']]]\n",
      "[[['', '\"s3:signatureAge\": 600000\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Sid\": \"NetworkRestrictionForIAMPrincipal\",\\n\"Effect\": \"Deny\",\\n\"Action\": \"*\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"NotIpAddressIfExists\": {\"aws:SourceIp\": \"IP-address-range\"},\\n\"BoolIfExists\": {\"aws:ViaAWSService\": \"false\"}\\n}\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3 presign s3://example-s3-bucket1/mydoc.txt --expires-in 604800', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor all AWS Regions launched after March 20, 2019 you need to specify the endpoint-\\nurl and AWS Region with the request. For a list of all the Amazon S3 Regions and\\nendpoints, see Regions and Endpoints in the AWS General Reference.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3 presign s3://example-s3-bucket1/mydoc.txt --expires-in 604800 --region af-\\nsouth-1 --endpoint-url https://s3.af-south-1.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor all AWS Regions launched after March 20, 2019 you need to specify the endpoint-\\nurl and AWS Region with the request. For a list of all the Amazon S3 Regions and\\nendpoints, see Regions and Endpoints in the AWS General Reference.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen using the AWS SDKs, the Tagging attribute must be a header and not a query\\nparameter. All other attributes can be passed as a parameter for the presigned URL.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nAt this time, the AWS Toolkit for Visual Studio does not support Visual Studio for Mac.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAt this time, the AWS Toolkit for Visual Studio does not support Visual Studio for Mac.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'curl -X PUT -T \"/path/to/file\" \"presigned URL\"', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor all AWS Regions launched after March 20, 2019 you need to specify the endpoint-\\nurl and AWS Region with the request. For a list of all the Amazon S3 Regions and\\nendpoints, see Regions and Endpoints in the AWS General Reference.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\nWhen you're using Object Lambda Access Points, make sure that the payload does not\\ncontain any confidential information.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo use the following commands, replace the user input placeholders with your own\\ninformation.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket Amazon S3 bucket name --key\\ns3objectlambda_deployment_package.zip --body release/\\ns3objectlambda_deployment_package.zip', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws cloudformation deploy --template-file s3objectlambda_defaultconfig.yaml \\\\\\n--stack-name AWS CloudFormation stack name \\\\\\n--parameter-overrides ObjectLambdaAccessPointName=Object Lambda Access Point name\\n\\\\\\nSupportingAccessPointName=Amazon S3 access point S3BucketName=Amazon S3 bucket \\\\\\nLambdaFunctionS3BucketName=Amazon S3 bucket containing your Lambda package \\\\\\nLambdaFunctionS3Key=Lambda object key LambdaFunctionS3ObjectVersion=Lambda object\\nversion \\\\\\nLambdaFunctionRuntime=Lambda function runtime --capabilities capability_IAM', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo use the following commands, replace the user input placeholders with your own\\ninformation.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"SupportingAccessPoint\" : \"arn:aws:s3:us-\\neast-1:111122223333:accesspoint/example-ap\",\\n\"TransformationConfigurations\": [{\\n\"Actions\" : [\"GetObject\", \"HeadObject\", \"ListObjects\", \"ListObjectsV2\"],\\n\"ContentTransformation\" : {\\n\"AwsLambda\": {\\n\"FunctionPayload\" : \"{\\\\\"compressionType\\\\\":\\\\\"gzip\\\\\"}\",\\n\"FunctionArn\" : \"arn:aws:lambda:us-east-1:111122223333:function/\\ncompress\"\\n}\\n}\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-access-point-for-object-lambda --account-id 111122223333 --\\nname my-object-lambda-ap --configuration file://my-olap-configuration.json', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2008-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Grant account 444455556666 GetObject access\",\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3-object-lambda:GetObject\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::444455556666:root\"\\n},\\n\"Resource\": \"your-object-lambda-access-point-arn\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-access-point-policy-for-object-lambda --account-id 111122223333\\n--name my-object-lambda-ap --policy file://my-olap-policy.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"SupportingAccessPoint\": \"AccessPointArn\",', '']]]\n",
      "[[['', '\"CloudWatchMetricsEnabled\": false,\\n\"TransformationConfigurations\": [{\\n\"Actions\": [\"GetObject\", \"HeadObject\", \"ListObjects\", \"ListObjectsV2\"],\\n\"ContentTransformation\": {\\n\"AwsLambda\": {\\n\"FunctionArn\": \"FunctionArn\",\\n\"FunctionPayload\": \"{\\\\\"res-x\\\\\": \\\\\"100\\\\\",\\\\\"res-y\\\\\": \\\\\"100\\\\\"}\"\\n}\\n}\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"SupportingAccessPoint\":\"AccessPointArn\",\\n\"CloudWatchMetricsEnabled\": false,\\n\"AllowedFeatures\": [\"GetObject-Range\", \"GetObject-PartNumber\", \"HeadObject-\\nRange\", \"HeadObject-PartNumber\"],\\n\"TransformationConfigurations\": [{\\n\"Action\": [\"GetObject\", \"HeadObject\", \"ListObjects\", \"ListObjectsV2\"],\\n\"ContentTransformation\": {\\n\"AwsLambda\": {\\n\"FunctionArn\":\"FunctionArn\",\\n\"FunctionPayload\": \"{\\\\\"compression-amount\\\\\": \\\\\"5\\\\\"}\"\\n}\\n}\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Important\\nWhen you're using Object Lambda Access Points, make sure that the payload does not\\ncontain any confidential information.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\nWhen you're using Object Lambda Access Points, make sure that the payload does\\nnot contain any confidential information.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThis is an access point that is associated with the Amazon S3 bucket that you\\nchose in the previous step. If you do not have any access points associated with\\nyour Amazon S3 bucket, you can configure the template to create one for you by\\nchoosing true for CreateNewSupportingAccessPoint.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'CreateNewSupportingAccessPoint=true', ''], ['', '', '']], [['', '', ''], ['', 'LambdaFunctionPayload=\"format=json\"', ''], ['', '', '']], [['', '', ''], ['', 'EnableCloudWatchMonitoring=true', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nAmazon CloudWatch usage will incur additional costs. For more information about Amazon\\nS3 request metrics, see Monitoring and logging access points.\\nFor pricing details, see CloudWatch pricing.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'LambdaFunctionVersion:\\nType: AWS::Lambda::Version\\nProperties:\\nFunctionName: !Ref LambdaFunction\\nProvisionedConcurrencyConfig:\\nProvisionedConcurrentExecutions: Integer', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou will incur additional charges for provisioning concurrency. For more information about\\nprovisioned concurrency, see Managing Lambda provisioned concurrency in the AWS\\nLambda Developer Guide.\\nFor pricing details, see AWS Lambda pricing.', ''], ['', '', '']], [['', '', ''], ['', 'async function writeResponse (s3Client: S3, requestContext: GetObjectContext,\\ntransformedObject: Buffer,\\nheaders: Headers): Promise<PromiseResult<{}, AWSError>> {\\nconst { algorithm, digest } = getChecksum(transformedObject);\\nreturn s3Client.writeGetObjectResponse({', '']]]\n",
      "[[['', \"RequestRoute: requestContext.outputRoute,\\nRequestToken: requestContext.outputToken,\\nBody: transformedObject,\\nMetadata: {\\n'body-checksum-algorithm': algorithm,\\n'body-checksum-digest': digest\\n},\\n...headers,\\nContentLanguage: 'my-new-language'\\n}).promise();\\n}\", ''], ['', '', '']], [['', '', ''], ['', \"async function writeResponse (s3Client: S3, requestContext: GetObjectContext,\\ntransformedObject: Buffer,\\nheaders: Headers): Promise<PromiseResult<{}, AWSError>> {\\nconst { algorithm, digest } = getChecksum(transformedObject);\\nreturn s3Client.writeGetObjectResponse({\\nRequestRoute: requestContext.outputRoute,\\nRequestToken: requestContext.outputToken,\\nBody: transformedObject,\\nMetadata: {\\n'body-checksum-algorithm': algorithm,\\n'body-checksum-digest': digest,\\n'my-new-header': 'my-new-value'\\n},\\n...headers\\n}).promise();\\n}\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"async function writeResponse (s3Client: S3, requestContext: GetObjectContext,\\ntransformedObject: Buffer,\\nheaders: Headers): Promise<PromiseResult<{}, AWSError>> {\\nconst { algorithm, digest } = getChecksum(transformedObject);\\nreturn s3Client.writeGetObjectResponse({\\nRequestRoute: requestContext.outputRoute,\\nRequestToken: requestContext.outputToken,\\nBody: transformedObject,\\nMetadata: {\\n'body-checksum-algorithm': algorithm,\\n'body-checksum-digest': digest\\n},\\n...headers,\\nStatusCode: Integer\\n}).promise();\\n}\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'AllowedFeatures:\\n- GetObject-Range\\n- GetObject-PartNumber\\n- HeadObject-Range\\n- HeadObject-PartNumber', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nThe Amazon Resource Names (ARNs) for Object Lambda Access Points use a service\\nname of s3-object-lambda. Thus, Object Lambda Access Point ARNs begin with\\narn:aws::s3-object-lambda, instead of arn:aws::s3, which is used with other access\\npoints.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control list-access-points-for-object-lambda --account-id 111122223333', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"ObjectLambdaAccessPointList\": [\\n{\\n\"Name\": \"my-object-lambda-ap\",\\n\"ObjectLambdaAccessPointArn\": \"arn:aws:s3-object-lambda:us-\\neast-1:111122223333:accesspoint/my-object-lambda-ap\"\\n},\\n...\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThe --ol-s3 suffix is reserved for Object Lambda Access Point alias names and can't\\nbe used for bucket or Object Lambda Access Point names. For more information about\\nAmazon S3 bucket-naming rules, see Bucket naming rules.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-access-point-for-object-lambda --account-id 111122223333 --\\nname my-object-lambda-access-point --configuration file://my-olap-configuration.json\\n{\\n\"ObjectLambdaAccessPointArn\": \"arn:aws:s3:region:111122223333:accesspoint/my-\\naccess-point\",\\n\"Alias\": {\\n\"Value\": \"my-object-lambda-acc-1a4n8yjrb3kda96f67zwrwiiuse1a--ol-s3\",\\n\"Status\": \"READY\"\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-location --bucket my-object-lambda-\\nacc-w7i37nq6xuzgax3jw3oqtifiusw2a--ol-s3\\n{\\n\"LocationConstraint\": \"us-west-2\"\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nBefore you save your policy, make sure to resolve security warnings, errors, general\\nwarnings, and suggestions from AWS Identity and Access Management Access Analyzer.\\nIAM Access Analyzer runs policy checks to validate your policy against IAM policy\\ngrammar and best practices. These checks generate findings and provide actionable\\nrecommendations to help you author policies that are functional and conform to security\\nbest practices.\\nTo learn more about validating policies by using IAM Access Analyzer, see IAM Access\\nAnalyzer policy validation in the IAM User Guide. To view a list of the warnings, errors, and\\nsuggestions that are returned by IAM Access Analyzer, see IAM Access Analyzer policy check\\nreference.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf you're using a Lambda function from your account, you must include the specific function\\nversion in your policy statement. In the following example ARN, the version is indicated by\\n1:\", '']]]\n",
      "[[['', \"arn:aws:lambda:us-\\neast-1:111122223333:function:MyObjectLambdaFunction:1\\nLambda doesn't support adding IAM policies to the version $LATEST. For more information\\nabout Lambda function versions, see Lambda function versions in the AWS Lambda\\nDeveloper Guide.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\" : [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\" : { \"AWS\":\"account-ARN\"},\\n\"Action\" : \"*\",\\n\"Resource\" : [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1\",\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*\"\\n],\\n\"Condition\": {\\n\"StringEquals\" : { \"s3:DataAccessPointAccount\" : \"Bucket owner\\'s account\\nID\" }\\n}\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowLambdaInvocation\",\\n\"Action\": [\\n\"lambda:InvokeFunction\"\\n],\\n\"Effect\": \"Allow\",\\n\"Resource\": \"arn:aws:lambda:us-\\neast-1:111122223333:function:MyObjectLambdaFunction:1\",\\n\"Condition\": {\\n\"ForAnyValue:StringEquals\": {\\n\"aws:CalledVia\": [\\n\"s3-object-lambda.amazonaws.com\"\\n]\\n}\\n}\\n},\\n{\\n\"Sid\": \"AllowStandardAccessPointAccess\",\\n\"Action\": [\\n\"s3:Get*\",\\n\"s3:List*\"\\n],\\n\"Effect\": \"Allow\",\\n\"Resource\": \"arn:aws:s3:us-east-1:111122223333:accesspoint/my-access-point/*\",\\n\"Condition\": {\\n\"ForAnyValue:StringEquals\": {\\n\"aws:CalledVia\": [\\n\"s3-object-lambda.amazonaws.com\"\\n]\\n}\\n}\\n},\\n{\\n\"Sid\": \"AllowObjectLambdaAccess\",\\n\"Action\": [\\n\"s3-object-lambda:Get*\",\\n\"s3-object-lambda:List*\"\\n],\\n\"Effect\": \"Allow\",\\n\"Resource\": \"arn:aws:s3-object-lambda:us-east-1:111122223333:accesspoint/my-\\nobject-lambda-ap\"\\n}', '']]]\n",
      "[[['', ']\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nYour function needs the s3-object-lambda:WriteGetObjectResponse permission\\nonly if you're making a GET request.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'package com.amazon.s3.objectlambda;\\nimport com.amazonaws.services.lambda.runtime.Context;\\nimport com.amazonaws.services.lambda.runtime.events.S3ObjectLambdaEvent;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3Client;\\nimport com.amazonaws.services.s3.model.WriteGetObjectResponseRequest;\\nimport java.io.ByteArrayInputStream;\\nimport java.net.URI;\\nimport java.net.http.HttpClient;\\nimport java.net.http.HttpRequest;\\nimport java.net.http.HttpResponse;\\npublic class Example1 {', '']]]\n",
      "[[['', 'public void handleRequest(S3ObjectLambdaEvent event, Context context) throws\\nException {\\nAmazonS3 s3Client = AmazonS3Client.builder().build();\\n// Check to see if the request contains all of the necessary information.\\n// If it does not, send a 4XX response and a custom error code and message.\\n// Otherwise, retrieve the object from S3 and stream it\\n// to the client unchanged.\\nvar tokenIsNotPresent = !\\nevent.getUserRequest().getHeaders().containsKey(\"requiredToken\");\\nif (tokenIsNotPresent) {\\ns3Client.writeGetObjectResponse(new WriteGetObjectResponseRequest()\\n.withRequestRoute(event.outputRoute())\\n.withRequestToken(event.outputToken())\\n.withStatusCode(403)\\n.withContentLength(0L).withInputStream(new\\nByteArrayInputStream(new byte[0]))\\n.withErrorCode(\"MissingRequiredToken\")\\n.withErrorMessage(\"The required token was not present in the\\nrequest.\"));\\nreturn;\\n}\\n// Prepare the presigned URL for use and make the request to S3.\\nHttpClient httpClient = HttpClient.newBuilder().build();\\nvar presignedResponse = httpClient.send(\\nHttpRequest.newBuilder(new URI(event.inputS3Url())).GET().build(),\\nHttpResponse.BodyHandlers.ofInputStream());\\n// Stream the original bytes back to the caller.\\ns3Client.writeGetObjectResponse(new WriteGetObjectResponseRequest()\\n.withRequestRoute(event.outputRoute())\\n.withRequestToken(event.outputToken())\\n.withInputStream(presignedResponse.body()));\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import boto3\\nimport requests\\ndef handler(event, context):', '']]]\n",
      "[[['', 's3 = boto3.client(\\'s3\\')\\n\"\"\"\\nRetrieve the operation context object from the event. This object indicates\\nwhere the WriteGetObjectResponse request\\nshould be delivered and contains a presigned URL in \\'inputS3Url\\' where we can\\ndownload the requested object from.\\nThe \\'userRequest\\' object has information related to the user who made this\\n\\'GetObject\\' request to\\nS3 Object Lambda.\\n\"\"\"\\nget_context = event[\"getObjectContext\"]\\nuser_request_headers = event[\"userRequest\"][\"headers\"]\\nroute = get_context[\"outputRoute\"]\\ntoken = get_context[\"outputToken\"]\\ns3_url = get_context[\"inputS3Url\"]\\n# Check for the presence of a \\'CustomHeader\\' header and deny or allow based on\\nthat header.\\nis_token_present = \"SuperSecretToken\" in user_request_headers\\nif is_token_present:\\n# If the user presented our custom \\'SuperSecretToken\\' header, we send the\\nrequested object back to the user.\\nresponse = requests.get(s3_url)\\ns3.write_get_object_response(RequestRoute=route, RequestToken=token,\\nBody=response.content)\\nelse:\\n# If the token is not present, we send an error back to the user.\\ns3.write_get_object_response(RequestRoute=route, RequestToken=token,\\nStatusCode=403,\\nErrorCode=\"NoSuperSecretTokenFound\", ErrorMessage=\"The request was not\\nsecret enough.\")\\n# Gracefully exit the Lambda function.\\nreturn { \\'status_code\\': 200 }', ''], ['', '', '']], [['', '', ''], ['', \"const { S3 } = require('aws-sdk');\\nconst axios = require('axios').default;\", '']]]\n",
      "[[['', 'exports.handler = async (event) => {\\nconst s3 = new S3();\\n// Retrieve the operation context object from the event. This object indicates\\nwhere the WriteGetObjectResponse request\\n// should be delivered and contains a presigned URL in \\'inputS3Url\\' where we can\\ndownload the requested object from.\\n// The \\'userRequest\\' object has information related to the user who made this\\n\\'GetObject\\' request to S3 Object Lambda.\\nconst { userRequest, getObjectContext } = event;\\nconst { outputRoute, outputToken, inputS3Url } = getObjectContext;\\n// Check for the presence of a \\'CustomHeader\\' header and deny or allow based on\\nthat header.\\nconst isTokenPresent = Object\\n.keys(userRequest.headers)\\n.includes(\"SuperSecretToken\");\\nif (!isTokenPresent) {\\n// If the token is not present, we send an error back to the user. The\\n\\'await\\' in front of the request\\n// indicates that we want to wait for this request to finish sending before\\nmoving on.\\nawait s3.writeGetObjectResponse({\\nRequestRoute: outputRoute,\\nRequestToken: outputToken,\\nStatusCode: 403,\\nErrorCode: \"NoSuperSecretTokenFound\",\\nErrorMessage: \"The request was not secret enough.\",\\n}).promise();\\n} else {\\n// If the user presented our custom \\'SuperSecretToken\\' header, we send the\\nrequested object back to the user.\\n// Again, note the presence of \\'await\\'.\\nconst presignedResponse = await axios.get(inputS3Url);\\nawait s3.writeGetObjectResponse({\\nRequestRoute: outputRoute,\\nRequestToken: outputToken,\\nBody: presignedResponse.data,\\n}).promise();\\n}\\n// Gracefully exit the Lambda function.\\nreturn { statusCode: 200 };', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', 'package com.amazon.s3.objectlambda;\\nimport com.amazonaws.services.lambda.runtime.Context;\\nimport com.amazonaws.services.lambda.runtime.events.S3ObjectLambdaEvent;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3Client;\\nimport com.amazonaws.services.s3.model.WriteGetObjectResponseRequest;\\nimport javax.imageio.ImageIO;\\nimport java.awt.image.BufferedImage;\\nimport java.awt.Image;\\nimport java.io.ByteArrayInputStream;\\nimport java.io.ByteArrayOutputStream;\\nimport java.net.URI;\\nimport java.net.http.HttpClient;\\nimport java.net.http.HttpRequest;\\nimport java.net.http.HttpResponse;\\npublic class Example2 {\\nprivate static final int HEIGHT = 250;\\nprivate static final int WIDTH = 250;\\npublic void handleRequest(S3ObjectLambdaEvent event, Context context) throws\\nException {\\nAmazonS3 s3Client = AmazonS3Client.builder().build();\\nHttpClient httpClient = HttpClient.newBuilder().build();\\n// Prepare the presigned URL for use and make the request to S3.\\nvar presignedResponse = httpClient.send(\\nHttpRequest.newBuilder(new URI(event.inputS3Url())).GET().build(),\\nHttpResponse.BodyHandlers.ofInputStream());', '']]]\n",
      "[[['', '// The entire image is loaded into memory here so that we can resize it.\\n// Once the resizing is completed, we write the bytes into the body\\n// of the WriteGetObjectResponse request.\\nvar originalImage = ImageIO.read(presignedResponse.body());\\nvar resizingImage = originalImage.getScaledInstance(WIDTH, HEIGHT,\\nImage.SCALE_DEFAULT);\\nvar resizedImage = new BufferedImage(WIDTH, HEIGHT,\\nBufferedImage.TYPE_INT_RGB);\\nresizedImage.createGraphics().drawImage(resizingImage, 0, 0, WIDTH, HEIGHT,\\nnull);\\nvar baos = new ByteArrayOutputStream();\\nImageIO.write(resizedImage, \"png\", baos);\\n// Stream the bytes back to the caller.\\ns3Client.writeGetObjectResponse(new WriteGetObjectResponseRequest()\\n.withRequestRoute(event.outputRoute())\\n.withRequestToken(event.outputToken())\\n.withInputStream(new ByteArrayInputStream(baos.toByteArray())));\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import boto3\\nimport requests\\nimport io\\nfrom PIL import Image\\ndef handler(event, context):\\n\"\"\"\\nRetrieve the operation context object from the event. This object indicates\\nwhere the WriteGetObjectResponse request\\nshould be delivered and has a presigned URL in \\'inputS3Url\\' where we can\\ndownload the requested object from.\\nThe \\'userRequest\\' object has information related to the user who made this\\n\\'GetObject\\' request to\\nS3 Object Lambda.\\n\"\"\"\\nget_context = event[\"getObjectContext\"]\\nroute = get_context[\"outputRoute\"]\\ntoken = get_context[\"outputToken\"]', '']]]\n",
      "[[['', 's3_url = get_context[\"inputS3Url\"]\\n\"\"\"\\nIn this case, we\\'re resizing .png images that are stored in S3 and are\\naccessible through the presigned URL\\n\\'inputS3Url\\'.\\n\"\"\"\\nimage_request = requests.get(s3_url)\\nimage = Image.open(io.BytesIO(image_request.content))\\nimage.thumbnail((256,256), Image.ANTIALIAS)\\ntransformed = io.BytesIO()\\nimage.save(transformed, \"png\")\\n# Send the resized image back to the client.\\ns3 = boto3.client(\\'s3\\')\\ns3.write_get_object_response(Body=transformed.getvalue(), RequestRoute=route,\\nRequestToken=token)\\n# Gracefully exit the Lambda function.\\nreturn { \\'status_code\\': 200 }', ''], ['', '', '']], [['', '', ''], ['', \"const { S3 } = require('aws-sdk');\\nconst axios = require('axios').default;\\nconst sharp = require('sharp');\\nexports.handler = async (event) => {\\nconst s3 = new S3();\\n// Retrieve the operation context object from the event. This object indicates\\nwhere the WriteGetObjectResponse request\\n// should be delivered and has a presigned URL in 'inputS3Url' where we can\\ndownload the requested object from.\\nconst { getObjectContext } = event;\\nconst { outputRoute, outputToken, inputS3Url } = getObjectContext;\\n// In this case, we're resizing .png images that are stored in S3 and are\\naccessible through the presigned URL\\n// 'inputS3Url'.\\nconst { data } = await axios.get(inputS3Url, { responseType: 'arraybuffer' });\", '']]]\n",
      "[[['', '// Resize the image.\\nconst resized = await sharp(data)\\n.resize({ width: 256, height: 256 })\\n.toBuffer();\\n// Send the resized image back to the client.\\nawait s3.writeGetObjectResponse({\\nRequestRoute: outputRoute,\\nRequestToken: outputToken,\\nBody: resized,\\n}).promise();\\n// Gracefully exit the Lambda function.\\nreturn { statusCode: 200 };\\n}', ''], ['', '', '']], [['', '', ''], ['', 'package com.amazon.s3.objectlambda;\\nimport com.amazonaws.services.lambda.runtime.events.S3ObjectLambdaEvent;\\nimport com.amazonaws.services.lambda.runtime.Context;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3Client;\\nimport com.amazonaws.services.s3.model.WriteGetObjectResponseRequest;\\nimport java.net.URI;\\nimport java.net.http.HttpClient;\\nimport java.net.http.HttpRequest;\\nimport java.net.http.HttpResponse;\\npublic class Example3 {', '']]]\n",
      "[[['', \"public void handleRequest(S3ObjectLambdaEvent event, Context context) throws\\nException {\\nAmazonS3 s3Client = AmazonS3Client.builder().build();\\nHttpClient httpClient = HttpClient.newBuilder().build();\\n// Request the original object from S3.\\nvar presignedResponse = httpClient.send(\\nHttpRequest.newBuilder(new URI(event.inputS3Url())).GET().build(),\\nHttpResponse.BodyHandlers.ofInputStream());\\n// Consume the incoming response body from the presigned request,\\n// apply our transformation on that data, and emit the transformed bytes\\n// into the body of the WriteGetObjectResponse request as soon as they're\\nready.\\n// This example compresses the data from S3, but any processing pertinent\\n// to your application can be performed here.\\nvar bodyStream = new GZIPCompressingInputStream(presignedResponse.body());\\n// Stream the bytes back to the caller.\\ns3Client.writeGetObjectResponse(new WriteGetObjectResponseRequest()\\n.withRequestRoute(event.outputRoute())\\n.withRequestToken(event.outputToken())\\n.withInputStream(bodyStream));\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'import boto3\\nimport requests\\nimport zlib\\nfrom botocore.config import Config\\n\"\"\"\\nA helper class to work with content iterators. Takes an interator and compresses the\\nbytes that come from it. It\\nimplements \\'read\\' and \\'__iter__\\' so that the SDK can stream the response.\\n\"\"\"\\nclass Compress:\\ndef __init__(self, content_iter):\\nself.content = content_iter', '']]]\n",
      "[[['', 'self.compressed_obj = zlib.compressobj()\\ndef read(self, _size):\\nfor data in self.__iter__()\\nreturn data\\ndef __iter__(self):\\nwhile True:\\ndata = next(self.content)\\nchunk = self.compressed_obj.compress(data)\\nif not chunk:\\nbreak\\nyield chunk\\nyield self.compressed_obj.flush()\\ndef handler(event, context):\\n\"\"\"\\nSetting the \\'payload_signing_enabled\\' property to False allows us to send a\\nstreamed response back to the client.\\nin this scenario, a streamed response means that the bytes are not buffered into\\nmemory as we\\'re compressing them,\\nbut instead are sent straight to the user.\\n\"\"\"\\nmy_config = Config(\\nregion_name=\\'eu-west-1\\',\\nsignature_version=\\'s3v4\\',\\ns3={\\n\"payload_signing_enabled\": False\\n}\\n)\\ns3 = boto3.client(\\'s3\\', config=my_config)\\n\"\"\"\\nRetrieve the operation context object from the event. This object indicates\\nwhere the WriteGetObjectResponse request\\nshould be delivered and has a presigned URL in \\'inputS3Url\\' where we can\\ndownload the requested object from.\\nThe \\'userRequest\\' object has information related to the user who made this\\n\\'GetObject\\' request to S3 Object Lambda.\\n\"\"\"\\nget_context = event[\"getObjectContext\"]', '']]]\n",
      "[[['', 'route = get_context[\"outputRoute\"]\\ntoken = get_context[\"outputToken\"]\\ns3_url = get_context[\"inputS3Url\"]\\n# Compress the \\'get\\' request stream.\\nwith requests.get(s3_url, stream=True) as r:\\ncompressed = Compress(r.iter_content())\\n# Send the stream back to the client.\\ns3.write_get_object_response(Body=compressed, RequestRoute=route,\\nRequestToken=token, ContentType=\"text/plain\",\\nContentEncoding=\"gzip\")\\n# Gracefully exit the Lambda function.\\nreturn {\\'status_code\\': 200}', ''], ['', '', '']], [['', '', ''], ['', \"const { S3 } = require('aws-sdk');\\nconst axios = require('axios').default;\\nconst zlib = require('zlib');\\nexports.handler = async (event) => {\\nconst s3 = new S3();\\n// Retrieve the operation context object from the event. This object indicates\\nwhere the WriteGetObjectResponse request\\n// should be delivered and has a presigned URL in 'inputS3Url' where we can\\ndownload the requested object from.\\nconst { getObjectContext } = event;\\nconst { outputRoute, outputToken, inputS3Url } = getObjectContext;\\n// Download the object from S3 and process it as a stream, because it might be a\\nhuge object and we don't want to\\n// buffer it in memory. Note the use of 'await' because we want to wait for\\n'writeGetObjectResponse' to finish\\n// before we can exit the Lambda function.\\nawait axios({\\nmethod: 'GET',\\nurl: inputS3Url,\\nresponseType: 'stream',\\n}).then(\\n// Gzip the stream.\", '']]]\n",
      "[[['', 'response => response.data.pipe(zlib.createGzip())\\n).then(\\n// Finally send the gzip-ed stream back to the client.\\nstream => s3.writeGetObjectResponse({\\nRequestRoute: outputRoute,\\nRequestToken: outputToken,\\nBody: stream,\\nContentType: \"text/plain\",\\nContentEncoding: \"gzip\",\\n}).promise()\\n);\\n// Gracefully exit the Lambda function.\\nreturn { statusCode: 200 };\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nAlthough S3 Object Lambda allows up to 60 seconds to send a complete response to\\nthe caller through the WriteGetObjectResponse request, the actual amount of time\\navailable might be less. For example, your Lambda function timeout might be less than 60\\nseconds. In other cases, the caller might have more stringent timeouts.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"xAmzRequestId\": \"requestId\",\\n\"**headObjectContext**\": {\\n\"**inputS3Url**\": \"https://my-s3-ap-111122223333.s3-accesspoint.us-\\neast-1.amazonaws.com/example?X-Amz-Security-Token=<snip>\"\\n},\\n\"configuration\": {\\n\"accessPointArn\": \"arn:aws:s3-object-lambda:us-\\neast-1:111122223333:accesspoint/example-object-lambda-ap\",\\n\"supportingAccessPointArn\": \"arn:aws:s3:us-\\neast-1:111122223333:accesspoint/example-ap\",\\n\"payload\": \"{}\"\\n},\\n\"userRequest\": {', '']]]\n",
      "[[['', '\"url\": \"https://object-lambda-111122223333.s3-object-lambda.us-\\neast-1.amazonaws.com/example\",\\n\"headers\": {\\n\"Host\": \"object-lambda-111122223333.s3-object-lambda.us-\\neast-1.amazonaws.com\",\\n\"Accept-Encoding\": \"identity\",\\n\"X-Amz-Content-SHA256\": \"e3b0c44298fc1example\"\\n}\\n},\\n\"userIdentity\": {\\n\"type\": \"AssumedRole\",\\n\"principalId\": \"principalId\",\\n\"arn\": \"arn:aws:sts::111122223333:assumed-role/Admin/example\",\\n\"accountId\": \"111122223333\",\\n\"accessKeyId\": \"accessKeyId\",\\n\"sessionContext\": {\\n\"attributes\": {\\n\"mfaAuthenticated\": \"false\",\\n\"creationDate\": \"Wed Mar 10 23:41:52 UTC 2021\"\\n},\\n\"sessionIssuer\": {\\n\"type\": \"Role\",\\n\"principalId\": \"principalId\",\\n\"arn\": \"arn:aws:iam::111122223333:role/Admin\",\\n\"accountId\": \"111122223333\",\\n\"userName\": \"Admin\"\\n}\\n}\\n},\\n\"protocolVersion\": \"1.00\"\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"statusCode\": <number>; // Required\\n\"errorCode\": <string>;\\n\"errorMessage\": <string>;\\n\"headers\": {\\n\"Accept-Ranges\": <string>,', '']]]\n",
      "[[['', '\"x-amz-archive-status\": <string>,\\n\"x-amz-server-side-encryption-bucket-key-enabled\": <boolean>,\\n\"Cache-Control\": <string>,\\n\"Content-Disposition\": <string>,\\n\"Content-Encoding\": <string>,\\n\"Content-Language\": <string>,\\n\"Content-Length\": <number>, // Required\\n\"Content-Type\": <string>,\\n\"x-amz-delete-marker\": <boolean>,\\n\"ETag\": <string>,\\n\"Expires\": <string>,\\n\"x-amz-expiration\": <string>,\\n\"Last-Modified\": <string>,\\n\"x-amz-missing-meta\": <number>,\\n\"x-amz-object-lock-mode\": <string>,\\n\"x-amz-object-lock-legal-hold\": <string>,\\n\"x-amz-object-lock-retain-until-date\": <string>,\\n\"x-amz-mp-parts-count\": <number>,\\n\"x-amz-replication-status\": <string>,\\n\"x-amz-request-charged\": <string>,\\n\"x-amz-restore\": <string>,\\n\"x-amz-server-side-encryption\": <string>,\\n\"x-amz-server-side-encryption-customer-algorithm\": <string>,\\n\"x-amz-server-side-encryption-aws-kms-key-id\": <string>,\\n\"x-amz-server-side-encryption-customer-key-MD5\": <string>,\\n\"x-amz-storage-class\": <string>,\\n\"x-amz-tagging-count\": <number>,\\n\"x-amz-version-id\": <string>,\\n<x-amz-meta-headers>: <string>, // user-defined metadata\\n\"x-amz-meta-meta1\": <string>, // example of the user-defined metadata header,\\nit will need the x-amz-meta prefix\\n\"x-amz-meta-meta2\": <string>\\n...\\n};\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import requests', '']]]\n",
      "[[['', 'def lambda_handler(event, context):\\nprint(event)\\n# Extract the presigned URL from the input.\\ns3_url = event[\"headObjectContext\"][\"inputS3Url\"]\\n# Get the head of the object from S3.\\nresponse = requests.head(s3_url)\\n# Return the error to S3 Object Lambda (if applicable).\\nif (response.status_code >= 400):\\nreturn {\\n\"statusCode\": response.status_code,\\n\"errorCode\": \"RequestFailure\",\\n\"errorMessage\": \"Request to S3 failed\"\\n}\\n# Store the headers in a dictionary.\\nresponse_headers = dict(response.headers)\\n# This obscures Content-Type in a transformation, it is optional to add\\nresponse_headers[\"Content-Type\"] = \"\"\\n# Return the headers to S3 Object Lambda.\\nreturn {\\n\"statusCode\": response.status_code,\\n\"headers\": response_headers\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nWe recommend that you use the newer version, ListObjectsV2, when developing\\napplications. For backward compatibility, Amazon S3 continues to support ListObjects.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"xAmzRequestId\": \"requestId\",\\n\"**listObjectsContext**\": {\\n\"**inputS3Url**\": \"https://my-s3-ap-111122223333.s3-accesspoint.us-\\neast-1.amazonaws.com/?X-Amz-Security-Token=<snip>\",\\n},\\n\"configuration\": {\\n\"accessPointArn\": \"arn:aws:s3-object-lambda:us-\\neast-1:111122223333:accesspoint/example-object-lambda-ap\",\\n\"supportingAccessPointArn\": \"arn:aws:s3:us-\\neast-1:111122223333:accesspoint/example-ap\",\\n\"payload\": \"{}\"\\n},\\n\"userRequest\": {\\n\"url\": \"https://object-lambda-111122223333.s3-object-lambda.us-\\neast-1.amazonaws.com/example\",\\n\"headers\": {\\n\"Host\": \"object-lambda-111122223333.s3-object-lambda.us-\\neast-1.amazonaws.com\",\\n\"Accept-Encoding\": \"identity\",\\n\"X-Amz-Content-SHA256\": \"e3b0c44298fc1example\"\\n}\\n},\\n\"userIdentity\": {\\n\"type\": \"AssumedRole\",\\n\"principalId\": \"principalId\",\\n\"arn\": \"arn:aws:sts::111122223333:assumed-role/Admin/example\",\\n\"accountId\": \"111122223333\",\\n\"accessKeyId\": \"accessKeyId\",', '']]]\n",
      "[[['', '\"sessionContext\": {\\n\"attributes\": {\\n\"mfaAuthenticated\": \"false\",\\n\"creationDate\": \"Wed Mar 10 23:41:52 UTC 2021\"\\n},\\n\"sessionIssuer\": {\\n\"type\": \"Role\",\\n\"principalId\": \"principalId\",\\n\"arn\": \"arn:aws:iam::111122223333:role/Admin\",\\n\"accountId\": \"111122223333\",\\n\"userName\": \"Admin\"\\n}\\n}\\n},\\n\"protocolVersion\": \"1.00\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import requests\\nimport xmltodict\\ndef lambda_handler(event, context):\\n# Extract the presigned URL from the input.\\ns3_url = event[\"listObjectsContext\"][\"inputS3Url\"]\\n# Get the head of the object from Amazon S3.\\nresponse = requests.get(s3_url)\\n# Return the error to S3 Object Lambda (if applicable).\\nif (response.status_code >= 400):', '']]]\n",
      "[[['', 'error = xmltodict.parse(response.content)\\nreturn {\\n\"statusCode\": response.status_code,\\n\"errorCode\": error[\"Error\"][\"Code\"],\\n\"errorMessage\": error[\"Error\"][\"Message\"]\\n}\\n# Store the XML result in a dict.\\nresponse_dict = xmltodict.parse(response.content)\\n# This obscures StorageClass in a transformation, it is optional to add\\nfor item in response_dict[\\'ListBucketResult\\'][\\'Contents\\']:\\nitem[\\'StorageClass\\'] = \"\"\\n# Convert back to XML.\\nlistResultXml = xmltodict.unparse(response_dict)\\n# Create response with listResultXml.\\nresponse_with_list_result_xml = {\\n\\'statusCode\\': 200,\\n\\'listResultXml\\': listResultXml\\n}\\n# Create response with listBucketResult.\\nresponse_dict[\\'ListBucketResult\\'] =\\nsanitize_response_dict(response_dict[\\'ListBucketResult\\'])\\nresponse_with_list_bucket_result = {\\n\\'statusCode\\': 200,\\n\\'listBucketResult\\': response_dict[\\'ListBucketResult\\']\\n}\\n# Return the list to S3 Object Lambda.\\n# Can return response_with_list_result_xml or response_with_list_bucket_result\\nreturn response_with_list_result_xml\\n# Converting the response_dict\\'s key to correct casing\\ndef sanitize_response_dict(response_dict: dict):\\nnew_response_dict = dict()\\nfor key, value in response_dict.items():\\nnew_key = key[0].lower() + key[1:] if key != \"ID\" else \\'id\\'\\nif type(value) == list:\\nnewlist = []\\nfor element in value:\\nif type(element) == type(dict()):', '']]]\n",
      "[[['', 'element = sanitize_response_dict(element)\\nnewlist.append(element)\\nvalue = newlist\\nelif type(value) == dict:\\nvalue = sanitize_response_dict(value)\\nnew_response_dict[new_key] = value\\nreturn new_response_dict', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"statusCode\": <number>; // Required\\n\"errorCode\": <string>;\\n\"errorMessage\": <string>;\\n\"listResultXml\": <string>; // This can also be Error XML string in case S3 returned\\nerror response when calling the pre-signed URL\\n\"listBucketResult\": { // listBucketResult can be provided instead of listResultXml,\\nhowever they can not both be provided in the JSON response\\n\"name\": <string>, // Required for \\'listBucketResult\\'\\n\"prefix\": <string>,\\n\"marker\": <string>,\\n\"nextMarker\": <string>,\\n\"maxKeys\": <int>, // Required for \\'listBucketResult\\'\\n\"delimiter\": <string>,\\n\"encodingType\": <string>\\n\"isTruncated\": <boolean>, // Required for \\'listBucketResult\\'\\n\"contents\": [ {\\n\"key\": <string>, // Required for \\'content\\'\\n\"lastModified\": <string>,\\n\"eTag\": <string>,\\n\"checksumAlgorithm\": <string>, // CRC32, CRC32C, SHA1, SHA256\\n\"size\": <int>, // Required for \\'content\\'\\n\"owner\": {\\n\"displayName\": <string>, // Required for \\'owner\\'\\n\"id\": <string>, // Required for \\'owner\\'\\n},\\n\"storageClass\": <string>\\n},\\n...\\n],\\n\"commonPrefixes\": [ {', '']]]\n",
      "[[['', '\"prefix\": <string> // Required for \\'commonPrefix\\'\\n},\\n...\\n],\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"xAmzRequestId\": \"requestId\",\\n\"**listObjectsV2Context**\": {\\n\"**inputS3Url**\": \"https://my-s3-ap-111122223333.s3-accesspoint.us-\\neast-1.amazonaws.com/?list-type=2&X-Amz-Security-Token=<snip>\",\\n},\\n\"configuration\": {\\n\"accessPointArn\": \"arn:aws:s3-object-lambda:us-\\neast-1:111122223333:accesspoint/example-object-lambda-ap\",\\n\"supportingAccessPointArn\": \"arn:aws:s3:us-\\neast-1:111122223333:accesspoint/example-ap\",\\n\"payload\": \"{}\"\\n},\\n\"userRequest\": {', '']]]\n",
      "[[['', '\"url\": \"https://object-lambda-111122223333.s3-object-lambda.us-\\neast-1.amazonaws.com/example\",\\n\"headers\": {\\n\"Host\": \"object-lambda-111122223333.s3-object-lambda.us-\\neast-1.amazonaws.com\",\\n\"Accept-Encoding\": \"identity\",\\n\"X-Amz-Content-SHA256\": \"e3b0c44298fc1example\"\\n}\\n},\\n\"userIdentity\": {\\n\"type\": \"AssumedRole\",\\n\"principalId\": \"principalId\",\\n\"arn\": \"arn:aws:sts::111122223333:assumed-role/Admin/example\",\\n\"accountId\": \"111122223333\",\\n\"accessKeyId\": \"accessKeyId\",\\n\"sessionContext\": {\\n\"attributes\": {\\n\"mfaAuthenticated\": \"false\",\\n\"creationDate\": \"Wed Mar 10 23:41:52 UTC 2021\"\\n},\\n\"sessionIssuer\": {\\n\"type\": \"Role\",\\n\"principalId\": \"principalId\",\\n\"arn\": \"arn:aws:iam::111122223333:role/Admin\",\\n\"accountId\": \"111122223333\",\\n\"userName\": \"Admin\"\\n}\\n}\\n},\\n\"protocolVersion\": \"1.00\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import requests\\nimport xmltodict\\ndef lambda_handler(event, context):\\n# Extract the presigned URL from the input.\\ns3_url = event[\"listObjectsV2Context\"][\"inputS3Url\"]\\n# Get the head of the object from Amazon S3.\\nresponse = requests.get(s3_url)\\n# Return the error to S3 Object Lambda (if applicable).\\nif (response.status_code >= 400):\\nerror = xmltodict.parse(response.content)\\nreturn {\\n\"statusCode\": response.status_code,\\n\"errorCode\": error[\"Error\"][\"Code\"],\\n\"errorMessage\": error[\"Error\"][\"Message\"]\\n}\\n# Store the XML result in a dict.\\nresponse_dict = xmltodict.parse(response.content)\\n# This obscures StorageClass in a transformation, it is optional to add\\nfor item in response_dict[\\'ListBucketResult\\'][\\'Contents\\']:\\nitem[\\'StorageClass\\'] = \"\"\\n# Convert back to XML.\\nlistResultXml = xmltodict.unparse(response_dict)\\n# Create response with listResultXml.\\nresponse_with_list_result_xml = {\\n\\'statusCode\\': 200,\\n\\'listResultXml\\': listResultXml\\n}\\n# Create response with listBucketResult.\\nresponse_dict[\\'ListBucketResult\\'] =\\nsanitize_response_dict(response_dict[\\'ListBucketResult\\'])\\nresponse_with_list_bucket_result = {\\n\\'statusCode\\': 200,\\n\\'listBucketResult\\': response_dict[\\'ListBucketResult\\']', '']]]\n",
      "[[['', '}\\n# Return the list to S3 Object Lambda.\\n# Can return response_with_list_result_xml or response_with_list_bucket_result\\nreturn response_with_list_result_xml\\n# Converting the response_dict\\'s key to correct casing\\ndef sanitize_response_dict(response_dict: dict):\\nnew_response_dict = dict()\\nfor key, value in response_dict.items():\\nnew_key = key[0].lower() + key[1:] if key != \"ID\" else \\'id\\'\\nif type(value) == list:\\nnewlist = []\\nfor element in value:\\nif type(element) == type(dict()):\\nelement = sanitize_response_dict(element)\\nnewlist.append(element)\\nvalue = newlist\\nelif type(value) == dict:\\nvalue = sanitize_response_dict(value)\\nnew_response_dict[new_key] = value\\nreturn new_response_dict', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"statusCode\": <number>; // Required\\n\"errorCode\": <string>;\\n\"errorMessage\": <string>;\\n\"listResultXml\": <string>; // This can also be Error XML string in case S3 returned\\nerror response when calling the pre-signed URL\\n\"listBucketResult\": { // listBucketResult can be provided instead of\\nlistResultXml, however they can not both be provided in the JSON response\\n\"name\": <string>, // Required for \\'listBucketResult\\'\\n\"prefix\": <string>,\\n\"startAfter\": <string>,\\n\"continuationToken\": <string>,\\n\"nextContinuationToken\": <string>,\\n\"keyCount\": <int>, // Required for \\'listBucketResult\\'\\n\"maxKeys\": <int>, // Required for \\'listBucketResult\\'\\n\"delimiter\": <string>,', '']]]\n",
      "[[['', '\"encodingType\": <string>\\n\"isTruncated\": <boolean>, // Required for \\'listBucketResult\\'\\n\"contents\": [ {\\n\"key\": <string>, // Required for \\'content\\'\\n\"lastModified\": <string>,\\n\"eTag\": <string>,\\n\"checksumAlgorithm\": <string>, // CRC32, CRC32C, SHA1, SHA256\\n\"size\": <int>, // Required for \\'content\\'\\n\"owner\": {\\n\"displayName\": <string>, // Required for \\'owner\\'\\n\"id\": <string>, // Required for \\'owner\\'\\n},\\n\"storageClass\": <string>\\n},\\n...\\n],\\n\"commonPrefixes\": [ {\\n\"prefix\": <string> // Required for \\'commonPrefix\\'\\n},\\n...\\n],\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"xAmzRequestId\": \"requestId\",\\n\"getObjectContext\": {\\n\"inputS3Url\": \"https://my-s3-ap-111122223333.s3-accesspoint.us-\\neast-1.amazonaws.com/example?X-Amz-Security-Token=<snip>\",\\n\"outputRoute\": \"io-use1-001\",\\n\"outputToken\": \"OutputToken\"\\n},\\n\"configuration\": {\\n\"accessPointArn\": \"arn:aws:s3-object-lambda:us-\\neast-1:111122223333:accesspoint/example-object-lambda-ap\",\\n\"supportingAccessPointArn\": \"arn:aws:s3:us-\\neast-1:111122223333:accesspoint/example-ap\",', '']]]\n",
      "[[['', '\"payload\": \"{}\"\\n},\\n\"userRequest\": {\\n\"url\": \"https://object-lambda-111122223333.s3-object-lambda.us-\\neast-1.amazonaws.com/example\",\\n\"headers\": {\\n\"Host\": \"object-lambda-111122223333.s3-object-lambda.us-\\neast-1.amazonaws.com\",\\n\"Accept-Encoding\": \"identity\",\\n\"X-Amz-Content-SHA256\": \"e3b0c44298fc1example\"\\n}\\n},\\n\"userIdentity\": {\\n\"type\": \"AssumedRole\",\\n\"principalId\": \"principalId\",\\n\"arn\": \"arn:aws:sts::111122223333:assumed-role/Admin/example\",\\n\"accountId\": \"111122223333\",\\n\"accessKeyId\": \"accessKeyId\",\\n\"sessionContext\": {\\n\"attributes\": {\\n\"mfaAuthenticated\": \"false\",\\n\"creationDate\": \"Wed Mar 10 23:41:52 UTC 2021\"\\n},\\n\"sessionIssuer\": {\\n\"type\": \"Role\",\\n\"principalId\": \"principalId\",\\n\"arn\": \"arn:aws:iam::111122223333:role/Admin\",\\n\"accountId\": \"111122223333\",\\n\"userName\": \"Admin\"\\n}\\n}\\n},\\n\"protocolVersion\": \"1.00\"\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['Parameter', 'Event context location'], ['Range (header)', 'userRequest.headers.Range'], ['Range (query\\nparameter)', 'userRequest.url (query parameter Range)'], ['partNumber', 'userRequest.url (query parameter partNumber )']], [['', '', ''], ['', \"Important\\nThe provided presigned URL for your Object Lambda Access Point doesn't contain the\\nRange or partNumber parameter from the original request. See the following options on\\nhow to handle these parameters in your AWS Lambda function.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nIn many cases, applying the Range parameter to the presigned URL will result in\\nunexpected behavior by the Lambda function or the requesting client. Unless you are\\nsure that your application will work properly when retrieving only a partial object from', '']]]\n",
      "[[['', 'Amazon S3, we recommend that you retrieve and transform full objects as described\\nearlier in approach A.', ''], ['', '', '']], [['', '', ''], ['', 'private HttpRequest.Builder applyRangeHeader(ObjectLambdaEvent event,\\nHttpRequest.Builder presignedRequest) {\\nvar header = event.getUserRequest().getHeaders().entrySet().stream()\\n.filter(e -> e.getKey().toLowerCase(Locale.ROOT).equals(\"range\"))\\n.findFirst();\\n// Add check in the query string itself.\\nheader.ifPresent(entry -> presignedRequest.header(entry.getKey(),\\nentry.getValue()));\\nreturn presignedRequest;\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe following examples can be used only with GetObject requests.', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:serverlessrepo:us-east-1:111122223333:applications/\\nComprehendPiiAccessControlS3ObjectLambda', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'arn:aws:serverlessrepo:us-east-1:111122223333::applications/\\nComprehendPiiRedactionS3ObjectLambda', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:serverlessrepo:us-east-1:111122223333::applications/S3ObjectLambdaDecompression', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nAmazon S3 server access logs aren't supported with S3 Express One Zone.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nTo use directory buckets with the high-level aws s3 commands, update your AWS CLI to\\nthe latest version. For more information about how to install and configure the AWS CLI,\\nsee Install or update the latest version of the AWS CLI in the AWS CLI Command Reference.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nDirectory buckets that have no request activity for a period of at least 90 days transition to\\nan inactive state. While in an inactive state, a directory bucket is temporarily inaccessible\\nfor reads and writes. Inactive buckets retain all storage, object metadata, and bucket\\nmetadata. Existing storage charges apply to inactive buckets. If you make an access request\\nto an inactive bucket, the bucket transitions to an active state, typically within a few\\nminutes. During this transition period, reads and writes return an HTTP 503 (Service\\nUnavailable) error code.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'bucket-base-name--azid--x-s3', ''], ['', '', '']], [['', '', ''], ['', 'bucket-base-name--usw2-az1--x-s3', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'base-name--azid--x-s3', ''], ['', '', '']], [['', '', ''], ['', 'bucket-base-name--usw2-az1--x-s3', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen you create a directory bucket using the console a suffix is automatically added to the\\nbase name that you provide. This suffix includes the Availability Zone ID of the Availability\\nZone that you chose.\\nWhen you create a directory bucket using an API you must provide the full suffix, including\\nthe Availability Zone ID, in your request. For a list of Availability Zone IDs, see S3 Express\\nOne Zone Availability Zones and Regions.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'bucket-base-name--azid--x-s3', ''], ['', '', '']], [['', '', ''], ['', 'bucket-base-name--usw2-az1--x-s3', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo minimize latency and costs and address regulatory requirements, choose a Region\\nclose to you. Objects stored in a Region never leave that Region unless you explicitly\\ntransfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS\\nservice endpoints in the Amazon Web Services General Reference.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\n• If you've chosen a Region that doesn't support directory buckets, the Bucket type\\noption disappears, and the bucket type defaults to a general purpose bucket. To\\ncreate a directory bucket, you must choose a supported Region. For a list of Regions\\nthat support directory buckets and the Amazon S3 Express One Zone storage class,\\nsee the section called “S3 Express One Zone Availability Zones and Regions”.\\n• After you create the bucket, you can't change the bucket type.\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThe Availability Zone can't be changed after the bucket is created.\", ''], ['', '', '']], [['', '', ''], ['', \"Important\\nAlthough directory buckets are stored across multiple devices within a single\\nAvailability Zone, directory buckets don't store data redundantly across Availability\\nZones.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nDo not include sensitive information, such as account numbers, in the bucket name.\\nThe bucket name is visible in the URLs that point to the objects in the bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'var bucket = \"...\"\\nfunc runCreateBucket(c *s3.Client) {\\nresp, err := c.CreateBucket(context.Background(), &s3.CreateBucketInput{\\nBucket: &bucket,\\nCreateBucketConfiguration: &types.CreateBucketConfiguration{\\nLocation: &types.LocationInfo{\\nName: aws.String(\"usw2-az1\"),\\nType: types.LocationTypeAvailabilityZone,\\n},\\nBucket: &types.BucketInfo{\\nDataRedundancy: types.DataRedundancySingleAvailabilityZone,\\nType: types.BucketTypeDirectory,\\n},\\n},\\n})\\nvar terr *types.BucketAlreadyOwnedByYou\\nif errors.As(err, &terr) {\\nfmt.Printf(\"BucketAlreadyOwnedByYou: %s\\\\n\", aws.ToString(terr.Message))\\nfmt.Printf(\"noop...\\\\n\")\\nreturn\\n}\\nif err != nil {\\nlog.Fatal(err)\\n}\\nfmt.Printf(\"bucket created at %s\\\\n\", aws.ToString(resp.Location))\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public static void createBucket(S3Client s3Client, String bucketName) {', '']]]\n",
      "[[['', '//Bucket name format is {base-bucket-name}--{az-id}--x-s3\\n//example: doc-example-bucket--usw2-az1--x-s3 is a valid name for a directory\\nbucket created in\\n//Region us-west-2, Availability Zone 2\\nCreateBucketConfiguration bucketConfiguration =\\nCreateBucketConfiguration.builder()\\n.location(LocationInfo.builder()\\n.type(LocationType.AVAILABILITY_ZONE)\\n.name(\"usw2-az1\").build()) //this must match the Region and\\nAvailability Zone in your bucket name\\n.bucket(BucketInfo.builder()\\n.type(BucketType.DIRECTORY)\\n.dataRedundancy(DataRedundancy.SINGLE_AVAILABILITY_ZONE)\\n.build()).build();\\ntry {\\nCreateBucketRequest bucketRequest =\\nCreateBucketRequest.builder().bucket(bucketName).createBucketConfiguration(bucketCon\\nCreateBucketResponse response = s3Client.createBucket(bucketRequest);\\nSystem.out.println(response);\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}', 'f'], ['', '', '']], [['', '', ''], ['', '// file.mjs, run with Node.js v16 or higher\\n// To use with the preview build, place this in a folder\\n// inside the preview build directory, such as /aws-sdk-js-v3/workspace/\\nimport { S3 } from \"@aws-sdk/client-s3\";', '']]]\n",
      "[[['', 'const region = \"us-east-1\";\\nconst zone = \"use1-az4\";\\nconst suffix = `${zone}--x-s3`;\\nconst s3 = new S3({ region });\\nconst bucketName = `...--${suffix}`;\\nconst createResponse = await s3.createBucket(\\n{ Bucket: bucketName,\\nCreateBucketConfiguration: {Location: {Type: \"AvailabilityZone\", Name: zone},\\nBucket: { Type: \"Directory\", DataRedundancy: \"SingleAvailabilityZone\" }}\\n}\\n);', ''], ['', '', '']], [['', '', ''], ['', 'using (var amazonS3Client = new AmazonS3Client())\\n{\\nvar putBucketResponse = await amazonS3Client.PutBucketAsync(new PutBucketRequest\\n{\\nBucketName = \"DOC-EXAMPLE-BUCKET--usw2-az1--x-s3\",\\nPutBucketConfiguration = new PutBucketConfiguration\\n{\\nBucketInfo = new BucketInfo { DataRedundancy =\\nDataRedundancy.SingleAvailabilityZone, Type = BucketType.Directory },\\nLocation = new LocationInfo { Name = \"usw2-az1\", Type =\\nLocationType.AvailabilityZone }\\n}\\n}).ConfigureAwait(false);\\n}', ''], ['', '', '']], [['', '', ''], ['', \"require 'vendor/autoload.php';\", '']]]\n",
      "[[['', '$s3Client = new S3Client([\\n\\'region\\' => \\'us-east-1\\',\\n]);\\n$result = $s3Client->createBucket([\\n\\'Bucket\\' => \\'doc-example-bucket--use1-az4--x-s3\\',\\n\\'CreateBucketConfiguration\\' => [\\n\\'Location\\' => [\\'Name\\'=> \\'use1-az4\\', \\'Type\\'=> \\'AvailabilityZone\\'],\\n\\'Bucket\\' => [\"DataRedundancy\" => \"SingleAvailabilityZone\" ,\"Type\" =>\\n\"Directory\"] ],\\n]);', ''], ['', '', '']], [['', '', ''], ['', \"import logging\\nimport boto3\\nfrom botocore.exceptions import ClientError\\ndef create_bucket(s3_client, bucket_name, availability_zone):\\n'''\\nCreate a directory bucket in a specified Availability Zone\\n:param s3_client: boto3 S3 client\\n:param bucket_name: Bucket to create; for example, 'doc-example-bucket--usw2-\\naz1--x-s3'\\n:param availability_zone: String; Availability Zone ID to create the bucket in,\\nfor example, 'usw2-az1'\\n:return: True if bucket is created, else False\\n'''\\ntry:\\nbucket_config = {\\n'Location': {\\n'Type': 'AvailabilityZone',\\n'Name': availability_zone\\n},\", '']]]\n",
      "[[['', \"'Bucket': {\\n'Type': 'Directory',\\n'DataRedundancy': 'SingleAvailabilityZone'\\n}\\n}\\ns3_client.create_bucket(\\nBucket = bucket_name,\\nCreateBucketConfiguration = bucket_config\\n)\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn False\\nreturn True\\nif __name__ == '__main__':\\nbucket_name = 'BUCKET_NAME'\\nregion = 'us-west-2'\\navailability_zone = 'usw2-az1'\\ns3_client = boto3.client('s3', region_name = region)\\ncreate_bucket(s3_client, bucket_name, availability_zone)\", ''], ['', '', '']], [['', '', ''], ['', 's3 = Aws::S3::Client.new(region:\\'us-west-2\\')\\ns3.create_bucket(\\nbucket: \"bucket_base_name--az_id--x-s3\",\\ncreate_bucket_configuration: {\\nlocation: { name: \\'usw2-az1\\', type: \\'AvailabilityZone\\' },\\nbucket: { data_redundancy: \\'SingleAvailabilityZone\\', type: \\'Directory\\' }\\n}\\n)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"aws s3api create-bucket\\n--bucket bucket-base-name--azid--x-s3\\n--create-bucket-configuration 'Location={Type=AvailabilityZone,Name=usw2-\\naz1},Bucket={DataRedundancy=SingleAvailabilityZone,Type=Directory}'\\n--region us-west-2\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor your convenience, the Edit bucket policy page displays the Bucket ARN\\n(Amazon Resource Name) of the current bucket above the Policy text field. You\\ncan copy this ARN for use in the statements on the AWS Policy Generator page.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nBucket policies are limited to 20 KB in size.', ''], ['', '', '']], [['', '', ''], ['', 'public static void setBucketPolicy(S3Client s3Client, String bucketName, String\\npolicyText) {\\n//sample policy text\\n/**', '']]]\n",
      "[[['', '* policy_statement = {\\n* \\'Version\\': \\'2012-10-17\\',\\n* \\'Statement\\': [\\n* {\\n* \\'Sid\\': \\'AdminPolicy\\',\\n* \\'Effect\\': \\'Allow\\',\\n* \\'Principal\\': {\\n* \"AWS\": \"111122223333\"\\n* },\\n* \\'Action\\': \\'s3express:*\\',\\n* \\'Resource\\':\\n\\'arn:aws:s3express:region:111122223333:bucket/bucket-base-name--azid--x-s3\\'\\n* }\\n* ]\\n* }\\n*/\\nSystem.out.println(\"Setting policy:\");\\nSystem.out.println(\"----\");\\nSystem.out.println(policyText);\\nSystem.out.println(\"----\");\\nSystem.out.format(\"On Amazon S3 bucket: \\\\\"%s\\\\\"\\\\n\", bucketName);\\ntry {\\nPutBucketPolicyRequest policyReq = PutBucketPolicyRequest.builder()\\n.bucket(bucketName)\\n.policy(policyText)\\n.build();\\ns3Client.putBucketPolicy(policyReq);\\nSystem.out.println(\"Done!\");\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-policy --bucket bucket-base-name--azid--x-s3 --policy file://\\nbucket_policy.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AdminPolicy\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"111122223333\"\\n},\\n\"Action\": \"s3express*\",\\n\"Resource\": \"arn:aws:s3express:us-west-2:111122223333:bucket/\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-policy --bucket bucket-base-name--azid--x-s3', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public static void deleteBucketPolicy(S3Client s3Client, String bucketName) {\\ntry {\\nDeleteBucketPolicyRequest deleteBucketPolicyRequest =\\nDeleteBucketPolicyRequest\\n.builder()\\n.bucket(bucketName)\\n.build()\\ns3Client.deleteBucketPolicy(deleteBucketPolicyRequest);\\nSystem.out.println(\"Successfully deleted bucket policy\");\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-bucket-policy --bucket bucket-base-name--azid--x-s3', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe s3 rm command through the AWS Command Line Interface (CLI), the delete\\noperation through Mountpoint, and the Empty bucket option button through the AWS\\nManagement Console are unable to delete in-progress multipart uploads in a directory\\nbucket. To delete these in-progress multipart uploads, use the ListMultipartUploads\\noperation to list the in-progress multipart uploads in the bucket and use the\\nAbortMultupartUpload operation to abort all the in-progress multipart uploads.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nDeleting a directory bucket can't be undone.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public static void deleteBucket(S3Client s3Client, String bucketName) {\\ntry {\\nDeleteBucketRequest del = DeleteBucketRequest.builder()\\n.bucket(bucketName)\\n.build();\\ns3Client.deleteBucket(del);\\nSystem.out.println(\"Bucket \" + bucketName + \" has been deleted\");\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"import logging\\nimport boto3\\nfrom botocore.exceptions import ClientError\\ndef delete_bucket(s3_client, bucket_name):\\n'''\\nDelete a directory bucket in a specified Region\\n:param s3_client: boto3 S3 client\\n:param bucket_name: Bucket to delete; for example, 'doc-example-bucket--usw2-\\naz1--x-s3'\\n:return: True if bucket is deleted, else False\\n'''\\ntry:\\ns3_client.delete_bucket(Bucket = bucket_name)\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn False\\nreturn True\", '']]]\n",
      "[[['', \"if __name__ == '__main__':\\nbucket_name = 'BUCKET_NAME'\\nregion = 'us-west-2'\\ns3_client = boto3.client('s3', region_name = region)\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-bucket --bucket bucket-base-name--azid--x-s3 --region us-west-2', ''], ['', '', '']], [['', '', ''], ['', 'public static void listBuckets(S3Client s3Client) {\\ntry {\\nListDirectoryBucketsRequest listDirectoryBucketsRequest =\\nListDirectoryBucketsRequest.builder().build();\\nListDirectoryBucketsResponse response =\\ns3Client.listDirectoryBuckets(listDirectoryBucketsRequest);\\nif (response.hasBuckets()) {\\nfor (Bucket bucket: response.buckets()) {\\nSystem.out.println(bucket.name());\\nSystem.out.println(bucket.creationDate());\\n}\\n}\\n}', '']]]\n",
      "[[['', 'catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"import logging\\nimport boto3\\nfrom botocore.exceptions import ClientError\\ndef list_directory_buckets(s3_client):\\n'''\\nPrints a list of all directory buckets in a Region\\n:param s3_client: boto3 S3 client\\n:return: True if there are buckets in the Region, else False\\n'''\\ntry:\\nresponse = s3_client.list_directory_buckets()\\nfor bucket in response['Buckets']:\\nprint (bucket['Name'])\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn False\\nreturn True\\nif __name__ == '__main__':\\nregion = 'us-east-1'\\ns3_client = boto3.client('s3', region_name = region)\\nlist_directory_buckets(s3_client)\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'var listDirectoryBuckets = await amazonS3Client.ListDirectoryBucketsAsync(new\\nListDirectoryBucketsRequest\\n{\\nMaxDirectoryBuckets = 10\\n}).ConfigureAwait(false);', ''], ['', '', '']], [['', '', ''], ['', \"require 'vendor/autoload.php';\\n$s3Client = new S3Client([\\n'region' => 'us-east-1',\\n]);\\n$result = $s3Client->listDirectoryBuckets();\", ''], ['', '', '']], [['', '', ''], ['', \"s3 = Aws::S3::Client.new(region:'us-west-1')\\ns3.list_directory_buckets\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-directory-buckets --region us-east-1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public static void headBucket(S3Client s3Client, String bucketName) {\\ntry {\\nHeadBucketRequest headBucketRequest = HeadBucketRequest\\n.builder()\\n.bucket(bucketName)\\n.build();\\ns3Client.headBucket(headBucketRequest);\\nSystem.out.format(\"Amazon S3 bucket: \\\\\"%s\\\\\" found.\", bucketName);\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api head-bucket --bucket bucket-base-name--azid--x-s3', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe following limitations apply to import jobs:\\n• The source bucket and the destination bucket must be in the same AWS Region and\\naccount.', '']]]\n",
      "[[['', '• The source bucket cannot be a directory bucket.\\n• Objects larger than 5GB are not supported and will be omitted from the copy operation.\\n• Objects in the Glacier Flexible Retrieval, Glacier Deep Archive, Intelligent-Tiering Archive\\nAccess tier, and Intelligent-Tiering Deep Archive tier storage classes must be restored\\nbefore they can be imported.\\n• Imported objects with MD5 checksum algorithms are converted to use CRC32 checksums.\\n• Imported objects use server-side encryption with Amazon S3 managed keys (SSE-S3).\\n• Imported objects use the Express One Zone storage class, which has a different pricing\\nstructure than the storage classes used by general purpose buckets. Consider this\\ndifference in cost when importing large numbers of objects.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf your source objects are encrypted with server-side encryption with AWS Key\\nManagement Service (AWS KMS) keys (SSE-KMS), don't choose the Create new\\nIAM role option. Instead, specify an existing IAM role that has the kms:Decrypt\\npermission.\\nAmazon S3 will use this permission to decrypt your objects. During the import\\nprocess, Amazon S3 will then re-encrypt those objects by using server-side\\nencryption with Amazon S3 managed keys (SSE-S3).\", ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'public static void putObject(S3Client s3Client, String bucketName, String objectKey,\\nPath filePath) {\\n//Using File Path to avoid loading the whole file into memory\\ntry {\\nPutObjectRequest putObj = PutObjectRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n//.metadata(metadata)\\n.build();\\ns3Client.putObject(putObj, filePath);\\nSystem.out.println(\"Successfully placed \" + objectKey +\" into bucket\\n\"+bucketName);\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.getMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import boto3\\nimport botocore', '']]]\n",
      "[[['', 'from botocore.exceptions import ClientError\\ndef put_object(s3_client, bucket_name, key_name, object_bytes):\\n\"\"\"\\nUpload data to a directory bucket.\\n:param s3_client: The boto3 S3 client\\n:param bucket_name: The bucket that will contain the object\\n:param key_name: The key of the object to be uploaded\\n:param object_bytes: The data to upload\\n\"\"\"\\ntry:\\nresponse = s3_client.put_object(Bucket=bucket_name, Key=key_name,\\nBody=object_bytes)\\nprint(f\"Upload object \\'{key_name}\\' to bucket \\'{bucket_name}\\'.\")\\nreturn response\\nexcept ClientError:\\nprint(f\"Couldn\\'t upload object \\'{key_name}\\' to bucket \\'{bucket_name}\\'.\")\\nraise\\ndef main():\\n# Share the client session with functions and objects to benefit from S3 Express\\nOne Zone auth key\\ns3_client = boto3.client(\\'s3\\')\\n# Directory bucket name must end with --azid--x-s3\\nresp = put_object(s3_client, \\'doc-bucket-example--use1-az5--x-s3\\', \\'sample.txt\\',\\nb\\'Hello, World!\\')\\nprint(resp)\\nif __name__ == \"__main__\":\\nmain()', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket bucket-base-name--azid--x-s3 --key sampleinut/file001.bin\\n--body bucket-seed/file001.bin', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nWhen using the preceding checksum algorithms, the multipart part numbers must\\nuse consecutive part numbers. If you try to complete a multipart upload request with\\nnonconsecutive part numbers, Amazon S3 generates an HTTP 400 Bad Request (Invalid\\nPart Order) error.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nVersioning isn’t supported for objects that are stored in directory buckets.', ''], ['', '', '']], [['', '', ''], ['', \"Important\\nIf the complete multipart upload request isn't sent successfully, the object parts aren'\\nassembled and an object isn'created. You are billed for all storage associated with uploaded\\nparts. It's important that you either complete the multipart upload to have the object\\ncreated or abort the multipart upload to remove any uploaded parts.\\nBefore you can delete a directory bucket, you must complete or abort all in-progress\\nmultipart uploads. Directory buckets don't support S3 Lifecycle configurations. If needed,\\nyou can list your active multipart uploads, then abort the uploads, and then delete your\\nbucket.\", ''], ['', '', '']]]\n",
      "[[['Action', 'Required permissions'], ['Create a\\nmultipart\\nupload', 'To create the multipart upload, you must be allowed to perform the\\ns3express:CreateSession action on the directory bucket.'], ['Initiate a\\nmultipart\\nupload', 'To initiate the multipart upload, you must be allowed to perform the\\ns3express:CreateSession action on the directory bucket.'], ['Upload a\\npart', 'To upload a part, you must be allowed to perform the s3express:CreateSe\\nssion action on the directory bucket.\\nFor the initiator to upload a part, the bucket owner must allow the initiator to\\nperform the s3express:CreateSession action on the directory bucket.'], ['Upload a\\npart (copy)', 'To upload a part, you must be allowed to perform the s3express:CreateSe\\nssion action on the directory bucket.\\nFor the initiator to upload a part for an object, the owner of the bucket must\\nallow the initiator to perform the s3express:CreateSession action on\\nthe object.'], ['Complete\\na multipart\\nupload', 'To complete a multipart upload, you must be allowed to perform the\\ns3express:CreateSession action on the directory bucket.\\nFor the initiator to complete a multipart upload, the bucket owner must allow\\nthe initiator to perform the s3express:CreateSession action on the\\nobject.'], ['Abort a\\nmultipart\\nupload', 'To abort a multipart upload, you must be allowed to perform the s3express\\n:CreateSession action.\\nFor the initiator to abort a multipart upload, the initiator must be granted\\nexplicit allow access to perform the s3express:CreateSession action.']]]\n",
      "[[['Action', 'Required permissions'], ['List parts', 'To list the parts in a multipart upload, you must be allowed to perform the\\ns3express:CreateSession action on the directory bucket.'], ['List in-progre\\nss multipart\\nuploads', 'To list the in-progress multipart uploads to a bucket, you must be allowed to\\nperform the s3:ListBucketMultipartUploads action on that bucket.']]]\n",
      "[[['', '', ''], ['', \"/**\\n* This method creates a multipart upload request that generates a unique upload ID\\nthat is used to track\\n* all the upload parts\\n*\\n* @param s3\\n* @param bucketName - for example, 'doc-example-bucket--use1-az4--x-s3'\\n* @param key\\n* @return\\n*/\\nprivate static String createMultipartUpload(S3Client s3, String bucketName, String\\nkey) {\\nCreateMultipartUploadRequest createMultipartUploadRequest =\\nCreateMultipartUploadRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.build();\\nString uploadId = null;\\ntry {\\nCreateMultipartUploadResponse response =\\ns3.createMultipartUpload(createMultipartUploadRequest);\\nuploadId = response.uploadId();\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\nreturn uploadId;\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"def create_multipart_upload(s3_client, bucket_name, key_name):\\n'''\\nCreate a multipart upload to a directory bucket\\n:param s3_client: boto3 S3 client\\n:param bucket_name: The destination bucket for the multipart upload\\n:param key_name: The key name for the object to be uploaded\\n:return: The UploadId for the multipart upload if created successfully, else None\\n'''\\ntry:\\nmpu = s3_client.create_multipart_upload(Bucket = bucket_name, Key =\\nkey_name)\\nreturn mpu['UploadId']\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn None\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api create-multipart-upload --bucket bucket-base-name--azid--x-s3 --key KEY_NAME', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '/**\\n* This method creates part requests and uploads individual parts to S3 and then\\nreturns all the completed parts\\n*\\n* @param s3\\n* @param bucketName\\n* @param key\\n* @param uploadId\\n* @throws IOException\\n*/\\nprivate static ListCompletedPartmultipartUpload(S3Client s3, String bucketName,\\nString key, String uploadId, String filePath) throws IOException {\\nint partNumber = 1;\\nListCompletedPart completedParts = new ArrayList<>();\\nByteBuffer bb = ByteBuffer.allocate(1024 * 1024 * 5); // 5 MB byte buffer\\n// read the local file, breakdown into chunks and process\\ntry (RandomAccessFile file = new RandomAccessFile(filePath, \"r\")) {\\nlong fileSize = file.length();\\nint position = 0;\\nwhile (position < fileSize) {\\nfile.seek(position);\\nint read = file.getChannel().read(bb);\\nbb.flip(); // Swap position and limit before reading from the\\nbuffer.\\nUploadPartRequest uploadPartRequest = UploadPartRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.uploadId(uploadId)\\n.partNumber(partNumber)\\n.build();\\nUploadPartResponse partResponse = s3.uploadPart(', '']]]\n",
      "[[['', 'uploadPartRequest,\\nRequestBody.fromByteBuffer(bb));\\nCompletedPart part = CompletedPart.builder()\\n.partNumber(partNumber)\\n.eTag(partResponse.eTag())\\n.build();\\ncompletedParts.add(part);\\nbb.clear();\\nposition += read;\\npartNumber++;\\n}\\n}\\ncatch (IOException e) {\\nthrow e;\\n}\\nreturn completedParts;\\n}', ''], ['', '', '']], [['', '', ''], ['', \"def multipart_upload(s3_client, bucket_name, key_name, mpu_id, part_size):\\n'''\\nBreak up a file into multiple parts and upload those parts to a directory bucket\\n:param s3_client: boto3 S3 client\\n:param bucket_name: Destination bucket for the multipart upload\\n:param key_name: Key name for object to be uploaded and for the local file\\nthat's being uploaded\\n:param mpu_id: The UploadId returned from the create_multipart_upload call\\n:param part_size: The size parts that the object will be broken into, in bytes.\\nMinimum 5 MiB, Maximum 5 GiB. There is no minimum size for the\\nlast part of your multipart upload.\\n:return: part_list for the multipart upload if all parts are uploaded\\nsuccessfully, else None\\n'''\", '']]]\n",
      "[[['', \"part_list = []\\ntry:\\nwith open(key_name, 'rb') as file:\\npart_counter = 1\\nwhile True:\\nfile_part = file.read(part_size)\\nif not len(file_part):\\nbreak\\nupload_part = s3_client.upload_part(\\nBucket = bucket_name,\\nKey = key_name,\\nUploadId = mpu_id,\\nBody = file_part,\\nPartNumber = part_counter\\n)\\npart_list.append({'PartNumber': part_counter, 'ETag':\\nupload_part['ETag']})\\npart_counter += 1\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn None\\nreturn part_list\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api upload-part --bucket bucket-base-name--azid--x-s3 --\\nkey KEY_NAME --part-number 1 --body LOCAL_FILE_NAME --upload-id\\n\"AS_mgt9RaQE9GEaifATue15dAAAAAAAAAAEMAAAAAAAAADQwNzI4MDU0MjUyMBYAAAAAAAAAAA0AAAAAAAAAAA', 'H'], ['', '', '']]]\n",
      "[[['', '', ''], ['', '/**\\n* This method completes the multipart upload request by collating all the upload\\nparts\\n* @param s3\\n* @param bucketName - for example, \\'doc-example-bucket--usw2-az1--x-s3\\'\\n* @param key\\n* @param uploadId\\n* @param uploadParts\\n*/\\nprivate static void completeMultipartUpload(S3Client s3, String bucketName, String\\nkey, String uploadId, ListCompletedPart uploadParts) {\\nCompletedMultipartUpload completedMultipartUpload =\\nCompletedMultipartUpload.builder()\\n.parts(uploadParts)\\n.build();\\nCompleteMultipartUploadRequest completeMultipartUploadRequest =\\nCompleteMultipartUploadRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.uploadId(uploadId)\\n.multipartUpload(completedMultipartUpload)\\n.build();\\ns3.completeMultipartUpload(completeMultipartUploadRequest);\\n}\\npublic static void multipartUploadTest(S3Client s3, String bucketName, String\\nkey, String localFilePath) {\\nSystem.out.println(\"Starting multipart upload for: \" + key);\\ntry {\\nString uploadId = createMultipartUpload(s3, bucketName, key);\\nSystem.out.println(uploadId);\\nListCompletedPart parts = multipartUpload(s3, bucketName, key, uploadId,\\nlocalFilePath);', '']]]\n",
      "[[['', 'completeMultipartUpload(s3, bucketName, key, uploadId, parts);\\nSystem.out.println(\"Multipart upload completed for: \" + key);\\n}\\ncatch (Exception e) {\\nSystem.err.println(e.getMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"def complete_multipart_upload(s3_client, bucket_name, key_name, mpu_id, part_list):\\n'''\\nCompletes a multipart upload to a directory bucket\\n:param s3_client: boto3 S3 client\\n:param bucket_name: The destination bucket for the multipart upload\\n:param key_name: The key name for the object to be uploaded\\n:param mpu_id: The UploadId returned from the create_multipart_upload call\\n:param part_list: The list of uploaded part numbers with their associated ETags\\n:return: True if the multipart upload was completed successfully, else False\\n'''\\ntry:\\ns3_client.complete_multipart_upload(\\nBucket = bucket_name,\\nKey = key_name,\\nUploadId = mpu_id,\\nMultipartUpload = {\\n'Parts': part_list\\n}\\n)\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn False\\nreturn True\\nif __name__ == '__main__':\\nMB = 1024 ** 2\", '']]]\n",
      "[[['', \"region = 'us-west-2'\\nbucket_name = 'BUCKET_NAME'\\nkey_name = 'OBJECT_NAME'\\npart_size = 10 * MB\\ns3_client = boto3.client('s3', region_name = region)\\nmpu_id = create_multipart_upload(s3_client, bucket_name, key_name)\\nif mpu_id is not None:\\npart_list = multipart_upload(s3_client, bucket_name, key_name, mpu_id,\\npart_size)\\nif part_list is not None:\\nif complete_multipart_upload(s3_client, bucket_name, key_name, mpu_id,\\npart_list):\\nprint (f'{key_name} successfully uploaded through a ultipart upload\\nto {bucket_name}')\\nelse:\\nprint (f'Could not upload {key_name} hrough a multipart upload to\\n{bucket_name}')\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api complete-multipart-upload --bucket bucket-base-name--azid--x-s3 --\\nkey KEY_NAME --upload-id\\n\"AS_mgt9RaQE9GEaifATue15dAAAAAAAAAAEMAAAAAAAAADQwNzI4MDU0MjUyMBYAAAAAAAAAAA0AAAAAAAAAAA\\n--multipart-upload file://parts.json', 'H'], ['', '', '']], [['', '', ''], ['', 'parts.json\\n{\\n\"Parts\": [\\n{\\n\"ETag\": \"6b78c4a64dd641a58dac8d9258b88147\",\\n\"PartNumber\": 1\\n}\\n]', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', 'public static void abortMultiPartUploads( S3Client s3, String bucketName ) {\\ntry {\\nListMultipartUploadsRequest listMultipartUploadsRequest =\\nListMultipartUploadsRequest.builder()\\n.bucket(bucketName)\\n.build();\\nListMultipartUploadsResponse response =\\ns3.listMultipartUploads(listMultipartUploadsRequest);\\nListMultipartUpload uploads = response.uploads();\\nAbortMultipartUploadRequest abortMultipartUploadRequest;\\nfor (MultipartUpload upload: uploads) {\\nabortMultipartUploadRequest = AbortMultipartUploadRequest.builder()\\n.bucket(bucketName)\\n.key(upload.key())\\n.uploadId(upload.uploadId())\\n.build();\\ns3.abortMultipartUpload(abortMultipartUploadRequest);\\n}\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.getMessage());\\nSystem.exit(1);', '']]]\n",
      "[[['', '}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"import logging\\nimport boto3\\nfrom botocore.exceptions import ClientError\\ndef abort_multipart_upload(s3_client, bucket_name, key_name, upload_id):\\n'''\\nAborts a partial multipart upload in a directory bucket.\\n:param s3_client: boto3 S3 client\\n:param bucket_name: Bucket where the multipart upload was initiated - for\\nexample, 'doc-example-bucket--usw2-az1--x-s3'\\n:param key_name: Name of the object for which the multipart upload needs to be\\naborted\\n:param upload_id: Multipart upload ID for the multipart upload to be aborted\\n:return: True if the multipart upload was successfully aborted, False if not\\n'''\\ntry:\\ns3_client.abort_multipart_upload(\\nBucket = bucket_name,\\nKey = key_name,\\nUploadId = upload_id\\n)\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn False\\nreturn True\\nif __name__ == '__main__':\\nregion = 'us-west-2'\\nbucket_name = 'BUCKET_NAME'\\nkey_name = 'KEY_NAME'\\nupload_id = 'UPLOAD_ID'\\ns3_client = boto3.client('s3', region_name = region)\", '']]]\n",
      "[[['', \"if abort_multipart_upload(s3_client, bucket_name, key_name, upload_id):\\nprint (f'Multipart upload for object {key_name} in {bucket_name} bucket has\\nbeen aborted')\\nelse:\\nprint (f'Unable to abort multipart upload for object {key_name} in\\n{bucket_name} bucket')\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api abort-multipart-upload --bucket bucket-base-name--azid--x-s3 --key KEY_NAME\\n--upload-id\\n\"AS_mgt9RaQE9GEaifATue15dAAAAAAAAAAEMAAAAAAAAADQwNzI4MDU0MjUyMBYAAAAAAAAAAA0AAAAAAAAAAA\\nMAQAAAAB0OxUFeA7LTbWWFS8WYwhrxDxTIDN-pdEEq_agIHqsbg\"', 'H'], ['', '', '']], [['', '', ''], ['', '/**\\n* This method creates a multipart upload request that generates a unique upload ID\\nthat is used to track\\n* all the upload parts.\\n*\\n* @param s3\\n* @param bucketName\\n* @param key\\n* @return', '']]]\n",
      "[[['', '*/\\nprivate static String createMultipartUpload(S3Client s3, String bucketName, String\\nkey) {\\nCreateMultipartUploadRequest createMultipartUploadRequest =\\nCreateMultipartUploadRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.build();\\nString uploadId = null;\\ntry {\\nCreateMultipartUploadResponse response =\\ns3.createMultipartUpload(createMultipartUploadRequest);\\nuploadId = response.uploadId();\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\nreturn uploadId;\\n}\\n/**\\n* Creates copy parts based on source object size and copies over individual parts\\n*\\n* @param s3\\n* @param sourceBucket\\n* @param sourceKey\\n* @param destnBucket\\n* @param destnKey\\n* @param uploadId\\n* @return\\n* @throws IOException\\n*/\\npublic static ListCompletedPart multipartUploadCopy(S3Client s3, String\\nsourceBucket, String sourceKey, String destnBucket, String destnKey, String\\nuploadId) throws IOException {\\n// Get the object size to track the end of the copy operation.\\nHeadObjectRequest headObjectRequest = HeadObjectRequest\\n.builder()\\n.bucket(sourceBucket)\\n.key(sourceKey)\\n.build();\\nHeadObjectResponse response = s3.headObject(headObjectRequest);\\nLong objectSize = response.contentLength();', '']]]\n",
      "[[['', 'System.out.println(\"Source Object size: \" + objectSize);\\n// Copy the object using 20 MB parts.\\nlong partSize = 20 * 1024 * 1024;\\nlong bytePosition = 0;\\nint partNum = 1;\\nListCompletedPart completedParts = new ArrayList<>();\\nwhile (bytePosition < objectSize) {\\n// The last part might be smaller than partSize, so check to make sure\\n// that lastByte isn\\'t beyond the end of the object.\\nlong lastByte = Math.min(bytePosition + partSize - 1, objectSize - 1);\\nSystem.out.println(\"part no: \" + partNum + \", bytePosition: \" +\\nbytePosition + \", lastByte: \" + lastByte);\\n// Copy this part.\\nUploadPartCopyRequest req = UploadPartCopyRequest.builder()\\n.uploadId(uploadId)\\n.sourceBucket(sourceBucket)\\n.sourceKey(sourceKey)\\n.destinationBucket(destnBucket)\\n.destinationKey(destnKey)\\n.copySourceRange(\"bytes=\"+bytePosition+\"-\"+lastByte)\\n.partNumber(partNum)\\n.build();\\nUploadPartCopyResponse res = s3.uploadPartCopy(req);\\nCompletedPart part = CompletedPart.builder()\\n.partNumber(partNum)\\n.eTag(res.copyPartResult().eTag())\\n.build();\\ncompletedParts.add(part);\\npartNum++;\\nbytePosition += partSize;\\n}\\nreturn completedParts;\\n}\\npublic static void multipartCopyUploadTest(S3Client s3, String srcBucket, String\\nsrcKey, String destnBucket, String destnKey) {\\nSystem.out.println(\"Starting multipart copy for: \" + srcKey);\\ntry {\\nString uploadId = createMultipartUpload(s3, destnBucket, destnKey);', '']]]\n",
      "[[['', 'System.out.println(uploadId);\\nListCompletedPart parts = multipartUploadCopy(s3, srcBucket,\\nsrcKey,destnBucket, destnKey, uploadId);\\ncompleteMultipartUpload(s3, destnBucket, destnKey, uploadId, parts);\\nSystem.out.println(\"Multipart copy completed for: \" + srcKey);\\n} catch (Exception e) {\\nSystem.err.println(e.getMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"import logging\\nimport boto3\\nfrom botocore.exceptions import ClientError\\ndef head_object(s3_client, bucket_name, key_name):\\n'''\\nReturns metadata for an object in a directory bucket\\n:param s3_client: boto3 S3 client\\n:param bucket_name: Bucket that contains the object to query for metadata\\n:param key_name: Key name to query for metadata\\n:return: Metadata for the specified object if successful, else None\\n'''\\ntry:\\nresponse = s3_client.head_object(\\nBucket = bucket_name,\\nKey = key_name\\n)\\nreturn response\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn None\\ndef create_multipart_upload(s3_client, bucket_name, key_name):\\n'''\", '']]]\n",
      "[[['', \"Create a multipart upload to a directory bucket\\n:param s3_client: boto3 S3 client\\n:param bucket_name: Destination bucket for the multipart upload\\n:param key_name: Key name of the object to be uploaded\\n:return: UploadId for the multipart upload if created successfully, else None\\n'''\\ntry:\\nmpu = s3_client.create_multipart_upload(Bucket = bucket_name, Key =\\nkey_name)\\nreturn mpu['UploadId']\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn None\\ndef multipart_copy_upload(s3_client, source_bucket_name, key_name,\\ntarget_bucket_name, mpu_id, part_size):\\n'''\\nCopy an object in a directory bucket to another bucket in multiple parts of a\\nspecified size\\n:param s3_client: boto3 S3 client\\n:param source_bucket_name: Bucket where the source object exists\\n:param key_name: Key name of the object to be copied\\n:param target_bucket_name: Destination bucket for copied object\\n:param mpu_id: The UploadId returned from the create_multipart_upload call\\n:param part_size: The size parts that the object will be broken into, in bytes.\\nMinimum 5 MiB, Maximum 5 GiB. There is no minimum size for the\\nlast part of your multipart upload.\\n:return: part_list for the multipart copy if all parts are copied successfully,\\nelse None\\n'''\\npart_list = []\\ncopy_source = {\\n'Bucket': source_bucket_name,\\n'Key': key_name\\n}\\ntry:\\npart_counter = 1\\nobject_size = head_object(s3_client, source_bucket_name, key_name)\\nif object_size is not None:\\nobject_size = object_size['ContentLength']\", '']]]\n",
      "[[['', \"while (part_counter - 1) * part_size <object_size:\\nbytes_start = (part_counter - 1) * part_size\\nbytes_end = (part_counter * part_size) - 1\\nupload_copy_part = s3_client.upload_part_copy (\\nBucket = target_bucket_name,\\nCopySource = copy_source,\\nCopySourceRange = f'bytes={bytes_start}-{bytes_end}',\\nKey = key_name,\\nPartNumber = part_counter,\\nUploadId = mpu_id\\n)\\npart_list.append({'PartNumber': part_counter, 'ETag':\\nupload_copy_part['CopyPartResult']['ETag']})\\npart_counter += 1\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn None\\nreturn part_list\\ndef complete_multipart_upload(s3_client, bucket_name, key_name, mpu_id, part_list):\\n'''\\nCompletes a multipart upload to a directory bucket\\n:param s3_client: boto3 S3 client\\n:param bucket_name: Destination bucket for the multipart upload\\n:param key_name: Key name of the object to be uploaded\\n:param mpu_id: The UploadId returned from the create_multipart_upload call\\n:param part_list: List of uploaded part numbers with associated ETags\\n:return: True if the multipart upload was completed successfully, else False\\n'''\\ntry:\\ns3_client.complete_multipart_upload(\\nBucket = bucket_name,\\nKey = key_name,\\nUploadId = mpu_id,\\nMultipartUpload = {\\n'Parts': part_list\\n}\\n)\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn False\\nreturn True\", '']]]\n",
      "[[['', \"if __name__ == '__main__':\\nMB = 1024 ** 2\\nregion = 'us-west-2'\\nsource_bucket_name = 'SOURCE_BUCKET_NAME'\\ntarget_bucket_name = 'TARGET_BUCKET_NAME'\\nkey_name = 'KEY_NAME'\\npart_size = 10 * MB\\ns3_client = boto3.client('s3', region_name = region)\\nmpu_id = create_multipart_upload(s3_client, target_bucket_name, key_name)\\nif mpu_id is not None:\\npart_list = multipart_copy_upload(s3_client, source_bucket_name, key_name,\\ntarget_bucket_name, mpu_id, part_size)\\nif part_list is not None:\\nif complete_multipart_upload(s3_client, target_bucket_name, key_name,\\nmpu_id, part_list):\\nprint (f'{key_name} successfully copied through multipart copy from\\n{source_bucket_name} to {target_bucket_name}')\\nelse:\\nprint (f'Could not copy {key_name} through multipart copy from\\n{source_bucket_name} to {target_bucket_name}')\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api upload-part-copy --bucket bucket-base-name--azid--x-s3 --key TARGET_KEY_NAME\\n--copy-source SOURCE_BUCKET_NAME/SOURCE_KEY_NAME --part-number 1 --upload-id\\n\"AS_mgt9RaQE9GEaifATue15dAAAAAAAAAAEMAAAAAAAAADQwNzI4MDU0MjUyMBYAAAAAAAAAAA0AAAAAAAAAAA', 'H'], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public static void listMultiPartUploads( S3Client s3, String bucketName) {\\ntry {\\nListMultipartUploadsRequest listMultipartUploadsRequest =\\nListMultipartUploadsRequest.builder()\\n.bucket(bucketName)\\n.build();\\nListMultipartUploadsResponse response =\\ns3.listMultipartUploads(listMultipartUploadsRequest);\\nList MultipartUpload uploads = response.uploads();\\nfor (MultipartUpload upload: uploads) {\\nSystem.out.println(\"Upload in progress: Key = \\\\\"\" + upload.key() +\\n\"\\\\\", id = \" + upload.uploadId());\\n}\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.getMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import logging\\nimport boto3\\nfrom botocore.exceptions import ClientError\\ndef list_multipart_uploads(s3_client, bucket_name):', '']]]\n",
      "[[['', \"'''\\nList any incomplete multipart uploads in a directory bucket in e specified gion\\n:param s3_client: boto3 S3 client\\n:param bucket_name: Bucket to check for incomplete multipart uploads\\n:return: List of incomplete multipart uploads if there are any, None if not\\n'''\\ntry:\\nresponse = s3_client.list_multipart_uploads(Bucket = bucket_name)\\nif 'Uploads' in response.keys():\\nreturn response['Uploads']\\nelse:\\nreturn None\\nexcept ClientError as e:\\nlogging.error(e)\\nif __name__ == '__main__':\\nbucket_name = 'BUCKET_NAME'\\nregion = 'us-west-2'\\ns3_client = boto3.client('s3', region_name = region)\\nmultipart_uploads = list_multipart_uploads(s3_client, bucket_name)\\nif multipart_uploads is not None:\\nprint (f'There are {len(multipart_uploads)} ncomplete multipart uploads for\\n{bucket_name}')\\nelse:\\nprint (f'There are no incomplete multipart uploads for {bucket_name}')\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-multipart-uploads --bucket bucket-base-name--azid--x-s3', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public static void listMultiPartUploadsParts( S3Client s3, String bucketName, String\\nobjKey, String uploadID) {\\ntry {\\nListPartsRequest listPartsRequest = ListPartsRequest.builder()\\n.bucket(bucketName)\\n.uploadId(uploadID)\\n.key(objKey)\\n.build();\\nListPartsResponse response = s3.listParts(listPartsRequest);\\nListPart parts = response.parts();\\nfor (Part part: parts) {\\nSystem.out.println(\"Upload in progress: Part number = \\\\\"\" +\\npart.partNumber() + \"\\\\\", etag = \" + part.eTag());\\n}\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.getMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import logging\\nimport boto3\\nfrom botocore.exceptions import ClientError', '']]]\n",
      "[[['', \"def list_parts(s3_client, bucket_name, key_name, upload_id):\\n'''\\nLists the parts that have been uploaded for a specific multipart upload to a\\ndirectory bucket.\\n:param s3_client: boto3 S3 client\\n:param bucket_name: Bucket that multipart uploads parts have been uploaded to\\n:param key_name: Name of the object that has parts uploaded\\n:param upload_id: Multipart upload ID that the parts are associated with\\n:return: List of parts associated with the specified multipart upload, None if\\nthere are no parts\\n'''\\nparts_list = []\\nnext_part_marker = ''\\ncontinuation_flag = True\\ntry:\\nwhile continuation_flag:\\nif next_part_marker == '':\\nresponse = s3_client.list_parts(\\nBucket = bucket_name,\\nKey = key_name,\\nUploadId = upload_id\\n)\\nelse:\\nresponse = s3_client.list_parts(\\nBucket = bucket_name,\\nKey = key_name,\\nUploadId = upload_id,\\nNextPartMarker = next_part_marker\\n)\\nif 'Parts' in response:\\nfor part in response['Parts']:\\nparts_list.append(part)\\nif response['IsTruncated']:\\nnext_part_marker = response['NextPartNumberMarker']\\nelse:\\ncontinuation_flag = False\\nelse:\\ncontinuation_flag = False\\nreturn parts_list\\nexcept ClientError as e:\\nlogging.error(e)\\nreturn None\", '']]]\n",
      "[[['', \"if __name__ == '__main__':\\nregion = 'us-west-2'\\nbucket_name = 'BUCKET_NAME'\\nkey_name = 'KEY_NAME'\\nupload_id = 'UPLOAD_ID'\\ns3_client = boto3.client('s3', region_name = region)\\nparts_list = list_parts(s3_client, bucket_name, key_name, upload_id)\\nif parts_list is not None:\\nprint (f'{key_name} has {len(parts_list)} parts uploaded to {bucket_name}')\\nelse:\\nprint (f'There are no multipart uploads with that upload ID for\\n{bucket_name} bucket')\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-parts --bucket bucket-base-name--azid--x-s3 --key KEY_NAME --upload-id\\n\"AS_mgt9RaQE9GEaifATue15dAAAAAAAAAAEMAAAAAAAAADQwNzI4MDU0MjUyMBYAAAAAAAAAAA0AAAAAAAAAAA', 'H'], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nEven if you opt to use the same checksum function, your checksum value might change\\nif the object is over 16 MB in size. The checksum value might change because of how\\nchecksums are calculated for multipart uploads. For more information about how the\\nchecksum might change when copying the object, see Using part-level checksums for\\nmultipart uploads.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nEven if you opt to use the same checksum function, your checksum value might change\\nif the object is over 16 MB in size. The checksum value might change because of how\\nchecksums are calculated for multipart uploads. For more information about how the\\nchecksum might change when copying the object, see Using part-level checksums for\\nmultipart uploads.', ''], ['', '', '']], [['', '', ''], ['', 'public static void copyBucketObject (S3Client s3, String sourceBucket, String\\nobjectKey, String targetBucket) {\\nCopyObjectRequest copyReq = CopyObjectRequest.builder()\\n.sourceBucket(sourceBucket)\\n.sourceKey(objectKey)\\n.destinationBucket(targetBucket)\\n.destinationKey(objectKey)\\n.build();\\nString temp = \"\";\\ntry {\\nCopyObjectResponse copyRes = s3.copyObject(copyReq);\\nSystem.out.println(\"Successfully copied \" + objectKey +\" from bucket \" +\\nsourceBucket +\" into bucket \"+targetBucket);\\n}\\ncatch (S3Exception e) {', '']]]\n",
      "[[['', 'System.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api copy-object --copy-source bucket SOURCE_BUCKET/SOURCE_KEY_NAME --\\nkey TARGET_KEY_NAME --bucket TARGET_BUCKET_NAME', ''], ['', '', '']], [['', '', ''], ['', \"Warning\\n• Deleting an object can't be undone.\\n• This action deletes all specified objects. When deleting folders, wait for the delete action\\nto finish before adding new objects to the folder. Otherwise, new objects might be\\ndeleted as well.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen you programmatically delete multiple objects from a directory bucket, note the\\nfollowing:\\n• Object keys in DeleteObjects requests must contain at least one non-white space\\ncharacter. Strings of all white space characters are not supported.', '']]]\n",
      "[[['', '• Object keys in DeleteObjects requests cannot contain Unicode control characters,\\nexcept for newline (\\\\n), tab (\\\\t), and carriage return (\\\\r).', ''], ['', '', '']], [['', '', ''], ['', 'static void deleteObject(S3Client s3Client, String bucketName, String objectKey) {\\ntry {\\nDeleteObjectRequest del = DeleteObjectRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.build();', '']]]\n",
      "[[['', 's3Client.deleteObject(del);\\nSystem.out.println(\"Object \" + objectKey + \" has been deleted\");\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"import logging\\nimport boto3\\nfrom botocore.exceptions import ClientError\\ndef delete_objects(s3_client, bucket_name, objects):\\n'''\\nDelete a list of objects in a directory bucket\\n:param s3_client: boto3 S3 client\\n:param bucket_name: Bucket that contains objects to be deleted; for example,\\n'doc-example-bucket--usw2-az1--x-s3'\\n:param objects: List of dictionaries that specify the key names to delete\\n:return: Response output, else False\\n'''\\ntry:\\nresponse = s3_client.delete_objects(\\nBucket = bucket_name,\\nDelete = {\\n'Objects': objects\\n}\\n)\\nreturn response\\nexcept ClientError as e:\", '']]]\n",
      "[[['', 'logging.error(e)\\nreturn False\\nif __name__ == \\'__main__\\':\\nregion = \\'us-west-2\\'\\nbucket_name = \\'BUCKET_NAME\\'\\nobjects = [\\n{\\n\\'Key\\': \\'0.txt\\'\\n},\\n{\\n\\'Key\\': \\'1.txt\\'\\n},\\n{\\n\\'Key\\': \\'2.txt\\'\\n},\\n{\\n\\'Key\\': \\'3.txt\\'\\n},\\n{\\n\\'Key\\': \\'4.txt\\'\\n}\\n]\\ns3_client = boto3.client(\\'s3\\', region_name = region)\\nresults = delete_objects(s3_client, bucket_name, objects)\\nif results is not None:\\nif \\'Deleted\\' in results:\\nprint (f\\'Deleted {len(results[\"Deleted\"])} objects from {bucket_name}\\')\\nif \\'Errors\\' in results:\\nprint (f\\'Failed to delete {len(results[\"Errors\"])} objects from\\n{bucket_name}\\')', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-object --bucket bucket-base-name--azid--x-s3 --key KEY_NAME', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public static void getObject(S3Client s3Client, String bucketName, String objectKey)\\n{\\ntry {\\nGetObjectRequest objectRequest = GetObjectRequest\\n.builder()\\n.key(objectKey)\\n.bucket(bucketName)\\n.build();\\nResponseBytes GetObjectResponse objectBytes =\\ns3Client.getObjectAsBytes(objectRequest);\\nbyte[] data = objectBytes.asByteArray();\\n//Print object contents to console\\nString s = new String(data, StandardCharsets.UTF_8);\\nSystem.out.println(s);\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import boto3\\nfrom botocore.exceptions import ClientError\\nfrom botocore.response import StreamingBody\\ndef get_object(s3_client: boto3.client, bucket_name: str, key_name: str) ->\\nStreamingBody:\\n\"\"\"\\nGets the object.\\n:param s3_client:\\n:param bucket_name: The bucket that contains the object.\\n:param key_name: The key of the object to be downloaded.\\n:return: The object data in bytes.\\n\"\"\"\\ntry:\\nresponse = s3_client.get_object(Bucket=bucket_name, Key=key_name)\\nbody = response[\\'Body\\'].read()\\nprint(f\"Got object \\'{key_name}\\' from bucket \\'{bucket_name}\\'.\")\\nexcept ClientError:\\nprint(f\"Couldn\\'t get object \\'{key_name}\\' from bucket \\'{bucket_name}\\'.\")\\nraise\\nelse:\\nreturn body\\ndef main():\\ns3_client = boto3.client(\\'s3\\')\\nresp = get_object(s3_client, \\'doc-example-bucket--use1-az4--x-s3\\', \\'sample.txt\\')\\nprint(resp)\\nif __name__ == \"__main__\":\\nmain()', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-object --bucket bucket-base-name--azid--x-s3 --\\nkey KEY_NAME LOCAL_FILE_NAME', ''], ['', '', '']], [['', '', ''], ['', 'public static void headObject(S3Client s3Client, String bucketName, String\\nobjectKey) {\\ntry {\\nHeadObjectRequest headObjectRequest = HeadObjectRequest\\n.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.build();\\nHeadObjectResponse response = s3Client.headObject(headObjectRequest);\\nSystem.out.format(\"Amazon S3 object: \\\\\"%s\\\\\" found in bucket: \\\\\"%s\\\\\" with\\nETag: \\\\\"%s\\\\\"\", objectKey, bucketName, response.eTag());\\n}\\ncatch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api head-object --bucket bucket-base-name--azid--x-s3 --key KEY_NAME', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nMulti-factor authentication (MFA) delete and S3 Versioning are not supported for S3\\nExpress One Zone.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'arn:aws:s3express:region:account-id:bucket/base-bucket-name--azid--x-s3', ''], ['', '', '']]]\n",
      "[[['Action', 'API', 'Description', 'Access\\nlevel', 'Condition keys'], ['s3express\\n:CreateBu\\ncket', 'CreateBuc\\nket', 'Grants permission to create a\\nnew bucket.', 'Write', 's3express\\n:authType\\ns3express\\n:Location\\nName\\ns3express\\n:Resource\\nAccount\\ns3express\\n:signatur\\neversion\\ns3express\\n:TlsVersi\\non\\ns3express\\n:x-amz-co\\nntent-sha\\n256']]]\n",
      "[[['Action', 'API', 'Description', 'Access\\nlevel', 'Condition keys'], ['s3express\\n:CreateSe\\nssion', 'CreateSes\\nsion', 'Grants permission to create\\na session token, which is\\nused for granting access to\\nall Zonal (object-level) API\\noperations, such as PutObject ,\\nGetObject , and so on.', 'Write', 's3express\\n:authType\\ns3express\\n:SessionM\\node\\ns3express\\n:Resource\\nAccount\\ns3express\\n:signatur\\neversion\\ns3express\\n:signatur\\neAge\\ns3express\\n:TlsVersi\\non\\ns3express\\n:x-amz-co\\nntent-sha\\n256']]]\n",
      "[[['Action', 'API', 'Description', 'Access\\nlevel', 'Condition keys'], ['s3express\\n:DeleteBu\\ncket', 'DeleteBuc\\nket', 'Grants permission to delete the\\nbucket named in the URI.', 'Write', 's3express\\n:authType\\ns3express\\n:Resource\\nAccount\\ns3express\\n:signatur\\neversion\\ns3express\\n:TlsVersi\\non\\ns3express\\n:x-amz-co\\nntent-sha\\n256']]]\n",
      "[[['Action', 'API', 'Description', 'Access\\nlevel', 'Condition keys'], ['s3express\\n:DeleteBu\\ncketPolicy', 'DeleteBuc\\nketPolicy', 'Grants permission to delete the\\npolicy on a specified bucket.', 'Permissi\\nns\\nmanage\\nt', 'o s3express\\n:authType\\nmen\\ns3express\\n:Resource\\nAccount\\ns3express\\n:signatur\\neversion\\ns3express\\n:TlsVersi\\non\\ns3express\\n:x-amz-co\\nntent-sha\\n256']]]\n",
      "[[['Action', 'API', 'Description', 'Access\\nlevel', 'Condition keys'], ['s3express\\n:GetBucke\\ntPolicy', 'GetBucket\\nPolicy', 'Grants permission to return the\\npolicy of the specified bucket.', 'Read', 's3express\\n:authType\\ns3express\\n:Resource\\nAccount\\ns3express\\n:signatur\\neversion\\ns3express\\n:TlsVersi\\non\\ns3express\\n:x-amz-co\\nntent-sha\\n256']]]\n",
      "[[['Action', 'API', 'Description', 'Access\\nlevel', 'Condition keys'], ['s3express\\n:ListAllM\\nyDirector\\nyBuckets', 'ListDirec\\ntoryBucke\\nts', 'Grants permission to list all\\ndirectory buckets owned by the\\nauthenticated sender of the\\nrequest.', 'List', 's3express\\n:authType\\ns3express\\n:Resource\\nAccount\\ns3express\\n:signatur\\neversion\\ns3express\\n:TlsVersi\\non\\ns3express\\n:x-amz-co\\nntent-sha\\n256']]]\n",
      "[[['Action', 'API', 'Description', 'Access\\nlevel', 'Condition keys'], ['s3express\\n:PutBucke\\ntPolicy', 'PutBucket\\nPolicy', 'Grants permission to add or\\nreplace a bucket policy on a\\nbucket.', 'Permissi\\nns\\nmanage\\nt', 'o s3express\\n:authType\\nmen\\ns3express\\n:Resource\\nAccount\\ns3express\\n:signatur\\neversion\\ns3express\\n:TlsVersi\\non\\ns3express\\n:x-amz-co\\nntent-sha\\n256']], [['Condition key', 'Description', 'Type'], ['s3express:authType', 'Filters access by authentication method. To\\nrestrict incoming requests to use a specific\\nauthentication method, you can use this\\noptional condition key. For example, you can\\nuse this condition key to allow only the HTTP', 'String']]]\n",
      "[[['Condition key', 'Description', 'Type'], ['', 'Authorization header to be used in request\\nauthentication.\\nValid values: REST-HEADER , REST-QUERY-\\nSTRING', ''], ['s3express:Location\\nName', 'Filters access to the CreateBucket API\\noperation by a specific Availability Zone ID (AZ\\nID), for example, usw2-az1.\\nExample value: usw2-az1', 'String'], ['s3express:Resource\\nAccount', \"Filters access by the resource owner's AWS\\naccount ID.\\nTo restrict user, role, or application access to\\nthe directory buckets that are owned by a\\nspecific AWS account ID, you can use either the\\naws:ResourceAccount or s3express\\n:ResourceAccount condition key. You can\\nuse this condition key in either AWS Identity\\nand Access Management (IAM) identity policies\\nor virtual private cloud (VPC) endpoint policies.\\nFor example, you can use this condition key to\\nrestrict clients within your VPC from accessing\\nbuckets that you don't own.\\nExample value: 111122223333\", 'String']]]\n",
      "[[['Condition key', 'Description', 'Type'], ['s3express:SessionMode', 'Filters access by the permission requested\\nby the CreateSession API operation.\\nBy default, the session is ReadWrite . You\\ncan use this condition key to limit access to\\nReadOnly or to explicitly deny ReadWrite\\naccess. For more information, see Example\\ndirectory bucket policies for S3 Express One\\nZone and CreateSession in the Amazon Simple\\nStorage Service API Reference.\\nValid values: ReadWrite , ReadOnly', 'String'], ['s3express:signatur\\neAge', 'Filters access by the age in milliseconds of the\\nrequest signature. This condition works only for\\npresigned URLs.\\nIn AWS Signature Version 4, the signing key\\nis valid for up to seven days. Therefore, the\\nsignatures are also valid for up to seven\\ndays. For more information, see Introduct\\nion to signing requests in the Amazon Simple\\nStorage Service API Reference. You can use this\\ncondition to further limit the signature age.\\nExample value: 600000', 'Numeric'], ['s3express:signatur\\neversion', 'Identifies the version of AWS Signature\\nthat you want to support for authenticated\\nrequests. For authenticated requests, S3\\nExpress One Zone supports Signature Version\\n4.\\nValid value: \"AWS4-HMAC-SHA256\" (identifi\\nes Signature Version 4)', 'String']]]\n",
      "[[['Condition key', 'Description', 'Type'], ['s3express:TlsVersion', \"Filters access by the TLS version that's used by\\nthe client.\\nYou can use the s3:TlsVersion condition\\nkey to write IAM, virtual private cloud endpoint\\n(VPCE), or bucket policies that restrict user or\\napplication access to directory buckets based\\non the TLS version that's used by the client.\\nYou can also use this condition key to write\\npolicies that require a minimum TLS version.\\nExample value: 1.3\", 'Numeric']]]\n",
      "[[['Condition key', 'Description', 'Type'], ['s3express:x-amz-co\\nntent-sha256', \"Filters access by unsigned content in your\\nbucket.\\nYou can use this condition key to disallow\\nunsigned content in your bucket.\\nWhen you use Signature Version 4 for requests\\nthat use the Authorization header, you\\nadd the x-amz-content-sha256 header in\\nthe signature calculation and then set its value\\nto the hash payload.\\nYou can use this condition key in your bucket\\npolicy to deny any uploads where the payloads\\naren't signed. For example:\\n• Deny uploads that use the Authorization\\nheader to authenticate requests but don't\\nsign the payload. For more information, see\\nTransferring payload in a single chunk in the\\nAmazon Simple Storage Service API Reference.\\n• Deny uploads that use presigned URLs.\\nPresigned URLs always have an UNSIGNED_\\nPAYLOAD . For more information, see\\nAuthenticating requests and Authentication\\nmethods in the Amazon Simple Storage\\nService API Reference.\\nValid value: UNSIGNED-PAYLOAD\", 'String']]]\n",
      "[[['API', 'Endpoint\\ntype', 'IAM action', 'Cross-account\\naccess'], ['CreateBucket', 'Regional', 's3express:CreateBucket', 'No'], ['DeleteBucket', 'Regional', 's3express:DeleteBucket', 'No'], ['ListDirec\\ntoryBuckets', 'Regional', 's3express:ListAllM\\nyDirectoryBuckets', 'No'], ['PutBucket\\nPolicy', 'Regional', 's3express:PutBucke\\ntPolicy', 'No'], ['GetBucket\\nPolicy', 'Regional', 's3express:GetBucke\\ntPolicy', 'No'], ['DeleteBuc\\nketPolicy', 'Regional', 's3express:DeleteBu\\ncketPolicy', 'No'], ['CreateSession', 'Zonal', 's3express:CreateSession', 'Yes'], ['CopyObject', 'Zonal', 's3express:CreateSession', 'Yes'], ['DeleteObject', 'Zonal', 's3express:CreateSession', 'Yes'], ['DeleteObjects', 'Zonal', 's3express:CreateSession', 'Yes'], ['HeadObject', 'Zonal', 's3express:CreateSession', 'Yes'], ['PutObject', 'Zonal', 's3express:CreateSession', 'Yes'], ['GetObject\\nAttributes', 'Zonal', 's3express:CreateSession', 'Yes'], ['ListObjectsV2', 'Zonal', 's3express:CreateSession', 'Yes'], ['HeadBucket', 'Zonal', 's3express:CreateSession', 'Yes']]]\n",
      "[[['API', 'Endpoint\\ntype', 'IAM action', 'Cross-account\\naccess'], ['CreateMul\\ntipartUpload', 'Zonal', 's3express:CreateSession', 'Yes'], ['UploadPart', 'Zonal', 's3express:CreateSession', 'Yes'], ['UploadPar\\ntCopy', 'Zonal', 's3express:CreateSession', 'Yes'], ['CompleteM\\nultipartU\\npload', 'Zonal', 's3express:CreateSession', 'Yes'], ['AbortMult\\nipartUpload', 'Zonal', 's3express:CreateSession', 'Yes'], ['ListParts', 'Zonal', 's3express:CreateSession', 'Yes'], ['ListMulti\\npartUploads', 'Zonal', 's3express:CreateSession', 'Yes']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [', '']]]\n",
      "[[['', '{\\n\"Sid\": \"AllowAccessRegionalEndpointAPIs\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3express:DeleteBucket\",\\n\"s3express:DeleteBucketPolicy\",\\n\"s3express:CreateBucket\",\\n\"s3express:PutBucketPolicy\",\\n\"s3express:GetBucketPolicy\",\\n\"s3express:ListAllMyDirectoryBuckets\"\\n],\\n\"Resource\": \"arn:aws:s3express:region:account_id:bucket/bucket-base-\\nname--azid--x-s3/*\"\\n},\\n{\\n\"Sid\": \"AllowCreateSession\",\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3express:CreateSession\",\\n\"Resource\": \"*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"ReadWriteAccess\",', '']]]\n",
      "[[['', '\"Effect\": \"Allow\",\\n\"Resource\": \"arn:aws:s3express:us-west-2:account-id:bucket/bucket-base-\\nname--azid--x-s3\",\\n\"Principal\": {\\n\"AWS\": [\\n\"111122223333\"\\n]\\n},\\n\"Action\": [\\n\"s3express:CreateSession\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"ReadOnlyAccess\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"111122223333\"\\n},\\n\"Action\": \"s3express:CreateSession\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3express:SessionMode\": \"ReadOnly\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"CrossAccount\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"111122223333\"\\n},\\n\"Action\": [\\n\"s3express:CreateSession\"\\n],\\n\"Resource\": \"arn:aws:s3express:us-west-2:444455556666:bucket/bucket-base-\\nname--azid--x-s3\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't grant access to objects stored in directory buckets. You can grant access only\\nto your directory buckets. The authorization model for S3 Express One Zone is different\\nthan the authorization model for Amazon S3. For more information, see CreateSession\\nauthorization.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor S3 Express One Zone, CloudTrail logging of Zonal endpoint (object-level, or data plane)\\nAPI operations (for example, PutObject or GetObject) is not supported.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['Region name', 'Region code', 'Availability\\nZone ID'], ['US East (N. Virginia)', 'us-east-1', 'use1-az4'], [None, None, 'use1-az5'], [None, None, 'use1-az6'], ['US West (Oregon)', 'us-west-2', 'usw2-az1'], [None, None, 'usw2-az3'], [None, None, 'usw2-az4']]]\n",
      "[[['Region name', 'Region code', 'Availability\\nZone ID'], ['Asia Pacific (Tokyo)', 'ap-northe\\nast-1', 'apne1-az1'], [None, None, 'apne1-az4'], ['Europe (Stockholm)', 'eu-north-\\n1', 'eun1-az1'], [None, None, 'eun1-az2'], [None, None, 'eun1-az3']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\n• You can only use access points to perform operations on objects. You can't use access\\npoints to perform other Amazon S3 operations, such as modifying or deleting buckets.\\nFor a complete list of S3 operations that support access points, see Access point\\ncompatibility with AWS services.\\n• Access points work with some, but not all, AWS services and features. For example,\\nyou can't configure Cross-Region Replication to operate through an access point. For a\\ncomplete list of AWS services that are compatible with S3 access points, see Access point\\ncompatibility with AWS services.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nAdding an S3 access point to a bucket doesn't change the bucket's behavior when the\\nbucket is accessed directly through the bucket's name or Amazon Resource Name (ARN). All\\nexisting operations against the bucket will continue to work as before. Restrictions that you\\ninclude in an access point policy apply only to requests made through that access point.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nPermissions granted in an access point policy are effective only if the underlying bucket\\nalso allows the same access. You can accomplish this in two ways:\\n1. (Recommended) Delegate access control from the bucket to the access point, as\\ndescribed in Delegating access control to access points.', '']]]\n",
      "[[['', \"2. Add the same permissions contained in the access point policy to the underlying bucket's\\npolicy. The Example 1 access point policy example demonstrates how to modify the\\nunderlying bucket policy to allow the necessary access.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Jane\"\\n},\\n\"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\\n\"Resource\": \"arn:aws:s3:us-west-2:123456789012:accesspoint/my-access-point/\\nobject/Jane/*\"\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor the access point policy to effectively grant access to Jane, the underlying bucket must\\nalso allow the same access to Jane. You can delegate access control from the bucket to the\\naccess point as described in Delegating access control to access points. Or, you can add the\\nfollowing policy to the underlying bucket to grant the necessary permissions to Jane. Note\\nthat the Resource entry differs between the access point and bucket policies.\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Jane\"', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Jane\"', '']]]\n",
      "[[['', '},\\n\"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/Jane/*\"\\n}]\\n}', ''], ['', '', '']], [['', '},\\n\"Action\": [\"s3:GetObject\", \"s3:PutObject\"],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/Jane/*\"\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\" : {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Mateo\"\\n},\\n\"Action\":\"s3:GetObject\",\\n\"Resource\" : \"arn:aws:s3:us-west-2:123456789012:accesspoint/my-access-point/\\nobject/*\",\\n\"Condition\" : {\\n\"StringEquals\": {\\n\"s3:ExistingObjectTag/data\": \"finance\"\\n}\\n}\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\": [', '']]]\n",
      "[[['', '{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Arnav\"\\n},\\n\"Action\": \"s3:ListBucket\",\\n\"Resource\": \"arn:aws:s3:us-west-2:123456789012:accesspoint/my-access-point\"\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Deny\",\\n\"Action\": \"s3:CreateAccessPoint\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"s3:AccessPointNetworkOrigin\": \"VPC\"\\n}\\n}\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Important\\nBefore using a statement like the one shown in this example, make sure that you don't\\nneed to use features that aren't supported by access points, such as Cross-Region\\nReplication.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": [\\n\"s3:AbortMultipartUpload\",\\n\"s3:BypassGovernanceRetention\",\\n\"s3:DeleteObject\",\\n\"s3:DeleteObjectTagging\",\\n\"s3:DeleteObjectVersion\",\\n\"s3:DeleteObjectVersionTagging\",\\n\"s3:GetObject\",\\n\"s3:GetObjectAcl\",\\n\"s3:GetObjectLegalHold\",\\n\"s3:GetObjectRetention\",\\n\"s3:GetObjectTagging\",\\n\"s3:GetObjectVersion\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:GetObjectVersionTagging\",\\n\"s3:ListMultipartUploadParts\",\\n\"s3:PutObject\",\\n\"s3:PutObjectAcl\",\\n\"s3:PutObjectLegalHold\",\\n\"s3:PutObjectRetention\",\\n\"s3:PutObjectTagging\",\\n\"s3:PutObjectVersionAcl\",\\n\"s3:PutObjectVersionTagging\",\\n\"s3:RestoreObject\"\\n],\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/*\",\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"s3:AccessPointNetworkOrigin\": \"VPC\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '\"Condition\" : {\\n\"StringLike\": {\\n\"s3:DataAccessPointArn\": \"arn:aws:s3:us-west-2:123456789012:accesspoint/*\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"Condition\" : {\\n\"StringEquals\": {\\n\"s3:DataAccessPointAccount\": \"123456789012\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"Condition\" : {\\n\"StringEquals\": {\\n\"s3:AccessPointNetworkOrigin\": \"VPC\"\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\" : [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\" : { \"AWS\": \"*\" },\\n\"Action\" : \"*\",\\n\"Resource\" : [ \"Bucket ARN\", \"Bucket ARN/*\"],\\n\"Condition\": {\\n\"StringEquals\" : { \"s3:DataAccessPointAccount\" : \"Bucket owner\\'s account\\nID\" }\\n}\\n}]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\" : [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\" : { \"AWS\": \"*\" },\\n\"Action\" : [\"s3:GetObject\",\"s3:ListBucket\"],\\n\"Resource\" : [ \"Bucket ARN\", \"Bucket ARN/*\"],\\n\"Condition\": {\\n\"StringEquals\" : { \"s3:DataAccessPointAccount\" : \"Access point owner\\'s\\naccount ID\" }\\n}\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nBecause you might want to publicize your access point name so that other users can use\\nthe access point, avoid including sensitive information in the access point name. Access\\npoint names are published in a publicly accessible database known as the Domain Name\\nSystem (DNS).', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nIf you're using a bucket in a different AWS account, the bucket owner must update\\nthe bucket policy to authorize requests from the access point. For an example bucket\\npolicy, see Granting permissions for cross-account access points.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nAfter you create an access point, you can't change its block public access settings.\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-access-point --name example-ap --account-id 111122223333 --\\nbucket DOC-EXAMPLE-BUCKET', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-access-point --name example-ap --account-id 111122223333 --\\nbucket DOC-EXAMPLE-BUCKET --bucket-account-id 444455556666', ''], ['', '', '']], [['', '', ''], ['', \"Important\\nYou can only specify an access point's network origin when you create the access point.\\nAfter you create the access point, you can't change its network origin.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-access-point --name example-vpc-ap --account-id 123456789012 --\\nbucket example-bucket --vpc-configuration VpcId=vpc-1a2b3c', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control get-access-point --name example-vpc-ap --account-id 123456789012\\n{\\n\"Name\": \"example-vpc-ap\",\\n\"Bucket\": \"example-bucket\",\\n\"NetworkOrigin\": \"VPC\",\\n\"VpcConfiguration\": {\\n\"VpcId\": \"vpc-1a2b3c\"\\n},\\n\"PublicAccessBlockConfiguration\": {\\n\"BlockPublicAcls\": true,\\n\"IgnorePublicAcls\": true,\\n\"BlockPublicPolicy\": true,\\n\"RestrictPublicBuckets\": true\\n},\\n\"CreationDate\": \"2019-11-27T00:00:00Z\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo make resources accessible only within a VPC, make sure to create a private hosted zone\\nfor your VPC endpoint. To use a private hosted zone, modify your VPC settings so that the\\nVPC network attributes enableDnsHostnames and enableDnsSupport are set to true.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Principal\": \"*\",\\n\"Action\": [\\n\"s3:GetObject\"\\n],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\\n\"arn:aws:s3:::awsexamplebucket1/*\",\\n\"arn:aws:s3:us-west-2:123456789012:accesspoint/example-vpc-ap/object/*\"\\n]\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe \"Resource\" declaration in this example uses an Amazon Resource Name (ARN) to\\nspecify the access point. For more information about access point ARNs, see Using access\\npoints.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\n• All block public access settings are enabled by default for access points. You must\\nexplicitly disable any settings that you don't want to apply to an access point.\\n• Amazon S3 currently doesn't support changing an access point's block public access\\nsettings after the access point has been created.\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-access-point --name example-ap --account-id\\n123456789012 --bucket example-bucket --public-access-block-configuration\\nBlockPublicAcls=false,IgnorePublicAcls=false,BlockPublicPolicy=true,RestrictPublicBu', 'c'], ['', '', '']], [['', '', ''], ['', 'aws s3control get-access-point --name example-ap --account-id 123456789012\\n{\\n\"Name\": \"example-ap\",\\n\"Bucket\": \"example-bucket\",\\n\"NetworkOrigin\": \"Internet\",\\n\"PublicAccessBlockConfiguration\": {\\n\"BlockPublicAcls\": false,\\n\"IgnorePublicAcls\": false,\\n\"BlockPublicPolicy\": true,\\n\"RestrictPublicBuckets\": true\\n},\\n\"CreationDate\": \"2019-11-27T00:00:00Z\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'https://AccessPointName-AccountId.s3-accesspoint.region.amazonaws.com', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\n• If your access point name includes dash (-) characters, include the dashes in the URL\\nand insert another dash before the account ID. For example, to use an access point\\nnamed finance-docs owned by account 123456789012 in Region us-west-2,\\nthe appropriate URL would be https://finance-docs-123456789012.s3-\\naccesspoint.us-west-2.amazonaws.com.\\n• S3 access points don't support access by HTTP, only secure access by HTTPS.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '\"resources\": [\\n{\"type\": \"AWS::S3::Object\",\\n\"ARN\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/my-image.jpg\"\\n},\\n{\"accountId\": \"123456789012\",', '']]]\n",
      "[[['', '\"type\": \"AWS::S3::Bucket\",\\n\"ARN\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET1\"\\n},\\n{\"accountId\": \"123456789012\",\\n\"type\": \"AWS::S3::AccessPoint\",\\n\"ARN\": \"arn:aws:s3:us-west-2:123456789012:accesspoint/my-bucket-ap\"\\n}\\n]', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\n• The console view always shows all objects in the bucket. Using an access point\\nas described in this procedure restricts the operations you can perform on those\\nobjects, but not whether you can see that they exist in the bucket.\\n• The S3 Management Console doesn't support using virtual private cloud (VPC)\\naccess points to access bucket resources. To access bucket resources from a VPC\\naccess point, use the AWS CLI, AWS SDKs, or Amazon S3 REST APIs.\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nYou can't change the Block Public Access settings for an access point after the access\\npoint is created.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nThe -s3alias suffix is reserved for access point alias names and can't be used for bucket\\nor access point names. For more information about Amazon S3 bucket-naming rules, see\\nBucket naming rules.\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-access-point --bucket example-s3-bucket1 --name my-access-point --\\naccount-id 111122223333\\n{\\n\"AccessPointArn\":\\n\"arn:aws:s3:region:111122223333:accesspoint/my-access-point\",\\n\"Alias\": \"my-access-point-aqfqprnstn7aefdfbarligizwgyfouse1a-s3alias\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object --bucket my-access-point-aqfqprnstn7aefdfbarligizwgyfouse1a-\\ns3alias --key dir/my_data.rtf my_data.rtf\\n{\\n\"AcceptRanges\": \"bytes\",\\n\"LastModified\": \"2020-01-08T22:16:28+00:00\",\\n\"ContentLength\": 910,\\n\"ETag\": \"\\\\\"00751974dc146b76404bb7290f8f51bb\\\\\"\",\\n\"VersionId\": \"null\",\\n\"ContentType\": \"text/rtf\",', '']]]\n",
      "[[['', '\"Metadata\": {}\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3api get-object --key my-image.jpg --bucket arn:aws:s3:us-\\nwest-2:123456789012:accesspoint/prod download.jpg', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket my-access-point-hrzrlukc5m36ft7okagglf3gmwluquse1b-\\ns3alias --key my-image.jpg --body my-image.jpg', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api delete-object --bucket arn:aws:s3:us-west-2:123456789012:accesspoint/prod\\n--key my-image.jpg', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-objects-v2 --bucket my-access-point-\\nhrzrlukc5m36ft7okagglf3gmwluquse1b-s3alias', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object-tagging --bucket arn:aws:s3:us-west-2:123456789012:accesspoint/\\nprod --key my-image.jpg --tagging TagSet=[{Key=\"finance\",Value=\"true\"}]', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-object-acl --bucket arn:aws:s3:us-west-2:123456789012:accesspoint/prod\\n--key my-image.jpg --acl private', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor an application or user to be able to access an object through a Multi-Region Access\\nPoint, both of the following policies must permit the request:\\n• The access policy for the Multi-Region Access Point\\n• The access policy for the underlying bucket that contains the object\\nWhen the two policies are different, the more restrictive policy takes precedence.\\nTo simplify permissions management for Multi-Region Access Points, you can delegate\\naccess control from the bucket to the Multi-Region Access Point. For more information, see\\nthe section called “Multi-Region Access Point policy examples”.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\n• You can specify the buckets that are associated with a Multi-Region Access Point only at\\nthe time that you create it. After it is created, you can’t add, modify, or remove buckets\\nfrom the Multi-Region Access Point configuration. To change the buckets, you must\\ndelete the entire Multi-Region Access Point and create a new one.\\n• You can't delete a bucket that is part of a Multi-Region Access Point. If you want to delete\\na bucket that's attached to a Multi-Region Access Point, delete the Multi-Region Access\\nPoint first.\\n• If you add a bucket that's owned by another account to your Multi-Region Access Point,\\nthe bucket owner must also update their bucket policy to grant access permissions to\\nthe Multi-Region Access Point. Otherwise, the Multi-Region Access Point won't be able to\\nretrieve data from that bucket. For example policies that show how to grant such access,\\nsee Multi-Region Access Point policy examples.\\n• Not all Regions support Multi-Region Access Points. To see the list of supported Regions,\\nsee Multi-Region Access Point restrictions and limitations.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nWhen you make a request to a Multi-Region Access Point, the Multi-Region Access Point\\nisn't aware of the data contents of the buckets in the Multi-Region Access Point. Therefore,\\nthe bucket that gets the request might not contain the requested data. To create consistent\\ndatasets in the Amazon S3 buckets that are associated with a Multi-Region Access Point, we\\nrecommend that you configure S3 Cross-Region Replication (CRR). For more information,\\nsee Configuring replication for use with Multi-Region Access Points.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou must add at least one bucket from either your account or other accounts. Also,\\nbe aware that Multi-Region Access Points support only one bucket per AWS Region.\\nTherefore, you can’t add two buckets from the same Region. AWS Regions that are\\ndisabled by default are not supported.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou must enter a valid AWS account ID and bucket name. The bucket must also be\\nin a supported Region, or you will encounter an error when you try to create your\\nMulti-Region Access Point. For the list of Regions that support Multi-Region Access\\nPoints, see Multi-Region Access Points restrictions and limitations.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can’t add or remove buckets to this Multi-Region Access Point after you’ve finished\\ncreating it.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nYou can't change the Block Public Access settings for a Multi-Region Access Point after\\nthe Multi-Region Access Point has been created. Therefore, if you're going to block\\npublic access, make sure that your applications work correctly without public access\\nbefore you create a Multi-Region Access Point.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nWhen you add a bucket that's owned by another account to your Multi-Region Access Point,\\nthe bucket owner must also update their bucket policy to grant access permissions to\\nthe Multi-Region Access Point. Otherwise, the Multi-Region Access Point won't be able to\\nretrieve data from that bucket. For example policies that show how to grant such access,\\nsee Multi-Region Access Point policy examples.\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-multi-region-access-point --account-id 111122223333 --details \\'{\\n\"Name\": \"simple-multiregionaccesspoint-with-two-regions\",\\n\"PublicAccessBlock\": {\\n\"BlockPublicAcls\": true,\\n\"IgnorePublicAcls\": true,\\n\"BlockPublicPolicy\": true,\\n\"RestrictPublicBuckets\": true\\n},\\n\"Regions\": [\\n{ \"Bucket\": \"example-s3-bucket1\" },\\n{ \"Bucket\": \"example-s3-bucket2\" }\\n]\\n}\\' --region us-west-2', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nYou can't change the Block Public Access settings for a Multi-Region Access Point after it\\nhas been created.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo retrieve replication status information from a bucket in an external account, the\\nbucket owner must grant you the s3:GetBucketReplication permission in their\\nbucket policy.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control get-multi-region-access-point --account-id 111122223333 --name example-\\ns3-bucket1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nMake sure to enter a valid bucket name. Otherwise, the Delete button will be disabled.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control delete-multi-region-access-point --account-id 123456789012 --details\\nName=example-multi-region-access-point-name', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nMake sure to create a com.amazonaws.s3-global.accesspoint endpoint. Other\\nendpoint types cannot access Multi-Region Access Points.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Read-buckets-and-MRAP-VPCE-policy\",\\n\"Principal\": \"*\",\\n\"Action\": [\\n\"s3:GetObject\"\\n],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*\",\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET2/*\",\\n\"arn:aws:s3::123456789012:accesspoint/mfzwi23gnjvgw.mrap/object/*\"', '']]]\n",
      "[[['', ']\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Open-read-MRAP-policy\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": [\\n\"s3:GetObject\"\\n],\\n\"Resource\": \"arn:aws:s3::123456789012:accesspoint/mfzwi23gnjvgw.mrap/object/*\"\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Public-read\",\\n\"Effect\":\"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:GetObject\",\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1\",\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET2/*\"]\\n}]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't delete an access policy for a Multi-Region Access Point. To remove access to a\\nMulti-Region Access Point, you must provide a new access policy with the modified access\\nthat you want.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe Multi-Region Access Point alias and ARN cannot be used interchangeably.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\nDelegating access control for a bucket to a Multi-Region Access Point policy doesn't\\nchange the bucket's behavior when the bucket is accessed directly through its bucket name\\nor Amazon Resource Name (ARN). All operations made directly against the bucket will\\ncontinue to work as before. Restrictions that you include in a Multi-Region Access Point\\npolicy apply only to requests made through that Multi-Region Access Point.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAny Block Public Access settings that are enabled under Block Public Access settings for\\nthis account (in your own account) or Block Public Settings for external buckets still apply\\neven if the independent Block Public Access settings for your Multi-Region Access Point are\\ndisabled.', ''], ['', '', '']], [['', '', ''], ['', \"Important\\nBy default, all Block Public Access settings are enabled for Multi-Region Access Points. You\\nmust explicitly turn off any settings that you don't want to apply to a Multi-Region Access\\nPoint.\\nYou can't change the Block Public Access settings for a Multi-Region Access Point after it\\nhas been created.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't edit the Block Public Access settings after the Multi-Region Access Point\\nis created. Therefore, if you're going to block public access, make sure that your\\napplications work correctly without public access before you create a Multi-Region\\nAccess Point.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"AWS\":\"arn:aws:iam::123456789012:user/JohnDoe\"\\n},\\n\"Action\":[\\n\"s3:ListBucket\",\\n\"s3:GetObject\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3::111122223333:accesspoint/MultiRegionAccessPoint_alias\",\\n\"arn:aws:s3::111122223333:accesspoint/MultiRegionAccessPoint_alias/object/\\n*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control put-multi-region-access-point-policy\\n--account-id 111122223333\\n--details { \"Name\": \"DOC-EXAMPLE-BUCKET-MultiRegionAccessPoint\",\\n\"Policy\": \"{ \\\\\"Version\\\\\": \\\\\"2012-10-17\\\\\", \\\\\"Statement\\\\\": { \\\\\"Effect\\\\\":\\n\\\\\"Allow\\\\\", \\\\\"Principal\\\\\": { \\\\\"AWS\\\\\": \\\\\"arn:aws:iam::111122223333:root\\n\\\\\" }, \\\\\"Action\\\\\": [\\\\\"s3:ListBucket\\\\\", \\\\\"s3:GetObject\\\\\"], \\\\\"Resource\\\\\":\\n[ \\\\\"arn:aws:s3::111122223333:accesspoint/MultiRegionAccessPoint_alias\",\\n\\\\\"arn:aws:s3::111122223333:accesspoint/MultiRegionAccessPoint_alias/object/*\\n\\\\\" ] } }\" }', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control describe-multi-region-access-point-operation\\n--account-id 111122223333\\n--request-token-arn requestArn', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control get-multi-region-access-point-policy\\n--account-id 111122223333\\n--name=DOC-EXAMPLE-BUCKET-MultiRegionAccessPoint', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf a bucket contains objects that are owned by other accounts, the Multi-Region Access\\nPoint policy doesn't apply to the objects that are owned by other AWS accounts.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe console automatically displays the Multi-Region Access Point Amazon Resource\\nName (ARN), which you can use in the policy. For example Multi-Region Access Point\\npolicies, see the section called “Multi-Region Access Point policy examples”.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nDelegating access control for a bucket to a Multi-Region Access Point policy doesn't\\nchange the bucket's behavior when the bucket is accessed directly through its bucket name\\nor Amazon Resource Name (ARN). All operations made directly against the bucket will\\ncontinue to work as before. Restrictions that you include in a Multi-Region Access Point\\npolicy apply only to requests made through that Multi-Region Access Point.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\" : [', '']]]\n",
      "[[['', '{\\n\"Effect\": \"Allow\",\\n\"Principal\" : { \"AWS\": \"*\" },\\n\"Action\" : \"*\",\\n\"Resource\" : [ \"Bucket ARN\", \"Bucket ARN/*\"],\\n\"Condition\": {\\n\"StringEquals\" : { \"s3:DataAccessPointArn\" : \"MultiRegionAccessPoint_ARN\" }\\n}\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf there are multiple Multi-Region Access Points that you're granting access to, make sure to\\nlist each Multi-Region Access Point.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\":\"arn:aws:iam::123456789012:user/JohnDoe\"\\n},\\n\"Action\":[\\n\"s3:ListBucket\",\\n\"s3:GetObject\"\\n],\\n\"Resource\":[\\n\"MultiRegionAccessPoint_ARN\",\\n\"MultiRegionAccessPoint_ARN/object/*\"\\n]\\n}\\n]', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/JohnDoe\"\\n},\\n\"Action\": \"s3:ListBucket\",\\n\"Resource\": \"MultiRegionAccessPoint_ARN\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo use SigV4A with temporary security credentials—for example, when using AWS\\nIdentity and Access Management (IAM) roles—you can request the temporary\\ncredentials from a Regional AWS Security Token Service (AWS STS) endpoint.\\nIf you request temporary credentials from the global AWS STS endpoint\\n(sts.amazonaws.com), then you must first set the Region compatibility of session\\ntokens for the global endpoint to be valid in all AWS Regions. For more information,\\nsee Managing AWS STS in an AWS Region in the IAM User Guide.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nAlthough failover is initiated between only two Regions at one time, you can separately\\nupdate the routing statuses for multiple Regions at the same time in your Multi-Region\\nAccess Point.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nTo initiate failover, at least one AWS Region must be designated as Active and one\\nRegion must be designated as Passive in your Multi-Region Access Point.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou can run Multi-Region Access Point AWS CLI routing commands against any of these five\\nRegions:\\n• ap-southeast-2\\n• ap-northeast-1\\n• us-east-1\\n• us-west-2\\n• eu-west-1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control submit-multi-region-access-point-routes\\n--region ap-southeast-2\\n--account-id 111122223333\\n--mrap MultiRegionAccessPoint_ARN\\n--route-updates Bucket=DOC-EXAMPLE-BUCKET-1,TrafficDialPercentage=100\\nBucket=DOC-EXAMPLE-BUCKET-2,TrafficDialPercentage=0', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-multi-region-access-point-routes\\n--region eu-west-1\\n--account-id 111122223333\\n--mrap MultiRegionAccessPoint_ARN', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nTo keep all metadata and objects in sync across buckets during data replication, we\\nrecommend that you create two-way replication rules and enable replica modification sync\\nbefore configuring your failover controls.\\nTwo-way replication rules help ensure that when data is written to the Amazon S3 bucket\\nthat traffic fails over to, that data is then replicated back to the source bucket. Replica\\nmodification sync helps ensure that object metadata is also synchronized between buckets\\nduring two-way replication.\\nFor more information about configuring replication to support failover, see the section\\ncalled “Bucket replication”.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo initiate failover, at least one AWS Region must be designated as Active and one\\nRegion must be designated as Passive in your Multi-Region Access Point. An active\\nstate allows traffic to be directed to a Region. A passive state stops any traffic from\\nbeing directed to the Region.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-multi-region-access-point-routes\\n--region eu-west-1\\n--account-id 111122223333\\n--mrap MultiRegionAccessPoint_ARN', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis command can only be executed against these five Regions:\\n• ap-southeast-2\\n• ap-northeast-1\\n• us-east-1\\n• us-west-2\\n• eu-west-1', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nWe recommend using one-way replication if your users will only be consuming the objects\\nin your destination buckets. If your users will be uploading or modifying the objects in\\nyour destination buckets, use two-way replication to keep all of your buckets in sync. We\\nalso recommend two-way replication if you plan to use your Multi-Region Access Point for\\nfailover. To set up two-way replication, see the section called “Create two-way replication\\nrules for your Multi-Region Access Point”.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou can create replication rules only for buckets in your own account. To create\\nreplication rules for external buckets, the bucket owners must create the replication\\nrules for those buckets.', ''], ['', '', '']], [['', '', ''], ['', \"Important\\nWhen you create replication rules by using this template, they replace any existing\\nreplication rules that are already assigned to the bucket.\\nTo add to or modify any existing replication rules instead of replacing them, go to each\\nbucket's Management tab in the console, and then edit the rules in the Replication\\nrules section. You can also add to or modify existing replication rules by using the AWS\\nCLI, SDKs, or REST API. For more information, see Replication configuration.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nMake sure that you have the required read and replicate permissions to establish\\nreplication, or you will encounter errors. For more information, see Creating an IAM\\nrole.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't enter a name in the Replication rule name box. Replication rule names are\\ngenerated based on your configuration when you create the replication rule.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWe recommend that you apply the following options:\\n• Replication time control (RTC) – To replicate your data across different Regions\\nwithin a predictable time frame, you can use S3 Replication Time Control (S3 RTC).\\nS3 RTC replicates 99.99 percent of new objects stored in Amazon S3 within 15\\nminutes (backed by a service-level agreement). For more information, see the section\\ncalled “Using S3 Replication Time Control”.\\n• Replication metrics and notifications – Enable Amazon CloudWatch metrics to\\nmonitor replication events.', '']]]\n",
      "[[['', '• Delete marker replication – Delete markers created by S3 delete operations will\\nbe replicated. Delete markers created by lifecycle rules are not replicated. For more\\ninformation, see Replicating delete markers between buckets.\\nThere are additional charges for S3 RTC and CloudWatch replication metrics and\\nnotifications. For more information, see Amazon S3 Pricing and Amazon CloudWatch\\npricing.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nWhen you create replication rules by using this template, they replace any existing\\nreplication rules that are already assigned to the bucket.\\nTo add to or modify any existing replication rules instead of replacing them, go to each\\nbucket's Management tab in the console, and then edit the rules in the Replication\\nrules section. You can also add to or modify existing replication rules by using the\\nAWS CLI, AWS SDKs, or Amazon S3 REST API. For more information, see Replication\\nconfiguration.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nMake sure that you have the required read and replicate permissions to establish\\nreplication, or you will encounter errors. For more information, see Creating an IAM\\nrole.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nYou can't enter a name in the Replication rule name box. Replication rule names are\\ngenerated based on your configuration when you create the replication rule.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWe recommend that you apply the following options, especially if you intend to\\nconfigure your Multi-Region Access Point to support failover:\\n• Replication time control (RTC) – To replicate your data across different Regions\\nwithin a predictable time frame, you can use S3 Replication Time Control (S3 RTC).\\nS3 RTC replicates 99.99 percent of new objects stored in Amazon S3 within 15\\nminutes (backed by a service-level agreement). For more information, see the section\\ncalled “Using S3 Replication Time Control”.\\n• Replication metrics and notifications – Enable Amazon CloudWatch metrics to\\nmonitor replication events.\\n• Delete marker replication – Delete markers created by S3 delete operations will\\nbe replicated. Delete markers created by lifecycle rules are not replicated. For more\\ninformation, see Replicating delete markers between buckets.\\n• Replica modification sync – Enable replica modification sync for each replication\\nrule to also keep metadata changes to your objects in sync. For more information,\\nsee Enabling replica modification sync.', '']]]\n",
      "[[['', 'There are additional charges for S3 RTC and CloudWatch replication metrics and\\nnotifications. For more information, see Amazon S3 Pricing and Amazon CloudWatch\\npricing.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you’ve added a bucket from another account to this Multi-Region Access Point, you\\nmust have the s3:GetBucketReplication permission from the bucket owner to\\nview the replication rules for that bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nRequests that are made through the Amazon S3 console are always synchronous. The\\nconsole waits until the request is completed before allowing you to submit another\\nrequest.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nMulti-Region Access Points supports copy operations using Multi-Region Access Points only\\nas a destination when using the Multi-Region Access Point ARN.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control get-multi-region-access-point-routes', '']]]\n",
      "[[['', '--region eu-west-1\\n--account-id 111122223333\\n--mrap arn:aws:s3::111122223333:accesspoint/abcdef0123456.mrap', ''], ['', '', '']], [['', '', ''], ['', 'S3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_EAST_1)\\n.credentialsProvider(credentialsProvider)\\n.build();\\nGetMultiRegionAccessPointRoutesRequest request =\\nGetMultiRegionAccessPointRoutesRequest.builder()\\n.accountId(\"111122223333\")\\n.mrap(\"arn:aws:s3::111122223333:accesspoint/abcdef0123456.mrap\")\\n.build();\\nGetMultiRegionAccessPointRoutesResponse response =\\ns3ControlClient.getMultiRegionAccessPointRoutes(request);', ''], ['', '', '']], [['', '', ''], ['', \"const REGION = 'us-east-1'\\nconst s3ControlClient = new S3ControlClient({\\nregion: REGION\\n})\\nexport const run = async () => {\\ntry {\\nconst data = await s3ControlClient.send(\\nnew GetMultiRegionAccessPointRoutesCommand({\\nAccountId: '111122223333',\\nMrap: 'arn:aws:s3::111122223333:accesspoint/abcdef0123456.mrap',\\n})\", '']]]\n",
      "[[['', \")\\nconsole.log('Success', data)\\nreturn data\\n} catch (err) {\\nconsole.log('Error', err)\\n}\\n}\\nrun()\", ''], ['', '', '']], [['', '', ''], ['', \"s3.get_multi_region_access_point_routes(\\nAccountId=111122223333,\\nMrap=arn:aws:s3::111122223333:accesspoint/abcdef0123456.mrap)['Routes']\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": {\\n\"Principal\": { \"AWS\": \"*\" },\\n\"Effect\": \"Allow\",\\n\"Action\": [\"s3:*\"],\\n\"Resource\": [\"arn:aws:s3:::111122223333/*\", \"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"],\\n\"Condition\": {\\n\"StringEquals\": {', '']]]\n",
      "[[['', '\"s3:DataAccessPointAccount\": \"444455556666\"\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-policy\\n--bucket DOC-EXAMPLE-BUCKET\\n--policy file:///tmp/policy.json', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control submit-multi-region-access-point-routes\\n--region ap-southeast-2\\n--account-id 111122223333', '']]]\n",
      "[[['', '--mrap arn:aws:s3::111122223333:accesspoint/abcdef0123456.mrap\\n--route-updates Bucket=DOC-EXAMPLE-BUCKET1,TrafficDialPercentage=100\\nBucket=DOC-EXAMPLE-BUCKET2,TrafficDialPercentage=0', ''], ['', '', '']], [['', '', ''], ['', 'S3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.ap-southeast-2)\\n.credentialsProvider(credentialsProvider)\\n.build();\\nSubmitMultiRegionAccessPointRoutesRequest request =\\nSubmitMultiRegionAccessPointRoutesRequest.builder()\\n.accountId(\"111122223333\")\\n.mrap(\"arn:aws:s3::111122223333:accesspoint/abcdef0123456.mrap\")\\n.routeUpdates(\\nMultiRegionAccessPointRoute.builder()\\n.region(\"eu-west-1\")\\n.trafficDialPercentage(100)\\n.build(),\\nMultiRegionAccessPointRoute.builder()\\n.region(\"ca-central-1\")\\n.bucket(\"111122223333\")\\n.trafficDialPercentage(0)\\n.build()\\n)\\n.build();\\nSubmitMultiRegionAccessPointRoutesResponse response =\\ns3ControlClient.submitMultiRegionAccessPointRoutes(request);', ''], ['', '', '']], [['', '', ''], ['', \"const REGION = 'ap-southeast-2'\", '']]]\n",
      "[[['', \"const s3ControlClient = new S3ControlClient({\\nregion: REGION\\n})\\nexport const run = async () => {\\ntry {\\nconst data = await s3ControlClient.send(\\nnew SubmitMultiRegionAccessPointRoutesCommand({\\nAccountId: '111122223333',\\nMrap: 'arn:aws:s3::111122223333:accesspoint/abcdef0123456.mrap',\\nRouteUpdates: [\\n{\\nRegion: 'eu-west-1',\\nTrafficDialPercentage: 100,\\n},\\n{\\nRegion: 'ca-central-1',\\nBucket: 'DOC-EXAMPLE-BUCKET1',\\nTrafficDialPercentage: 0,\\n},\\n],\\n})\\n)\\nconsole.log('Success', data)\\nreturn data\\n} catch (err) {\\nconsole.log('Error', err)\\n}\\n}\\nrun()\", ''], ['', '', '']], [['', '', ''], ['', \"s3.submit_multi_region_access_point_routes(\\nAccountId=111122223333,\\nMrap=arn:aws:s3::111122223333:accesspoint/abcdef0123456.mrap,\\nRouteUpdates= [{\\n'Bucket': DOC-EXAMPLE-BUCKET,\", '']]]\n",
      "[[['', \"'Region': ap-southeast-2,\\n'TrafficDialPercentage': 10\\n}])\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo use this operation, you must have the s3:PutObject permission for the Multi-\\nRegion Access Point. For more information about Multi-Region Access Point permission\\nrequirements, see Permissions.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket\\narn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap --key example.txt --\\nbody example.txt', ''], ['', '', '']], [['', '', ''], ['', 'S3Client s3Client = S3Client.builder()\\n.build();\\nPutObjectRequest objectRequest = PutObjectRequest.builder()\\n.bucket(\"arn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap\")\\n.key(\"example.txt\")\\n.build();\\ns3Client.putObject(objectRequest, RequestBody.fromString(\"Hello S3!\"));', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'const client = new S3Client({});\\nasync function putObjectExample() {\\nconst command = new PutObjectCommand({\\nBucket: \"arn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap\",\\nKey: \"example.txt\",\\nBody: \"Hello S3!\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"import boto3\\nclient = boto3.client('s3')\\nclient.put_object(\\nBucket='arn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap',\\nKey='example.txt',\\nBody='Hello S3!'\\n)\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo use this API operation, you must have the s3:GetObject permission for the Multi-\\nRegion Access Point. For more information about Multi-Region Access Point permissions\\nrequirements, see Permissions.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-object --bucket\\narn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap --\\nkey example.txt downloaded_example.txt', ''], ['', '', '']], [['', '', ''], ['', 'S3Client s3 = S3Client\\n.builder()\\n.build();\\nGetObjectRequest getObjectRequest = GetObjectRequest.builder()\\n.bucket(\"arn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap\")\\n.key(\"example.txt\")\\n.build();\\ns3Client.getObject(getObjectRequest);', ''], ['', '', '']], [['', '', ''], ['', 'const client = new S3Client({})\\nasync function getObjectExample() {\\nconst command = new GetObjectCommand({\\nBucket: \"arn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap\",\\nKey: \"example.txt\"\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"import boto3\\nclient = boto3.client('s3')\\nclient.get_object(\\nBucket='arn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap',\\nKey='example.txt'\\n)\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo use this API operation, you must have the s3:ListBucket permission for the Multi-\\nRegion Access Point and the underlying bucket. For more information about Multi-Region\\nAccess Point permissions requirements, see Permissions.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-objects-v2 --bucket\\narn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap', ''], ['', '', '']], [['', '', ''], ['', 'S3Client s3Client = S3Client.builder()\\n.build();', '']]]\n",
      "[[['', 'String bucketName = \"arn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap\";\\nListObjectsV2Request listObjectsRequest = ListObjectsV2Request\\n.builder()\\n.bucket(bucketName)\\n.build();\\ns3Client.listObjectsV2(listObjectsRequest);', ''], ['', '', '']], [['', '', ''], ['', 'const client = new S3Client({});\\nasync function listObjectsExample() {\\nconst command = new ListObjectsV2Command({\\nBucket: \"arn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"import boto3\\nclient = boto3.client('s3')\\nclient.list_objects_v2(\\nBucket='arn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap'\\n)\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3 presign\\narn:aws:s3::123456789012:accesspoint/MultiRegionAccessPoint_alias/example-file.txt', ''], ['', '', '']], [['', '', ''], ['', 'import logging\\nimport boto3\\nfrom botocore.exceptions import ClientError\\ns3_client = boto3.client(\\'s3\\',aws_access_key_id=\\'xxx\\',aws_secret_access_key=\\'xxx\\')\\ns3_client.generate_presigned_url(HttpMethod=\\'PUT\\',ClientMethod=\"put_object\",\\nParams={\\'Bucket\\':\\'arn:aws:s3::123456789012:accesspoint/\\nabcdef0123456.mrap\\',\\'Key\\':\\'example-file\\'})', ''], ['', '', '']], [['', '', ''], ['', 'S3Presigner s3Presigner = S3Presigner.builder()\\n.credentialsProvider(StsAssumeRoleCredentialsProvider.builder()\\n.refreshRequest(assumeRole)\\n.stsClient(stsClient)\\n.build())\\n.build();\\nGetObjectRequest getObjectRequest = GetObjectRequest.builder()\\n.bucket(\"arn:aws:s3::123456789012:accesspoint/abcdef0123456.mrap\")\\n.key(\"example-file\")', '']]]\n",
      "[[['', '.build();\\nGetObjectPresignRequest preSignedReq = GetObjectPresignRequest.builder()\\n.getObjectRequest(getObjectRequest)\\n.signatureDuration(Duration.ofMinutes(10))\\n.build();\\nPresignedGetObjectRequest presignedGetObjectRequest =\\ns3Presigner.presignGetObject(preSignedReq);', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nTo use SigV4A with temporary security credentials—for example, when using IAM roles\\n—make sure that you request the temporary credentials from a Regional endpoint in\\nAWS Security Token Service (AWS STS), instead of a global endpoint. If you use the\\nglobal endpoint for AWS STS (sts.amazonaws.com), AWS STS will generate temporary\\ncredentials from a global endpoint, which isn't supported by Sig4A. As a result, you'll get an\\nerror. To resolve this issue, use any of the listed Regional endpoints for AWS STS.\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object --bucket MultiRegionAccessPoint_ARN --request-payer requester\\n--key example-file-in-bucket.txt example-location-of-downloaded-file.txt', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'GetObjectResponse getObjectResponse = s3Client.getObject(GetObjectRequest.builder()\\n.key(\"example-file.txt\")\\n.bucket(\"arn:aws:s3::\\n123456789012:accesspoint/abcdef0123456.mrap\")\\n.requestPayer(RequestPayer.REQUESTER)\\n.build()\\n).response();', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nMulti-Region Access Points aren't aware of the data contents of the underlying buckets.\\nTherefore, the bucket that gets the request might not contain the requested data. For\\nexample, if Amazon S3 determines that the my-bucket-euc1 bucket is the closest, your\\nlogs will reflect a failed GET request for my-image.jpg from my-bucket-euc1, using a\\nhostname of mfzwi23gnjvgw.mrap.accesspoint.s3-global.amazonaws.com. If the\\nrequest was routed to my-bucket-usw2 instead, your logs would indicate a successful GET\\nrequest.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe console displays only the status of asynchronous requests that were made within the\\nprevious 14 days. To view the status of older requests, use the AWS CLI, SDKs, or REST API.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API', '']]]\n",
      "[[['', 'response header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nYou can't apply different types of server-side encryption to the same object simultaneously.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nHashiCorp Terraform users that aren't using an updated version of the AWS Provider might\\nsee an unexpected drift after creating new S3 buckets with no customer defined encryption\\nconfiguration. To avoid this drift, update your Terraform AWS Provider version to one of the\\nfollowing versions: any 4.x release, 3.76.1, or 2.70.4.\", ''], ['', '', '']], [['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"PutObjectPolicy\",\\n\"Statement\": [\\n{\\n\"Sid\": \"DenyObjectsThatAreNotSSES3\",\\n\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/*\",\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"s3:x-amz-server-side-encryption\": \"AES256\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nServer-side encryption encrypts only the object data, not the object metadata.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen using a POST operation to upload an object, instead of providing the request header,\\nyou provide the same information in the form fields. For more information, see POST\\nObject.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\n• If you change an object's encryption, a new object is created to replace the old one. If\\nS3 Versioning is enabled, a new version of the object is created, and the existing object\\nbecomes an older version. The role that changes the property also becomes the owner of\\nthe new object (or object version).\\n• If you change the encryption type for an object that has user-defined tags, you must\\nhave the s3:GetObjectTagging permission. If you're changing the encryption type\\nfor an object that doesn't have user-defined tags but is over 16 MB in size, you must also\\nhave the s3:GetObjectTagging permission.\\nIf the destination bucket policy denies the s3:GetObjectTagging action, the\\nencryption type for the object will be updated, but the user-defined tags will be removed\\nfrom the object, and you will receive an error.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThis action applies encryption to all specified objects. When you're encrypting folders, wait\\nfor the save operation to finish before adding new objects to the folder.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nDo not send encryption request headers for GET requests and HEAD requests if your object\\nuses SSE-S3, or you'll get an HTTP status code 400 (Bad Request) error.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.internal.SSEResultBase;\\nimport com.amazonaws.services.s3.model.*;\\nimport java.io.ByteArrayInputStream;\\npublic class SpecifyServerSideEncryption {\\npublic static void main(String[] args) {', '']]]\n",
      "[[['', 'Regions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString keyNameToEncrypt = \"*** Key name for an object to upload and encrypt\\n***\";\\nString keyNameToCopyAndEncrypt = \"*** Key name for an unencrypted object to\\nbe encrypted by copying ***\";\\nString copiedObjectKeyName = \"*** Key name for the encrypted copy of the\\nunencrypted object ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withRegion(clientRegion)\\n.withCredentials(new ProfileCredentialsProvider())\\n.build();\\n// Upload an object and encrypt it with SSE.\\nuploadObjectWithSSEEncryption(s3Client, bucketName, keyNameToEncrypt);\\n// Upload a new unencrypted object, then change its encryption state\\n// to encrypted by making a copy.\\nchangeSSEEncryptionStatusByCopying(s3Client,\\nbucketName,\\nkeyNameToCopyAndEncrypt,\\ncopiedObjectKeyName);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\nprivate static void uploadObjectWithSSEEncryption(AmazonS3 s3Client, String\\nbucketName, String keyName) {\\nString objectContent = \"Test object encrypted with SSE\";\\nbyte[] objectBytes = objectContent.getBytes();\\n// Specify server-side encryption.\\nObjectMetadata objectMetadata = new ObjectMetadata();\\nobjectMetadata.setContentLength(objectBytes.length);', '']]]\n",
      "[[['', 'objectMetadata.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);\\nPutObjectRequest putRequest = new PutObjectRequest(bucketName,\\nkeyName,\\nnew ByteArrayInputStream(objectBytes),\\nobjectMetadata);\\n// Upload the object and check its encryption status.\\nPutObjectResult putResult = s3Client.putObject(putRequest);\\nSystem.out.println(\"Object \\\\\"\" + keyName + \"\\\\\" uploaded with SSE.\");\\nprintEncryptionStatus(putResult);\\n}\\nprivate static void changeSSEEncryptionStatusByCopying(AmazonS3 s3Client,\\nString bucketName,\\nString sourceKey,\\nString destKey) {\\n// Upload a new, unencrypted object.\\nPutObjectResult putResult = s3Client.putObject(bucketName, sourceKey,\\n\"Object example to encrypt by copying\");\\nSystem.out.println(\"Unencrypted object \\\\\"\" + sourceKey + \"\\\\\" uploaded.\");\\nprintEncryptionStatus(putResult);\\n// Make a copy of the object and use server-side encryption when storing the\\n// copy.\\nCopyObjectRequest request = new CopyObjectRequest(bucketName,\\nsourceKey,\\nbucketName,\\ndestKey);\\nObjectMetadata objectMetadata = new ObjectMetadata();\\nobjectMetadata.setSSEAlgorithm(ObjectMetadata.AES_256_SERVER_SIDE_ENCRYPTION);\\nrequest.setNewObjectMetadata(objectMetadata);\\n// Perform the copy operation and display the copy\\'s encryption status.\\nCopyObjectResult response = s3Client.copyObject(request);\\nSystem.out.println(\"Object \\\\\"\" + destKey + \"\\\\\" uploaded with SSE.\");\\nprintEncryptionStatus(response);\\n// Delete the original, unencrypted object, leaving only the encrypted copy\\nin\\n// Amazon S3.\\ns3Client.deleteObject(bucketName, sourceKey);\\nSystem.out.println(\"Unencrypted object \\\\\"\" + sourceKey + \"\\\\\" deleted.\");', '']]]\n",
      "[[['', '}\\nprivate static void printEncryptionStatus(SSEResultBase response) {\\nString encryptionStatus = response.getSSEAlgorithm();\\nif (encryptionStatus == null) {\\nencryptionStatus = \"Not encrypted with SSE\";\\n}\\nSystem.out.println(\"Object encryption status is: \" + encryptionStatus);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'ServerSideEncryptionMethod = ServerSideEncryptionMethod.AES256', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass SpecifyServerSideEncryptionTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\nprivate const string keyName = \"*** key name for object created ***\";\\n// Specify your bucket region (an example region is shown).', '']]]\n",
      "[[['', 'private static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nclient = new AmazonS3Client(bucketRegion);\\nWritingAnObjectAsync().Wait();\\n}\\nstatic async Task WritingAnObjectAsync()\\n{\\ntry\\n{\\nvar putRequest = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nContentBody = \"sample text\",\\nServerSideEncryptionMethod = ServerSideEncryptionMethod.AES256\\n};\\nvar putResponse = await client.PutObjectAsync(putRequest);\\n// Determine the encryption state of an object.\\nGetObjectMetadataRequest metadataRequest = new\\nGetObjectMetadataRequest\\n{\\nBucketName = bucketName,\\nKey = keyName\\n};\\nGetObjectMetadataResponse response = await\\nclient.GetObjectMetadataAsync(metadataRequest);\\nServerSideEncryptionMethod objectEncryption =\\nresponse.ServerSideEncryptionMethod;\\nConsole.WriteLine(\"Encryption method used: {0}\",\\nobjectEncryption.ToString());\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered ***. Message:\\'{0}\\' when writing\\nan object\", e.Message);\\n}', '']]]\n",
      "[[['', 'catch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"require 'vendor/autoload.php';\\nuse Aws\\\\S3\\\\S3Client;\\n$bucket = '*** Your Bucket Name ***';\\n$keyname = '*** Your Object Key ***';\\n// $filepath should be an absolute path to a file on disk.\\n$filepath = '*** Your File Path ***';\\n$s3 = new S3Client([\\n'version' => 'latest',\\n'region' => 'us-east-1'\\n]);\\n// Upload a file with server-side encryption.\\n$result = $s3->putObject([\\n'Bucket' => $bucket,\\n'Key' => $keyname,\\n'SourceFile' => $filepath,\\n'ServerSideEncryption' => 'AES256',\\n]);\", '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"require 'vendor/autoload.php';\\nuse Aws\\\\S3\\\\S3Client;\\n$bucket = '*** Your Bucket Name ***';\\n$keyname = '*** Your Object Key ***';\\n$s3 = new S3Client([\\n'version' => 'latest',\\n'region' => 'us-east-1'\\n]);\\n// Check which server-side encryption algorithm is used.\\n$result = $s3->headObject([\\n'Bucket' => $bucket,\\n'Key' => $keyname,\\n]);\\necho $result['ServerSideEncryption'];\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'require \\'vendor/autoload.php\\';\\nuse Aws\\\\S3\\\\S3Client;\\n$sourceBucket = \\'*** Your Source Bucket Name ***\\';\\n$sourceKeyname = \\'*** Your Source Object Key ***\\';\\n$targetBucket = \\'*** Your Target Bucket Name ***\\';\\n$targetKeyname = \\'*** Your Target Object Key ***\\';\\n$s3 = new S3Client([\\n\\'version\\' => \\'latest\\',\\n\\'region\\' => \\'us-east-1\\'\\n]);\\n// Copy an object and add server-side encryption.\\n$s3->copyObject([\\n\\'Bucket\\' => $targetBucket,\\n\\'Key\\' => $targetKeyname,\\n\\'CopySource\\' => \"$sourceBucket/$sourceKeyname\",\\n\\'ServerSideEncryption\\' => \\'AES256\\',\\n]);', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectPutSseWrapper\\nattr_reader :object\\n# @param object [Aws::S3::Object] An existing Amazon S3 object.\\ndef initialize(object)\\n@object = object\\nend\\ndef put_object_encrypted(object_content, encryption)\\n@object.put(body: object_content, server_side_encryption: encryption)\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t put your content to #{object.key}. Here\\'s why: #{e.message}\"\\nfalse\\nend\\nend\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nobject_key = \"my-encrypted-content\"\\nobject_content = \"This is my super-secret content.\"\\nencryption = \"AES256\"\\nwrapper = ObjectPutSseWrapper.new(Aws::S3::Object.new(bucket_name,\\nobject_content))\\nreturn unless wrapper.put_object_encrypted(object_content, encryption)\\nputs \"Put your content into #{bucket_name}:#{object_key} and encrypted it with\\n#{encryption}.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectGetEncryptionWrapper\\nattr_reader :object\\n# @param object [Aws::S3::Object] An existing Amazon S3 object.\\ndef initialize(object)\\n@object = object\\nend\\n# Gets the object into memory.\\n#\\n# @return [Aws::S3::Types::GetObjectOutput, nil] The retrieved object data if\\nsuccessful; otherwise nil.\\ndef get_object\\n@object.get\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t get object #{@object.key}. Here\\'s why: #{e.message}\"\\nend\\nend\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nobject_key = \"my-object.txt\"\\nwrapper = ObjectGetEncryptionWrapper.new(Aws::S3::Object.new(bucket_name,\\nobject_key))\\nobj_data = wrapper.get_object\\nreturn unless obj_data\\nencryption = obj_data.server_side_encryption.nil? ? \"no\" :\\nobj_data.server_side_encryption\\nputs \"Object #{object_key} uses #{encryption} encryption.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectCopyEncryptWrapper\\nattr_reader :source_object\\n# @param source_object [Aws::S3::Object] An existing Amazon S3 object. This is\\nused as the source object for\\n# copy actions.\\ndef initialize(source_object)\\n@source_object = source_object\\nend\\n# Copy the source object to the specified target bucket, rename it with the target\\nkey, and encrypt it.\\n#\\n# @param target_bucket [Aws::S3::Bucket] An existing Amazon S3 bucket where the\\nobject is copied.\\n# @param target_object_key [String] The key to give the copy of the object.\\n# @return [Aws::S3::Object, nil] The copied object when successful; otherwise,\\nnil.\\ndef copy_object(target_bucket, target_object_key, encryption)\\n@source_object.copy_to(bucket: target_bucket.name, key: target_object_key,\\nserver_side_encryption: encryption)\\ntarget_bucket.object(target_object_key)\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t copy #{@source_object.key} to #{target_object_key}. Here\\'s why:\\n#{e.message}\"\\nend\\nend\\n# Example usage:\\ndef run_demo\\nsource_bucket_name = \"doc-example-bucket1\"\\nsource_key = \"my-source-file.txt\"\\ntarget_bucket_name = \"doc-example-bucket2\"', '']]]\n",
      "[[['', 'target_key = \"my-target-file.txt\"\\ntarget_encryption = \"AES256\"\\nsource_bucket = Aws::S3::Bucket.new(source_bucket_name)\\nwrapper = ObjectCopyEncryptWrapper.new(source_bucket.object(source_key))\\ntarget_bucket = Aws::S3::Bucket.new(target_bucket_name)\\ntarget_object = wrapper.copy_object(target_bucket, target_key, target_encryption)\\nreturn unless target_object\\nputs \"Copied #{source_key} from #{source_bucket_name} to\\n#{target_object.bucket_name}:#{target_object.key} and \"\\\\\\n\"encrypted the target with #{target_object.server_side_encryption}\\nencryption.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket example-s3-bucket1 --key object-key-name --server-side-\\nencryption AES256 --body file path', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all', '']]]\n",
      "[[['', 'new object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nCarefully review the permissions that are granted in your KMS key policies. Always restrict\\ncustomer-managed KMS key policy permissions only to the IAM principals and AWS\\nservices that must access the relevant AWS KMS key action. For more information, see Key\\npolicies in AWS KMS.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nObjects encrypted using SSE-KMS with AWS managed keys can't be shared cross-account.\\nIf you need to share SSE-KMS data cross-account, you must use a customer managed key\\nfrom AWS KMS.\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nWhen you use server-side encryption with a customer managed key that's stored in\\nan external key store, unlike standard KMS keys, you are responsible for ensuring the\\navailability and durability of your key material. For more information about external key\\nstores and how they shift the shared responsibility model, see External key stores in the\\nAWS Key Management Service Developer Guide.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nWhen you use an AWS KMS key for server-side encryption in Amazon S3, you must choose\\na symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS\\nkeys. For more information about these keys, see Symmetric encryption KMS keys in the\\nAWS Key Management Service Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Id\":\"PutObjectPolicy\",\\n\"Statement\":[{\\n\"Sid\":\"DenyObjectsThatAreNotSSEKMS\",\\n\"Effect\":\"Deny\",\\n\"Principal\":\"*\",\\n\"Action\":\"s3:PutObject\",\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket1/*\",\\n\"Condition\":{\\n\"Null\":{\\n\"s3:x-amz-server-side-encryption-aws-kms-key-id\":\"true\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen you upload an object, you can specify the KMS key by using the x-amz-server-\\nside-encryption-aws-kms-key-id header. If the header is not present in the request,\\nAmazon S3 assumes that you want to use the AWS managed key. Regardless, the AWS KMS\\nkey ID that Amazon S3 uses for object encryption must match the AWS KMS key ID in the\\npolicy, otherwise Amazon S3 denies the request.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'arn:aws:s3:::object_ARN', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3:::bucket_ARN', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '\"encryptionContext\": {\\n\"aws:s3:arn\": \"arn:aws:s3:::example-s3-bucket1/file_name\"\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"encryptionContext\": {\\n\"aws:s3:arn\": \"arn:aws:s3:::example-s3-bucket1\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nAll GET and PUT requests for AWS KMS encrypted objects must be made using Secure\\nSockets Layer (SSL) or Transport Layer Security (TLS). Requests must also be signed using\\nvalid credentials, such as AWS Signature Version 4 (or AWS Signature Version 2).', ''], ['', '', '']], [['', '', ''], ['', \"Important\\nIf your object uses SSE-KMS, don't send encryption request headers for GET requests and\\nHEAD requests. Otherwise, you’ll get an HTTP 400 Bad Request error.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can use multi-Region AWS KMS keys in Amazon S3. However, Amazon S3 currently\\ntreats multi-Region keys as though they were single-Region keys, and does not use the', '']]]\n",
      "[[['', 'multi-Region features of the key. For more information, see Using multi-Region keys in\\nAWS Key Management Service Developer Guide.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf you want to use a KMS key that's owned by a different account, you must have\\npermission to use the key. For more information about cross-account permissions for KMS\\nkeys, see Creating KMS keys that other accounts can use in the AWS Key Management\\nService Developer Guide.\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\n• If you change an object's encryption, a new object is created to replace the old one. If\\nS3 Versioning is enabled, a new version of the object is created, and the existing object\\nbecomes an older version. The role that changes the property also becomes the owner of\\nthe new object (or object version).\\n• If you change the encryption type for an object that has user-defined tags, you must\\nhave the s3:GetObjectTagging permission. If you're changing the encryption type\\nfor an object that doesn't have user-defined tags but is over 16 MB in size, you must also\\nhave the s3:GetObjectTagging permission.\\nIf the destination bucket policy denies the s3:GetObjectTagging action, the\\nencryption type for the object will be updated, but the user-defined tags will be removed\\nfrom the object, and you will receive an error.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf you use the SSE-KMS option for your default encryption configuration, you are\\nsubject to the requests per second (RPS) quotas of AWS KMS. For more information\\nabout AWS KMS quotas and how to request a quota increase, see Quotas in the AWS\\nKey Management Service Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nYou can use only KMS keys that are available in the same AWS Region as the bucket.\\nThe Amazon S3 console lists only the first 100 KMS keys in the same Region as the\\nbucket. To use a KMS key that is not listed, you must enter your KMS key ARN. If\\nyou want to use a KMS key that is owned by a different account, you must first have\\npermission to use the key and then you must enter the KMS key ARN.\\nAmazon S3 supports only symmetric encryption KMS keys, and not asymmetric KMS\\nkeys. For more information, see Identifying symmetric and asymmetric KMS keys in the\\nAWS Key Management Service Developer Guide.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThis action applies encryption to all specified objects. When you're encrypting folders, wait\\nfor the save operation to finish before adding new objects to the folder.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\n• All GET and PUT requests for an object protected by AWS KMS fail if you don't make\\nthese requests by using Secure Sockets Layer (SSL), Transport Layer Security (TLS), or\\nSignature Version 4.\\n• If your object uses SSE-KMS, don't send encryption request headers for GET requests and\\nHEAD requests, or you’ll get an HTTP 400 BadRequest error.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nWhen you use an AWS KMS key for server-side encryption in Amazon S3, you must choose\\na symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS\\nkeys. For more information about these keys, see Symmetric encryption KMS keys in the\\nAWS Key Management Service Developer Guide.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket example-s3-bucket --key example-object-key --server-side-\\nencryption aws:kms --ssekms-key-id example-key-id --ssekms-encryption-context example-\\nencryption-context --body filepath', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-object --bucket example-s3-bucket --key example-object-key --server-side-\\nencryption aws:kms --bucket-key-enabled --body filepath', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api copy-object --copy-source example-s3-bucket/example-object-key --\\nbucket example-s3-bucket2 --key example-object-key --server-side-encryption aws:kms --\\nsse-kms-key-id example-key-id --ssekms-encryption-context example-encryption-context', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you use an AWS KMS key for server-side encryption in Amazon S3, you must choose\\na symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS\\nkeys. For more information about these keys, see Symmetric encryption KMS keys in the\\nAWS Key Management Service Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'PutObjectRequest putRequest = new PutObjectRequest(bucketName,\\nkeyName, file).withSSEAwsKeyManagementParams(new SSEAwsKeyManagementParams());', ''], ['', '', '']], [['', '', ''], ['', 'PutObjectRequest putRequest = new PutObjectRequest(bucketName,\\nkeyName, file).withSSEAwsKeyManagementParams(new\\nSSEAwsKeyManagementParams(keyID));', ''], ['', '', '']], [['', '', ''], ['', 'PutObjectRequest putRequest = new PutObjectRequest\\n{', '']]]\n",
      "[[['', 'BucketName = example-s3-bucket,\\nKey = keyName,\\n// other properties\\nServerSideEncryptionMethod = ServerSideEncryptionMethod.AWSKMS\\n};', ''], ['', '', '']], [['', '', ''], ['', 'PutObjectRequest putRequest1 = new PutObjectRequest\\n{\\nBucketName = example-s3-bucket,\\nKey = keyName,\\n// other properties\\nServerSideEncryptionMethod = ServerSideEncryptionMethod.AWSKMS,\\nServerSideEncryptionKeyManagementServiceKeyId = keyId\\n};', ''], ['', '', '']], [['', '', ''], ['', 'ClientConfiguration clientConfiguration = new ClientConfiguration();', '']]]\n",
      "[[['', 'clientConfiguration.setSignerOverride(\"AWSS3V4SignerType\");\\nAmazonS3Client s3client = new AmazonS3Client(\\nnew ProfileCredentialsProvider(), clientConfiguration);\\n...', ''], ['', '', '']], [['', '', ''], ['', 'AWSConfigs.S3Config.UseSignatureVersion4 = true;', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nS3 Bucket Keys aren't supported for dual-layer server-side encryption with AWS Key\\nManagement Service (AWS KMS) keys (DSSE-KMS).\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nUsing S3 Bucket Keys allows you to save on AWS KMS request costs by decreasing your\\nrequests to AWS KMS for Encrypt, GenerateDataKey, and Decrypt operations through\\nthe use of a bucket-level key. By design, subsequent requests that take advantage of this\\nbucket-level key do not result in AWS KMS API requests or validate access against the AWS\\nKMS key policy.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'AmazonS3 s3client = AmazonS3ClientBuilder.standard()\\n.withRegion(Regions.DEFAULT_REGION)\\n.build();\\nServerSideEncryptionByDefault serverSideEncryptionByDefault = new\\nServerSideEncryptionByDefault()\\n.withSSEAlgorithm(SSEAlgorithm.KMS);\\nServerSideEncryptionRule rule = new ServerSideEncryptionRule()\\n.withApplyServerSideEncryptionByDefault(serverSideEncryptionByDefault)\\n.withBucketKeyEnabled(true);\\nServerSideEncryptionConfiguration serverSideEncryptionConfiguration =', '']]]\n",
      "[[['', 'new ServerSideEncryptionConfiguration().withRules(Collections.singleton(rule));\\nSetBucketEncryptionRequest setBucketEncryptionRequest = new\\nSetBucketEncryptionRequest()\\n.withServerSideEncryptionConfiguration(serverSideEncryptionConfiguration)\\n.withBucketName(bucketName);\\ns3client.setBucketEncryption(setBucketEncryptionRequest);', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-encryption --bucket example-s3-bucket --server-side-encryption-\\nconfiguration \\'{\\n\"Rules\": [\\n{\\n\"ApplyServerSideEncryptionByDefault\": {\\n\"SSEAlgorithm\": \"aws:kms\",\\n\"KMSMasterKeyID\": \"KMS-Key-ARN\"\\n},\\n\"BucketKeyEnabled\": true\\n}\\n]\\n}\\'', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'AmazonS3 s3client = AmazonS3ClientBuilder.standard()\\n.withRegion(Regions.DEFAULT_REGION)\\n.build();\\nString bucketName = \"DOC-EXAMPLE-BUCKET1\";\\nString keyName = \"key name for object\";\\nString contents = \"file contents\";\\nPutObjectRequest putObjectRequest = new PutObjectRequest(bucketName, keyName,\\ncontents)\\n.withBucketKeyEnabled(true);\\ns3client.putObject(putObjectRequest);', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-object --bucket example-s3-bucket --key object key name --server-side-\\nencryption aws:kms --bucket-key-enabled --body filepath', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-encryption --bucket example-s3-bucket1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api head-object --bucket example-s3-bucket1 --key my_images.tar.bz2', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nS3 Bucket Keys aren't supported for DSSE-KMS.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Id\":\"PutObjectPolicy\",\\n\"Statement\":[{\\n\"Sid\":\"DenyUnEncryptedObjectUploads\",\\n\"Effect\":\"Deny\",\\n\"Principal\":\"*\",\\n\"Action\":\"s3:PutObject\",\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket1/*\",\\n\"Condition\":{\\n\"StringNotEquals\":{\\n\"s3:x-amz-server-side-encryption\":\"aws:kms:dsse\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can use multi-Region AWS KMS keys in Amazon S3. However, Amazon S3 currently\\ntreats multi-Region keys as though they were single-Region keys, and does not use the', '']]]\n",
      "[[['', 'multi-Region features of the key. For more information, see Using multi-Region keys in\\nAWS Key Management Service Developer Guide.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you want to use a KMS key that is owned by a different account, you must have\\npermission to use the key. For more information about cross-account permissions for KMS\\nkeys, see Creating KMS keys that other accounts can use in the AWS Key Management\\nService Developer Guide.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\n• If you change an object's method of encryption, a new object is created to replace the old\\none. If S3 Versioning is enabled, a new version of the object is created, and the existing\\nobject becomes an older version. The role that changes the property also becomes the\\nowner of the new object (or object version).\\n• If you change the encryption type for an object that has user-defined tags, you must\\nhave the s3:GetObjectTagging permission. If you're changing the encryption type\\nfor an object that doesn't have user-defined tags but is over 16 MB in size, you must also\\nhave the s3:GetObjectTagging permission.\\nIf the destination bucket policy denies the s3:GetObjectTagging action, the\\nencryption type for the object will be updated, but the user-defined tags will be removed\\nfrom the object, and you will receive an error.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nYou can use only KMS keys that are available in the same AWS Region as the bucket.\\nThe Amazon S3 console lists only the first 100 KMS keys in the same Region as the\\nbucket. To use a KMS key that is not listed, you must enter your KMS key ARN. If\\nyou want to use a KMS key that is owned by a different account, you must first have\\npermission to use the key, and then you must enter the KMS key ARN.', '']]]\n",
      "[[['', 'Amazon S3 supports only symmetric encryption KMS keys, and not asymmetric KMS\\nkeys. For more information, see Identifying asymmetric KMS keys in the AWS Key\\nManagement Service Developer Guide.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThis action applies encryption to all specified objects. When you're encrypting folders, wait\\nfor the save operation to finish before adding new objects to the folder.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\n• All GET and PUT requests for an object that's protected by AWS KMS fail if you don't\\nmake them by using Secure Sockets Layer (SSL), Transport Layer Security (TLS), or\\nSignature Version 4.\\n• If your object uses DSSE-KMS, don't send encryption request headers for GET requests\\nand HEAD requests, or you'll get an HTTP 400 (Bad Request) error.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nWhen you use an AWS KMS key for server-side encryption in Amazon S3, you must choose\\na symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS\\nkeys. For more information about these keys, see Symmetric encryption KMS keys in the\\nAWS Key Management Service Developer Guide.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket DOC-EXAMPLE-BUCKET --key example-object-key --server-\\nside-encryption aws:kms:dsse --ssekms-key-id example-key-id --body filepath', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api copy-object --bucket DOC-EXAMPLE-BUCKET --key example-object-key --\\nbody filepath --bucket DOC-EXAMPLE-BUCKET --key example-object-key --sse aws:kms:dsse\\n--sse-kms-key-id example-key-id --body filepath', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAmazon S3 does not store the encryption key that you provide. Instead, it stores a\\nrandomly salted Hash-based Message Authentication Code (HMAC) value of the encryption\\nkey to validate future requests. The salted HMAC value cannot be used to derive the value\\nof the encryption key or to decrypt the contents of the encrypted object. That means if you\\nlose the encryption key, you lose the object.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nAmazon S3 rejects any requests made over HTTP when using SSE-C. For security\\nconsiderations, we recommend that you consider any key that you erroneously send over\\nHTTP to be compromised. Discard the key and rotate as appropriate.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nIf you lose the encryption key, any GET request for an object without its encryption key\\nfails, and you lose the object.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"PutObjectPolicy\",\\n\"Statement\": [\\n{\\n\"Sid\": \"RequireSSECObjectUploads\",\\n\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/*\",\\n\"Condition\": {\\n\"Null\": {\\n\"s3:x-amz-server-side-encryption-customer-algorithm\": \"true\"\\n}\\n}\\n}', '']]]\n",
      "[[['', ']\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"PutObjectPolicy\",\\n\"Statement\": [\\n{\\n\"Sid\": \"RestrictSSECObjectUploads\",\\n\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/*\",\\n\"Condition\": {\\n\"Null\": {\\n\"s3:x-amz-server-side-encryption-customer-algorithm\": \"false\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nIf you use a bucket policy to require SSE-C on s3:PutObject, you must include the x-\\namz-server-side-encryption-customer-algorithm header in all multipart upload\\nrequests (CreateMultipartUpload, UploadPart, and CompleteMultipartUpload).', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nFor non-SSE-C objects, you can generate a presigned URL and directly paste that URL\\ninto a browser to access the data.\\nHowever, you cannot do this for SSE-C objects, because in addition to the presigned URL,\\nyou also must include HTTP headers that are specific to SSE-C objects. Therefore, you can\\nuse presigned URLs for SSE-C objects only programmatically.', ''], ['', '', '']], [['Name', 'Description'], ['x-amz-server-\\nside-encryption\\n-customer-algor\\nithm', 'Use this header to specify the encryption algorithm. The header\\nvalue must be AES256.'], ['x-amz-server-\\nside-encryption\\n-customer-key', 'Use this header to provide the 256-bit, base64-encoded encryption\\nkey for Amazon S3 to use to encrypt or decrypt your data.'], ['x-amz-server-\\nside-encryption\\n-customer-key-M\\nD5', 'Use this header to provide the base64-encoded 128-bit MD5 digest\\nof the encryption key according to RFC 1321. Amazon S3 uses this\\nheader for a message integrity check to ensure that the encryption\\nkey was transmitted without error.']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou cannot use the Amazon S3 console to upload an object and request SSE-C. You also\\ncannot use the console to update (for example, change the storage class or add metadata)\\nan existing object stored using SSE-C.', ''], ['', '', '']]]\n",
      "[[['Name', 'Description'], ['x-amz-copy-\\nsource-server-s\\nide-encryption\\n-customer-a\\nlgorithm', 'Include this header to specify the algorithm Amazon S3 should\\nuse to decrypt the source object. This value must be AES256.'], ['x-amz-copy-\\nsource-server-s\\nide-encryption\\n-customer-key', 'Include this header to provide the base64-encoded encryptio\\nn key for Amazon S3 to use to decrypt the source object. This\\nencryption key must be the one that you provided Amazon S3\\nwhen you created the source object. Otherwise, Amazon S3\\ncannot decrypt the object.'], ['x-amz-copy-\\nsource-server-s\\nide-encryption\\n-customer-key-\\nMD5', 'Include this header to provide the base64-encoded 128-bit MD5\\ndigest of the encryption key according to RFC 1321.']]]\n",
      "[[['', '', ''], ['', 'Note\\nThis example shows how to upload an object in a single operation. When using the\\nMultipart Upload API to upload large objects, you provide encryption information in the\\nsame way shown in this example. For examples of multipart uploads that use the AWS\\nSDK for Java, see Uploading an object using multipart upload.', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.*;\\nimport javax.crypto.KeyGenerator;\\nimport java.io.BufferedReader;\\nimport java.io.File;\\nimport java.io.IOException;\\nimport java.io.InputStreamReader;\\nimport java.security.NoSuchAlgorithmException;\\nimport java.security.SecureRandom;', '']]]\n",
      "[[['', 'public class ServerSideEncryptionUsingClientSideEncryptionKey {\\nprivate static SSECustomerKey SSE_KEY;\\nprivate static AmazonS3 S3_CLIENT;\\nprivate static KeyGenerator KEY_GENERATOR;\\npublic static void main(String[] args) throws IOException,\\nNoSuchAlgorithmException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString keyName = \"*** Key name ***\";\\nString uploadFileName = \"*** File path ***\";\\nString targetKeyName = \"*** Target key name ***\";\\n// Create an encryption key.\\nKEY_GENERATOR = KeyGenerator.getInstance(\"AES\");\\nKEY_GENERATOR.init(256, new SecureRandom());\\nSSE_KEY = new SSECustomerKey(KEY_GENERATOR.generateKey());\\ntry {\\nS3_CLIENT = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\n// Upload an object.\\nuploadObject(bucketName, keyName, new File(uploadFileName));\\n// Download the object.\\ndownloadObject(bucketName, keyName);\\n// Verify that the object is properly encrypted by attempting to\\nretrieve it\\n// using the encryption key.\\nretrieveObjectMetadata(bucketName, keyName);\\n// Copy the object into a new object that also uses SSE-C.\\ncopyObject(bucketName, keyName, targetKeyName);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.', '']]]\n",
      "[[['', 'e.printStackTrace();\\n}\\n}\\nprivate static void uploadObject(String bucketName, String keyName, File file) {\\nPutObjectRequest putRequest = new PutObjectRequest(bucketName, keyName,\\nfile).withSSECustomerKey(SSE_KEY);\\nS3_CLIENT.putObject(putRequest);\\nSystem.out.println(\"Object uploaded\");\\n}\\nprivate static void downloadObject(String bucketName, String keyName) throws\\nIOException {\\nGetObjectRequest getObjectRequest = new GetObjectRequest(bucketName,\\nkeyName).withSSECustomerKey(SSE_KEY);\\nS3Object object = S3_CLIENT.getObject(getObjectRequest);\\nSystem.out.println(\"Object content: \");\\ndisplayTextInputStream(object.getObjectContent());\\n}\\nprivate static void retrieveObjectMetadata(String bucketName, String keyName) {\\nGetObjectMetadataRequest getMetadataRequest = new\\nGetObjectMetadataRequest(bucketName, keyName)\\n.withSSECustomerKey(SSE_KEY);\\nObjectMetadata objectMetadata =\\nS3_CLIENT.getObjectMetadata(getMetadataRequest);\\nSystem.out.println(\"Metadata retrieved. Object size: \" +\\nobjectMetadata.getContentLength());\\n}\\nprivate static void copyObject(String bucketName, String keyName, String\\ntargetKeyName)\\nthrows NoSuchAlgorithmException {\\n// Create a new encryption key for target so that the target is saved using\\n// SSE-C.\\nSSECustomerKey newSSEKey = new SSECustomerKey(KEY_GENERATOR.generateKey());\\nCopyObjectRequest copyRequest = new CopyObjectRequest(bucketName, keyName,\\nbucketName, targetKeyName)\\n.withSourceSSECustomerKey(SSE_KEY)\\n.withDestinationSSECustomerKey(newSSEKey);\\nS3_CLIENT.copyObject(copyRequest);', '']]]\n",
      "[[['', 'System.out.println(\"Object copied\");\\n}\\nprivate static void displayTextInputStream(S3ObjectInputStream input) throws\\nIOException {\\n// Read one line at a time from the input stream and display each line.\\nBufferedReader reader = new BufferedReader(new InputStreamReader(input));\\nString line;\\nwhile ((line = reader.readLine()) != null) {\\nSystem.out.println(line);\\n}\\nSystem.out.println();\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor examples of uploading large objects using the multipart upload API, see Uploading\\nan object using multipart upload and Using the AWS SDKs (low-level API).', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.IO;\\nusing System.Security.Cryptography;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass SSEClientEncryptionKeyObjectOperationsTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";', '']]]\n",
      "[[['', 'private const string keyName = \"*** key name for new object created ***\";\\nprivate const string copyTargetKeyName = \"*** key name for object copy ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nclient = new AmazonS3Client(bucketRegion);\\nObjectOpsUsingClientEncryptionKeyAsync().Wait();\\n}\\nprivate static async Task ObjectOpsUsingClientEncryptionKeyAsync()\\n{\\ntry\\n{\\n// Create an encryption key.\\nAes aesEncryption = Aes.Create();\\naesEncryption.KeySize = 256;\\naesEncryption.GenerateKey();\\nstring base64Key = Convert.ToBase64String(aesEncryption.Key);\\n// 1. Upload the object.\\nPutObjectRequest putObjectRequest = await\\nUploadObjectAsync(base64Key);\\n// 2. Download the object and verify that its contents matches what\\nyou uploaded.\\nawait DownloadObjectAsync(base64Key, putObjectRequest);\\n// 3. Get object metadata and verify that the object uses AES-256\\nencryption.\\nawait GetObjectMetadataAsync(base64Key);\\n// 4. Copy both the source and target objects using server-side\\nencryption with\\n// a customer-provided encryption key.\\nawait CopyObjectAsync(aesEncryption, base64Key);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered ***. Message:\\'{0}\\' when writing\\nan object\", e.Message);\\n}\\ncatch (Exception e)\\n{', '']]]\n",
      "[[['', 'Console.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\nprivate static async Task<PutObjectRequest> UploadObjectAsync(string\\nbase64Key)\\n{\\nPutObjectRequest putObjectRequest = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nContentBody = \"sample text\",\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key\\n};\\nPutObjectResponse putObjectResponse = await\\nclient.PutObjectAsync(putObjectRequest);\\nreturn putObjectRequest;\\n}\\nprivate static async Task DownloadObjectAsync(string base64Key,\\nPutObjectRequest putObjectRequest)\\n{\\nGetObjectRequest getObjectRequest = new GetObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\n// Provide encryption information for the object stored in Amazon\\nS3.\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key\\n};\\nusing (GetObjectResponse getResponse = await\\nclient.GetObjectAsync(getObjectRequest))\\nusing (StreamReader reader = new\\nStreamReader(getResponse.ResponseStream))\\n{\\nstring content = reader.ReadToEnd();\\nif (String.Compare(putObjectRequest.ContentBody, content) == 0)\\nConsole.WriteLine(\"Object content is same as we uploaded\");\\nelse', '']]]\n",
      "[[['', 'Console.WriteLine(\"Error...Object content is not same.\");\\nif (getResponse.ServerSideEncryptionCustomerMethod ==\\nServerSideEncryptionCustomerMethod.AES256)\\nConsole.WriteLine(\"Object encryption method is AES256, same as\\nwe set\");\\nelse\\nConsole.WriteLine(\"Error...Object encryption method is not the\\nsame as AES256 we set\");\\n// Assert.AreEqual(putObjectRequest.ContentBody, content);\\n// Assert.AreEqual(ServerSideEncryptionCustomerMethod.AES256,\\ngetResponse.ServerSideEncryptionCustomerMethod);\\n}\\n}\\nprivate static async Task GetObjectMetadataAsync(string base64Key)\\n{\\nGetObjectMetadataRequest getObjectMetadataRequest = new\\nGetObjectMetadataRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\n// The object stored in Amazon S3 is encrypted, so provide the\\nnecessary encryption information.\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key\\n};\\nGetObjectMetadataResponse getObjectMetadataResponse = await\\nclient.GetObjectMetadataAsync(getObjectMetadataRequest);\\nConsole.WriteLine(\"The object metadata show encryption method used is:\\n{0}\", getObjectMetadataResponse.ServerSideEncryptionCustomerMethod);\\n// Assert.AreEqual(ServerSideEncryptionCustomerMethod.AES256,\\ngetObjectMetadataResponse.ServerSideEncryptionCustomerMethod);\\n}\\nprivate static async Task CopyObjectAsync(Aes aesEncryption, string\\nbase64Key)\\n{\\naesEncryption.GenerateKey();\\nstring copyBase64Key = Convert.ToBase64String(aesEncryption.Key);\\nCopyObjectRequest copyRequest = new CopyObjectRequest', '']]]\n",
      "[[['', \"{\\nSourceBucket = bucketName,\\nSourceKey = keyName,\\nDestinationBucket = bucketName,\\nDestinationKey = copyTargetKeyName,\\n// Information about the source object's encryption.\\nCopySourceServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nCopySourceServerSideEncryptionCustomerProvidedKey = base64Key,\\n// Information about the target object's encryption.\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = copyBase64Key\\n};\\nawait client.CopyObjectAsync(copyRequest);\\n}\\n}\\n}\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.CopyObjectRequest;\\nimport com.amazonaws.services.s3.model.PutObjectRequest;\\nimport com.amazonaws.services.s3.model.SSECustomerKey;\\nimport com.amazonaws.services.s3.transfer.Copy;\\nimport com.amazonaws.services.s3.transfer.TransferManager;\\nimport com.amazonaws.services.s3.transfer.TransferManagerBuilder;\\nimport com.amazonaws.services.s3.transfer.Upload;\\nimport javax.crypto.KeyGenerator;\\nimport java.io.File;\\nimport java.security.SecureRandom;\\npublic class ServerSideEncryptionCopyObjectUsingHLwithSSEC {\\npublic static void main(String[] args) throws Exception {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString fileToUpload = \"*** File path ***\";\\nString keyName = \"*** New object key name ***\";\\nString targetKeyName = \"*** Key name for object copy ***\";', '']]]\n",
      "[[['', 'try {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withRegion(clientRegion)\\n.withCredentials(new ProfileCredentialsProvider())\\n.build();\\nTransferManager tm = TransferManagerBuilder.standard()\\n.withS3Client(s3Client)\\n.build();\\n// Create an object from a file.\\nPutObjectRequest putObjectRequest = new PutObjectRequest(bucketName,\\nkeyName, new File(fileToUpload));\\n// Create an encryption key.\\nKeyGenerator keyGenerator = KeyGenerator.getInstance(\"AES\");\\nkeyGenerator.init(256, new SecureRandom());\\nSSECustomerKey sseCustomerEncryptionKey = new\\nSSECustomerKey(keyGenerator.generateKey());\\n// Upload the object. TransferManager uploads asynchronously, so this\\ncall\\n// returns immediately.\\nputObjectRequest.setSSECustomerKey(sseCustomerEncryptionKey);\\nUpload upload = tm.upload(putObjectRequest);\\n// Optionally, wait for the upload to finish before continuing.\\nupload.waitForCompletion();\\nSystem.out.println(\"Object created.\");\\n// Copy the object and store the copy using SSE-C with a new key.\\nCopyObjectRequest copyObjectRequest = new CopyObjectRequest(bucketName,\\nkeyName, bucketName, targetKeyName);\\nSSECustomerKey sseTargetObjectEncryptionKey = new\\nSSECustomerKey(keyGenerator.generateKey());\\ncopyObjectRequest.setSourceSSECustomerKey(sseCustomerEncryptionKey);\\ncopyObjectRequest.setDestinationSSECustomerKey(sseTargetObjectEncryptionKey);\\n// Copy the object. TransferManager copies asynchronously, so this call\\nreturns\\n// immediately.\\nCopy copy = tm.copy(copyObjectRequest);', '']]]\n",
      "[[['', '// Optionally, wait for the upload to finish before continuing.\\ncopy.waitForCompletion();\\nSystem.out.println(\"Copy complete.\");\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'TransferUtilityUploadRequest request = new TransferUtilityUploadRequest()\\n{\\nFilePath = filePath,\\nBucketName = existingBucketName,\\nKey = keyName,\\n// Provide encryption information.\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key,\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.IO;\\nusing System.Security.Cryptography;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass SSECLowLevelMPUcopyObjectTest\\n{\\nprivate const string existingBucketName = \"*** bucket name ***\";\\nprivate const string sourceKeyName = \"*** source object key name\\n***\";\\nprivate const string targetKeyName = \"*** key name for the target\\nobject ***\";\\nprivate const string filePath = @\"*** file path ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\nstatic void Main()\\n{\\ns3Client = new AmazonS3Client(bucketRegion);\\nCopyObjClientEncryptionKeyAsync().Wait();\\n}', '']]]\n",
      "[[['', 'private static async Task CopyObjClientEncryptionKeyAsync()\\n{\\nAes aesEncryption = Aes.Create();\\naesEncryption.KeySize = 256;\\naesEncryption.GenerateKey();\\nstring base64Key = Convert.ToBase64String(aesEncryption.Key);\\nawait CreateSampleObjUsingClientEncryptionKeyAsync(base64Key,\\ns3Client);\\nawait CopyObjectAsync(s3Client, base64Key);\\n}\\nprivate static async Task CopyObjectAsync(IAmazonS3 s3Client, string\\nbase64Key)\\n{\\nList<CopyPartResponse> uploadResponses = new List<CopyPartResponse>();\\n// 1. Initialize.\\nInitiateMultipartUploadRequest initiateRequest = new\\nInitiateMultipartUploadRequest\\n{\\nBucketName = existingBucketName,\\nKey = targetKeyName,\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key,\\n};\\nInitiateMultipartUploadResponse initResponse =\\nawait s3Client.InitiateMultipartUploadAsync(initiateRequest);\\n// 2. Upload Parts.\\nlong partSize = 5 * (long)Math.Pow(2, 20); // 5 MB\\nlong firstByte = 0;\\nlong lastByte = partSize;\\ntry\\n{\\n// First find source object size. Because object is stored\\nencrypted with\\n// customer provided key you need to provide encryption\\ninformation in your request.\\nGetObjectMetadataRequest getObjectMetadataRequest = new\\nGetObjectMetadataRequest()', '']]]\n",
      "[[['', '{\\nBucketName = existingBucketName,\\nKey = sourceKeyName,\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key // \" *\\n**source object encryption key ***\"\\n};\\nGetObjectMetadataResponse getObjectMetadataResponse = await\\ns3Client.GetObjectMetadataAsync(getObjectMetadataRequest);\\nlong filePosition = 0;\\nfor (int i = 1; filePosition <\\ngetObjectMetadataResponse.ContentLength; i++)\\n{\\nCopyPartRequest copyPartRequest = new CopyPartRequest\\n{\\nUploadId = initResponse.UploadId,\\n// Source.\\nSourceBucket = existingBucketName,\\nSourceKey = sourceKeyName,\\n// Source object is stored using SSE-C. Provide encryption\\ninformation.\\nCopySourceServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nCopySourceServerSideEncryptionCustomerProvidedKey =\\nbase64Key, //\"***source object encryption key ***\",\\nFirstByte = firstByte,\\n// If the last part is smaller then our normal part size\\nthen use the remaining size.\\nLastByte = lastByte >\\ngetObjectMetadataResponse.ContentLength ?\\ngetObjectMetadataResponse.ContentLength - 1 :\\nlastByte,\\n// Target.\\nDestinationBucket = existingBucketName,\\nDestinationKey = targetKeyName,\\nPartNumber = i,\\n// Encryption information for the target object.\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key', '']]]\n",
      "[[['', '};\\nuploadResponses.Add(await\\ns3Client.CopyPartAsync(copyPartRequest));\\nfilePosition += partSize;\\nfirstByte += partSize;\\nlastByte += partSize;\\n}\\n// Step 3: complete.\\nCompleteMultipartUploadRequest completeRequest = new\\nCompleteMultipartUploadRequest\\n{\\nBucketName = existingBucketName,\\nKey = targetKeyName,\\nUploadId = initResponse.UploadId,\\n};\\ncompleteRequest.AddPartETags(uploadResponses);\\nCompleteMultipartUploadResponse completeUploadResponse =\\nawait s3Client.CompleteMultipartUploadAsync(completeRequest);\\n}\\ncatch (Exception exception)\\n{\\nConsole.WriteLine(\"Exception occurred: {0}\", exception.Message);\\nAbortMultipartUploadRequest abortMPURequest = new\\nAbortMultipartUploadRequest\\n{\\nBucketName = existingBucketName,\\nKey = targetKeyName,\\nUploadId = initResponse.UploadId\\n};\\ns3Client.AbortMultipartUpload(abortMPURequest);\\n}\\n}\\nprivate static async Task\\nCreateSampleObjUsingClientEncryptionKeyAsync(string base64Key, IAmazonS3\\ns3Client)\\n{\\n// List to store upload part responses.\\nList<UploadPartResponse> uploadResponses = new\\nList<UploadPartResponse>();\\n// 1. Initialize.', '']]]\n",
      "[[['', 'InitiateMultipartUploadRequest initiateRequest = new\\nInitiateMultipartUploadRequest\\n{\\nBucketName = existingBucketName,\\nKey = sourceKeyName,\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key\\n};\\nInitiateMultipartUploadResponse initResponse =\\nawait s3Client.InitiateMultipartUploadAsync(initiateRequest);\\n// 2. Upload Parts.\\nlong contentLength = new FileInfo(filePath).Length;\\nlong partSize = 5 * (long)Math.Pow(2, 20); // 5 MB\\ntry\\n{\\nlong filePosition = 0;\\nfor (int i = 1; filePosition < contentLength; i++)\\n{\\nUploadPartRequest uploadRequest = new UploadPartRequest\\n{\\nBucketName = existingBucketName,\\nKey = sourceKeyName,\\nUploadId = initResponse.UploadId,\\nPartNumber = i,\\nPartSize = partSize,\\nFilePosition = filePosition,\\nFilePath = filePath,\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key\\n};\\n// Upload part and add response to our list.\\nuploadResponses.Add(await\\ns3Client.UploadPartAsync(uploadRequest));\\nfilePosition += partSize;\\n}\\n// Step 3: complete.', '']]]\n",
      "[[['', 'CompleteMultipartUploadRequest completeRequest = new\\nCompleteMultipartUploadRequest\\n{\\nBucketName = existingBucketName,\\nKey = sourceKeyName,\\nUploadId = initResponse.UploadId,\\n//PartETags = new List<PartETag>(uploadResponses)\\n};\\ncompleteRequest.AddPartETags(uploadResponses);\\nCompleteMultipartUploadResponse completeUploadResponse =\\nawait s3Client.CompleteMultipartUploadAsync(completeRequest);\\n}\\ncatch (Exception exception)\\n{\\nConsole.WriteLine(\"Exception occurred: {0}\", exception.Message);\\nAbortMultipartUploadRequest abortMPURequest = new\\nAbortMultipartUploadRequest\\n{\\nBucketName = existingBucketName,\\nKey = sourceKeyName,\\nUploadId = initResponse.UploadId\\n};\\nawait s3Client.AbortMultipartUploadAsync(abortMPURequest);\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['Gateway endpoints for Amazon S3', 'Interface endpoints for Amazon S3'], ['In both cases, your network traffic remains on the AWS network.', None], ['Use Amazon S3 public IP addresses', 'Use private IP addresses from\\nyour VPC to access Amazon S3'], ['Use the same Amazon S3 DNS names', 'Require endpoint-specific\\nAmazon S3 DNS names'], ['Do not allow access from on premises', 'Allow access from on premises'], ['Do not allow access from another AWS Region', 'Allow access from a VPC in\\nanother AWS Region by using VPC\\npeering or AWS Transit Gateway'], ['Not billed', 'Billed']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nTo take advantage of the lowest cost network path when using Enable private DNS only\\nfor inbound endpoints, a gateway endpoint must be present in your VPC. The presence\\nof a gateway endpoint helps ensure that in-VPC traffic always routes over the AWS private\\nnetwork when the Enable private DNS only for inbound endpoints option is selected.\\nYou must maintain this gateway endpoint while you have the Enable private DNS only for\\ninbound endpoints option selected. If you want to delete your gateway endpoint you must\\nfirst clear Enable private DNS only for inbound endpoints.\\nIf you want to update an existing interface endpoint to Enable private DNS only for\\ninbound endpoints, first confirm that your VPC has an S3 gateway endpoint. For more\\ninformation about gateway endpoints and managing private DNS names, see Gateway VPC\\nendpoints and Manage DNS names respectively in the AWS PrivateLink Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws ec2 create-vpc-endpoint \\\\\\n--region us-east-1 \\\\\\n--service-name s3-service-name \\\\', '']]]\n",
      "[[['', '--vpc-id client-vpc-id \\\\\\n--subnet-ids client-subnet-id \\\\\\n--vpc-endpoint-type Interface \\\\\\n--private-dns-enabled \\\\\\n--ip-address-type ip-address-type \\\\\\n--dns-options PrivateDnsOnlyForInboundResolverEndpoint=true \\\\\\n--security-group-ids client-sg-id', ''], ['', '', '']], [['', '', ''], ['', 'aws ec2 modify-vpc-endpoint \\\\\\n--region us-east-1 \\\\\\n--vpc-endpoint-id client-vpc-id \\\\\\n--private-dns-enabled \\\\\\n--dns-options PrivateDnsOnlyForInboundResolverEndpoint=false', ''], ['', '', '']], [['', '', ''], ['', 'aws ec2 modify-vpc-endpoint \\\\\\n--region us-east-1 \\\\\\n--vpc-endpoint-id client-vpc-id \\\\\\n--private-dns-enabled \\\\\\n--dns-options PrivateDnsOnlyForInboundResolverEndpoint=true', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3 ls s3://my-bucket/ --region us-east-1 --endpoint-url\\nhttps://bucket.vpce-1a2b3c4d-5e6f.s3.us-east-1.vpce.amazonaws.com', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api list-objects-v2 --bucket arn:aws:s3:us-east-1:123456789012:accesspoint/\\naccesspointexamplename --region us-east-1 --endpoint-url\\nhttps://accesspoint.vpce-1a2b3c4d-5e6f.s3.us-east-1.vpce.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-objects-v2 --\\nbucket accesspointexamplename-8tyekmigicmhun8n9kwpfur39dnw4use1a-s3alias\\n--region us-east-1 --endpoint-url https://bucket.vpce-1a2b3c4d-5e6f.s3.us-\\neast-1.vpce.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'aws configure set default.s3.addressing_style virtual', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api list-objects-v2 --\\nbucket accesspointexamplename-8tyekmigicmhun8n9kwpfur39dnw4use1a-s3alias --\\nregion us-east-1 --endpoint-url https://accesspoint.vpce-1a2b3c4d-5e6f.s3.us-\\neast-1.vpce.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control --region us-east-1 --endpoint-url\\nhttps://control.vpce-1a2b3c4d-5e6f.s3.us-east-1.vpce.amazonaws.com list-jobs --\\naccount-id 12345678', ''], ['', '', '']], [['', '', ''], ['', \"s3_client = session.client(\\nservice_name='s3',\\nregion_name='us-east-1',\\nendpoint_url='https://bucket.vpce-1a2b3c4d-5e6f.s3.us-east-1.vpce.amazonaws.com'\\n)\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"ap_client = session.client(\\nservice_name='s3',\\nregion_name='us-east-1',\\nendpoint_url='https://accesspoint.vpce-1a2b3c4d-5e6f.s3.us-\\neast-1.vpce.amazonaws.com'\\n)\", ''], ['', '', '']], [['', '', ''], ['', \"control_client = session.client(\\nservice_name='s3control',\\nregion_name='us-east-1',\\nendpoint_url='https://control.vpce-1a2b3c4d-5e6f.s3.us-east-1.vpce.amazonaws.com'\\n)\", ''], ['', '', '']], [['', '', ''], ['', '// bucket client\\nfinal AmazonS3 s3 = AmazonS3ClientBuilder.standard().withEndpointConfiguration(\\nnew AwsClientBuilder.EndpointConfiguration(\\n\"https://bucket.vpce-1a2b3c4d-5e6f.s3.us-east-1.vpce.amazonaws.com\",\\nRegions.DEFAULT_REGION.getName()\\n)\\n).build();\\nList<Bucket> buckets = s3.listBuckets();', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '// accesspoint client\\nfinal AmazonS3 s3accesspoint =\\nAmazonS3ClientBuilder.standard().withEndpointConfiguration(\\nnew AwsClientBuilder.EndpointConfiguration(\\n\"https://accesspoint.vpce-1a2b3c4d-5e6f.s3.us-\\neast-1.vpce.amazonaws.com\",\\nRegions.DEFAULT_REGION.getName()\\n)\\n).build();\\nObjectListing objects = s3accesspoint.listObjects(\"arn:aws:s3:us-\\neast-1:123456789012:accesspoint/prod\");', ''], ['', '', '']], [['', '', ''], ['', '// control client\\nfinal AWSS3Control s3control =\\nAWSS3ControlClient.builder().withEndpointConfiguration(\\nnew AwsClientBuilder.EndpointConfiguration(\\n\"https://control.vpce-1a2b3c4d-5e6f.s3.us-\\neast-1.vpce.amazonaws.com\",\\nRegions.DEFAULT_REGION.getName()\\n)\\n).build();\\nfinal ListJobsResult jobs = s3control.listJobs(new ListJobsRequest());', ''], ['', '', '']], [['', '', ''], ['', '// bucket client\\nRegion region = Region.US_EAST_1;\\ns3Client = S3Client.builder().region(region)\\n.endpointOverride(URI.create(\"https://bucket.vpce-1a2b3c4d-5e6f.s3.us-\\neast-1.vpce.amazonaws.com\"))', '']]]\n",
      "[[['', '.build()', ''], ['', '', '']], [['', '', ''], ['', '// accesspoint client\\nRegion region = Region.US_EAST_1;\\ns3Client = S3Client.builder().region(region)\\n.endpointOverride(URI.create(\"https://accesspoint.vpce-1a2b3c4d-5e6f.s3.us-\\neast-1.vpce.amazonaws.com\"))\\n.build()', ''], ['', '', '']], [['', '', ''], ['', '// control client\\nRegion region = Region.US_EAST_1;\\ns3ControlClient = S3ControlClient.builder().region(region)\\n.endpointOverride(URI.create(\"https://control.vpce-1a2b3c4d-5e6f.s3.us-\\neast-1.vpce.amazonaws.com\"))\\n.build()', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"Policy1415115909151\",\\n\"Statement\": [\\n{ \"Sid\": \"Access-to-specific-bucket-only\",\\n\"Principal\": \"*\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:PutObject\"\\n],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\"arn:aws:s3:::example-s3-bucket1\",\\n\"arn:aws:s3:::example-s3-bucket1/*\"]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Statement\": [\\n{\\n\"Sid\": \"Access-to-bucket-in-specific-account-only\",', '']]]\n",
      "[[['', '\"Principal\": \"*\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:PutObject\"\\n],\\n\"Effect\": \"Deny\",\\n\"Resource\": \"arn:aws:s3:::*\",\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"aws:ResourceAccount\": \"111122223333\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo specify the AWS account ID of the resource being accessed, you can use either the\\naws:ResourceAccount or the s3:ResourceAccount key in your IAM policy. However,\\nbe aware that some AWS services rely on access to AWS managed buckets. Therefore, using\\nthe aws:ResourceAccount or s3:ResourceAccount key in your IAM policy might also\\naffect access to these resources.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\n• When applying the following Amazon S3 bucket policy to restrict access to only certain\\nVPC endpoints, you might block your access to the bucket without intending to do', '']]]\n",
      "[[['', \"so. Bucket policies that are intended to specifically limit bucket access to connections\\noriginating from your VPC endpoint can block all connections to the bucket. For\\ninformation about how to fix this issue, see My bucket policy has the wrong VPC or VPC\\nendpoint ID. How can I fix the policy so that I can access the bucket? in the AWS Support\\nKnowledge Center.\\n• Before using the following example policy, replace the VPC endpoint ID with an\\nappropriate value for your use case. Otherwise, you won't be able to access your bucket.\\n• This policy disables console access to the specified bucket, because console requests don't\\noriginate from the specified VPC endpoint.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"Policy1415115909152\",\\n\"Statement\": [\\n{ \"Sid\": \"Access-to-specific-VPCE-only\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:*\",\\n\"Effect\": \"Deny\",\\n\"Resource\": [\"arn:aws:s3:::example-s3-bucket2\",\\n\"arn:aws:s3:::example-s3-bucket2/*\"],\\n\"Condition\": {\"StringNotEquals\": {\"aws:sourceVpce\": \"vpce-1a2b3c4d\"}}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['Resource type', 'Amazon S3\\nfeature', 'Description'], ['bucket', 'Core features', 'A bucket is a container for objects. To store an object in S3,\\ncreate a bucket and then upload one or more objects to\\nthe bucket. For more information, see Creating, configuri\\nng, and working with Amazon S3 buckets.'], ['object', None, 'An object can be a file and any metadata that describes\\nthat file. When an object is in the bucket, you can open\\nit, download it, and move it. For more information, see\\nUploading, downloading, and working with objects in\\nAmazon S3.'], ['accesspoi\\nnt', 'Access Points', 'Access Points are named network endpoints that\\nare attached to buckets that you can use to perform\\nAmazon S3 object operations, such as GetObject and\\nPutObject . Each access point has distinct permissions,\\nnetwork controls, and a customized access point policy\\nthat works in conjunction with the bucket policy that is\\nattached to the underlying bucket. You can configure any\\naccess point to accept requests only from a virtual private\\ncloud (VPC) or configure custom block public access\\nsettings for each access point. For more information, see\\nManaging data access with Amazon S3 access points.'], ['objectlam\\nbdaaccess\\npoint', '', \"An Object Lambda Access Point is an access point for a\\nbucket that is also associated with a Lambda function.\\nWith Object Lambda Access Point, you can add your own\\ncode to Amazon S3 G ET, LIST, and HEAD requests to\\nmodify and process data as it's returned to an application.\\nFor more information, see Creating Object Lambda Access\\nPoints.\"], ['multiregi\\nonaccessp\\noint', '', 'Multi-Region Access Points provide a global endpoint\\nthat applications can use to fulfill requests from Amazo\\nn S3 buckets that are located in multiple AWS Regions.']]]\n",
      "[[['Resource type', 'Amazon S3\\nfeature', 'Description'], ['', '', \"You can use Multi-Region Access Points to build multi-\\nRegion applications with the same architecture that's\\nused in a single Region, and then run those applications\\nanywhere in the world. Instead of sending requests over\\nthe congested public internet, application requests made\\nto a Multi-Region Access Point global endpoint automatic\\nally route through the AWS global network to the closest\\nproximity Amazon S3 bucket. For more information, see\\nMulti-Region Access Points in Amazon S3.\"], ['job', 'S3 Batch\\nOperations', 'A job is a resource of the S3 Batch Operations feature.\\nYou can use S3 Batch Operations to perform large-sca\\nle batch operations on lists of Amazon S3 objects that\\nyou specify. Amazon S3 tracks the progress of the batch\\noperation job, sends notifications, and stores a detailed\\ncompletion report of all actions, providing you with a fully\\nmanaged, auditable, and serverless experience. For more\\ninformation, see Performing large-scale batch operations\\non Amazon S3 objects.'], ['storagele\\nnsconfigu\\nration', 'S3 Storage\\nLens', 'An S3 Storage Lens configuration collects organization-\\nwide storage metrics and user data across accounts.\\nS3 Storage Lens provides admins with a single view of\\nobject storage usage and activity across hundreds, or even\\nthousands, of accounts in an organization, with details to\\ngenerate insights at multiple aggregation levels. For more\\ninformation, see A ssessing your storage activity and usage\\nwith Amazon S3 Storage Lens.']]]\n",
      "[[['Resource type', 'Amazon S3\\nfeature', 'Description'], ['storagele\\nnsgroup', '', 'An S3 Storage Lens group aggregates metrics by using\\ncustom filters based on object metadata. S3 Storage Lens\\ngroups help you investigate characteristics of your data,\\nsuch as distribution of objects by age, your most common\\nfile types, and more. For more information, see Working\\nwith S3 Storage Lens groups.'], ['accessgra\\nntsinstan\\nce', 'S3 Access\\nGrants', 'An S3 Access Grants instance is a container for the S3\\ngrants that you create. With S3 Access Grants, you can\\ncreate grants to your Amazon S3 data for IAM identitie\\ns within your account, IAM identities in other accounts\\n(cross-account), and directory identities added to AWS IAM\\nIdentity Center from your corporate directory. For more\\ninformation about S3 Access Grants, see Managing access\\nwith S3 Access Grants.'], ['accessgra\\nntslocati\\non', None, 'An Access Grants Location is a bucket, prefix within a\\nbucket, or an object that you register in your S3 Access\\nGrants instance. You must register locations within the\\nS3 Access Grants instance before you can create a grant to\\nthat location. Then, with S3 Access Grants, you can grant\\naccess to the bucket, prefix, or object for IAM identitie\\ns within your account, IAM identities in other accounts\\n(cross-account), and directory identities added to AWS IAM\\nIdentity Center from your corporate directory. For more\\ninformation about S3 Access Grants, see Managing access\\nwith S3 Access Grants']]]\n",
      "[[['Resource type', 'Amazon S3\\nfeature', 'Description'], ['accessgra\\nnt', '', 'An Access Grant is an individual grant to your Amazon S3\\ndata. With S3 Access Grants, you can create grants to your\\nAmazon S3 data for IAM identities within your account,\\nIAM identities in other accounts (cross-account), and\\ndirectory identities added to AWS IAM Identity Center from\\nyour corporate directory. For more information about S3\\nAccess Grants, see Managing access with S3 Access Grants']], [['', '', ''], ['', 'Note\\nThe following information applies to general purpose buckets. Directory buckets do not\\nsupport tagging, and they have prefix limitations. For more information, see AWS Identity\\nand Access Management (IAM) for S3 Express One Zone.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\nWe recommend that you don't use the AWS account root user credentials to make\\nauthenticated requests. Instead, create an IAM role and grant that role full access. We\\nrefer to users with this role as administrator users. You can use credentials assigned to\\nthe administrator role, instead of AWS account root user credentials, to interact with AWS\\nand perform tasks, such as create a bucket, create users, and grant permissions. For more\\ninformation, see AWS account root user credentials and IAM user credentials in the AWS\\nGeneral Reference, and see Security best practices in IAM in the IAM User Guide.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nPermission delegation — If an AWS account owns a resource, it can grant those\\npermissions to another AWS account. That account can then delegate those permissions, or\\na subset of them, to users in the same account. This is referred to as permission delegation.\\nBut an account that receives permissions from another account cannot delegate those\\npermissions \"cross-account\" to another AWS account.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\":\"BucketLevelReadPermissions\",\\n\"Effect\":\"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789101:role/s3-role\"\\n},\\n\"Action\":[\"s3:GetObject\"],\\n\"Resource\":[\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*\"]\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen creating policies, avoid the use of wildcard characters (*) in the Principal\\nelement because using a wildcard character allows anyone to access your Amazon S3\\nresources. Instead, explicitly list users or groups that are allowed to access the bucket, or\\nlist conditions that must be met by using a condition clause in the policy. Also, rather than\\nincluding a wildcard character for the actions of your users or groups, grant them specific\\npermissions when applicable.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AssignARoleActions\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\",\\n\"s3:GetObject\",\\n\"s3:ListBucket\",\\n\"s3:DeleteObject\",\\n\"s3:GetBucketLocation\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*\",\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1\"\\n]\\n},\\n{\\n\"Sid\": \"AssignARoleActions2\",\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:ListAllMyBuckets\",\\n\"Resource\": \"*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nThe majority of modern use cases in Amazon S3 do not require the use of ACLs.', ''], ['', '', '']], [['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<AccessControlPolicy xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Owner>', '']]]\n",
      "[[['', '<ID>Owner-Canonical-User-ID</ID>\\n<DisplayName>owner-display-name</DisplayName>\\n</Owner>\\n<AccessControlList>\\n<Grant>\\n<Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"Canonical\\nUser\">\\n<ID>Owner-Canonical-User-ID</ID>\\n<DisplayName>display-name</DisplayName>\\n</Grantee>\\n<Permission>FULL_CONTROL</Permission>\\n</Grant>\\n</AccessControlList>\\n</AccessControlPolicy>', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIn a resource-based access policy, such as a bucket policy, or in an identity-based policy, you\\ncan specify the following:\\n• An action or an array of actions in the Action element of the policy statement.\\n• In the Effect element of the policy statement, you can specify Allow to grant the\\nactions listed, or you can specify Deny to block the listed actions. To further maintain the\\npractice of least privileges, Deny statements in the Effect element of the access policy\\nshould be as broad as possible, and Allow statements should be as narrow as possible.\\nDeny effects paired with the s3:* action are another good way to implement opt-in best\\npractices for the identities that are included in policy condition statements.\\n• A condition key in the Condition element of a policy statement.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Warning\\nWe do not recommend this method. Instead, we recommend you use Amazon S3 static\\nwebsites as a part of Amazon CloudFront. For more information, see the previous option,\\nor see Getting started with a secure static website.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['IAM feature', 'Amazon S3 support'], ['Identity-based policies', 'Yes'], ['Resource-based policies', 'Yes'], ['Policy actions', 'Yes'], ['Policy resources', 'Yes'], ['Policy condition keys (service-specific)', 'Yes']]]\n",
      "[[['IAM feature', 'Amazon S3 support'], ['ACLs', 'Yes'], ['ABAC (tags in policies)', 'Partial'], ['Temporary credentials', 'Yes'], ['Forward access sessions (FAS)', 'Yes'], ['Service roles', 'Yes'], ['Service-linked roles', 'Partial']], [['Supports identity-based policies', 'Yes']]]\n",
      "[[['Supports resource-based policies', 'Yes']]]\n",
      "[[['', '', ''], ['', '\"AWS\":\"account-ARN\"', ''], ['', '', '']], [['', '', ''], ['', '\"Principal\":{\"AWS\":\"arn:aws:iam::AccountIDWithoutHyphens:root\"}', ''], ['', '', '']], [['', '', ''], ['', '\"Principal\":{\"AWS\":\\n[\"arn:aws:iam::AccountID1WithoutHyphens:root\",\"arn:aws:iam::AccountID2WithoutHyphens:roo', 't'], ['', '', '']], [['', '', ''], ['', '\"Principal\":{\"AWS\":\"arn:aws:iam::account-number-without-hyphens:user/username\"}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf an IAM identity is deleted after you update your bucket policy, the bucket policy will\\nshow a unique identifier in the principal element instead of an ARN. These unique IDs are\\nnever reused, so you can safely remove principals with unique identifiers from all of your\\npolicy statements. For more information about unique identifiers, see IAM identifiers in the\\nIAM User Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nUse caution when granting anonymous access to your Amazon S3 bucket. When you grant\\nanonymous access, anyone in the world can access your bucket. We highly recommend that\\nyou never grant any kind of anonymous write access to your S3 bucket.', ''], ['', '', '']], [['', '', ''], ['', '\"Principal\":\"*\"', ''], ['', '', '']], [['', '', ''], ['', '\"Principal\":{\"AWS\":\"*\"}', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nBecause anyone can create an AWS account, the security level of these two methods is\\nequivalent, even though they function differently.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:*\",\\n\"Resource\": <bucket ARN>,\\n\"Condition\": {\\n\"Boolean\": { \"aws:SecureTransport\" : \"false\"}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"Principal\":{\"Service\":\"cloudfront.amazonaws.com\"}', ''], ['', '', '']], [['', '', ''], ['', '\"Condition\": {\\n\"StringEquals\": {\\n\"AWS:SourceArn\":\\n\"arn:aws:cloudfront::111122223333:distribution/CloudFront-distribution-ID\"\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['Supports policy actions', 'Yes']]]\n",
      "[[['', '', ''], ['', 's3', ''], ['', '', '']], [['', '', ''], ['', '\"Action\": [\\n\"s3:action1\",\\n\"s3:action2\"\\n]', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Allow Akua to list objects in the bucket\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::12345678901:user/Akua\"\\n},\\n\"Action\": [\\n\"s3:ListBucket\"', '']]]\n",
      "[[['', '],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:us-west-2:123456789012:accesspoint/DOC-EXAMPLE-ACCESS-POINT\"', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Allow Akua to list objects in the bucket through access point\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::12345678901:user/Akua\"\\n},\\n\"Action\": [\\n\"s3:ListBucket\"\\n],\\n\"Resource\": \"arn:aws:s3:us-west-2:123456789012:accesspoint/DOC-EXAMPLE-\\nACCESS-POINT\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nNot all bucket operations are supported by S3 Access Point. For more information, see\\nAccess point compatibility with S3 operations.', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/prefix/*\"', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe object ARN must contain a forward slash after the bucket name, as seen in the previous\\nexamples.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Allow Akua to upload objects\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::12345678901:user/Akua\"\\n},\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n}', '']]]\n",
      "[[['', ']\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:us-west-2:123456789012:accesspoint/DOC-EXAMPLE-ACCESS-POINT/\\nobject/*\"', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:us-west-2:123456789012:accesspoint/DOC-EXAMPLE-ACCESS-POINT/\\nobject/prefix/*\"', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Allow Akua to get objects through access point\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::12345678901:user/Akua\"\\n},\\n\"Action\": [\\n\"s3:GetObject\"\\n],\\n\"Resource\": \"arn:aws:s3:us-west-2:123456789012:accesspoint/DOC-EXAMPLE-\\nACCESS-POINT/object/*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nNot all object operations are supported by S3 Access Point. For more information, see\\nAccess point compatibility with S3 operations.', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:us-west-2:123456789012:accesspoint/DOC-EXAMPLE-ACCESS-POINT\"', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Grant permission to retrieve the access point policy of access\\npoint DOC-EXAMPLE-ACCESS-POINT\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetAccessPointPolicy\"\\n],\\n\"Resource\": \"arn:aws:s3:*:123456789012:access point/DOC-EXAMPLE-ACCESS-\\nPOINT\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '\"Resource\": \"arn:aws:s3:*:123456789012:job/*\"', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Allow describing the Batch operation job DOC-EXAMPLE-JOB\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:DescribeJob\"\\n],', '']]]\n",
      "[[['', '\"Resource\": \"arn:aws:s3:*:123456789012:job/DOC-EXAMPLE-JOB\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Sid\":\"Allow retrieving the account-level Public Access Block settings\",\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetAccountPublicAccessBlock\"\\n],\\n\"Resource\":[\\n\"*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['Supports policy resources', 'Yes']], [['', '', ''], ['', '\"Resource\": \"*\"', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": [\\n\"EXAMPLE-RESOURCE-1\",\\n\"EXAMPLE-RESOURCE-2\"', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'arn:aws:s3:::examplebucket/*', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3:::*', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3:::example?bucket/*', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3:::bucket_name/developers/${aws:username}/', ''], ['', '', '']]]\n",
      "[[['Supports service-specific policy condition keys', 'Yes']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"statement1\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Dave\"\\n},\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket1/*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3:x-amz-storage-class\": [\\n\"STANDARD_IA\"\\n]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['Supports ACLs', 'Yes']], [['', '', ''], ['', 'Important\\nA majority of modern use cases in Amazon S3 no longer require the use of ACLs.', ''], ['', '', '']], [['Supports ABAC (tags in policies)', 'Partial']]]\n",
      "[[['Supports temporary credentials', 'Yes']]]\n",
      "[[['Supports forward access sessions (FAS)', 'Yes']], [['Supports service roles', 'Yes']]]\n",
      "[[['', '', ''], ['', 'Warning\\nChanging the permissions for a service role might break Amazon S3 functionality. Edit\\nservice roles only when Amazon S3 provides guidance to do so.', ''], ['', '', '']], [['Supports service-linked roles', 'Partial']], [['Service name in the policy', 'S3 feature', 'More information'], ['s3.amazonaws.com', 'S3 Replication', 'Setting up live replication'], ['s3.amazonaws.com', 'S3 event notifications', 'Amazon S3 Event Notificat\\nions'], ['s3.amazonaws.com', 'S3 Inventory', 'Amazon S3 Inventory'], ['access-grants.s3.a\\nmazonaws.com', 'S3 Access Grants', 'Register a location'], ['batchoperations.s3\\n.amazonaws.com', 'S3 Batch Operations', 'Granting permissions for\\nAmazon S3 Batch Operations'], ['logging.s3.amazona\\nws.com', 'S3 Server Access Logging', 'Enabling Amazon S3 server\\naccess logging']]]\n",
      "[[['Service name in the policy', 'S3 feature', 'More information'], ['storage-lens.s3.am\\nazonaws.com', 'S3 Storage Lens', 'Viewing Amazon S3 Storage\\nLens metrics using a data\\nexport']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"ExamplePolicy01\",\\n\"Statement\": [\\n{\\n\"Sid\": \"ExampleStatement01\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Akua\"\\n},\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetBucketLocation\",\\n\"s3:ListBucket\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::awsexamplebucket1/*\",\\n\"arn:aws:s3:::awsexamplebucket1\"\\n]\\n}\\n]', '']]]\n",
      "[[['', '}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nWe recommend that you don't use the AWS account root user credentials to make\\nauthenticated requests. Instead, create an IAM role and grant that role full access. We\\nrefer to users with this role as administrator users. You can use credentials assigned to\\nthe administrator role, instead of AWS account root user credentials, to interact with AWS\\nand perform tasks, such as create a bucket, create users, and grant permissions. For more\\ninformation, see AWSsecurity credentials in the IAM User Guide and Security best practices\\nin IAM in the IAM User Guide.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor your convenience, the Edit bucket policy page displays the Bucket ARN\\n(Amazon Resource Name) of the current bucket above the Policy text field. You\\ncan copy this ARN for use in the statements on the AWS Policy Generator page.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nBucket policies are limited to 20 KB in size.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nWhen applying the Amazon S3 bucket policies for VPC endpoints described in this section,\\nyou might block your access to the bucket unintentionally. Bucket permissions that are\\nintended to specifically limit bucket access to connections originating from your VPC\\nendpoint can block all connections to the bucket. For information about how to fix this\\nissue, see How do I fix my bucket policy when it has the wrong VPC or VPC endpoint ID? in\\nthe AWS Support Knowledge Center.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\n• Before using the following example policy, replace the VPC endpoint ID with an\\nappropriate value for your use case. Otherwise, you won't be able to access your bucket.\\n• This policy disables console access to the specified bucket because console requests don't\\noriginate from the specified VPC endpoint.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"Policy1415115909152\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Access-to-specific-VPCE-only\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:*\",\\n\"Effect\": \"Deny\",\\n\"Resource\": [\"arn:aws:s3:::awsexamplebucket1\",\\n\"arn:aws:s3:::awsexamplebucket1/*\"],\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"aws:SourceVpce\": \"vpce-1a2b3c4d\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\n• Before using the following example policy, replace the VPC ID with an appropriate value\\nfor your use case. Otherwise, you won't be able to access your bucket.\\n• This policy disables console access to the specified bucket because console requests don't\\noriginate from the specified VPC.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"Policy1415115909153\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Access-to-specific-VPC-only\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:*\",\\n\"Effect\": \"Deny\",\\n\"Resource\": [\"arn:aws:s3:::awsexamplebucket1\",\\n\"arn:aws:s3:::awsexamplebucket1/*\"],\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"aws:SourceVpc\": \"vpc-111bbb22\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen testing permissions by using the Amazon S3 console, you must grant\\nadditional permissions that the console requires—s3:ListAllMyBuckets,\\ns3:GetBucketLocation, and s3:ListBucket. For an example walkthrough that grants\\npermissions to users and tests those permissions by using the console, see Controlling\\naccess to a bucket with user policies.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nBefore you complete this step, review Blocking public access to your Amazon S3 storage to\\nensure that you understand and accept the risks involved with allowing public access. When\\nyou turn off block public access settings to make your bucket public, anyone on the internet\\ncan access your bucket. We recommend that you block all public access to your buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nBefore you complete this step, review Blocking public access to your Amazon S3\\nstorage to ensure you understand and accept the risks involved with allowing public\\naccess. When you turn off block public access settings to make your bucket public,\\nanyone on the internet can access your bucket. We recommend that you block all\\npublic access to your buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"PutObjPolicy\",\\n\"Statement\": [{\\n\"Sid\": \"DenyObjectsThatAreNotSSEKMS\",\\n\"Principal\": \"*\",\\n\"Effect\": \"Deny\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\\n\"Condition\": {\\n\"Null\": {\\n\"s3:x-amz-server-side-encryption-aws-kms-key-id\": \"true\"\\n}\\n}\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"PutObjPolicy\",\\n\"Statement\": [{\\n\"Sid\": \"DenyObjectsThatAreNotSSEKMSWithSpecificKey\",\\n\"Principal\": \"*\",\\n\"Effect\": \"Deny\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",', '']]]\n",
      "[[['', '\"Condition\": {\\n\"ArnNotEqualsIfExists\": {\\n\"s3:x-amz-server-side-encryption-aws-kms-key-id\": \"arn:aws:kms:us-\\neast-2:111122223333:key/01234567-89ab-cdef-0123-456789abcdef\"\\n}\\n}\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nThe public-read canned ACL allows anyone in the world to view the objects in your\\nbucket. Use caution when granting anonymous access to your Amazon S3 bucket or\\ndisabling block public access settings. When you grant anonymous access, anyone in the\\nworld can access your bucket. We recommend that you never grant anonymous access to\\nyour Amazon S3 bucket unless you specifically need to, such as with static website hosting.\\nIf you want to enable block public access settings for static website hosting, see Tutorial:\\nConfiguring a static website on Amazon S3.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AddPublicReadCannedAcl\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:root\",\\n\"arn:aws:iam::444455556666:root\"\\n]', '']]]\n",
      "[[['', '},\\n\"Action\": [\\n\"s3:PutObject\",\\n\"s3:PutObjectAcl\"\\n],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3:x-amz-acl\": [\\n\"public-read\"\\n]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Sid\":\"PolicyForAllowUploadWithACL\",\\n\"Effect\":\"Allow\",\\n\"Principal\":{\"AWS\":\"111122223333\"},\\n\"Action\":\"s3:PutObject\",\\n\"Resource\":\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\\n\"Condition\": {\\n\"StringEquals\": {\"s3:x-amz-acl\":\"bucket-owner-full-control\"}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Principal\":{\\n\"AWS\":\"arn:aws:iam::111122223333:role/JohnDoe\"\\n},\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\":\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\\n\"Condition\":{\\n\"StringEquals\":{\\n\"s3:ExistingObjectTag/environment\":\"production\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\"Principal\":{\"AWS\":[\\n\"arn:aws:iam::111122223333:role/JohnDoe\"\\n]\\n},\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObjectTagging\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n],\\n\"Condition\": {\"ForAnyValue:StringEquals\": {\"s3:RequestObjectTagKeys\": [\\n\"Owner\",\\n\"CreationDate\"\\n]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\"Principal\":{\"AWS\":[\\n\"arn:aws:iam::111122223333:user/JohnDoe\"\\n]\\n},\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObjectTagging\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"', '']]]\n",
      "[[['', '],\\n\"Condition\": {\"StringEquals\": {\"s3:RequestObjectTag/Project\": \"X\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [{\\n\"Principal\":{\\n\"AWS\":[\\n\"arn:aws:iam::111122223333:user/JohnDoe\"\\n]\\n},\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n],\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3:RequestObjectTag/Department\": \"Finance\"\\n}\\n}\\n}]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowPutObjectS3ServerAccessLogsPolicy\",\\n\"Principal\": {\\n\"Service\": \"logging.s3.amazonaws.com\"\\n},\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET-logs/*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"111111111111\"\\n},\\n\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:::EXAMPLE-SOURCE-BUCKET\"\\n}\\n}\\n},\\n{\\n\"Sid\": \"RestrictToS3ServerAccessLogs\",\\n\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET-logs/*\",\\n\"Condition\": {\\n\"ForAllValues:StringNotEquals\": {\\n\"aws:PrincipalServiceNamesList\": \"logging.s3.amazonaws.com\"', '']]]\n",
      "[[['', '}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [{\\n\"Sid\": \"AllowGetObject\",\\n\"Principal\": {\\n\"AWS\": \"*\"\\n},\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:GetObject\",\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:PrincipalOrgID\": [\"o-aa111bb222\"]\\n}\\n}', '']]]\n",
      "[[['', '}]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen restricting access to a specific IP address, make sure that you also specify which\\nVPC endpoints, VPC source IP addresses, or external IP addresses can access the S3\\nbucket. Otherwise, you might lose access to the bucket if your policy denies all users from\\nperforming any S3 operations on objects in your bucket without the proper permissions\\nalready in place.', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nBefore using this policy, replace the 192.0.2.0/24 IP address range in this example with\\nan appropriate value for your use case. Otherwise, you will lose the ability to access your\\nbucket.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"S3PolicyId1\",\\n\"Statement\": [', '']]]\n",
      "[[['', '{\\n\"Sid\": \"IPAllow\",\\n\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:*\",\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\",\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n],\\n\"Condition\": {\\n\"NotIpAddress\": {\\n\"aws:SourceIp\": \"192.0.2.0/24\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nReplace the IP address ranges in this example with appropriate values for your use case\\nbefore using this policy. Otherwise, you might lose the ability to access your bucket.', ''], ['', '', '']], [['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"Id\": \"PolicyId2\",\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowIPmix\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:*\",\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\",\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n],\\n\"Condition\": {\\n\"IpAddress\": {\\n\"aws:SourceIp\": [\\n\"192.0.2.0/24\",\\n\"2001:DB8:1234:5678::/64\"\\n]\\n},\\n\"NotIpAddress\": {\\n\"aws:SourceIp\": [\\n\"203.0.113.0/24\",\\n\"2001:DB8:1234:5678:ABCD::/80\"\\n]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [{\\n\"Sid\": \"RestrictToTLSRequestsOnly\",\\n\"Action\": \"s3:*\",\\n\"Effect\": \"Deny\",\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\",\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n],\\n\"Condition\": {\\n\"Bool\": {\\n\"aws:SecureTransport\": \"false\"\\n}\\n},\\n\"Principal\": \"*\"\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Id\":\"HTTP referer policy example\",\\n\"Statement\":[\\n{\\n\"Sid\":\"Allow only GET requests originating from www.example.com and\\nexample.com.\",\\n\"Effect\":\"Allow\",\\n\"Principal\":\"*\",', '']]]\n",
      "[[['', '\"Action\":[\"s3:GetObject\",\"s3:GetObjectVersion\"],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket/*\",\\n\"Condition\":{\\n\"StringLike\":{\"aws:Referer\":[\"http://www.example.com/*\",\"http://example.com/\\n*\"]}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nWe recommend that you use caution when using the aws:Referer condition key. It is\\ndangerous to include a publicly known HTTP referer header value. Unauthorized parties\\ncan use modified or custom browsers to provide any aws:Referer value that they choose.\\nTherefore, do not use aws:Referer to prevent unauthorized parties from making direct\\nAWS requests.\\nThe aws:Referer condition key is offered only to allow customers to protect their digital\\ncontent, such as content stored in Amazon S3, from being referenced on unauthorized\\nthird-party sites. For more information, see aws:Referer in the IAM User Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowRootAndHomeListingOfCompanyBucket\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:user/JohnDoe\"\\n]\\n},\\n\"Effect\": \"Allow\",\\n\"Action\": [\"s3:ListBucket\"],\\n\"Resource\": [\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"],\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3:prefix\": [\"\", \"home/\", \"home/JohnDoe\"],\\n\"s3:delimiter\": [\"/\"]\\n}\\n}\\n},\\n{\\n\"Sid\": \"AllowListingOfUserFolder\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:user/JohnDoe\"\\n]\\n},\\n\"Action\": [\"s3:ListBucket\"],', '']]]\n",
      "[[['', '\"Effect\": \"Allow\",\\n\"Resource\": [\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"],\\n\"Condition\": {\\n\"StringLike\": {\\n\"s3:prefix\": [\"home/JohnDoe/*\"]\\n}\\n}\\n},\\n{\\n\"Sid\": \"AllowAllS3ActionsInUserFolder\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:user/JohnDoe\"\\n]\\n},\\n\"Action\": [\"s3:*\"],\\n\"Resource\": [\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/home/JohnDoe/*\"]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::elb-account-id:root\"\\n},\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/prefix/AWSLogs/111122223333/*\"', '']]]\n",
      "[[['', '}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nMake sure to replace elb-account-id with the AWS account ID for Elastic Load Balancing\\nfor your AWS Region. For the list of Elastic Load Balancing Regions, see Attach a policy to\\nyour Amazon S3 bucket in the Elastic Load Balancing User Guide.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Principal\": {\\n\"Service\": \"logdelivery.elasticloadbalancing.amazonaws.com\"\\n},\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/prefix/AWSLogs/111122223333/*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"PolicyForCloudFrontPrivateContent\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access\\nIdentity EH1HDMB1FH2TC\"\\n},\\n\"Action\": \"s3:GetObject\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"S3StorageLensExamplePolicy\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"storage-lens.s3.amazonaws.com\"\\n},\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": [\\n\"arn:aws:s3:::destination-bucket/destination-prefix/\\nStorageLens/111122223333/*\"\\n],\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3:x-amz-acl\": \"bucket-owner-full-control\",\\n\"aws:SourceAccount\": \"111122223333\",\\n\"aws:SourceArn\": \"arn:aws:s3:region-code:111122223333:storage-\\nlens/storage-lens-dashboard-configuration-id\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:::destination-bucket/destination-prefix/StorageLens/your-\\norganization-id/*\",', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"InventoryAndAnalyticsExamplePolicy\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"s3.amazonaws.com\"\\n},\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-DESTINATION-BUCKET/*\"\\n],\\n\"Condition\": {\\n\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:::DOC-EXAMPLE-SOURCE-BUCKET\"\\n},\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"111122223333\",\\n\"s3:x-amz-acl\": \"bucket-owner-full-control\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Id\": \"InventoryConfigPolicy\",\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [{\\n\"Sid\": \"AllowInventoryCreationConditionally\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::111122223333:user/Ana\"', '']]]\n",
      "[[['', '},\\n\"Action\":\\n\"s3:PutInventoryConfiguration\",\\n\"Resource\":\\n\"arn:aws:s3:::DOC-EXAMPLE-SOURCE-BUCKET\",\\n\"Condition\": {\\n\"ForAllValues:StringEquals\": {\\n\"s3:InventoryAccessibleOptionalFields\": [\\n\"Size\",\\n\"StorageClass\"\\n]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Id\": \"InventoryConfigSomeFields\",\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [{\\n\"Sid\": \"AllowInventoryCreation\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::111122223333:user/Ana\"\\n},\\n\"Action\": \"s3:PutInventoryConfiguration\",\\n\"Resource\":\\n\"arn:aws:s3:::DOC-EXAMPLE-SOURCE-BUCKET\",\\n},\\n{\\n\"Sid\": \"DenyCertainInventoryFieldCreation\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {', '']]]\n",
      "[[['', '\"AWS\": \"arn:aws:iam::111122223333:user/Ana\"\\n},\\n\"Action\": \"s3:PutInventoryConfiguration\",\\n\"Resource\":\\n\"arn:aws:s3:::DOC-EXAMPLE-SOURCE-BUCKET\",\\n\"Condition\": {\\n\"ForAnyValue:StringEquals\": {\\n\"s3:InventoryAccessibleOptionalFields\": [\\n\"ObjectOwner\",\\n\"ObjectAccessControlList\"\\n]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThe use of the s3:InventoryAccessibleOptionalFields condition key in bucket\\npolicies doesn't affect the delivery of inventory reports based on the existing inventory\\nconfigurations.\", ''], ['', '', '']], [['', '', ''], ['', \"Important\\nWe recommend that you use ForAllValues with an Allow effect or ForAnyValue with a\\nDeny effect, as shown in the prior examples.\\nDon't use ForAllValues with a Deny effect nor ForAnyValue with an Allow effect,\\nbecause these combinations can be overly restrictive and block inventory configuration\\ndeletion.\\nTo learn more about the ForAllValues and ForAnyValue condition set operators, see\\nMultivalued context keys in the IAM User Guide.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"123\",\\n\"Statement\": [\\n{\\n\"Sid\": \"\",\\n\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:*\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/taxdocuments/*\",\\n\"Condition\": { \"Null\": { \"aws:MultiFactorAuthAge\": true }}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"123\",\\n\"Statement\": [\\n{\\n\"Sid\": \"\",\\n\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:*\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/taxdocuments/*\",\\n\"Condition\": { \"Null\": { \"aws:MultiFactorAuthAge\": true } }\\n},\\n{\\n\"Sid\": \"\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": [\"s3:GetObject\"],\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"123\",\\n\"Statement\": [\\n{\\n\"Sid\": \"\",\\n\"Effect\": \"Deny\",', '']]]\n",
      "[[['', '\"Principal\": \"*\",\\n\"Action\": \"s3:*\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/taxdocuments/*\",\\n\"Condition\": {\"Null\": {\"aws:MultiFactorAuthAge\": true }}\\n},\\n{\\n\"Sid\": \"\",\\n\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:*\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/taxdocuments/*\",\\n\"Condition\": {\"NumericGreaterThan\": {\"aws:MultiFactorAuthAge\": 3600 }}\\n},\\n{\\n\"Sid\": \"\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": [\"s3:GetObject\"],\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [', '']]]\n",
      "[[['', '{\\n\"Sid\": \"statement1\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Dave\"\\n},\\n\"Action\": [\\n\"s3:GetObjectVersion\",\\n\"s3:GetBucketAcl\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::example-s3-bucket1\",\\n\"arn:aws:s3:::example-s3-bucket1/*\"\\n]\\n},\\n{\\n\"Sid\": \"statement2\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Dave\"\\n},\\n\"Action\": [\\n\"s3:DeleteObject\",\\n\"s3:DeleteObjectVersion\",\\n\"s3:PutLifecycleConfiguration\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::example-s3-bucket1\",\\n\"arn:aws:s3:::example-s3-bucket1/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '\"Condition\": {\\n\"StringNotEquals\": {\\n\"s3:x-amz-server-side-encryption\": \"AES256\"\\n}}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket example1bucket --key HappyFace.jpg --body c:\\n\\\\HappyFace.jpg --server-side-encryption \"AES256\" --profile AccountBadmin', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"cross-account permission to user in your own account\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Dave\"\\n},\\n\"Action\": \"s3:PutObject\",', '']]]\n",
      "[[['', '\"Resource\": \"arn:aws:s3:::awsexamplebucket1/*\"\\n},\\n{\\n\"Sid\": \"Deny your user permission to upload object if copy source is not /\\nbucket/folder\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Dave\"\\n},\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::awsexamplebucket1/*\",\\n\"Condition\": {\\n\"StringNotLike\": {\\n\"s3:x-amz-copy-source\": \"awsexamplebucket1/public/*\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api copy-object --bucket awsexamplebucket1 --key HappyFace.jpg\\n--copy-source examplebucket/public/PublicHappyFace1.jpg --profile AccountADave', ''], ['', '', '']], [['', '', ''], ['', '\"Condition\": {\\n\"StringNotEquals\": {\\n\"s3:x-amz-copy-source\": \"awsexamplebucket1/public/PublicHappyFace1.jpg\"\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"statement1\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Dave\"\\n},\\n\"Action\": \"s3:GetObjectVersion\",\\n\"Resource\": \"arn:aws:s3:::examplebucketversionenabled/HappyFace.jpg\"\\n},\\n{\\n\"Sid\": \"statement2\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/Dave\"\\n},\\n\"Action\": \"s3:GetObjectVersion\",\\n\"Resource\": \"arn:aws:s3:::examplebucketversionenabled/HappyFace.jpg\",\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"s3:VersionId\": \"AaaHbAQitwiL_h47_44lRO2DDfLlBO5e\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-object --bucket examplebucketversionenabled --key HappyFace.jpg\\nOutputFile.jpg --version-id AaaHbAQitwiL_h47_44lRO2DDfLlBO5e --profile AccountADave', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": [\\n\"arn:aws:s3:::example-s3-bucket1\",\\n\"arn:aws:s3:::example-s3-bucket1/*\"\\n],\\n\"Condition\": {\\n\"NumericLessThan\": {\\n\"s3:TlsVersion\": 1.2\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": [\\n\"arn:aws:s3:::example-s3-bucket1\",\\n\"arn:aws:s3:::example-s3-bucket1/*\"', '']]]\n",
      "[[['', '],\\n\"Condition\": {\\n\"NumericGreaterThan\": {\\n\"s3:TlsVersion\": 1.1\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe IPAddress and NotIpAddress key values specified in the condition uses CIDR\\nnotation as described in RFC 4632. For more information, see http://www.rfc-editor.org/\\nrfc/rfc4632.txt.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"S3PolicyId1\",', '']]]\n",
      "[[['', '\"Statement\": [\\n{\\n\"Sid\": \"statement1\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\":\"s3:GetObject\",\\n\"Resource\": \"arn:aws:s3:::awsexamplebucket1/*\",\\n\"Condition\" : {\\n\"IpAddress\" : {\\n\"aws:SourceIp\": \"192.0.2.0/24\"\\n},\\n\"NotIpAddress\" : {\\n\"aws:SourceIp\": \"192.0.2.188/32\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nFor some AWS global condition keys, only certain resource types are supported. Therefore,\\ncheck whether Amazon S3 supports the global condition key and resource type that you\\nwant to use, or if you'll need to use an Amazon S3-specific condition key instead. For a\\ncomplete list of supported resource types and condition keys for Amazon S3, see Actions,\\nresources, and condition keys for Amazon S3 in the Service Authorization Reference.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Sid\":\"statement1\",\\n\"Effect\":\"Allow\",\\n\"Action\": \"s3:ListBucket\",\\n\"Resource\":\"arn:aws:s3:::awsexamplebucket1\",\\n\"Condition\" : {\\n\"StringEquals\" : {\\n\"s3:prefix\": \"projects\"\\n}\\n}\\n},\\n{\\n\"Sid\":\"statement2\",\\n\"Effect\":\"Deny\",\\n\"Action\": \"s3:ListBucket\",\\n\"Resource\": \"arn:aws:s3:::awsexamplebucket1\",\\n\"Condition\" : {\\n\"StringNotEquals\" : {\\n\"s3:prefix\": \"projects\"\\n}\\n}\\n}\\n]', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Sid\":\"statement1\",\\n\"Effect\":\"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/bucket-owner\"\\n},\\n\"Action\": \"s3:ListBucket\",\\n\"Resource\": \"arn:aws:s3:::awsexamplebucket1\",\\n\"Condition\" : {\\n\"StringEquals\" : {\\n\"s3:prefix\": \"projects\"\\n}\\n}\\n},\\n{\\n\"Sid\":\"statement2\",\\n\"Effect\":\"Deny\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:user/bucket-owner\"\\n},\\n\"Action\": \"s3:ListBucket\",\\n\"Resource\": \"arn:aws:s3:::awsexamplebucket1\",\\n\"Condition\" : {\\n\"StringNotEquals\" : {\\n\"s3:prefix\": \"projects\"', '']]]\n",
      "[[['', '}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-objects --bucket awsexamplebucket1 --prefix examplefolder --profile\\nAccountADave', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\": \"s3:ListAllMyBuckets\",\\n\"Resource\":\"*\"\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\"s3:ListBucket\",\"s3:GetBucketLocation\"],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket1\"\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\",\\n\"s3:PutObjectAcl\",\\n\"s3:GetObject\",\\n\"s3:GetObjectAcl\",\\n\"s3:DeleteObject\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket1/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'example-s3-bucket1', '']]]\n",
      "[[['', 'Mary/\\nCarlos/', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\",\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:DeleteObject\",\\n\"s3:DeleteObjectVersion\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket1/Mary/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[', '']]]\n",
      "[[['', '\"s3:PutObject\",\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:DeleteObject\",\\n\"s3:DeleteObjectVersion\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket1/${aws:username}/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen using policy variables, you must explicitly specify version 2012-10-17 in the policy.\\nThe default version of the IAM policy language, 2008-10-17, does not support policy\\nvariables.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowGroupToSeeBucketListInTheConsole\",\\n\"Action\": [\\n\"s3:ListAllMyBuckets\",\\n\"s3:GetBucketLocation\"\\n],\\n\"Effect\": \"Allow\",\\n\"Resource\": \"arn:aws:s3:::*\"\\n},\\n{\\n\"Sid\": \"AllowRootLevelListingOfTheBucket\",\\n\"Action\": \"s3:ListBucket\",\\n\"Effect\": \"Allow\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket1\",\\n\"Condition\":{\\n\"StringEquals\":{\\n\"s3:prefix\":[\"\"], \"s3:delimiter\":[\"/\"]', '']]]\n",
      "[[['', '}\\n}\\n},\\n{\\n\"Sid\": \"AllowListBucketOfASpecificUserPrefix\",\\n\"Action\": \"s3:ListBucket\",\\n\"Effect\": \"Allow\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket1\",\\n\"Condition\":{ \"StringLike\":{\"s3:prefix\":[\"${aws:username}/*\"] }\\n}\\n},\\n{\\n\"Sid\": \"AllowUserSpecificActionsOnlyInTheSpecificUserPrefix\",\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\",\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:DeleteObject\",\\n\"s3:DeleteObjectVersion\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket1/${aws:username}/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIn the 2012-10-17 version of the policy, policy variables start with $. This change in syntax\\ncan potentially create a conflict if your object key (object name) includes a $.\\nTo avoid this conflict, specify the $ character by using ${$}. For example, to include the\\nobject key my$file in a policy, specify it as my${$}file.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\",\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:DeleteObject\",\\n\"s3:DeleteObjectVersion\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket1/home/${aws:userid}/*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\",\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:DeleteObject\",\\n\"s3:DeleteObjectVersion\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket1/share/marketing/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket1/readonly/*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":\"s3:PutObject\",\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket1/uploads/anycompany/*\"\\n},\\n{\\n\"Effect\":\"Deny\",\\n\"Action\":\"s3:*\",\\n\"NotResource\":\"arn:aws:s3:::example-s3-bucket1/uploads/anycompany/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"DenyS3AccessOutsideMyBoundary\",\\n\"Effect\": \"Deny\",\\n\"Action\": [\\n\"s3:*\"\\n],', '']]]\n",
      "[[['', '\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"aws:ResourceAccount\": [\\n\"222222222222\"\\n]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThis policy doesn't replace your existing IAM access controls, because it doesn't grant any\\naccess. Instead, this policy acts as an additional guardrail for your other IAM permissions,\\nregardless of the permissions granted through other IAM policies.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowS3AccessOutsideMyBoundary\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:*\"\\n],', '']]]\n",
      "[[['', '\"Resource\": \"*\",\\n\"Condition\": {\\n\"ForAllValues:StringNotLike\": {\\n\"aws:ResourceOrgPaths\": [\\n\"o-acorg/r-acroot/ou-acroot-exampleou/\"\\n]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThis policy doesn't grant any access. Instead, this policy acts as a backstop for your other\\nIAM permissions, preventing your principals from accessing Amazon S3 objects outside of\\nan OU-defined boundary.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"DenyS3AccessOutsideMyBoundary\",', '']]]\n",
      "[[['', '\"Effect\": \"Deny\",\\n\"Action\": [\\n\"s3:*\"\\n],\\n\"Resource\": \"arn:aws:s3:::*/*\",\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"aws:ResourceOrgID\": \"${aws:PrincipalOrgID}\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThis policy doesn't grant any access. Instead, this policy acts as a backstop for your other\\nIAM permissions, preventing your principals from accessing any Amazon S3 objects outside\\nof your organization. This policy also applies to Amazon S3 resources that are created after\\nthe policy is put into effect.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Sid\":\"statement1\",\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetAccountPublicAccessBlock\"\\n],', '']]]\n",
      "[[['', '\"Resource\":[\\n\"*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIn this example, the bucket owner is granting permission to one of its users, so either a\\nbucket policy or a user policy can be used. This example shows a user policy.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Sid\":\"statement1\",\\n\"Effect\":\"Allow\",\\n\"Action\": \"s3:CreateBucket\",\\n\"Resource\": \"arn:aws:s3:::*\",\\n\"Condition\": {\\n\"StringLike\": {\\n\"s3:LocationConstraint\": \"sa-east-1\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Sid\":\"statement1\",\\n\"Effect\":\"Allow\",\\n\"Action\": \"s3:CreateBucket\",\\n\"Resource\": \"arn:aws:s3:::*\",\\n\"Condition\": {\\n\"StringLike\": {\\n\"s3:LocationConstraint\": \"sa-east-1\"\\n}\\n}\\n},\\n{\\n\"Sid\":\"statement2\",\\n\"Effect\":\"Deny\",\\n\"Action\": \"s3:CreateBucket\",\\n\"Resource\": \"arn:aws:s3:::*\",\\n\"Condition\": {\\n\"StringNotLike\": {\\n\"s3:LocationConstraint\": \"sa-east-1\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api create-bucket --bucket examplebucket --profile AccountADave --create-bucket-\\nconfiguration file://c:/Users/someUser/bucketconfig.txt', ''], ['', '', '']], [['', '', ''], ['', '{\"LocationConstraint\": \"sa-east-1\"}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nThe walkthrough uses companybucket as the bucket name, Alice and Bob as the IAM\\nusers, and Consultants as the group name. Because Amazon S3 requires that bucket\\nnames be globally unique, you must replace the bucket name with a name that you create.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Warning\\nWhen you add users and a group, do not attach any policies that grant permissions to these\\nusers. At first, these users don't have any permissions. In the following sections, you grant\\npermissions incrementally. First you must ensure that you have assigned passwords to\", '']]]\n",
      "[[['', 'these IAM users. You use these user credentials to test Amazon S3 actions and verify that\\nthe permissions work as expected.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nBecause you are granting user permissions, sign in using your AWS account credentials,\\nnot as an IAM user.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowGroupToSeeBucketListInTheConsole\",\\n\"Action\": [\"s3:ListAllMyBuckets\"],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\"arn:aws:s3:::*\"]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe Summary entry displays a message stating that the policy does not grant any\\npermissions. For this walkthrough, you can safely ignore this message.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis example uses companybucket for illustration. You must use the name of the bucket\\nthat you created.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'GET ?prefix=&delimiter=/ HTTP/1.1\\nHost: companybucket.s3.amazonaws.com\\nDate: Wed, 01 Aug 2012 12:00:00 GMT\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:xQE0diMbLRepdf3YB+FIEXAMPLE=', ''], ['', '', '']], [['', '', ''], ['', '<ListBucketResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Name>companybucket</Name>\\n<Prefix></Prefix>\\n<Delimiter>/</Delimiter>\\n...\\n<Contents>\\n<Key>s3-dg.pdf</Key>\\n...\\n</Contents>\\n<CommonPrefixes>\\n<Prefix>Development/</Prefix>\\n</CommonPrefixes>\\n<CommonPrefixes>\\n<Prefix>Finance/</Prefix>\\n</CommonPrefixes>\\n<CommonPrefixes>\\n<Prefix>Private/</Prefix>\\n</CommonPrefixes>\\n</ListBucketResult>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<ListBucketResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Name>companybucket</Name>\\n<Prefix>Development</Prefix>\\n<Delimiter>/</Delimiter>\\n...\\n<Contents>\\n<Key>Project1.xls</Key>\\n...\\n</Contents>\\n<Contents>\\n<Key>Project2.xls</Key>\\n...\\n</Contents>\\n</ListBucketResult>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Sid\": \"AllowRootLevelListingOfCompanyBucket\",\\n\"Action\": [\"s3:ListBucket\"],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\"arn:aws:s3:::companybucket\"],\\n\"Condition\":{\\n\"StringEquals\":{\\n\"s3:prefix\":[\"\"], \"s3:delimiter\":[\"/\"]\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Sid\": \"RequiredByS3Console\",\\n\"Action\": [\"s3:GetBucketLocation\"],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\"arn:aws:s3:::*\"]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\":\\n\"AllowGroupToSeeBucketListAndAlsoAllowGetBucketLocationRequiredForListBucket\",\\n\"Action\": [ \"s3:ListAllMyBuckets\", \"s3:GetBucketLocation\" ],\\n\"Effect\": \"Allow\",\\n\"Resource\": [ \"arn:aws:s3:::*\" ]\\n},\\n{\\n\"Sid\": \"AllowRootLevelListingOfCompanyBucket\",\\n\"Action\": [\"s3:ListBucket\"],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\"arn:aws:s3:::companybucket\"],\\n\"Condition\":{\\n\"StringEquals\":{\\n\"s3:prefix\":[\"\"], \"s3:delimiter\":[\"/\"]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowListBucketIfSpecificPrefixIsIncludedInRequest\",\\n\"Action\": [\"s3:ListBucket\"],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\"arn:aws:s3:::companybucket\"],\\n\"Condition\":{ \"StringLike\":{\"s3:prefix\":[\"Development/*\"] }\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Sid\":\"AllowUserToReadWriteObjectData\",\\n\"Action\":[\"s3:GetObject\", \"s3:PutObject\"],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\"arn:aws:s3:::companybucket/Development/*\"]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Sid\":\"AllowListBucketIfSpecificPrefixIsIncludedInRequest\",\\n\"Action\":[\"s3:ListBucket\"],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\"arn:aws:s3:::companybucket\"],\\n\"Condition\":{\\n\"StringLike\":{\"s3:prefix\":[\"Development/*\"]\\n}\\n}\\n},\\n{\\n\"Sid\":\"AllowUserToReadWriteObjectDataInDevelopmentFolder\",', '']]]\n",
      "[[['', '\"Action\":[\"s3:GetObject\", \"s3:PutObject\"],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\"arn:aws:s3:::companybucket/Development/*\"]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Sid\": \"ExplicitlyDenyAnyRequestsForAllOtherFoldersExceptDevelopment\",\\n\"Action\": [\"s3:ListBucket\"],\\n\"Effect\": \"Deny\",\\n\"Resource\": [\"arn:aws:s3:::companybucket\"],\\n\"Condition\":{ \"StringNotLike\": {\"s3:prefix\":[\"Development/*\",\"\"] },\\n\"Null\" : {\"s3:prefix\":false }\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Sid\":\"AllowListBucketIfSpecificPrefixIsIncludedInRequest\",\\n\"Action\":[\"s3:ListBucket\"],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\"arn:aws:s3:::companybucket\"],\\n\"Condition\":{\\n\"StringLike\":{\"s3:prefix\":[\"Development/*\"]\\n}\\n}\\n},\\n{\\n\"Sid\":\"AllowUserToReadWriteObjectDataInDevelopmentFolder\",\\n\"Action\":[\"s3:GetObject\", \"s3:PutObject\"],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\"arn:aws:s3:::companybucket/Development/*\"]\\n},\\n{', '']]]\n",
      "[[['', '\"Sid\": \"ExplicitlyDenyAnyRequestsForAllOtherFoldersExceptDevelopment\",\\n\"Action\": [\"s3:ListBucket\"],\\n\"Effect\": \"Deny\",\\n\"Resource\": [\"arn:aws:s3:::companybucket\"],\\n\"Condition\":{ \"StringNotLike\": {\"s3:prefix\":[\"Development/*\",\"\"] },\\n\"Null\" : {\"s3:prefix\":false }\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Sid\": \"ExplictDenyAccessToPrivateFolderToEveryoneInTheGroup\",\\n\"Action\": [\"s3:*\"],\\n\"Effect\": \"Deny\",\\n\"Resource\":[\"arn:aws:s3:::companybucket/Private/*\"]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Sid\": \"DenyListBucketOnPrivateFolder\",\\n\"Action\": [\"s3:ListBucket\"],\\n\"Effect\": \"Deny\",\\n\"Resource\": [\"arn:aws:s3:::*\"],\\n\"Condition\":{\\n\"StringLike\":{\"s3:prefix\":[\"Private/\"]}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\":\\n\"AllowGroupToSeeBucketListAndAlsoAllowGetBucketLocationRequiredForListBucket\",\\n\"Action\": [\"s3:ListAllMyBuckets\", \"s3:GetBucketLocation\"],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\"arn:aws:s3:::*\"]\\n},', '']]]\n",
      "[[['', '{\\n\"Sid\": \"AllowRootLevelListingOfCompanyBucket\",\\n\"Action\": [\"s3:ListBucket\"],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\"arn:aws:s3:::companybucket\"],\\n\"Condition\":{\\n\"StringEquals\":{\"s3:prefix\":[\"\"]}\\n}\\n},\\n{\\n\"Sid\": \"RequireFolderStyleList\",\\n\"Action\": [\"s3:ListBucket\"],\\n\"Effect\": \"Deny\",\\n\"Resource\": [\"arn:aws:s3:::*\"],\\n\"Condition\":{\\n\"StringNotEquals\":{\"s3:delimiter\":\"/\"}\\n}\\n},\\n{\\n\"Sid\": \"ExplictDenyAccessToPrivateFolderToEveryoneInTheGroup\",\\n\"Action\": [\"s3:*\"],\\n\"Effect\": \"Deny\",\\n\"Resource\":[\"arn:aws:s3:::companybucket/Private/*\"]\\n},\\n{\\n\"Sid\": \"DenyListBucketOnPrivateFolder\",\\n\"Action\": [\"s3:ListBucket\"],\\n\"Effect\": \"Deny\",\\n\"Resource\": [\"arn:aws:s3:::*\"],\\n\"Condition\":{\\n\"StringLike\":{\"s3:prefix\":[\"Private/\"]}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', '[default]\\naws_access_key_id = access key ID\\naws_secret_access_key = secret access key\\nregion = us-west-2', ''], ['', '', '']], [['', '', ''], ['', 'aws help', ''], ['', '', '']], [['', '', ''], ['', 'aws s3 ls', ''], ['', '', '']], [['', '', ''], ['', '[profile AccountAadmin]\\naws_access_key_id = User AccountAadmin access key ID\\naws_secret_access_key = User AccountAadmin secret access key\\nregion = us-west-2', '']]]\n",
      "[[['', '[profile AccountBadmin]\\naws_access_key_id = Account B access key ID\\naws_secret_access_key = Account B secret access key\\nregion = us-east-1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3 ls s3://examplebucket --profile AccountBadmin', ''], ['', '', '']], [['', '', ''], ['', '$ export AWS_DEFAULT_PROFILE=AccountAadmin', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo load the AWS Tools for Windows PowerShell module, you must enable PowerShell\\nscript execution. For more information, see Enable Script Execution in the AWS Tools\\nfor Windows PowerShell User Guide.', ''], ['', '', '']], [['', '', ''], ['', 'Set-AWSCredentials -AccessKey AccessKeyID -SecretKey SecretAccessKey -\\nstoreas string', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Get-Command -module awspowershell -noun s3* -StoredCredentials string', ''], ['', '', '']], [['', '', ''], ['', 'Get-S3Object -BucketName bucketname -StoredCredentials string', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nGranting permissions to IAM roles is a better practice than granting permissions to\\nindividual users.For more information about how to grant permissions to IAM roles, see\\nUnderstanding cross-account permissions and using IAM roles.', ''], ['', '', '']]]\n",
      "[[['Account ID', 'Account referred to as', 'Administrator user in the\\naccount'], ['1111-1111-1111', 'Account A', 'AccountAadmin']], [['', '', ''], ['', 'Note\\nThe administrator user in this example is AccountAadmin, which refers to Account A, and\\nnot AccountAdmin.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"statement1\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountA-ID:user/Dave\"\\n},\\n\"Action\": [\\n\"s3:GetBucketLocation\",\\n\"s3:ListBucket\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::awsexamplebucket1\"\\n]\\n},\\n{\\n\"Sid\": \"statement2\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountA-ID:user/Dave\"\\n},\\n\"Action\": [\\n\"s3:GetObject\"', '']]]\n",
      "[[['', '],\\n\"Resource\": [\\n\"arn:aws:s3:::awsexamplebucket1/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"PermissionForObjectOperations\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::awsexamplebucket1/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '[profile UserDaveAccountA]\\naws_access_key_id = access-key\\naws_secret_access_key = secret-access-key\\nregion = us-east-1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket awsexamplebucket1 --key HappyFace.jpg --\\nbody HappyFace.jpg --profile UserDaveAccountA', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object --bucket awsexamplebucket1 --key HappyFace.jpg OutputFile.jpg\\n--profile UserDaveAccountA', ''], ['', '', '']], [['', '', ''], ['', 'set-awscredentials -AccessKey AccessKeyID -SecretKey SecretAccessKey -storeas\\nAccountADave', ''], ['', '', '']], [['', '', ''], ['', 'Write-S3Object -bucketname awsexamplebucket1 -key HappyFace.jpg -file HappyFace.jpg\\n-StoredCredentials AccountADave', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Read-S3Object -bucketname awsexamplebucket1 -key HappyFace.jpg -file Output.jpg -\\nStoredCredentials AccountADave', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nGranting permissions to IAM roles is a better practice than granting permissions to\\nindividual users. To learn how to do this, see Understanding cross-account permissions and\\nusing IAM roles.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nAccount A can also directly grant a user in Account B permissions using a bucket policy.\\nHowever, the user will still need permission from the parent account, Account B, to which\\nthe user belongs, even if Account B doesn't have permissions from Account A. As long as\\nthe user has permission from both the resource owner and the parent account, the user will\\nbe able to access the resource.\", ''], ['', '', '']]]\n",
      "[[['AWS account ID', 'Account referred to as', 'Administrator user in the\\naccount'], ['1111-1111-1111', 'Account A', 'AccountAadmin'], ['2222-2222-2222', 'Account B', 'AccountBadmin']]]\n",
      "[[['', '', ''], ['', '[AccountAadmin]\\naws_access_key_id = access-key-ID\\naws_secret_access_key = secret-access-key', '']]]\n",
      "[[['', 'region = us-east-1\\n[AccountBadmin]\\naws_access_key_id = access-key-ID\\naws_secret_access_key = secret-access-key\\nregion = us-east-1', ''], ['', '', '']], [['', '', ''], ['', 'set-awscredentials –AccessKey AcctA-access-key-ID –SecretKey AcctA-secret-\\naccess-key –storeas AccountAadmin\\nset-awscredentials –AccessKey AcctB-access-key-ID –SecretKey AcctB-secret-\\naccess-key –storeas AccountBadmin', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Example permissions\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountB-ID:root\"\\n},\\n\"Action\": [\\n\"s3:GetLifecycleConfiguration\",\\n\"s3:ListBucket\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3 ls s3://DOC-EXAMPLE-BUCKET --profile AccountBadmin\\naws s3api get-bucket-lifecycle-configuration --bucket DOC-EXAMPLE-BUCKET --\\nprofile AccountBadmin', ''], ['', '', '']], [['', '', ''], ['', 'get-s3object -BucketName DOC-EXAMPLE-BUCKET -StoredCredentials AccountBadmin\\nget-s3bucketlifecycleconfiguration -BucketName DOC-EXAMPLE-BUCKET -\\nStoredCredentials AccountBadmin', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Example\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:ListBucket\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '[profile UserDave]\\naws_access_key_id = access-key\\naws_secret_access_key = secret-access-key\\nregion = us-east-1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3 ls s3://DOC-EXAMPLE-BUCKET --profile UserDave', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-lifecycle-configuration --bucket DOC-EXAMPLE-BUCKET --profile\\nUserDave', ''], ['', '', '']], [['', '', ''], ['', 'set-awscredentials -AccessKey AccessKeyID -SecretKey SecretAccessKey -storeas\\nAccountBDave', ''], ['', '', '']], [['', '', ''], ['', 'get-s3object -BucketName DOC-EXAMPLE-BUCKET -StoredCredentials AccountBDave', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'get-s3bucketlifecycleconfiguration -BucketName DOC-EXAMPLE-BUCKET -\\nStoredCredentials AccountBDave', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Example permissions\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountB-ID:root\"\\n},\\n\"Action\": [\\n\"s3:GetLifecycleConfiguration\",\\n\"s3:ListBucket\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"\\n]\\n},\\n{\\n\"Sid\": \"Deny permission\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountB-ID:root\"\\n},\\n\"Action\": [\\n\"s3:ListBucket\"', '']]]\n",
      "[[['', '],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3 ls s3://DOC-EXAMPLE-BUCKET --profile AccountBadmin', ''], ['', '', '']], [['', '', ''], ['', 'get-s3object -BucketName DOC-EXAMPLE-BUCKET -StoredCredentials AccountBDave', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nGranting permissions to IAM roles is a better practice than granting permissions to\\nindividual users. To learn how to do this, see Understanding cross-account permissions and\\nusing IAM roles.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nS3 Object Ownership is an Amazon S3 bucket-level setting that you can use to both control\\nownership of the objects that are uploaded to your bucket and to disable or enable ACLs.\\nBy default, Object Ownership is set to the Bucket owner enforced setting, and all ACLs are', '']]]\n",
      "[[['', 'disabled. When ACLs are disabled, the bucket owner owns all the objects in the bucket and\\nmanages access to them exclusively by using access-management policies.\\nA majority of modern use cases in Amazon S3 no longer require the use of ACLs. We\\nrecommend that you keep ACLs disabled, except in unusual circumstances where you need\\nto control access for each object individually. With ACLs disabled, you can use policies to\\ncontrol access to all objects in your bucket, regardless of who uploaded the objects to your\\nbucket. For more information, see Controlling ownership of objects and disabling ACLs for\\nyour bucket.', ''], ['', '', '']]]\n",
      "[[['AWS account ID', 'Account referred to as', 'Administrator in the account'], ['1111-1111-1111', 'Account A', 'AccountAadmin'], ['2222-2222-2222', 'Account B', 'AccountBadmin']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{', '']]]\n",
      "[[['', '\"Sid\": \"Statement1\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountB-ID:root\"\\n},\\n\"Action\": [\\n\"s3:PutObject\",\\n\"s3:ListBucket\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::example-s3-bucket1/*\",\\n\"arn:aws:s3:::example-s3-bucket1\"\\n]\\n},\\n{\\n\"Sid\": \"Statement3\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountA-ID:user/Dave\"\\n},\\n\"Action\": [\\n\"s3:GetObject\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::example-s3-bucket1/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-object --bucket example-s3-bucket1 --key HappyFace.jpg --body\\nHappyFace.jpg --profile AccountBadmin', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object-acl --bucket example-s3-bucket1 --key HappyFace.jpg --grant-\\nfull-control id=\"AccountA-CanonicalUserID\" --profile AccountBadmin', ''], ['', '', '']], [['', '', ''], ['', 'Write-S3Object -BucketName example-s3-bucket1 -key HappyFace.jpg -file\\nHappyFace.jpg -StoredCredentials AccountBadmin', ''], ['', '', '']], [['', '', ''], ['', 'Set-S3ACL -BucketName example-s3-bucket1 -Key HappyFace.jpg -CannedACLName \"bucket-\\nowner-full-control\" -StoredCreden', ''], ['', '', '']], [['', '', ''], ['', '[profile UserDaveAccountA]', '']]]\n",
      "[[['', 'aws_access_key_id = access-key\\naws_secret_access_key = secret-access-key\\nregion = us-east-1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object --bucket example-s3-bucket1 --key HappyFace.jpg Outputfile.jpg\\n--profile UserDaveAccountA', ''], ['', '', '']], [['', '', ''], ['', 'Set-AWSCredentials -AccessKey UserDave-AccessKey -SecretKey UserDave-\\nSecretAccessKey -storeas UserDaveAccountA', ''], ['', '', '']], [['', '', ''], ['', 'Read-S3Object -BucketName example-s3-bucket1 -Key HappyFace.jpg -file HappyFace.jpg\\n-StoredCredentials UserDaveAccountA', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nS3 Object Ownership is an Amazon S3 bucket-level setting that you can use to both control\\nownership of the objects that are uploaded to your bucket and to disable or enable ACLs.\\nBy default, Object Ownership is set to the Bucket owner enforced setting, and all ACLs are\\ndisabled. When ACLs are disabled, the bucket owner owns all the objects in the bucket and\\nmanages access to them exclusively by using access-management policies.\\nA majority of modern use cases in Amazon S3 no longer require the use of ACLs. We\\nrecommend that you keep ACLs disabled, except in unusual circumstances where you need\\nto control access for each object individually. With ACLs disabled, you can use policies to\\ncontrol access to all objects in your bucket, regardless of who uploaded the objects to your\\nbucket. For more information, see Controlling ownership of objects and disabling ACLs for\\nyour bucket.', ''], ['', '', '']]]\n",
      "[[['AWS account ID', 'Account referred to as', 'Administrator user in the\\naccount'], ['1111-1111-1111', 'Account A', 'AccountAadmin']]]\n",
      "[[['AWS account ID', 'Account referred to as', 'Administrator user in the\\naccount'], ['2222-2222-2222', 'Account B', 'AccountBadmin'], ['3333-3333-3333', 'Account C', 'AccountCadmin']], [['', '', ''], ['', 'Note\\nYou might want to open a text editor, and write down some of the information as you go\\nthrough the steps. In particular, you will need account IDs, canonical user IDs, IAM user\\nSign-in URLs for each account to connect to the console, and Amazon Resource Names\\n(ARNs) of the IAM users, and roles.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe canonical user ID is the Amazon S3–only concept. It is a 64-character obfuscated\\nversion of the account ID.', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:iam::AccountB-ID:user/AccountBadmin', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"111\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountB-ID:user/AccountBadmin\"', '']]]\n",
      "[[['', '},\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket1/*\"\\n},\\n{\\n\"Sid\": \"112\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountB-ID:user/AccountBadmin\"\\n},\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket1/*\",\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"s3:x-amz-grant-full-control\": \"id=CanonicalUserId-of-\\nAWSaccountA-BucketOwner\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",', '']]]\n",
      "[[['', '\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:GetObject\",\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket1/*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountC-ID:root\"\\n},\\n\"Action\": \"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket example-s3-bucket1 --key HappyFace.jpg --\\nbody HappyFace.jpg --grant-full-control id=\"canonicalUserId-ofTheBucketOwner\" --\\nprofile AccountBadmin', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\"sts:AssumeRole\"],\\n\"Resource\": \"arn:aws:iam::AccountA-ID:role/examplerole\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '[profile AccountCDave]\\naws_access_key_id = UserDaveAccessKeyID\\naws_secret_access_key = UserDaveSecretAccessKey\\nregion = us-west-2', ''], ['', '', '']], [['', '', ''], ['', 'aws sts assume-role --role-arn arn:aws:iam::AccountA-ID:role/examplerole --profile\\nAccountCDave --role-session-name test', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '[profile TempCred]\\naws_access_key_id = temp-access-key-ID\\naws_secret_access_key = temp-secret-access-key\\naws_session_token = session-token\\nregion = us-west-2', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object --bucket example-s3-bucket1 --key HappyFace.jpg SaveFileAs.jpg\\n--profile TempCred', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object-acl --bucket example-s3-bucket1 --key HappyFace.jpg --profile\\nTempCred', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf the Amazon S3 permission check fails to find valid permissions, an Access Denied (403\\nForbidden)permission denied error is returned. For more information, see Troubleshoot\\nAccess Denied (403 Forbidden) errors in Amazon S3.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf the parent AWS account owns the resource (bucket or object), it can grant resource\\npermissions to its IAM principal by using either the user policy or the resource policy.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf bucket and object owners are the same, access to the object can be granted in the\\nbucket policy, which is evaluated at the bucket context. If the owners are different, the', '']]]\n",
      "[[['', 'object owners must use an object ACL to grant permissions. If the AWS account that\\nowns the object is also the parent account to which the IAM principal belongs, it can\\nconfigure object permissions in a user policy, which is evaluated at the user context. For\\nmore information about using these access policy alternatives, see Walkthroughs that\\nuse policies to manage access to your Amazon S3 resources.\\nIf you as the bucket owner want to own all the objects in your bucket and use bucket\\npolicies or policies based on IAMto manage access to these objects, you can apply the\\nbucket owner enforced setting for Object Ownership. With this setting, you as the\\nbucket owner automatically own and have full control over every object in your bucket.\\nBucket and object ACLs can’t be edited and are no longer considered for access. For more\\ninformation, see Controlling ownership of objects and disabling ACLs for your bucket.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['Change', 'Description', 'Date'], ['Amazon S3 added Describe\\npermissions to AmazonS3R\\neadOnlyAccess', 'Amazon S3 added\\ns3:Describe* permissions\\nto AmazonS3ReadOnlyAc\\ncess .', 'August 11, 2023'], ['Amazon S3 added S3 Object\\nLambda permissions to\\nAmazonS3FullAccess\\nand AmazonS3ReadOnlyAc\\ncess', 'Amazon S3 updated the\\nAmazonS3FullAccess\\nand AmazonS3ReadOnlyAc\\ncess policies to include\\npermissions for S3 Object\\nLambda.', 'September 27, 2021'], ['Amazon S3 added\\nAmazonS3ObjectLamb\\ndaExecutionRolePol\\nicy', 'Amazon S3 added a new\\nAWS-managed policy called\\nAmazonS3ObjectLamb\\ndaExecutionRolePol\\nicy that provides Lambda\\nfunctions permissions to\\ninteract with S3 Object\\nLambda and write to\\nCloudWatch logs.', 'August 18, 2021'], ['Amazon S3 started tracking\\nchanges', 'Amazon S3 started tracking\\nchanges for its AWS managed\\npolicies.', 'August 18, 2021']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nS3 Storage Lens will support a maximum of five delegated administrators per organization.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AwsOrgsAccess\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"organizations:DescribeOrganization\",\\n\"organizations:ListAccounts\",\\n\"organizations:ListAWSServiceAccessForOrganization\",\\n\"organizations:ListDelegatedAdministrators\"\\n],\\n\"Resource\": [\\n\"*\"\\n]', '']]]\n",
      "[[['', '}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf the Amazon S3 Storage Lens service is using the role when you try to delete the\\nresources, then the deletion might fail. If that happens, wait for a few minutes and try the\\noperation again.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo delete the service-linked role, you must delete all the organization-level S3 Storage Lens\\nconfigurations in all the Regions where they exist.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'User: arn:aws:iam::123456789012:user/mateojackson is not authorized to perform:\\ns3:GetWidget on resource: my-example-widget', ''], ['', '', '']], [['', '', ''], ['', 'User: arn:aws:iam::123456789012:user/marymajor is not authorized to perform:\\niam:PassRole', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws sso-admin create-application \\\\\\n--instance-arn \"arn:aws:sso:::instance/ssoins-ssoins-1234567890abcdef\" \\\\\\n--application-provider-arn \"arn:aws:sso::aws:applicationProvider/custom\" \\\\\\n--name MyDataApplication', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"ApplicationArn\": \"arn:aws:sso::123456789012:application/ssoins-\\nssoins-1234567890abcdef/apl-abcd1234a1b2c3d\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"OidcJwtConfiguration\":\\n{\\n\"IssuerUrl\": \"https://login.microsoftonline.com/a1b2c3d4-abcd-1234-b7d5-\\nb154440ac123/v2.0\",\\n\"ClaimAttributePath\": \"preferred_username\",\\n\"IdentityStoreAttributePath\": \"userName\",\\n\"JwksRetrievalOption\": \"OPEN_ID_DISCOVERY\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws sso-admin create-trusted-token-issuer \\\\\\n--instance-arn \"arn:aws:sso:::instance/ssoins-1234567890abcdef\" \\\\\\n--name MyEntraIDTrustedIssuer \\\\\\n--trusted-token-issuer-type OIDC_JWT \\\\\\n--trusted-token-issuer-configuration file://./oidc-configuration.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"TrustedTokenIssuerArn\": \"arn:aws:sso::123456789012:trustedTokenIssuer/\\nssoins-1234567890abcdef/tti-43b4a822-1234-1234-1234-a1b2c3d41234\"\\n}', ''], ['', '', '']], [['', '', ''], ['', '1234973b-abcd-1234-abcd-345c5a9c1234', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"JwtBearer\":\\n{\\n\"AuthorizedTokenIssuers\":\\n[\\n{\\n\"TrustedTokenIssuerArn\": \"arn:aws:sso::123456789012:trustedTokenIssuer/\\nssoins-1234567890abcdef/tti-43b4a822-1234-1234-1234-a1b2c3d41234\",\\n\"AuthorizedAudiences\":\\n[\\n\"1234973b-abcd-1234-abcd-345c5a9c1234\"\\n]\\n}\\n]\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws sso-admin put-application-grant \\\\\\n--application-arn \"arn:aws:sso::123456789012:application/ssoins-\\nssoins-1234567890abcdef/apl-abcd1234a1b2c3d\" \\\\\\n--grant-type \"urn:ietf:params:oauth:grant-type:jwt-bearer\" \\\\\\n--grant file://./grant.json \\\\', ''], ['', '', '']], [['', '', ''], ['', 'aws sso-admin put-application-access-scope \\\\\\n--application-arn \"arn:aws:sso::111122223333:application/ssoins-\\nssoins-111122223333abcdef/apl-abcd1234a1b2c3d\" \\\\\\n--scope \"s3:access_grants:read_write\"', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Iam\":\\n{\\n\"ActorPolicy\":\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\":\\n[\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\":\\n{\\n\"AWS\": \"arn:aws:iam::123456789012:role/webapp\"\\n},\\n\"Action\": \"sso-oauth:CreateTokenWithIAM\",\\n\"Resource\": \"*\"\\n}\\n]\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws sso-admin put-application-authentication-method \\\\\\n--application-arn \"arn:aws:sso::123456789012:application/ssoins-\\nssoins-1234567890abcdef/apl-abcd1234a1b2c3d\" \\\\\\n--authentication-method-type IAM \\\\\\n--authentication-method file://./authentication-method.json', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws sso-oidc create-token-with-iam \\\\\\n--client-id \"arn:aws:sso::123456789012:application/ssoins-ssoins-1234567890abcdef/\\napl-abcd1234a1b2c3d\" \\\\\\n--grant-type urn:ietf:params:oauth:grant-type:jwt-bearer \\\\\\n--assertion IdToken', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"accessToken\": \"<suppressed long string to reduce space>\",\\n\"tokenType\": \"Bearer\",\\n\"expiresIn\": 3600,\\n\"refreshToken\": \"<suppressed long string to reduce space>\",\\n\"idToken\": \"<suppressed long string to reduce space>\",\\n\"issuedTokenType\": \"urn:ietf:params:oauth:token-type:refresh_token\",\\n\"scope\": [\\n\"sts:identity_context\",\\n\"s3:access_grants:read_write\",\\n\"openid\",\\n\"aws\"\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"aws:identity_store_id\": \"d-996773e796\",\\n\"sts:identity_context\": \"AQoJb3JpZ2luX2VjEOTtl;<SUPRESSED>\",\\n\"sub\": \"83d43802-00b1-7054-db02-f1d683aacba5\",\\n\"aws:instance_account\": \"123456789012\",\\n\"iss\": \"https://identitycenter.amazonaws.com/ssoins-1234567890abcdef\",\\n\"sts:audit_context\": \"AQoJb3JpZ2luX2VjEOT<SUPRESSED>==\",\\n\"aws:identity_store_arn\": \"arn:aws:identitystore::232642235904:identitystore/\\nd-996773e796\",\\n\"aud\": \"abcd12344U0gi7n4Yyp0-WV1LWNlbnRyYWwtMQ\",\\n\"aws:instance_arn\": \"arn:aws:sso:::instance/ssoins-6987d7fb04cf7a51\",\\n\"aws:credential_id\": \"EXAMPLEHI5glPh40y9TpApJn8...\",\\n\"act\": {', '']]]\n",
      "[[['', '\"sub\": \"arn:aws:sso::232642235904:trustedTokenIssuer/\\nssoins-6987d7fb04cf7a51/43b4a822-1020-7053-3631-cb2d3e28d10e\"\\n},\\n\"auth_time\": \"2023-11-01T20:24:28Z\",\\n\"exp\": 1698873868,\\n\"iat\": 1698870268\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws sts assume-role \\\\\\n--role-arn \"arn:aws:iam::123456789012:role/temp-role\" \\\\\\n--role-session-name \"TempDirectoryUserRole\" \\\\\\n--provided-contexts ProviderArn=\"arn:aws:iam::aws:contextProvider/\\nIdentityCenter\",ContextAssertion=\"value from sts:identity_context\"', ''], ['', '', '']]]\n",
      "[[['Steps', 'Description'], ['1', 'Create an S3 Access Grants instance\\nTo get started, initiate an S3 Access Grants instance that will\\ncontain your individual access grants.'], ['2', 'Register a location\\nSecond, register an S3 data location (such as the default,\\ns3://) and then specify a default IAM role that S3 Access\\nGrants assumes when providing access to the S3 data location.\\nYou can also add custom locations to specific buckets or\\nprefixes and map those to custom IAM roles.'], ['3', 'Create grants\\nCreate individual permission grants. Specify in these permissio\\nn grants the registered S3 location, the scope of data access\\nwithin the location, the identity of the grantee, and their\\naccess level (READ, WRITE, or R EADWRITE ).'], ['4', \"Request access to S3 data\\nWhen users, applications, and AWS services want to access\\nS3 data, they first make an access request. S3 Access Grants\\ndetermines if the request should be authorized. If there is a\\ncorresponding grant that authorizes access, S3 Access Grants\\nuses the registered location's IAM role that's associated with\\nthat grant to vend temporary credentials back to the request\\ner.\"]]]\n",
      "[[['Steps', 'Description'], ['5', 'Access S3 data\\nApplications use the temporary credentials vended by S3\\nAccess Grants to access S3 data.']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3control create-access-grants-instance \\\\\\n--account-id 111122223333 \\\\\\n--region us-east-2', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"CreatedAt\": \"2023-05-31T17:54:07.893000+00:00\",\\n\"AccessGrantsInstanceId\": \"default\",\\n\"AccessGrantsInstanceArn\": \"arn:aws:s3:us-east-2:111122223333:access-grants/\\ndefault\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public void createAccessGrantsInstance() {\\nCreateAccessGrantsInstanceRequest createRequest =\\nCreateAccessGrantsInstanceRequest.builder().accountId(\"111122223333\").build();\\nCreateAccessGrantsInstanceResponse createResponse =\\ns3Control.createAccessGrantsInstance(createRequest);LOGGER.info(\"CreateAccessGrantsI\\n\" + createResponse);\\n}', 'n'], ['', '', '']], [['', '', ''], ['', 'CreateAccessGrantsInstanceResponse(\\nCreatedAt=2023-06-07T01:46:20.507Z,\\nAccessGrantsInstanceId=default,\\nAccessGrantsInstanceArn=arn:aws:s3:us-east-2:111122223333:access-grants/default)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-access-grants-instance \\\\\\n--account-id 111122223333 \\\\\\n--region us-east-2', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"AccessGrantsInstanceArn\": \"arn:aws:s3:us-east-2: 111122223333:access-grants/\\ndefault\",\\n\"AccessGrantsInstanceId\": \"default\",\\n\"CreatedAt\": \"2023-05-31T17:54:07.893000+00:00\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control list-access-grants-instances \\\\\\n--account-id 111122223333 \\\\\\n--region us-east-2', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"AccessGrantsInstanceArn\": \"arn:aws:s3:us-east-2: 111122223333:access-grants/\\ndefault\",\\n\"AccessGrantsInstanceId\": \"default\",\\n\"CreatedAt\": \"2023-05-31T17:54:07.893000+00:00\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public void getAccessGrantsInstance() {\\nGetAccessGrantsInstanceRequest getRequest = GetAccessGrantsInstanceRequest.builder()\\n.accountId(\"111122223333\")\\n.build();\\nGetAccessGrantsInstanceResponse getResponse =\\ns3Control.getAccessGrantsInstance(getRequest);', '']]]\n",
      "[[['', 'LOGGER.info(\"GetAccessGrantsInstanceResponse: \" + getResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'GetAccessGrantsInstanceResponse(\\nAccessGrantsInstanceArn=arn:aws:s3:us-east-2: 111122223333:access-grants/default,\\nCreatedAt=2023-06-07T01:46:20.507Z)', ''], ['', '', '']], [['', '', ''], ['', 'public void listAccessGrantsInstances() {\\nListAccessGrantsInstancesRequest listRequest =\\nListAccessGrantsInstancesRequest.builder()\\n.accountId(\"111122223333\")\\n.build();\\nListAccessGrantsInstancesResponse listResponse =\\ns3Control.listAccessGrantsInstances(listRequest);\\nLOGGER.info(\"ListAccessGrantsInstancesResponse: \" + listResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'ListAccessGrantsInstancesResponse(\\nAccessGrantsInstancesList=[\\nListAccessGrantsInstanceEntry(\\nAccessGrantsInstanceId=default,\\nAccessGrantsInstanceArn=arn:aws:s3:us-east-2:111122223333:access-grants/default,\\nCreatedAt=2023-06-07T04:28:11.728Z\\n)\\n]\\n)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control associate-access-grants-identity-center \\\\\\n--account-id 111122223333 \\\\\\n--identity-center-arn arn:aws:sso:::instance/ssoins-1234a567bb89012c \\\\\\n--profile access-grants-profile \\\\', '']]]\n",
      "[[['', '--region eu-central-1\\n// No response body', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control dissociate-access-grants-identity-center \\\\\\n--account-id 111122223333 \\\\\\n--profile access-grants-profile \\\\\\n--region eu-central-1\\n// No response body', ''], ['', '', '']], [['', '', ''], ['', \"Important\\nIf you delete an S3 Access Grants instance, the deletion is permanent and can't be undone.\\nAll grantees that were given access through the grants in this S3 Access Grants instance will\\nlose access to your S3 data.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nBefore you can delete an S3 Access Grants instance, you must first delete all grants and\\nlocations created within the S3 Access Grants instance. If you have associated an IAM\\nIdentity Center center instance with your S3 Access Grants instance, you must disassociate\\nit first.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control delete-access-grants-instance \\\\\\n--account-id 111122223333 \\\\\\n--profile access-grants-profile \\\\\\n--region us-east-2 \\\\\\n--endpoint-url https://s3-control.us-east-2.amazonaws.com \\\\', '']]]\n",
      "[[['', '// No response body', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nBefore you can delete an S3 Access Grants instance, you must first delete all grants\\nand locations created within the S3 Access Grants instance. If you have associated an\\nIAM Identity Center center instance with your S3 Access Grants instance, you must\\ndisassociate it first.', ''], ['', '', '']], [['', '', ''], ['', 'public void deleteAccessGrantsInstance() {\\nDeleteAccessGrantsInstanceRequest deleteRequest =\\nDeleteAccessGrantsInstanceRequest.builder()\\n.accountId(\"111122223333\")\\n.build();\\nDeleteAccessGrantsInstanceResponse deleteResponse =\\ns3Control.deleteAccessGrantsInstance(deleteRequest);\\nLOGGER.info(\"DeleteAccessGrantsInstanceResponse: \" + deleteResponse);\\n}', ''], ['', '', '']]]\n",
      "[[['S3 URI', 'IAM role', 'Description'], ['s3://', 'Default-IAM-\\nrole', 'The default location, s3://, includes all\\nbuckets in the AWS Region.'], ['s3://example-s\\n3-bucket1 /', 'IAM-role-For-\\nbucket', 'This location includes all objects in the\\nspecified bucket.']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Stmt1234567891011\",\\n\"Action\": [\"sts:AssumeRole\", \"sts:SetSourceIdentity\", \"sts:SetContext\"],\\n\"Effect\": \"Allow\",', '']]]\n",
      "[[['', '\"Principal\": {\"Service\":\"access-grants.s3.amazonaws.com\"}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\n• If you use server-side encryption with AWS Key Management Service (AWS KMS)\\nkeys to encrypt your data, the following example includes the necessary AWS KMS\\npermissions for the IAM role in the policy. If you do not use this feature, you can\\nremove these permissions from your IAM policy.\\n• You can restrict the IAM role to access S3 data only if the credentials are vended by\\nS3 Access Grants. This example shows you how to add a Condition statement for a\\nspecific S3 Access Grants instance. To do this, replace the S3 Access Grants instance\\nARN in the condition statement with your S3 Access Grants instance ARN, which has\\nthe format: arn:aws:s3:region:accountId:access-grants/default', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"ObjectLevelReadPermissions\",\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:GetObjectAcl\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:ListMultipartUploadParts\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::*\"', '']]]\n",
      "[[['', '],\\n\"Condition\":{\\n\"StringEquals\": { \"aws:ResourceAccount\": \"accountId\" },\\n\"ArnEquals\": {\\n\"s3:AccessGrantsInstanceArn\": [\"arn:aws:s3:region:accountId:access-\\ngrants/default\"]\\n}\\n}\\n},\\n{\\n\"Sid\": \"ObjectLevelWritePermissions\",\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\",\\n\"s3:PutObjectAcl\",\\n\"s3:PutObjectVersionAcl\",\\n\"s3:DeleteObject\",\\n\"s3:DeleteObjectVersion\",\\n\"s3:AbortMultipartUpload\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::*\"\\n],\\n\"Condition\":{\\n\"StringEquals\": { \"aws:ResourceAccount\": \"accountId\" },\\n\"ArnEquals\": {\\n\"s3:AccessGrantsInstanceArn\": [\"arn:aws:s3:AWS\\nRegion:accountId:access-grants/default\"]\\n}\\n}\\n},\\n{\\n\"Sid\": \"BucketLevelReadPermissions\",\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:ListBucket\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::*\"\\n],\\n\"Condition\":{\\n\"StringEquals\": { \"aws:ResourceAccount\": \"accountId\" },\\n\"ArnEquals\": {', '']]]\n",
      "[[['', '\"s3:AccessGrantsInstanceArn\": [\"arn:aws:s3:AWS\\nRegion:accountId:access-grants/default\"]\\n}\\n}\\n},\\n{\\n\"Sid\": \"KMSPermissions\",\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"kms:Decrypt\",\\n\"kms:GenerateDataKey\"\\n],\\n\"Resource\":[\\n\"*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Stmt1234567891011\",\\n\"Action\": [\"sts:AssumeRole\", \"sts:SetSourceIdentity\"],\\n\"Effect\": \"Allow\",\\n\"Principal\": {\"Service\":\"access-grants.s3.amazonaws.com\"}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws iam create-role --role-name accessGrantsTestRole \\\\\\n--region us-east-2 \\\\\\n--assume-role-policy-document file://TestRolePolicy.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Role\": {\\n\"Path\": \"/\",\\n\"RoleName\": \"accessGrantsTestRole\",\\n\"RoleId\": \"AROASRDGX4WM4GH55GIDA\",\\n\"Arn\": \"arn:aws:iam::111122223333:role/accessGrantsTestRole\",\\n\"CreateDate\": \"2023-05-31T18:11:06+00:00\",\\n\"AssumeRolePolicyDocument\": {\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Stmt1685556427189\",', '']]]\n",
      "[[['', '\"Action\": [\\n\"sts:AssumeRole\",\\n\"sts:SetSourceIdentity\"\\n],\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\":\"access-grants.s3.amazonaws.com\"\\n}\\n}\\n]\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you use server-side encryption with AWS Key Management Service (AWS KMS) keys to\\nencrypt your data, the following example adds the necessary AWS KMS permissions for\\nthe IAM role in the policy. If you do not use this feature, you can remove these permissions\\nfrom your IAM policy.\\nTo make sure that the IAM role can only be used to access data in S3 if the credentials\\nare vended out by S3 Access Grants, this example shows you how to add a Condition\\nstatement that specifies the S3 Access Grants instance (s3:AccessGrantsInstance:\\nInstanceArn) in your IAM policy. When using following example policy, replace the user\\ninput placeholders with your own information.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"ObjectLevelReadPermissions\",\\n\"Effect\":\"Allow\",', '']]]\n",
      "[[['', '\"Action\":[\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:GetObjectAcl\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:ListMultipartUploadParts\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::*\"\\n],\\n\"Condition\":{\\n\"StringEquals\": { \"aws:ResourceAccount\": \"accountId\" },\\n\"ArnEquals\": {\\n\"s3:AccessGrantsInstanceArn\": [\"arn:aws:s3:region:accountId:access-\\ngrants/default\"]\\n}\\n}\\n},\\n{\\n\"Sid\": \"ObjectLevelWritePermissions\",\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\",\\n\"s3:PutObjectAcl\",\\n\"s3:PutObjectVersionAcl\",\\n\"s3:DeleteObject\",\\n\"s3:DeleteObjectVersion\",\\n\"s3:AbortMultipartUpload\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::*\"\\n],\\n\"Condition\":{\\n\"StringEquals\": { \"aws:ResourceAccount\": \"accountId\" },\\n\"ArnEquals\": {\\n\"s3:AccessGrantsInstanceArn\": [\"arn:aws:s3:AWS Region:accountId:access-\\ngrants/default\"]\\n}\\n}\\n},\\n{\\n\"Sid\": \"BucketLevelReadPermissions\",\\n\"Effect\":\"Allow\",\\n\"Action\":[', '']]]\n",
      "[[['', '\"s3:ListBucket\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::*\"\\n],\\n\"Condition\":{\\n\"StringEquals\": { \"aws:ResourceAccount\": \"accountId\" },\\n\"ArnEquals\": {\\n\"s3:AccessGrantsInstanceArn\": [\"arn:aws:s3:AWS Region:accountId:access-\\ngrants/default\"]\\n}\\n}\\n},\\n{\\n\"Sid\": \"KMSPermissions\",\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"kms:Decrypt\",\\n\"kms:GenerateDataKey\"\\n],\\n\"Resource\":[\\n\"*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws iam put-role-policy \\\\\\n--role-name accessGrantsTestRole \\\\\\n--policy-name accessGrantsTestRole \\\\\\n--policy-document file://iam-policy.json', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-access-grants-location \\\\\\n--account-id 111122223333 \\\\\\n--location-scope s3:// \\\\\\n--iam-role-arn arn:aws:iam::111122223333:role/accessGrantsTestRole', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\"CreatedAt\": \"2023-05-31T18:23:48.107000+00:00\",\\n\"AccessGrantsLocationId\": \"default\",\\n\"AccessGrantsLocationArn\": \"arn:aws:s3:us-east-2:111122223333:access-grants/\\ndefault/location/default\",\\n\"LocationScope\": \"s3://”\\n\"IAMRoleArn\": \"arn:aws:iam::111122223333:role/accessGrantsTestRole\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-access-grants-location \\\\\\n--account-id 111122223333 \\\\\\n--location-scope s3://DOC-BUCKET-EXAMPLE/ \\\\\\n--iam-role-arn arn:aws:iam::123456789012:role/accessGrantsTestRole', ''], ['', '', '']], [['', '', ''], ['', '{\"CreatedAt\": \"2023-05-31T18:23:48.107000+00:00\",\\n\"AccessGrantsLocationId\": \"635f1139-1af2-4e43-8131-a4de006eb456\",\\n\"AccessGrantsLocationArn\": \"arn:aws:s3:us-east-2: 111122223333:access-grants/\\ndefault/location/635f1139-1af2-4e43-8131-a4de006eb888\",\\n\"LocationScope\": \"s3://DOC-BUCKET-EXAMPLE/\",\\n\"IAMRoleArn\": \"arn:aws:iam::111122223333:role/accessGrantsTestRole\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public void createAccessGrantsLocation() {\\nCreateAccessGrantsLocationRequest createRequest =\\nCreateAccessGrantsLocationRequest.builder()\\n.accountId(\"111122223333\")\\n.locationScope(\"s3://\")\\n.iamRoleArn(\"arn:aws:iam::123456789012:role/accessGrantsTestRole\")\\n.build();\\nCreateAccessGrantsLocationResponse createResponse =\\ns3Control.createAccessGrantsLocation(createRequest);\\nLOGGER.info(\"CreateAccessGrantsLocationResponse: \" + createResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'CreateAccessGrantsLocationResponse(\\nCreatedAt=2023-06-07T04:35:11.027Z,\\nAccessGrantsLocationId=default,\\nAccessGrantsLocationArn=arn:aws:s3:us-east-2:111122223333:access-grants/default/\\nlocation/default,\\nLocationScope=s3://,\\nIAMRoleArn=arn:aws:iam::111122223333:role/accessGrantsTestRole', '']]]\n",
      "[[['', ')', ''], ['', '', '']], [['', '', ''], ['', 'public void createAccessGrantsLocation() {\\nCreateAccessGrantsLocationRequest createRequest =\\nCreateAccessGrantsLocationRequest.builder()\\n.accountId(\"111122223333\")\\n.locationScope(\"s3://DOC-BUCKET-EXAMPLE/\")\\n.iamRoleArn(\"arn:aws:iam::111122223333:role/accessGrantsTestRole\")\\n.build();\\nCreateAccessGrantsLocationResponse createResponse =\\ns3Control.createAccessGrantsLocation(createRequest);\\nLOGGER.info(\"CreateAccessGrantsLocationResponse: \" + createResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'CreateAccessGrantsLocationResponse(\\nCreatedAt=2023-06-07T04:35:10.027Z,\\nAccessGrantsLocationId=18cfe6fb-eb5a-4ac5-aba9-8d79f04c2012,\\nAccessGrantsLocationArn=arn:aws:s3:us-east-2:111122223333:access-grants/default/\\nlocation/18cfe6fb-eb5a-4ac5-aba9-8d79f04c2666,\\nLocationScope= s3://test-bucket-access-grants-user123/,\\nIAMRoleArn=arn:aws:iam::111122223333:role/accessGrantsTestRole\\n)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-access-grants-location \\\\\\n--account-id 111122223333 \\\\\\n--access-grants-location-id default', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"CreatedAt\": \"2023-05-31T18:23:48.107000+00:00\",\\n\"AccessGrantsLocationId\": \"default\",\\n\"AccessGrantsLocationArn\": \"arn:aws:s3:us-east-2:111122223333:access-grants/\\ndefault/location/default\",\\n\"IAMRoleArn\": \"arn:aws:iam::111122223333:role/accessGrantsTestRole\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control list-access-grants-locations \\\\\\n--account-id 111122223333 \\\\\\n--region us-east-2', ''], ['', '', '']], [['', '', ''], ['', '{\"AccessGrantsLocationsList\": [\\n{\\n\"CreatedAt\": \"2023-05-31T18:23:48.107000+00:00\",\\n\"AccessGrantsLocationId\": \"default\",\\n\"AccessGrantsLocationArn\": \"arn:aws:s3:us-east-2:111122223333:access-grants/\\ndefault/location/default\",\\n\"LocationScope\": \"s3://\"\\n\"IAMRoleArn\": \"arn:aws:iam::111122223333:role/accessGrantsTestRole\"\\n},\\n{\\n\"CreatedAt\": \"2023-05-31T18:23:48.107000+00:00\",\\n\"AccessGrantsLocationId\": \"635f1139-1af2-4e43-8131-a4de006eb456\",\\n\"AccessGrantsLocationArn\": \"arn:aws:s3:us-east-2:111122223333:access-grants/\\ndefault/location/635f1139-1af2-4e43-8131-a4de006eb888\",\\n\"LocationScope\": \"s3://DOC-EXAMPLE-BUCKET/prefixA*\",\\n\"IAMRoleArn\": \"arn:aws:iam::111122223333:role/accessGrantsTestRole\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public void getAccessGrantsLocation() {\\nGetAccessGrantsLocationRequest getAccessGrantsLocationRequest =\\nGetAccessGrantsLocationRequest.builder()\\n.accountId(\"111122223333\")\\n.accessGrantsLocationId(\"default\")\\n.build();\\nGetAccessGrantsLocationResponse getAccessGrantsLocationResponse =\\ns3Control.getAccessGrantsLocation(getAccessGrantsLocationRequest);\\nLOGGER.info(\"GetAccessGrantsLocationResponse: \" + getAccessGrantsLocationResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'GetAccessGrantsLocationResponse(\\nCreatedAt=2023-06-07T04:35:10.027Z,\\nAccessGrantsLocationId=default,\\nAccessGrantsLocationArn=arn:aws:s3:us-east-2:111122223333:access-grants/default/\\nlocation/default,\\nLocationScope= s3://,\\nIAMRoleArn=arn:aws:iam::111122223333:role/accessGrantsTestRole\\n)', ''], ['', '', '']], [['', '', ''], ['', 'public void listAccessGrantsLocations() {\\nListAccessGrantsLocationsRequest listRequest =\\nListAccessGrantsLocationsRequest.builder()', '']]]\n",
      "[[['', '.accountId(\"111122223333\")\\n.build();\\nListAccessGrantsLocationsResponse listResponse =\\ns3Control.listAccessGrantsLocations(listRequest);\\nLOGGER.info(\"ListAccessGrantsLocationsResponse: \" + listResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'ListAccessGrantsLocationsResponse(\\nAccessGrantsLocationsList=[\\nListAccessGrantsLocationsEntry(\\nCreatedAt=2023-06-07T04:35:11.027Z,\\nAccessGrantsLocationId=default,\\nAccessGrantsLocationArn=arn:aws:s3:us-east-2:111122223333:access-grants/default/\\nlocation/default,\\nLocationScope=s3://,\\nIAMRoleArn=arn:aws:iam::111122223333:role/accessGrantsTestRole\\n),\\nListAccessGrantsLocationsEntry(\\nCreatedAt=2023-06-07T04:35:10.027Z,\\nAccessGrantsLocationId=635f1139-1af2-4e43-8131-a4de006eb456,\\nAccessGrantsLocationArn=arn:aws:s3:us-east-2:111122223333:access-grants/default/\\nlocation/635f1139-1af2-4e43-8131-a4de006eb888,\\nLocationScope=s3://DOC-EXAMPLE-BUCKET/prefixA*,\\nIAMRoleArn=arn:aws:iam::111122223333:role/accessGrantsTestRole\\n)\\n]\\n)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control update-access-grants-location \\\\\\n--account-id 111122223333 \\\\\\n--access-grants-location-id 635f1139-1af2-4e43-8131-a4de006eb999 \\\\\\n--iam-role-arn arn:aws:iam::777788889999:role/accessGrantsTestRole', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"CreatedAt\": \"2023-05-31T18:23:48.107000+00:00\",\\n\"AccessGrantsLocationId\": \"635f1139-1af2-4e43-8131-a4de006eb999\",\\n\"AccessGrantsLocationArn\": \"arn:aws:s3:us-east-2:777788889999:access-grants/\\ndefault/location/635f1139-1af2-4e43-8131-a4de006eb888\",\\n\"LocationScope\": \"s3://DOC-EXAMPLE-BUCKET/prefixB*\",\\n\"IAMRoleArn\": \"arn:aws:iam::777788889999:role/accessGrantsTestRole\"', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', 'public void updateAccessGrantsLocation() {\\nUpdateAccessGrantsLocationRequest updateRequest =\\nUpdateAccessGrantsLocationRequest.builder()\\n.accountId(\"111122223333\")\\n.accessGrantsLocationId(\"635f1139-1af2-4e43-8131-a4de006eb999\")\\n.iamRoleArn(\"arn:aws:iam::777788889999:role/accessGrantsTestRole\")\\n.build();\\nUpdateAccessGrantsLocationResponse updateResponse =\\ns3Control.updateAccessGrantsLocation(updateRequest);\\nLOGGER.info(\"UpdateAccessGrantsLocationResponse: \" + updateResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'UpdateAccessGrantsLocationResponse(\\nCreatedAt=2023-06-07T04:35:10.027Z,\\nAccessGrantsLocationId=635f1139-1af2-4e43-8131-a4de006eb999,\\nAccessGrantsLocationArn=arn:aws:s3:us-east-2:777788889999:access-grants/default/\\nlocation/635f1139-1af2-4e43-8131-a4de006eb888,\\nLocationScope=s3://DOC-EXAMPLE-BUCKET/prefixB*,\\nIAMRoleArn=arn:aws:iam::777788889999:role/accessGrantsTestRole\\n)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control delete-access-grants-location \\\\', '']]]\n",
      "[[['', '--account-id 111122223333 \\\\\\n--access-grants-location-id a1b2c3d4-5678-90ab-cdef-EXAMPLE11111\\n// No response body', ''], ['', '', '']], [['', '', ''], ['', 'public void deleteAccessGrantsLocation() {\\nDeleteAccessGrantsLocationRequest deleteRequest =\\nDeleteAccessGrantsLocationRequest.builder()\\n.accountId(\"111122223333\")\\n.accessGrantsLocationId(\"a1b2c3d4-5678-90ab-cdef-EXAMPLE11111\")\\n.build();\\nDeleteAccessGrantsLocationResponse deleteResponse =\\ns3Control.deleteAccessGrantsLocation(deleteRequest);\\nLOGGER.info(\"DeleteAccessGrantsLocationResponse: \" + deleteResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'DeleteAccessGrantsLocationResponse()', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nYou cannot create a grant to a bucket if the bucket does not yet exist. However, you can\\ncreate grant to a prefix that does not exist, yet.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf you're creating an access grant that grants access to only one object, include the required\\nparameter --s3-prefix-type Object.\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-access-grant \\\\\\n--account-id 111122223333 \\\\\\n--access-grants-location-id a1b2c3d4-5678-90ab-cdef-EXAMPLE22222 \\\\\\n--access-grants-location-configuration S3SubPrefix=prefixB* \\\\\\n--permission READ \\\\\\n--grantee GranteeType=IAM,GranteeIdentifier=arn:aws:iam::123456789012:user/data-\\nconsumer-3', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\"CreatedAt\": \"2023-05-31T18:41:34.663000+00:00\",\\n\"AccessGrantId\": \"a1b2c3d4-5678-90ab-cdef-EXAMPLE11111\",\\n\"AccessGrantArn\": \"arn:aws:s3:us-east-2:111122223333:access-grants/default/\\ngrant/a1b2c3d4-5678-90ab-cdef-EXAMPLE11111\",\\n\"Grantee\": {\\n\"GranteeType\": \"IAM\",\\n\"GranteeIdentifier\": \"arn:aws:iam::111122223333:user/data-consumer-3\"\\n},\\n\"AccessGrantsLocationId\": \"a1b2c3d4-5678-90ab-cdef-EXAMPLE22222\",\\n\"AccessGrantsLocationConfiguration\": {\\n\"S3SubPrefix\": \"prefixB*\"\\n},\\n\"GrantScope\": \"s3://DOC-BUCKET-EXAMPLE/prefix*\",\\n\"Permission\": \"READ\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws identitystore list-users --identity-store-id d-1a2b3c4d1234', ''], ['', '', '']], [['', '', ''], ['', 'aws identitystore list-groups --identity-store-id d-1a2b3c4d1234', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-access-grant \\\\\\n--account-id 123456789012 \\\\\\n--access-grants-location-id default \\\\\\n--access-grants-location-configuration S3SubPrefix=\"DOC-EXAMPLE-BUCKET/rafael/*\" \\\\\\n--permission READWRITE \\\\\\n--grantee GranteeType=DIRECTORY_USER,GranteeIdentifier=83d43802-00b1-7054-db02-\\nf1d683aacba5 \\\\', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you are creating an access grant that grants access to only one object, include the\\nrequired parameter .s3PrefixType(S3PrefixType.Object).', ''], ['', '', '']], [['', '', ''], ['', 'public void createAccessGrant() {\\nCreateAccessGrantRequest createRequest = CreateAccessGrantRequest.builder()\\n.accountId(\"111122223333\")', '']]]\n",
      "[[['', '.accessGrantsLocationId(\"a1b2c3d4-5678-90ab-cdef-EXAMPLEaaaaa\")\\n.permission(\"READ\")\\n.accessGrantsLocationConfiguration(AccessGrantsLocationConfiguration.builder().s3SubP\\n.grantee(Grantee.builder().granteeType(\"IAM\").granteeIdentifier(\"arn:aws:iam::1111222\\ndata-consumer-3\").build())\\n.build();\\nCreateAccessGrantResponse createResponse =\\ns3Control.createAccessGrant(createRequest);\\nLOGGER.info(\"CreateAccessGrantResponse: \" + createResponse);\\n}', 'r\\n2'], ['', '', '']], [['', '', ''], ['', 'CreateAccessGrantResponse(\\nCreatedAt=2023-06-07T05:20:26.330Z,\\nAccessGrantId=a1b2c3d4-5678-90ab-cdef-EXAMPLE33333,\\nAccessGrantArn=arn:aws:s3:us-east-2:444455556666:access-grants/default/grant/\\na1b2c3d4-5678-90ab-cdef-EXAMPLE33333,\\nGrantee=Grantee(\\nGranteeType=IAM,\\nGranteeIdentifier=arn:aws:iam::111122223333:user/data-consumer-3\\n),\\nAccessGrantsLocationId=a1b2c3d4-5678-90ab-cdef-EXAMPLEaaaaa,\\nAccessGrantsLocationConfiguration=AccessGrantsLocationConfiguration(\\nS3SubPrefix=prefixB*\\n),\\nGrantScope=s3://DOC-BUCKET-EXAMPLE/prefixB,\\nPermission=READ\\n)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-access-grant \\\\\\n--account-id 111122223333 \\\\\\n--access-grant-id a1b2c3d4-5678-90ab-cdef-EXAMPLE22222', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"CreatedAt\": \"2023-05-31T18:41:34.663000+00:00\",\\n\"AccessGrantId\": \"a1b2c3d4-5678-90ab-cdef-EXAMPLE22222\",\\n\"AccessGrantArn\": \"arn:aws:s3:us-east-2:111122223333:access-grants/default/\\ngrant-a1b2c3d4-5678-90ab-cdef-EXAMPLE22222\",\\n\"Grantee\": {\\n\"GranteeType\": \"IAM\",\\n\"GranteeIdentifier\": \"arn:aws:iam::111122223333:user/data-consumer-3\"\\n},', '']]]\n",
      "[[['', '\"Permission\": \"READ\",\\n\"AccessGrantsLocationId\": \"12a6710f-5af8-41f5-b035-0bc795bf1a2b\",\\n\"AccessGrantsLocationConfiguration\": {\\n\"S3SubPrefix\": \"prefixB*\"\\n},\\n\"GrantScope\": \"s3://DOC-EXAMPLE-BUCKET/\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control list-access-grants \\\\\\n--account-id 111122223333', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"AccessGrantsList\": [{\"CreatedAt\": \"2023-06-14T17:54:46.542000+00:00\",\\n\"AccessGrantId\": \"dd8dd089-b224-4d82-95f6-975b4185bbaa\",\\n\"AccessGrantArn\": \"arn:aws:s3:us-east-2:111122223333:access-grants/default/\\ngrant/dd8dd089-b224-4d82-95f6-975b4185bbaa\",\\n\"Grantee\": {\\n\"GranteeType\": \"IAM\",\\n\"GranteeIdentifier\": \"arn:aws:iam::111122223333:user/data-consumer-3\"\\n},\\n\"Permission\": \"READ\",\\n\"AccessGrantsLocationId\": \"23514a34-ea2e-4ddf-b425-d0d4bfcarda1\",\\n\"GrantScope\": \"s3://DOC-EXAMPLE-BUCKET/prefixA*\"\\n},\\n{\"CreatedAt\": \"2023-06-24T17:54:46.542000+00:00\",\\n\"AccessGrantId\": \"ee8ee089-b224-4d72-85f6-975b4185a1b2\",\\n\"AccessGrantArn\": \"arn:aws:s3:us-east-2:111122223333:access-grants/default/\\ngrant/ee8ee089-b224-4d72-85f6-975b4185a1b2\",\\n\"Grantee\": {\\n\"GranteeType\": \"IAM\",', '']]]\n",
      "[[['', '\"GranteeIdentifier\": \"arn:aws:iam::111122223333:user/data-consumer-9\"\\n},\\n\"Permission\": \"READ\",\\n\"AccessGrantsLocationId\": \"12414a34-ea2e-4ddf-b425-d0d4bfcacao0\",\\n\"GrantScope\": \"s3://DOC-EXAMPLE-BUCKET/prefixB*\"\\n},\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public void getAccessGrant() {\\nGetAccessGrantRequest getRequest = GetAccessGrantRequest.builder()\\n.accountId(\"111122223333\")\\n.accessGrantId(\"a1b2c3d4-5678-90ab-cdef-EXAMPLE22222\")\\n.build();\\nGetAccessGrantResponse getResponse = s3Control.getAccessGrant(getRequest);\\nLOGGER.info(\"GetAccessGrantResponse: \" + getResponse);\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'GetAccessGrantResponse(\\nCreatedAt=2023-06-07T05:20:26.330Z,\\nAccessGrantId=a1b2c3d4-5678-90ab-cdef-EXAMPLE22222,\\nAccessGrantArn=arn:aws:s3:us-east-2:111122223333:access-grants/default/\\ngrant-fd3a5086-42f7-4b34-9fad-472e2942c70e,\\nGrantee=Grantee(\\nGranteeType=IAM,\\nGranteeIdentifier=arn:aws:iam::111122223333:user/data-consumer-3\\n),\\nPermission=READ,\\nAccessGrantsLocationId=12a6710f-5af8-41f5-b035-0bc795bf1a2b,\\nAccessGrantsLocationConfiguration=AccessGrantsLocationConfiguration(\\nS3SubPrefix=prefixB*\\n),\\nGrantScope=s3://DOC-EXAMPLE-BUCKET/\\n)', ''], ['', '', '']], [['', '', ''], ['', 'public void listAccessGrants() {\\nListAccessGrantsRequest listRequest = ListAccessGrantsRequest.builder()\\n.accountId(\"111122223333\")\\n.build();\\nListAccessGrantsResponse listResponse = s3Control.listAccessGrants(listRequest);\\nLOGGER.info(\"ListAccessGrantsResponse: \" + listResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'ListAccessGrantsResponse(\\nAccessGrantsList=[\\nListAccessGrantEntry(\\nCreatedAt=2023-06-14T17:54:46.540z,', '']]]\n",
      "[[['', 'AccessGrantId=dd8dd089-b224-4d82-95f6-975b4185bbaa,\\nAccessGrantArn=arn:aws:s3:us-east-2:111122223333:access-grants/default/\\ngrant/dd8dd089-b224-4d82-95f6-975b4185bbaa,\\nGrantee=Grantee(\\nGranteeType=IAM, GranteeIdentifier= arn:aws:iam::111122223333:user/data-consumer-3\\n),\\nPermission=READ,\\nAccessGrantsLocationId=23514a34-ea2e-4ddf-b425-d0d4bfcarda1,\\nGrantScope=s3://DOC-EXAMPLE-BUCKET/prefixA\\n),\\nListAccessGrantEntry(\\nCreatedAt=2023-06-24T17:54:46.540Z,\\nAccessGrantId=ee8ee089-b224-4d72-85f6-975b4185a1b2,\\nAccessGrantArn=arn:aws:s3:us-east-2:111122223333:access-grants/default/\\ngrant/ee8ee089-b224-4d72-85f6-975b4185a1b2,\\nGrantee=Grantee(\\nGranteeType=IAM, GranteeIdentifier= arn:aws:iam::111122223333:user/data-consumer-9\\n),\\nPermission=READ,\\nAccessGrantsLocationId=12414a34-ea2e-4ddf-b425-d0d4bfcacao0,\\nGrantScope=s3://DOC-EXAMPLE-BUCKET/prefixB*\\n)\\n]\\n)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control delete-access-grant \\\\\\n--account-id 111122223333 \\\\\\n--access-grant-id a1b2c3d4-5678-90ab-cdef-EXAMPLE11111\\n// No response body', ''], ['', '', '']], [['', '', ''], ['', 'public void deleteAccessGrant() {', '']]]\n",
      "[[['', 'DeleteAccessGrantRequest deleteRequest = DeleteAccessGrantRequest.builder()\\n.accountId(\"111122223333\")\\n.accessGrantId(\"a1b2c3d4-5678-90ab-cdef-EXAMPLE11111\")\\n.build();\\nDeleteAccessGrantResponse deleteResponse =\\ns3Control.deleteAccessGrant(deleteRequest);\\nLOGGER.info(\"DeleteAccessGrantResponse: \" + deleteResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'DeleteAccessGrantResponse()', ''], ['', '', '']]]\n",
      "[[['Grant\\nscope', 'Requested\\nscope', 'Privilege', 'Returned scope', 'Effect'], ['S3://examp\\ns3-\\nbucket1\\n/bob/*', 'leex-ample-\\ns3-\\nbucket1\\n/bob/*', 'Default', 'example-s3-bucket1\\n/bob/*', 'The requester has access\\nto all objects that have\\nkey names that start\\nwith the prefix bob/\\nin the example-s3-\\nbucket1 bucket.'], ['S3://examp\\ns3-\\nbucket1\\n/bob/*', 'leex-ample-\\ns3-\\nbucket1\\n/bob/', 'Minimal', 'example-s3-bucket1\\n/bob/', \"Without a wild card\\n* character after the\\nprefix name b ob/, the\\nrequester has access to\\nonly the object named\\nbob/ in the example-\\ns3-bucket1 bucket.\\nIt's not common to\\nhave such an object.\\nThe requester doesn't\\nhave access to any other\\nobjects, including those\\nthat have key names\\nthat start with the bob/\\nprefix.\"], ['S3://examp\\ns3-\\nbucket1\\n/bob/*', 'leex-ample-\\ns3-\\nbucket1\\n/bob/', 'Minimal', 'example-s3-bucket1\\n/bob/images/*', 'The requester has access\\nto all objects that have\\nkey names that start\\nwith the prefix bob/\\nimages/* in the']]]\n",
      "[[['Grant\\nscope', 'Requested\\nscope', 'Privilege', 'Returned scope', 'Effect'], ['', 'images/\\n*', '', '', 'example-s3-bucket1\\nbucket.'], ['S3://examp\\ns3-\\nbucket1\\n/bob/\\nrepo\\nrts/*', 'leex-ample-\\ns3-\\nbucket1\\n/bob/\\nrepo\\nrts/\\nfile.\\ntxt', 'Default', 'example-s3-bucket1\\n/bob/reports/*', 'The requester has access\\nto all objects that have\\nkey names that start\\nwith the bob/reports\\nprefix in the e xample-\\ns3-bucket1 bucket,\\nwhich is the scope of the\\nmatching grant.'], ['S3://examp\\ns3-\\nbucket1\\n/bob/\\nrepo\\nrts/*', 'leex-ample-\\ns3-\\nbucket1\\n/bob/\\nrepo\\nrts/\\nfile.\\ntxt', 'Minimal', 'example-s3-bucket1\\n/bob/reports/\\nfile.txt', 'The requester has access\\nonly to the object with\\nthe key name bob/\\nreports/file.txt\\nin the example-s3-\\nbucket1 bucket. The\\nrequester has no access\\nto any other object.']], [['', '', ''], ['', \"Note\\nIn your request for a temporary token, if the location is an object, set the value of the\\ntargetType parameter in your request to Object. This parameter is required only if the\\nlocation is an object and the privilege level is Minimal. If the location is a bucket or a\\nprefix, you don't need to specify this parameter.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-data-access \\\\\\n--account-id 111122223333 \\\\\\n--target s3://example-s3-bucket/prefixA* \\\\\\n--permission READ \\\\\\n--privilege Default \\\\\\n--region us-east-2', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Credentials\": {\\n\"AccessKeyId\": \"Example-key-id\",\\n\"SecretAccessKey\": \"Example-access-key\",\\n\"SessionToken\": \"Example-session-token\",\\n\"Expiration\": \"2023-06-14T18:56:45+00:00\"},\\n\"MatchedGrantTarget\": \"s3://example-s3-bucket/prefixA**\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public void getDataAccess() {\\nGetDataAccessRequest getDataAccessRequest = GetDataAccessRequest.builder()\\n.accountId(\"111122223333\")\\n.permission(Permission.READ)\\n.privilege(Privilege.MINIMAL)\\n.target(\"s3://example-s3-bucket/prefixA*\")\\n.build();\\nGetDataAccessResponse getDataAccessResponse =\\ns3Control.getDataAccess(getDataAccessRequest);\\nLOGGER.info(\"GetDataAccessResponse: \" + getDataAccessResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'GetDataAccessResponse(\\nCredentials=Credentials(\\nAccessKeyId=\"Example-access-key-id\",\\nSecretAccessKey=\"Example-secret-access-key\",\\nSessionToken=\"Example-session-token\",\\nExpiration=2023-06-07T06:55:24Z\\n))', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws configure set aws_access_key_id \"$accessKey\" --profile access-grants-consumer-\\naccess-profile\\naws configure set aws_secret_access_key \"$secretKey\" --profile access-grants-consumer-\\naccess-profile\\naws configure set aws_session_token \"$sessionToken\" --profile access-grants-consumer-\\naccess-profile', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object \\\\\\n--bucket example-s3-bucket1 \\\\\\n--key myprefix \\\\\\n--region us-east-2 \\\\\\n--profile access-grants-consumer-access-profile', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",', '']]]\n",
      "[[['', '\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"444455556666\"\\n},\\n\"Action\": [\\n\"s3:ListAccessGrants\",\\n\"s3:ListAccessGrantsLocations\",\\n\"s3:GetDataAccess\",\\n\"s3:GetAccessGrantsInstanceForPrefix\"\\n],\\n\"Resource\": \"arn:aws:s3:us-east-2:111122223333:access-grants/default\"\\n} ]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-access-grants-instance-resource-policy \\\\\\n--account-id 111122223333 \\\\\\n--policy file://resourcePolicy.json \\\\\\n--region us-east-2\\n{\\n\"Policy\": \"{\\\\n\\n\\\\\"Version\\\\\": \\\\\"2012-10-17\\\\\",\\\\n\\n\\\\\"Statement\\\\\": [{\\\\n\\n\\\\\"Effect\\\\\": \\\\\"Allow\\\\\",\\\\n\\n\\\\\"Principal\\\\\": {\\\\n\\n\\\\\"AWS\\\\\": \\\\\"444455556666\\\\\"\\\\n\\n},\\\\n\\n\\\\\"Action\\\\\": [\\\\n\\n\\\\\"s3:ListAccessGrants\\\\\",\\\\n\\n\\\\\"s3:ListAccessGrantsLocations\\\\\",\\\\n\\n\\\\\"s3:GetDataAccess\\\\\",\\\\n\\n\\\\\"s3:GetAccessGrantsInstanceForPrefix\\\\\"\\\\n\\n],\\\\n\\n\\\\\"Resource\\\\\": \\\\\"arn:aws:s3:us-east-2:111122223333:access-grants/default\\\\\"\\\\n\\n}\\\\n', '']]]\n",
      "[[['', ']\\\\n\\n}\\\\n\",\\n\"CreatedAt\": \"2023-06-16T00:07:47.473000+00:00\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control get-access-grants-instance-resource-policy \\\\\\n--account-id 111122223333 \\\\\\n--region us-east-2\\n{\\n\"Policy\": \"{\\\\\"Version\\\\\":\\\\\"2012-10-17\\\\\",\\\\\"Statement\\\\\":[{\\\\\"Effect\\\\\":\\\\\"Allow\\\\\",\\\\\"Principal\\n\\\\\":{\\\\\"AWS\\\\\":\\\\\"arn:aws:iam::111122223333:root\\\\\"},\\\\\"Action\\\\\":[\\\\\"s3:ListAccessGrants\\\\\",\\n\\\\\"s3:ListAccessGrantsLocations\\\\\",\\\\\"s3:GetDataAccess\\\\\"],\\\\\"Resource\\\\\":\\\\\"arn:aws:s3:us-\\neast-2:111122223333:access-grants/default\\\\\"}]}\",\\n\"CreatedAt\": \"2023-06-16T00:07:47.473000+00:00\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control delete-access-grants-instance-resource-policy \\\\\\n--account-id 111122223333 \\\\\\n--region us-east-2\\n// No response body', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"444455556666\"\\n},\\n\"Action\": [\\n\"s3:ListAccessGrants\",\\n\"s3:ListAccessGrantsLocations\",\\n\"s3:GetDataAccess\",\\n\"s3:GetAccessGrantsInstanceForPrefix\"\\n],\\n\"Resource\": \"arn:aws:s3:us-east-2:111122223333:access-grants/default\"\\n} ]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"444455556666\"\\n},\\n\"Action\": [\\n\"s3:ListAccessGrants\",\\n\"s3:ListAccessGrantsLocations\",\\n\"s3:GetDataAccess\",\\n\"s3:GetAccessGrantsInstanceForPrefix\"\\n],\\n\"Resource\": \"arn:aws:s3:us-east-2:111122223333:access-grants/default\"', '']]]\n",
      "[[['', '} ]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public void putAccessGrantsInstanceResourcePolicy() {\\nPutAccessGrantsInstanceResourcePolicyRequest putRequest =\\nPutAccessGrantsInstanceResourcePolicyRequest.builder()\\n.accountId(111122223333)\\n.policy(RESOURCE_POLICY)\\n.build();\\nPutAccessGrantsInstanceResourcePolicyResponse putResponse =\\ns3Control.putAccessGrantsInstanceResourcePolicy(putRequest);\\nLOGGER.info(\"PutAccessGrantsInstanceResourcePolicyResponse: \" + putResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'PutAccessGrantsInstanceResourcePolicyResponse(\\nPolicy={\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"444455556666\"\\n},\\n\"Action\": [\\n\"s3:ListAccessGrants\",\\n\"s3:ListAccessGrantsLocations\",\\n\"s3:GetDataAccess\",\\n\"s3:GetAccessGrantsInstanceForPrefix\"\\n],\\n\"Resource\": \"arn:aws:s3:us-east-2:111122223333:access-grants/default\"\\n}]\\n}\\n)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public void getAccessGrantsInstanceResourcePolicy() {\\nGetAccessGrantsInstanceResourcePolicyRequest getRequest =\\nGetAccessGrantsInstanceResourcePolicyRequest.builder()\\n.accountId(111122223333)\\n.build();\\nGetAccessGrantsInstanceResourcePolicyResponse getResponse =\\ns3Control.getAccessGrantsInstanceResourcePolicy(getRequest);\\nLOGGER.info(\"GetAccessGrantsInstanceResourcePolicyResponse: \" + getResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'GetAccessGrantsInstanceResourcePolicyResponse(\\nPolicy={\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Principal\":\\n{\"AWS\":\"arn:aws:iam::444455556666:root\"},\"Action\":\\n[\"s3:ListAccessGrants\",\"s3:ListAccessGrantsLocations\",\"s3:GetDataAccess\"],\"Resource\":\\neast-2:111122223333:access-grants/default\"}]},\\nCreatedAt=2023-06-15T22:54:44.319Z\\n)', '\"'], ['', '', '']], [['', '', ''], ['', 'public void deleteAccessGrantsInstanceResourcePolicy() {\\nDeleteAccessGrantsInstanceResourcePolicyRequest deleteRequest =\\nDeleteAccessGrantsInstanceResourcePolicyRequest.builder()\\n.accountId(111122223333)\\n.build();\\nDeleteAccessGrantsInstanceResourcePolicyResponse deleteResponse =\\ns3Control.putAccessGrantsInstanceResourcePolicy(deleteRequest);\\nLOGGER.info(\"DeleteAccessGrantsInstanceResourcePolicyResponse: \" + deleteResponse);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'DeleteAccessGrantsInstanceResourcePolicyResponse()', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetDataAccess\",\\n],\\n\"Resource\": \"arn:aws:s3:us-east-2:111122223333:access-grants/default\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-access-grant \\\\\\n--account-id 111122223333 \\\\\\n--access-grants-location-id default \\\\\\n--access-grants-location-configuration S3SubPrefix=prefixA* \\\\\\n--permission READ \\\\\\n--grantee GranteeType=IAM,GranteeIdentifier=arn:aws:iam::444455556666:role/data-\\nconsumer-1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTagging in S3 Access Grants uses different API operations than object tagging. S3 Access\\nGrants uses the TagResource, UntagResource, and ListTagsForResource API operations,\\nwhere a resource can be either an S3 Access Grants instance, a registered location, or an\\naccess grant.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-access-grants-instance \\\\\\n--account-id 111122223333 \\\\\\n--profile access-grants-profile \\\\\\n--region us-east-2 \\\\\\n--tags Key=tagKey1,Value=tagValue1', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"CreatedAt\": \"2023-10-25T01:09:46.719000+00:00\",\\n\"AccessGrantsInstanceId\": \"default\",\\n\"AccessGrantsInstanceArn\": \"arn:aws:s3:us-east-2:111122223333:access-grants/\\ndefault\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control tag-resource \\\\\\n--account-id 111122223333 \\\\\\n--resource-arn \"arn:aws:s3:us-east-2:111122223333:access-grants/default\" \\\\\\n--profile access-grants-profile \\\\\\n--region us-east-2 \\\\\\n--tags Key=tagKey2,Value=tagValue2', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control list-tags-for-resource \\\\\\n--account-id 111122223333 \\\\\\n--resource-arn \"arn:aws:s3:us-east-2:111122223333:access-grants/default\" \\\\\\n--profile access-grants-profile \\\\\\n--region us-east-2', ''], ['', '', '']], [['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"Tags\": [\\n{\\n\"Key\": \"tagKey1\",\\n\"Value\": \"tagValue1\"\\n},\\n{\\n\"Key\": \"tagKey2\",\\n\"Value\": \"tagValue2\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control untag-resource \\\\\\n--account-id 111122223333 \\\\\\n--resource-arn \"arn:aws:s3:us-east-2:111122223333:access-grants/default\" \\\\\\n--profile access-grants-profile \\\\\\n--region us-east-2 \\\\\\n--tag-keys \"tagKey2\"', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf your use case exceeds these limitations, contact AWS support to request higher limits.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nIf your bucket uses the Bucket owner enforced setting for S3 Object Ownership, you must\\nuse policies to grant access to your bucket and the objects in it. With the Bucket owner\\nenforced setting enabled, requests to set access control lists (ACLs) or update ACLs fail and\\nreturn the AccessControlListNotSupported error code. Requests to read ACLs are still\\nsupported.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf your bucket uses the Bucket owner enforced setting for S3 Object Ownership, you must\\nuse policies to grant access to your bucket and the objects in it. With the Bucket owner\\nenforced setting enabled, requests to set access control lists (ACLs) or update ACLs fail and\\nreturn the AccessControlListNotSupported error code. Requests to read ACLs are still\\nsupported.', ''], ['', '', '']], [['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<AccessControlPolicy xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Owner>\\n<ID>*** Owner-Canonical-User-ID ***</ID>\\n<DisplayName>owner-display-name</DisplayName>\\n</Owner>\\n<AccessControlList>\\n<Grant>\\n<Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"', '']]]\n",
      "[[['', 'xsi:type=\"Canonical User\">\\n<ID>*** Owner-Canonical-User-ID ***</ID>\\n<DisplayName>display-name</DisplayName>\\n</Grantee>\\n<Permission>FULL_CONTROL</Permission>\\n</Grant>\\n</AccessControlList>\\n</AccessControlPolicy>', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nAn ACL can have up to 100 grants.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nUsing email addresses to specify a grantee is only supported in the following AWS Regions:\\n• US East (N. Virginia)\\n• US West (N. California)\\n• US West (Oregon)\\n• Asia Pacific (Singapore)\\n• Asia Pacific (Sydney)\\n• Asia Pacific (Tokyo)\\n• Europe (Ireland)\\n• South America (São Paulo)\\nFor a list of all the Amazon S3 supported regions and endpoints, see Regions and\\nEndpoints in the Amazon Web Services General Reference.', ''], ['', '', '']], [['', '', ''], ['', 'x-amz-grant-read: emailAddress=\"xyz@example.com\", emailAddress=\"abc@example.com\"', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nWhen you grant other AWS accounts access to your resources, be aware that the AWS\\naccounts can delegate their permissions to users under their accounts. This is known as\\ncross-account access. For information about using cross-account access, see Creating a Role\\nto Delegate Permissions to an IAM User in the IAM User Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf you make your bucket public (not recommended), any unauthenticated user can upload\\nobjects to the bucket. These anonymous users don't have an AWS account. When an\\nanonymous user uploads an object to your bucket, Amazon S3 adds a special canonical user\\nID (65a011a29cdf8ec533ec3d1ccaae921c) as the object owner in the ACL. For more\\ninformation, see Amazon S3 bucket and object ownership.\", ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nWhen you grant access to the Authenticated Users group, any AWS authenticated user in\\nthe world can access your resource.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nWe highly recommend that you never grant the All Users group WRITE, WRITE_ACP, or\\nFULL_CONTROL permissions. For example, while WRITE permissions do not allow non-\\nowners to overwrite or delete existing objects, WRITE permissions still allow anyone\\nto store objects in your bucket, for which you are billed. For more details about these\\npermissions, see the following section What permissions can I grant?.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen using ACLs, a grantee can be an AWS account or one of the predefined Amazon S3\\ngroups. However, the grantee cannot be an IAM user. For more information about AWS\\nusers and permissions within IAM, see Using AWS Identity and Access Management.', ''], ['', '', '']]]\n",
      "[[['Permissio\\nn', 'When granted on a bucket', 'When granted on an object'], ['READ', 'Allows grantee to list the objects in the\\nbucket', 'Allows grantee to read the object data\\nand its metadata'], ['WRITE', 'Allows grantee to create new objects in\\nthe bucket. For the bucket and object\\nowners of existing objects, also allows\\ndeletions and overwrites of those obje\\ncts', 'Not applicable'], ['READ_ACP', 'Allows grantee to read the bucket ACL', 'Allows grantee to read the object ACL'], ['WRITE_ACP', 'Allows grantee to write the ACL for the\\napplicable bucket', 'Allows grantee to write the ACL for the\\napplicable object'], ['FULL_CONT\\nROL', 'Allows grantee the READ, WRITE,\\nREAD_ACP, and WRITE_ACP permissio\\nns on the bucket', 'Allows grantee the READ, READ_ACP,\\nand W RITE_ACP permissions on the\\nobject']], [['', '', ''], ['', 'Warning\\nUse caution when granting access permissions to your S3 buckets and objects. For example,\\ngranting WRITE access to a bucket allows the grantee to create objects in the bucket. We\\nhighly recommend that you read through the entire Access control list (ACL) overview\\nsection before granting permissions.', ''], ['', '', '']]]\n",
      "[[['ACL permissio\\nn', 'Corresponding access policy\\npermissions when the ACL permi\\nssion is granted on a bucket', 'Corresponding access policy\\npermissions when the ACL permi\\nssion is granted on an object'], ['READ', 's3:ListBucket , s 3:ListBu\\ncketVersions , and s 3:ListBu\\ncketMultipartUploads', 's3:GetObject and s3:GetObj\\nectVersion'], ['WRITE', 's3:PutObject\\nBucket owner can create, overwrite,\\nand delete any object in the bucket,\\nand object owner has FULL_CONT\\nROL over their object.\\nIn addition, when the grantee is\\nthe bucket owner, granting WRITE\\npermission in a bucket ACL allows\\nthe s3:DeleteObjectVersion\\naction to be performed on any\\nversion in that bucket.', 'Not applicable'], ['READ_ACP', 's3:GetBucketAcl', 's3:GetObjectAcl and\\ns3:GetObjectVersionAcl'], ['WRITE_ACP', 's3:PutBucketAcl', 's3:PutObjectAcl and\\ns3:PutObjectVersionAcl'], ['FULL_CONT\\nROL', 'Equivalent to granting READ, WRITE,\\nREAD_ACP, and WRITE_ACP ACL\\npermissions. Accordingly, this ACL\\npermission maps to a combinati', 'Equivalent to granting READ,\\nREAD_ACP, and WRITE_ACP ACL\\npermissions. Accordingly, this ACL\\npermission maps to a combinati']]]\n",
      "[[['ACL permissio\\nn', 'Corresponding access policy\\npermissions when the ACL permi\\nssion is granted on a bucket', 'Corresponding access policy\\npermissions when the ACL permi\\nssion is granted on an object'], ['', 'on of corresponding access policy\\npermissions.', 'on of corresponding access policy\\npermissions.']]]\n",
      "[[['', '', ''], ['', 'Note\\nPutObject operations in the following table, unless specified otherwise, indicate requests\\nthat do not set an ACL, unless the ACL is a bucket-owner-full-control ACL. A null\\nvalue for aclRequired indicates that aclRequired is absent in AWS CloudTrail logs.', ''], ['', '', '']], [['Operation\\nname', 'Requester', 'Object\\nowner', 'Bucket\\nowner', 'Bucket\\npolicy\\ngrants\\naccess', 'aclRequir\\ned value', 'Reason'], ['GetObject', 'A', 'A', 'A', 'Yes or No', 'null', 'Same-acco\\nunt access'], [None, 'A', 'B', 'A', 'Yes or No', 'null', 'Same-\\naccount\\naccess with\\nbucket\\nowner\\nenforced'], [None, 'A', 'A', 'B', 'Yes', 'null', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy'], [None, 'A', 'A', 'B', 'No', 'Yes', 'Cross-\\naccount\\naccess']]]\n",
      "[[['Operation\\nname', 'Requester', 'Object\\nowner', 'Bucket\\nowner', 'Bucket\\npolicy\\ngrants\\naccess', 'aclRequir\\ned value', 'Reason'], ['', '', '', '', '', '', 'relies on\\nACL'], [None, 'A', 'A', 'B', 'Yes', 'null', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy'], [None, 'A', 'B', 'B', 'No', 'Yes', 'Cross-\\naccount\\naccess\\nrelies on\\nACL'], [None, 'A', 'B', 'C', 'Yes', 'null', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy'], [None, 'A', 'B', 'C', 'No', 'Yes', 'Cross-\\naccount\\naccess\\nrelies on\\nACL'], ['PutObject', 'A', 'Not\\napplicable', 'A', 'Yes or No', 'null', 'Same-acco\\nunt access']]]\n",
      "[[['Operation\\nname', 'Requester', 'Object\\nowner', 'Bucket\\nowner', 'Bucket\\npolicy\\ngrants\\naccess', 'aclRequir\\ned value', 'Reason'], ['', 'A', 'Not\\napplicable', 'B', 'Yes', 'null', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy'], [None, 'A', 'Not\\napplicable', 'B', 'No', 'Yes', 'Cross-\\naccount\\naccess\\nrelies on\\nACL'], ['PutObject\\nwith\\nan ACL\\n(except for\\nbucket-\\nowner-\\nfull-\\ncontrol )', '*', 'Not\\napplicable', '*', 'Yes or No', 'Yes', 'Request\\ngrants ACL'], ['ListObjec\\nts', 'A', 'Not\\napplicable', 'A', 'Yes or No', 'null', 'Same-acco\\nunt access'], [None, 'A', 'Not\\napplicable', 'B', 'Yes', 'null', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy']]]\n",
      "[[['Operation\\nname', 'Requester', 'Object\\nowner', 'Bucket\\nowner', 'Bucket\\npolicy\\ngrants\\naccess', 'aclRequir\\ned value', 'Reason'], ['', 'A', 'Not\\napplicable', 'B', 'No', 'Yes', 'Cross-\\naccount\\naccess\\nrelies on\\nACL'], ['DeleteObj\\nect', 'A', 'Not\\napplicable', 'A', 'Yes or No', 'null', 'Same-acco\\nunt access'], [None, 'A', 'Not\\napplicable', 'B', 'Yes', 'null', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy'], [None, 'A', 'Not\\napplicable', 'B', 'No', 'Yes', 'Cross-\\naccount\\naccess\\nrelies on\\nACL'], ['PutObject\\nAcl', '*', '*', '*', 'Yes or No', 'Yes', 'Request\\ngrants ACL'], ['PutBucket\\nAcl', '*', 'Not\\napplicable', '*', 'Yes or No', 'Yes', 'Request\\ngrants ACL']]]\n",
      "[[['', '', ''], ['', 'Note\\nREST.PUT.OBJECT operations in the following table, unless specified otherwise, indicate\\nrequests that do not set an ACL, unless the ACL is a bucket-owner-full-control ACL.\\nAn aclRequired value string of \"-\" indicates a null value in Amazon S3 server access logs.', ''], ['', '', '']], [['Operation\\nname', 'Requester', 'Object\\nowner', 'Bucket\\nowner', 'Bucket\\npolicy\\ngrants\\naccess', 'aclRequir\\ned value', 'Reason'], ['REST.GET.\\nOBJECT', 'A', 'A', 'A', 'Yes or No', '-', 'Same-acco\\nunt access'], [None, 'A', 'B', 'A', 'Yes or No', '-', 'Same-\\naccount\\naccess with\\nbucket\\nowner\\nenforced'], [None, 'A', 'A', 'B', 'Yes', '-', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy'], [None, 'A', 'A', 'B', 'No', 'Yes', 'Cross-\\naccount\\naccess\\nrelies on\\nACL']]]\n",
      "[[['Operation\\nname', 'Requester', 'Object\\nowner', 'Bucket\\nowner', 'Bucket\\npolicy\\ngrants\\naccess', 'aclRequir\\ned value', 'Reason'], ['', 'A', 'B', 'B', 'Yes', '-', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy'], [None, 'A', 'B', 'B', 'No', 'Yes', 'Cross-\\naccount\\naccess\\nrelies on\\nACL'], [None, 'A', 'B', 'C', 'Yes', '-', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy'], [None, 'A', 'B', 'C', 'No', 'Yes', 'Cross-\\naccount\\naccess\\nrelies on\\nACL'], ['REST.PUT.\\nOBJECT', 'A', 'Not\\napplicable', 'A', 'Yes or No', '-', 'Same-acco\\nunt access']]]\n",
      "[[['Operation\\nname', 'Requester', 'Object\\nowner', 'Bucket\\nowner', 'Bucket\\npolicy\\ngrants\\naccess', 'aclRequir\\ned value', 'Reason'], ['', 'A', 'Not\\napplicable', 'B', 'Yes', '-', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy'], [None, 'A', 'Not\\napplicable', 'B', 'No', 'Yes', 'Cross-\\naccount\\naccess\\nrelies on\\nACL'], ['REST.PUT.\\nOBJECT\\nwith\\nan ACL\\n(except for\\nbucket-\\nowner-\\nfull-\\ncontrol )', '*', 'Not\\napplicable', '*', 'Yes or No', 'Yes', 'Request\\ngrants ACL'], ['REST.GET.\\nBUCKET', 'A', 'Not\\napplicable', 'A', 'Yes or No', '-', 'Same-acco\\nunt access'], [None, 'A', 'Not\\napplicable', 'B', 'Yes', '-', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy']]]\n",
      "[[['Operation\\nname', 'Requester', 'Object\\nowner', 'Bucket\\nowner', 'Bucket\\npolicy\\ngrants\\naccess', 'aclRequir\\ned value', 'Reason'], ['', 'A', 'Not\\napplicable', 'B', 'No', 'Yes', 'Cross-\\naccount\\naccess\\nrelies on\\nACL'], ['REST.DELE\\nTE.OBJECT', 'A', 'Not\\napplicable', 'A', 'Yes or No', '-', 'Same-acco\\nunt access'], [None, 'A', 'Not\\napplicable', 'B', 'Yes', '-', 'Cross-\\naccount\\naccess\\ngranted\\nby bucket\\npolicy'], [None, 'A', 'Not\\napplicable', 'B', 'No', 'Yes', 'Cross-\\naccount\\naccess\\nrelies on\\nACL'], ['REST.PUT.\\nACL', '*', '*', '*', 'Yes or No', 'Yes', 'Request\\ngrants ACL']]]\n",
      "[[['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<AccessControlPolicy xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Owner>\\n<ID>Owner-canonical-user-ID</ID>\\n<DisplayName>display-name</DisplayName>\\n</Owner>\\n<AccessControlList>\\n<Grant>\\n<Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\nxsi:type=\"CanonicalUser\">\\n<ID>Owner-canonical-user-ID</ID>\\n<DisplayName>display-name</DisplayName>\\n</Grantee>\\n<Permission>FULL_CONTROL</Permission>\\n</Grant>\\n<Grant>\\n<Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\nxsi:type=\"CanonicalUser\">\\n<ID>user1-canonical-user-ID</ID>\\n<DisplayName>display-name</DisplayName>\\n</Grantee>\\n<Permission>WRITE</Permission>\\n</Grant>\\n<Grant>\\n<Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\nxsi:type=\"CanonicalUser\">\\n<ID>user2-canonical-user-ID</ID>\\n<DisplayName>display-name</DisplayName>\\n</Grantee>\\n<Permission>READ</Permission>\\n</Grant>\\n<Grant>\\n<Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"Group\">\\n<URI>http://acs.amazonaws.com/groups/global/AllUsers</URI>\\n</Grantee>\\n<Permission>READ</Permission>\\n</Grant>\\n<Grant>\\n<Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"Group\">', '']]]\n",
      "[[['', '<URI>http://acs.amazonaws.com/groups/s3/LogDelivery</URI>\\n</Grantee>\\n<Permission>WRITE</Permission>\\n</Grant>\\n</AccessControlList>\\n</AccessControlPolicy>', ''], ['', '', '']], [['Canned ACL', 'Applies\\nto', 'Permissions added to ACL'], ['private', 'Bucket\\nand\\nobject', 'Owner gets FULL_CONTROL . No one else has access\\nrights (default).'], ['public-read', 'Bucket\\nand\\nobject', 'Owner gets FULL_CONTROL . The AllUsers group\\n(see Who is a grantee?) gets READ access.'], ['public-read-write', 'Bucket\\nand\\nobject', 'Owner gets FULL_CONTROL . The A llUsers group\\ngets READ and WRITE access. Granting this on a\\nbucket is generally not recommended.'], ['aws-exec-read', 'Bucket\\nand\\nobject', 'Owner gets FULL_CONTROL . Amazon EC2 gets READ\\naccess to GET an Amazon Machine Image (AMI) bundle\\nfrom Amazon S3.'], ['authenticated-read', 'Bucket\\nand\\nobject', 'Owner gets FULL_CONTROL . The A uthentic\\natedUsers group gets READ access.']]]\n",
      "[[['Canned ACL', 'Applies\\nto', 'Permissions added to ACL'], ['bucket-owner-read', 'Object', 'Object owner gets FULL_CONTROL . Bucket owner\\ngets READ access. If you specify this canned ACL when\\ncreating a bucket, Amazon S3 ignores it.'], ['bucket-owner-full-\\ncontrol', 'Object', 'Both the object owner and the bucket owner get\\nFULL_CONTROL over the object. If you specify t\\nhis canned ACL when creating a bucket, Amazon S3\\nignores it.'], ['log-delivery-write', 'Bucket', 'The LogDelivery group gets WRITE and READ_ACP\\npermissions on the bucket. For more information\\nabout logs, see (Logging requests with server access\\nlogging).']], [['', '', ''], ['', 'Note\\nYou can specify only one of these canned ACLs in your request.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf your bucket uses the Bucket owner enforced setting for S3 Object Ownership, you must\\nuse policies to grant access to your bucket and the objects in it. With the Bucket owner\\nenforced setting enabled, requests to set access control lists (ACLs) or update ACLs fail and\\nreturn the AccessControlListNotSupported error code. Requests to read ACLs are still\\nsupported.', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nWe highly recommend that you avoid granting write access to the Everyone (public\\naccess) or Authenticated Users group (all AWS authenticated users) groups. For more\\ninformation about the effects of granting write access to these groups, see Amazon S3\\npredefined groups.', ''], ['', '', '']]]\n",
      "[[['Console\\npermission', 'ACL\\npermission', 'Access'], ['Objects - List', 'READ', 'Allows grantee to list the objects in the bucket.'], ['Objects -\\nWrite', 'WRITE', 'Allows grantee to create new objects in the bucket. For the\\nbucket and object owners of existing objects, also allows\\ndeletions and overwrites of those objects.'], ['Bucket ACL -\\nRead', 'READ_ACP', 'Allows grantee to read the bucket ACL.'], ['Bucket ACL -\\nWrite', 'WRITE_ACP', 'Allows grantee to write the ACL for the applicable bucket.'], ['Everyone\\n(public\\naccess):\\nObjects - List', 'READ', 'Grants public read access for the objects in the bucket. When\\nyou grant list access to Everyone (public access), anyone in\\nthe world can access the objects in the bucket.'], ['Everyone\\n(public\\naccess):\\nBucket ACL -\\nRead', 'READ_ACP', 'Grants public read access for the bucket ACL. When you grant\\nread access to Everyone (public access), anyone in the world\\ncan access the bucket ACL.']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf your bucket uses the Bucket owner enforced setting for S3 Object Ownership, you must\\nuse policies to grant access to your bucket and the objects in it. With the Bucket owner\\nenforced setting enabled, requests to set access control lists (ACLs) or update ACLs fail and\\nreturn the AccessControlListNotSupported error code. Requests to read ACLs are still\\nsupported.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nUse caution when granting the Everyone group public access to your S3 bucket. When\\nyou grant access to this group, anyone in the world can access your bucket. We highly\\nrecommend that you never grant any kind of public write access to your S3 bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nWhen you grant other AWS accounts access to your resources, be aware that the AWS\\naccounts can delegate their permissions to users under their accounts. This is known as\\ncross-account access. For information about using cross-account access, see Creating a\\nRole to Delegate Permissions to an IAM User in the IAM User Guide.', ''], ['', '', '']], [['Console\\npermission', 'ACL\\npermission', 'Access'], ['Object - Read', 'READ', 'Allows grantee to read the object data and its metadata.'], ['Object ACL -\\nRead', 'READ_ACP', 'Allows grantee to read the object ACL.'], ['Object ACL -\\nWrite', 'WRITE_ACP', 'Allows grantee to write the ACL for the applicable object']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf your bucket uses the Bucket owner enforced setting for S3 Object Ownership, you must\\nuse policies to grant access to your bucket and the objects in it. With the Bucket owner\\nenforced setting enabled, requests to set access control lists (ACLs) or update ACLs fail and\\nreturn the AccessControlListNotSupported error code. Requests to read ACLs are still\\nsupported.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\n• Use caution when granting the Everyone group anonymous access to your\\nAmazon S3 objects. When you grant access to this group, anyone in the world\\ncan access your object. If you need to grant access to everyone, we highly\\nrecommend that you only grant permissions to Read objects.', '']]]\n",
      "[[['', '• We highly recommend that you do not grant the Everyone group write object\\npermissions. Doing so allows anyone to overwrite the ACL permissions for the\\nobject.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nIf your bucket uses the Bucket owner enforced setting for S3 Object Ownership, you must\\nuse policies to grant access to your bucket and the objects in it. With the Bucket owner\\nenforced setting enabled, requests to set access control lists (ACLs) or update ACLs fail and\\nreturn the AccessControlListNotSupported error code. Requests to read ACLs are still\\nsupported.', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.*;', '']]]\n",
      "[[['', 'import java.io.IOException;\\nimport java.util.ArrayList;\\npublic class CreateBucketWithACL {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString userEmailForReadPermission = \"*** user@example.com ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withRegion(clientRegion)\\n.build();\\n// Create a bucket with a canned ACL. This ACL will be replaced by the\\n// setBucketAcl()\\n// calls below. It is included here for demonstration purposes.\\nCreateBucketRequest createBucketRequest = new\\nCreateBucketRequest(bucketName, clientRegion.getName())\\n.withCannedAcl(CannedAccessControlList.LogDeliveryWrite);\\ns3Client.createBucket(createBucketRequest);\\n// Create a collection of grants to add to the bucket.\\nArrayList<Grant> grantCollection = new ArrayList<Grant>();\\n// Grant the account owner full control.\\nGrant grant1 = new Grant(new\\nCanonicalGrantee(s3Client.getS3AccountOwner().getId()),\\nPermission.FullControl);\\ngrantCollection.add(grant1);\\n// Grant the LogDelivery group permission to write to the bucket.\\nGrant grant2 = new Grant(GroupGrantee.LogDelivery, Permission.Write);\\ngrantCollection.add(grant2);\\n// Save grants by replacing all current ACL grants with the two we just\\ncreated.\\nAccessControlList bucketAcl = new AccessControlList();\\nbucketAcl.grantAllPermissions(grantCollection.toArray(new Grant[0]));\\ns3Client.setBucketAcl(bucketName, bucketAcl);', '']]]\n",
      "[[['', \"// Retrieve the bucket's ACL, add another grant, and then save the new\\nACL.\\nAccessControlList newBucketAcl = s3Client.getBucketAcl(bucketName);\\nGrant grant3 = new Grant(new\\nEmailAddressGrantee(userEmailForReadPermission), Permission.Read);\\nnewBucketAcl.grantAllPermissions(grant3);\\ns3Client.setBucketAcl(bucketName, newBucketAcl);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.AccessControlList;\\nimport com.amazonaws.services.s3.model.CanonicalGrantee;\\nimport com.amazonaws.services.s3.model.EmailAddressGrantee;\\nimport com.amazonaws.services.s3.model.Permission;', '']]]\n",
      "[[['', 'import java.io.IOException;\\npublic class ModifyACLExistingObject {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString keyName = \"*** Key name ***\";\\nString emailGrantee = \"*** user@example.com ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\n// Get the existing object ACL that we want to modify.\\nAccessControlList acl = s3Client.getObjectAcl(bucketName, keyName);\\n// Clear the existing list of grants.\\nacl.getGrantsAsList().clear();\\n// Grant a sample set of permissions, using the existing ACL owner for\\nFull\\n// Control permissions.\\nacl.grantPermission(new CanonicalGrantee(acl.getOwner().getId()),\\nPermission.FullControl);\\nacl.grantPermission(new EmailAddressGrantee(emailGrantee),\\nPermission.WriteAcp);\\n// Save the modified ACL back to the object.\\ns3Client.setObjectAcl(bucketName, keyName, acl);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass ManagingBucketACLTest\\n{\\nprivate const string newBucketName = \"*** bucket name ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nclient = new AmazonS3Client(bucketRegion);\\nCreateBucketUseCannedACLAsync().Wait();\\n}\\nprivate static async Task CreateBucketUseCannedACLAsync()\\n{\\ntry\\n{\\n// Add bucket (specify canned ACL).\\nPutBucketRequest putBucketRequest = new PutBucketRequest()\\n{\\nBucketName = newBucketName,\\nBucketRegion = S3Region.EUW1, // S3Region.US,', '']]]\n",
      "[[['', '// Add canned ACL.\\nCannedACL = S3CannedACL.LogDeliveryWrite\\n};\\nPutBucketResponse putBucketResponse = await\\nclient.PutBucketAsync(putBucketRequest);\\n// Retrieve bucket ACL.\\nGetACLResponse getACLResponse = await client.GetACLAsync(new\\nGetACLRequest\\n{\\nBucketName = newBucketName\\n});\\n}\\ncatch (AmazonS3Exception amazonS3Exception)\\n{\\nConsole.WriteLine(\"S3 error occurred. Exception: \" +\\namazonS3Exception.ToString());\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Exception: \" + e.ToString());\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;', '']]]\n",
      "[[['', 'using Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass ManagingObjectACLTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\nprivate const string keyName = \"*** object key name ***\";\\nprivate const string emailAddress = \"*** email address ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nclient = new AmazonS3Client(bucketRegion);\\nTestObjectACLTestAsync().Wait();\\n}\\nprivate static async Task TestObjectACLTestAsync()\\n{\\ntry\\n{\\n// Retrieve the ACL for the object.\\nGetACLResponse aclResponse = await client.GetACLAsync(new\\nGetACLRequest\\n{\\nBucketName = bucketName,\\nKey = keyName\\n});\\nS3AccessControlList acl = aclResponse.AccessControlList;\\n// Retrieve the owner (we use this to re-add permissions after\\nwe clear the ACL).\\nOwner owner = acl.Owner;\\n// Clear existing grants.\\nacl.Grants.Clear();', '']]]\n",
      "[[['', '// Add a grant to reset the owner\\'s full permission (the\\nprevious clear statement removed all permissions).\\nS3Grant fullControlGrant = new S3Grant\\n{\\nGrantee = new S3Grantee { CanonicalUser = owner.Id },\\nPermission = S3Permission.FULL_CONTROL\\n};\\n// Describe the grant for the permission using an email address.\\nS3Grant grantUsingEmail = new S3Grant\\n{\\nGrantee = new S3Grantee { EmailAddress = emailAddress },\\nPermission = S3Permission.WRITE_ACP\\n};\\nacl.Grants.AddRange(new List<S3Grant> { fullControlGrant,\\ngrantUsingEmail });\\n// Set a new ACL.\\nPutACLResponse response = await client.PutACLAsync(new\\nPutACLRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nAccessControlList = acl\\n});\\n}\\ncatch (AmazonS3Exception amazonS3Exception)\\n{\\nConsole.WriteLine(\"An AmazonS3Exception was thrown. Exception: \" +\\namazonS3Exception.ToString());\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Exception: \" + e.ToString());\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf your bucket uses the Bucket owner enforced setting for S3 Object Ownership, you must\\nuse policies to grant access to your bucket and the objects in it. With the Bucket owner\\nenforced setting enabled, requests to set access control lists (ACLs) or update ACLs fail and\\nreturn the AccessControlListNotSupported error code. Requests to read ACLs are still\\nsupported.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf your bucket uses the Bucket owner enforced setting for S3 Object Ownership, you must\\nuse policies to grant access to your bucket and the objects in it. With the Bucket owner\\nenforced setting enabled, requests to set access control lists (ACLs) or update ACLs fail and\\nreturn the AccessControlListNotSupported error code. Requests to read ACLs are still\\nsupported.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"statement1\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountB-ID:user/Dave\"\\n},\\n\"Action\": \"s3:PutObject\",', '']]]\n",
      "[[['', '\"Resource\": \"arn:aws:s3:::awsexamplebucket1/*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3:x-amz-grant-full-control\": \"id=AccountA-CanonicalUserID\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis example is about cross-account permission. However, if Dave (who is getting the\\npermission) belongs to the AWS account that owns the bucket, this conditional permission\\nis not necessary. This is because the parent account to which Dave belongs owns objects\\nthat the user uploads.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"statement1\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountB-ID:user/AccountBadmin\"\\n},\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::awsexamplebucket1/*\",\\n\"Condition\": {', '']]]\n",
      "[[['', '\"StringEquals\": {\\n\"s3:x-amz-grant-full-control\": \"id=AccountA-CanonicalUserID\"\\n}\\n}\\n},\\n{\\n\"Sid\": \"statement2\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::AccountB-ID:user/AccountBadmin\"\\n},\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::awsexamplebucket1/*\",\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"s3:x-amz-grant-full-control\": \"id=AccountA-CanonicalUserID\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket examplebucket --key HappyFace.jpg --body c:\\\\HappyFace.jpg\\n--grant-full-control id=\"AccountA-CanonicalUserID\" --profile AccountBUserProfile', ''], ['', '', '']], [['', '', ''], ['', '\"Condition\": {', '']]]\n",
      "[[['', '\"StringEquals\": {\\n\"s3:x-amz-acl\": \"bucket-owner-full-control\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket examplebucket --key HappyFace.jpg --body c:\\\\HappyFace.jpg\\n--acl \"bucket-owner-full-control\" --profile AccountBadmin', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\":\"AddCannedAcl\",\\n\"Effect\":\"Allow\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::Account1-ID:root\",\\n\"arn:aws:iam::Account2-ID:root\"\\n]\\n},\\n\"Action\":\"s3:PutObject\",\\n\"Resource\": [\"arn:aws:s3:::awsexamplebucket1/*\"],\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3:x-amz-acl\":[\"public-read\"]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nNot all conditions make sense for all actions. For example, it makes sense to include an\\ns3:LocationConstraint condition on a policy that grants the s3:CreateBucket\\nAmazon S3 permission. However, it does not make sense to include this condition on a\\npolicy that grants the s3:GetObject permission. Amazon S3 can test for semantic errors\\nof this type that involve Amazon S3–specific conditions. However, if you are creating a\\npolicy for an IAM user or role and you include a semantically invalid Amazon S3 condition,\\nno error is reported because IAM cannot validate Amazon S3 conditions.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nPublic access is granted to buckets and objects through access control lists (ACLs), access\\npoint policies, bucket policies, or all. To help ensure that all of your Amazon S3 access\\npoints, buckets, and objects have their public access blocked, we recommend that you turn\\non all four settings for block public access for your account. These settings block public\\naccess for all current and future buckets and access points.\\nBefore applying these settings, verify that your applications will work correctly without\\npublic access. If you require some level of public access to your buckets or objects, for\\nexample to host a static website as described at Hosting a static website using Amazon S3,\\nyou can customize the individual settings to suit your storage use cases.\\nEnabling Block Public Access helps protect your resources by preventing public access from\\nbeing granted through the resource policies or access control lists (ACLs) that are directly\\nattached to S3 resources. In addition to enabling Block Public Access, carefully inspect the\\nfollowing policies to confirm that they do not grant public access:\\n• Identity-based policies attached to associated AWS principals (for example, IAM roles)\\n• Resource-based policies attached to associated AWS resources (for example, AWS Key\\nManagement Service (KMS) keys)', ''], ['', '', '']], [['', '', ''], ['', \"Note\\n• You can enable block public access settings only for access points, buckets, and AWS\\naccounts. Amazon S3 doesn't support block public access settings on a per-object basis.\\n• When you apply block public access settings to an account, the settings apply to all\\nAWS Regions globally. The settings might not take effect in all Regions immediately or\\nsimultaneously, but they eventually propagate to all Regions.\", ''], ['', '', '']]]\n",
      "[[['Name', 'Description'], ['BlockPubl\\nicAcls', 'Setting this option to TRUE causes the following behavior:\\n•\\nPUT Bucket acl and PUT Object acl calls fail if the specified access\\ncontrol list (ACL) is public.\\n•\\nPUT Object calls fail if the request includes a public ACL.\\n•\\nIf this setting is applied to an account, then PUT Bucket calls fail if the\\nrequest includes a public ACL.\\nWhen this setting is set to TRUE, the specified operations fail (whether\\nmade through the REST API, AWS CLI, or AWS SDKs). However, existing\\npolicies and ACLs for buckets and objects are not modified. This setting\\nenables you to protect against public access while allowing you to audit,\\nrefine, or otherwise alter the existing policies and ACLs for your buckets\\nand objects.']]]\n",
      "[[['Name', 'Description'], ['', \"Note\\nAccess points don't have ACLs associated with them. If you apply\\nthis setting to an access point, it acts as a passthrough to the\\nunderlying bucket. If an access point has this setting enabled,\\nrequests made through the access point behave as though the\\nunderlying bucket has this setting enabled, regardless of whether\\nthe bucket actually has this setting enabled.\"], ['IgnorePub\\nlicAcls', \"Setting this option to TRUE causes Amazon S3 to ignore all public ACLs\\non a bucket and any objects that it contains. This setting enables you to\\nsafely block public access granted by ACLs while still allowing PUT Object\\ncalls that include a public ACL (as opposed to BlockPublicAcls , which\\nrejects PUT Object calls that include a public ACL). Enabling this setting\\ndoesn't affect the persistence of any existing ACLs and doesn't prevent\\nnew public ACLs from being set.\\nNote\\nAccess points don't have ACLs associated with them. If you apply\\nthis setting to an access point, it acts as a passthrough to the\\nunderlying bucket. If an access point has this setting enabled,\\nrequests made through the access point behave as though the\\nunderlying bucket has this setting enabled, regardless of whether\\nthe bucket actually has this setting enabled.\"]], [['', '', ''], ['', \"Note\\nAccess points don't have ACLs associated with them. If you apply\\nthis setting to an access point, it acts as a passthrough to the\\nunderlying bucket. If an access point has this setting enabled,\\nrequests made through the access point behave as though the\\nunderlying bucket has this setting enabled, regardless of whether\\nthe bucket actually has this setting enabled.\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nAccess points don't have ACLs associated with them. If you apply\\nthis setting to an access point, it acts as a passthrough to the\\nunderlying bucket. If an access point has this setting enabled,\\nrequests made through the access point behave as though the\\nunderlying bucket has this setting enabled, regardless of whether\\nthe bucket actually has this setting enabled.\", ''], ['', '', '']]]\n",
      "[[['Name', 'Description'], ['BlockPubl\\nicPolicy', \"Setting this option to TRUE for a bucket causes Amazon S3 to reject calls\\nto PUT Bucket policy if the specified bucket policy allows public access.\\nSetting this option to TRUE for a bucket also causes Amazon S3 to reject\\ncalls to PUT access point policy for all of the bucket's same-account access\\npoints if the specified policy allows public access.\\nSetting this option to TRUE for an access point causes Amazon S3 to\\nreject calls to PUT access point policy and PUT Bucket policy that are\\nmade through the access point if the specified policy (for either the access\\npoint or the underlying bucket) allows public access.\\nYou can use this setting to allow users to manage access point and bucket\\npolicies without allowing them to publicly share the bucket or the object\\ns it contains. Enabling this setting doesn't affect existing access point or\\nbucket policies.\\nImportant\\nTo use this setting effectively, we recommend that you apply it\\nat the account level. A bucket policy can allow users to alter a\\nbucket's block public access settings. Therefore, users who have\\npermission to change a bucket policy could insert a policy that\\nallows them to disable the block public access settings for the\\nbucket. If this setting is enabled for the entire account, rather\\nthan for a specific bucket, Amazon S3 blocks public policies even\\nif a user alters the bucket policy to disable this setting.\"]], [['', '', ''], ['', \"Important\\nTo use this setting effectively, we recommend that you apply it\\nat the account level. A bucket policy can allow users to alter a\\nbucket's block public access settings. Therefore, users who have\\npermission to change a bucket policy could insert a policy that\\nallows them to disable the block public access settings for the\\nbucket. If this setting is enabled for the entire account, rather\\nthan for a specific bucket, Amazon S3 blocks public policies even\\nif a user alters the bucket policy to disable this setting.\", ''], ['', '', '']]]\n",
      "[[['Name', 'Description'], ['RestrictP\\nublicBuckets', \"Setting this option to TRUE restricts access to an access point or bucket\\nwith a public policy to only AWS service principals and authorized users\\nwithin the bucket owner's account and access point owner's account.\\nThis setting blocks all cross-account access to the access point or bucket\\n(except by AWS service principals), while still allowing users within the\\naccount to manage the access point or bucket.\\nEnabling this setting doesn't affect existing access point or bucket pol\\nicies, except that Amazon S3 blocks public and cross-account access der\\nived from any public access point or bucket policy, including non-public\\ndelegation to specific accounts.\"]], [['', '', ''], ['', \"Important\\n• Calls to GET Bucket acl and GET Object acl always return the effective permissions in\\nplace for the specified bucket or object. For example, suppose that a bucket has an\\nACL that grants public access, but the bucket also has the IgnorePublicAcls setting\\nenabled. In this case, GET Bucket acl returns an ACL that reflects the access permissions\\nthat Amazon S3 is enforcing, rather than the actual ACL that is associated with the\\nbucket.\\n• Block public access settings don't alter existing policies or ACLs. Therefore, removing a\\nblock public access setting causes a bucket or object with a public policy or ACL to again\\nbe publicly accessible.\", ''], ['', '', '']], [['', '', ''], ['', \"Important\\nNote that it isn't currently possible to change an access point's block public access settings\\nafter creating the access point. Thus, the only way to specify block public access settings\\nfor an access point is by including them when creating the access point.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nBucket policies that grant access conditioned on the aws:SourceIp condition key with\\nvery broad IP ranges (for example, 0.0.0.0/1) are evaluated as \"public.\" This includes\\nvalues broader than /8 for IPv4 and /32 for IPv6 (excluding RFC1918 private ranges).\\nBlock public access will reject these \"public\" policies and prevent cross-account access to\\nbuckets already using these \"public\" policies.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen used in a bucket policy, this value can contain a wildcard for the access point\\nname without rendering the policy public, as long as the account id is fixed. For example,\\nallowing access to arn:aws:s3:us-west-2:123456789012:accesspoint/* would\\npermit access to any access point associated with account 123456789012 in Region us-\\nwest-2, without rendering the bucket policy public. Note that this behavior is different\\nfor access point policies. For more information, see Access points.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Principal\": \"*\",\\n\"Resource\": \"*\",\\n\"Action\": \"s3:PutObject\",\\n\"Effect\": \"Allow\"\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Principal\": \"*\",\\n\"Resource\": \"*\",\\n\"Action\": \"s3:PutObject\",\\n\"Effect\": \"Allow\",\\n\"Condition\": { \"StringLike\": {\"aws:SourceVpc\": \"vpc-*\"}}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Principal\": \"*\",', '']]]\n",
      "[[['', '\"Resource\": \"*\",\\n\"Action\": \"s3:PutObject\",\\n\"Effect\": \"Allow\",\\n\"Condition\": {\"StringEquals\": {\"aws:SourceVpc\": \"vpc-91237329\"}}\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['Operation', 'Required permissions'], ['GET bucket policy status', 's3:GetBucketPolicyStatus'], ['GET bucket Block Public Access settings', 's3:GetBucketPublicAccessBlock'], ['PUT bucket Block Public Access settings', 's3:PutBucketPublicAccessBlock'], ['DELETE bucket Block Public Access settings', 's3:PutBucketPublicAccessBlock'], ['GET account Block Public Access settings', 's3:GetAccountPublicAccessBlock'], ['PUT account Block Public Access settings', 's3:PutAccountPublicAccessBlock'], ['DELETE account Block Public Access settings', 's3:PutAccountPublicAccessBlock'], ['PUT access point Block Public Access settings', 's3:CreateAccessPoint']], [['', '', ''], ['', 'Note\\nThe DELETE operations require the same permissions as the PUT operations. There are no\\nseparate permissions for the DELETE operations.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAccount level settings override settings on individual objects. Configuring your account\\nto block public access will override any public access settings made to individual objects\\nwithin your account.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'AWSS3ControlClientBuilder controlClientBuilder =\\nAWSS3ControlClientBuilder.standard();\\ncontrolClientBuilder.setRegion(<region>);\\ncontrolClientBuilder.setCredentials(<credentials>);\\nAWSS3Control client = controlClientBuilder.build();\\nclient.putPublicAccessBlock(new PutPublicAccessBlockRequest()\\n.withAccountId(<account-id>)\\n.withPublicAccessBlockConfiguration(new PublicAccessBlockConfiguration()\\n.withIgnorePublicAcls(<value>)\\n.withBlockPublicAcls(<value>)\\n.withBlockPublicPolicy(<value>)\\n.withRestrictPublicBuckets(<value>)));', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nThis example pertains only to account-level operations, which use the AWSS3Control\\nclient class. For bucket-level operations, see the preceding example.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 's3:GetAccountPublicAccessBlock\\ns3:GetBucketPublicAccessBlock\\ns3:GetBucketPolicyStatus\\ns3:GetBucketLocation\\ns3:GetBucketAcl\\ns3:ListAccessPoints\\ns3:ListAllMyBuckets', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'AmazonS3 client = AmazonS3ClientBuilder.standard()\\n.withCredentials(<credentials>)\\n.build();\\nclient.setPublicAccessBlock(new SetPublicAccessBlockRequest()\\n.withBucketName(<bucket-name>)\\n.withPublicAccessBlockConfiguration(new PublicAccessBlockConfiguration()', '']]]\n",
      "[[['', '.withBlockPublicAcls(<value>)\\n.withIgnorePublicAcls(<value>)\\n.withBlockPublicPolicy(<value>)\\n.withRestrictPublicBuckets(<value>)));', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nThis example pertains only to bucket-level operations, which use the AmazonS3 client\\nclass. For account-level operations, see the following example.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\n• IAM Access Analyzer for S3 requires an account-level analyzer. To use IAM Access\\nAnalyzer for S3, you must visit IAM Access Analyzer and create an analyzer that has an\\naccount as the zone of trust. For more information, see Enabling IAM Access Analyzer in\\nIAM User Guide.\\n• IAM Access Analyzer for S3 doesn't analyze the access point policy that's attached to\\ncross-account access points. This behavior occurs because the access point and its policy\\nare outside the zone of trust, that is, the account. Buckets that delegate access to a cross-\\naccount access point are listed under Buckets with public access if you haven't applied\\nthe RestrictPublicBuckets block public access setting to the bucket or account.\\nWhen you apply the RestrictPublicBuckets block public access setting, the bucket is\\nreported under Buckets with access from other AWS accounts — including third-party\\nAWS accounts.\\n• When a bucket policy or bucket ACL is added or modified, IAM Access Analyzer generates\\nand updates findings based on the change within 30 minutes. Findings related to account\", '']]]\n",
      "[[['', 'level block public access settings might not be generated or updated for up to 6 hours\\nafter you change the settings. Findings related to Multi-Region Access Points might\\nnot be generated or updated for up to six hours after the Multi-Region Access Point is\\ncreated, deleted, or you change its policy.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['Access method', 'Parameter for non-\\ncopy operations', 'Copy operation\\nsource parameter', 'Copy operation\\ndestination\\nparameter'], ['AWS CLI', '--expected-\\nbucket-owner', '--expected-\\nsource-bucket-\\nowner', '--expected-\\nbucket-owner'], ['Amazon S3 REST APIs', 'x-amz-expected-\\nbucket-owner\\nheader', 'x-amz-source-\\nexpected-bucke\\nt-owner header', 'x-amz-expected-\\nbucket-owner\\nheader']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-object \\\\\\n--bucket example-s3-bucket1 --key exampleobject --\\nbody example_file.txt \\\\\\n--expected-bucket-owner 111122223333', ''], ['', '', '']], [['', '', ''], ['', 'public void putObjectExample() {\\nS3Client s3Client = S3Client.create();;\\nPutObjectRequest request = PutObjectRequest.builder()\\n.bucket(\"example-s3-bucket1\")\\n.key(\"exampleobject\")\\n.expectedBucketOwner(\"111122223333\")\\n.build();\\nPath path = Paths.get(\"example_file.txt\");\\ns3Client.putObject(request, path);\\n}', ''], ['', '', '']]]\n",
      "[[['Bucket', 'Expected owner'], ['example-s3-bucket1', '111122223333'], ['example-s3-bucket2', '444455556666']], [['', '', ''], ['', 'aws s3api copy-object --copy-source example-s3-bucket1/object1 \\\\\\n--bucket example-s3-bucket2 --key object1copy \\\\\\n--expected-source-bucket-owner 111122223333 --expected-\\nbucket-owner 444455556666', ''], ['', '', '']], [['', '', ''], ['', 'public void copyObjectExample() {\\nS3Client s3Client = S3Client.create();\\nCopyObjectRequest request = CopyObjectRequest.builder()\\n.copySource(\"example-s3-bucket1/object1\")\\n.destinationBucket(\"example-s3-bucket2\")\\n.destinationKey(\"object1copy\")\\n.expectedSourceBucketOwner(\"111122223333\")\\n.expectedBucketOwner(\"444455556666\")\\n.build();\\ns3Client.copyObject(request);\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-policy --bucket example-s3-bucket1 --expected-bucket-\\nowner 111122223333', ''], ['', '', '']], [['', '', ''], ['', 'public void getBucketPolicyExample() {\\nS3Client s3Client = S3Client.create();\\nGetBucketPolicyRequest request = GetBucketPolicyRequest.builder()\\n.bucket(\"example-s3-bucket1\")\\n.expectedBucketOwner(\"111122223333\")\\n.build();\\ntry {\\nGetBucketPolicyResponse response = s3Client.getBucketPolicy(request);\\n}\\ncatch (S3Exception e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n}\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']], [['Setting', 'Applies to', 'Effect on object\\nownership', 'Effect on ACLs', 'Uploads\\naccepted'], ['Bucket owner\\nenforced\\n(default)', 'All new and\\nexisting objects', 'Bucket owner\\nowns every\\nobject.', 'ACLs are\\ndisabled and\\nno longer', 'Uploads with\\nbucket owner\\nfull control ACLs']]]\n",
      "[[['Setting', 'Applies to', 'Effect on object\\nownership', 'Effect on ACLs', 'Uploads\\naccepted'], ['', '', '', 'affect access\\npermissions to\\nyour bucket.\\nRequests to set\\nor update ACLs\\nfail. However,\\nrequests to\\nread ACLs are\\nsupported.\\nBucket owner\\nhas full\\nownership and\\ncontrol.\\nObject writer no\\nlonger has full\\nownership and\\ncontrol.', \"or uploads that\\ndon't specify an\\nACL\"]]]\n",
      "[[['Setting', 'Applies to', 'Effect on object\\nownership', 'Effect on ACLs', 'Uploads\\naccepted'], ['Bucket owner\\npreferred', 'New objects', 'If an object\\nupload includes\\nthe bucket-\\nowner-full-\\ncontrol\\ncanned ACL, the\\nbucket owner\\nowns the object.\\nObjects\\nuploaded with\\nother ACLs are\\nowned by the\\nwriting account.', 'ACLs can be\\nupdated and can\\ngrant permissio\\nns.\\nIf an object\\nupload includes\\nthe bucket-\\nowner-full-\\ncontrol\\ncanned ACL, the\\nbucket owner\\nhas full control\\naccess, and the\\nobject writer no\\nlonger has full\\ncontrol access.', 'All uploads'], ['Object writer', 'New objects', 'Object writer\\nowns the object.', 'ACLs can be\\nupdated and can\\ngrant permissio\\nns.\\nObject writer\\nhas full control\\naccess.', 'All uploads']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3api put-object --bucket DOC-EXAMPLE-BUCKET --key key-name --body path-to-file --\\nacl bucket-owner-full-control', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket DOC-EXAMPLE-BUCKET --key key-name --body path-to-file', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf other AWS accounts need access to objects after uploading, you must grant additional\\npermissions to those accounts through bucket policies. For more information, see\\nWalkthroughs that use policies to manage access to your Amazon S3 resources.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nDo not remove object ACLs. Otherwise, applications that rely on object ACLs for\\npermissions will lose access.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Only allow writes to my bucket with bucket owner full control\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:user/ExampleUser\"\\n]\\n},\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3:x-amz-acl\": \"bucket-owner-full-control\"', '']]]\n",
      "[[['', '}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Only allow writes to my bucket with public read access\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:user/ExampleUser\" ]\\n},\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3:x-amz-acl\": \"public-read\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['REST API', 'AWS CLI', 'Description'], ['PutBucketOwnershipControls', 'put-bucket-ownership-contro\\nls', 'Creates or modifies the\\nObject Ownership setting for\\nan existing S3 bucket.'], ['CreateBucket', 'create-bucket', 'Creates a bucket using the x-\\namz-object-ownership\\nrequest header to specify the\\nObject Ownership setting.']]]\n",
      "[[['REST API', 'AWS CLI', 'Description'], ['GetBucketOwnershipControls', 'get-bucket-ownership-contro\\nls', 'Retrieves the Object\\nOwnership setting for an\\nAmazon S3 bucket.'], ['DeleteBucketOwners\\nhipControls', 'delete-bucket-ownership-con\\ntrols', 'Deletes the Object Ownership\\nsetting for an Amazon S3\\nbucket.']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Permission to list the objects in a bucket\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:root\"\\n]\\n},\\n\"Action\": [\\n\"s3:ListBucket\",\\n\"s3:ListBucketVersions\",\\n\"s3:ListBucketMultipartUploads\"\\n],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"\\n}', '']]]\n",
      "[[['', ']\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Read permission for every object in a bucket\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:root\"\\n]\\n},\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/OBJECT-KEY\"', ''], ['', '', '']], [['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Permission to write objects to a bucket\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:root\"\\n]\\n},\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-acl --bucket DOC-EXAMPLE-BUCKET', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Owner\": {\\n\"DisplayName\": \"DOC-EXAMPLE-ACCOUNT-OWNER\",\\n\"ID\": \"852b113e7a2f25102679df27bb0ae12b3f85be6BucketOwnerCanonicalUserID\"\\n},\\n\"Grants\": [\\n{\\n\"Grantee\": {\\n\"DisplayName\": \"DOC-EXAMPLE-ACCOUNT-OWNER\",\\n\"ID\":\\n\"852b113e7a2f25102679df27bb0ae12b3f85be6BucketOwnerCanonicalUserID\",\\n\"Type\": \"CanonicalUser\"\\n},\\n\"Permission\": \"FULL_CONTROL\"', '']]]\n",
      "[[['', '},\\n{\\n\"Grantee\": {\\n\"DisplayName\": \"THIRD-PARTY-EXAMPLE-ACCOUNT\",\\n\"ID\":\\n\"72806de9d1ae8b171cca9e2494a8d1335dfced4ThirdPartyAccountCanonicalUserID\",\\n\"Type\": \"CanonicalUser\"\\n},\\n\"Permission\": \"READ\"\\n},\\n{\\n\"Grantee\": {\\n\"DisplayName\": \"THIRD-PARTY-EXAMPLE-ACCOUNT\",\\n\"ID\":\\n\"72806de9d1ae8b171cca9e2494a8d1335dfced4ThirdPartyAccountCanonicalUserID\",\\n\"Type\": \"CanonicalUser\"\\n},\\n\"Permission\": \"WRITE\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-policy --bucket DOC-EXAMPLE-BUCKET --policy file://policy.json\\npolicy.json:\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"PolicyForCrossAccountAllowUpload\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:root\"\\n]', '']]]\n",
      "[[['', '},\\n\"Action\": [\\n\"s3:PutObject\",\\n\"s3:ListBucket\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\",\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object-acl --bucket DOC-EXAMPLE-BUCKET --key EXAMPLE-OBJECT-KEY', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/EXAMPLE-OBJECT-KEY\"', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-acl --bucket DOC-EXAMPLE-BUCKET --acl private', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Owner\": {\\n\"DisplayName\": \"DOC-EXAMPLE-ACCOUNT-OWNER\",\\n\"ID\": \"852b113e7a2f25102679df27bb0ae12b3f85be6BucketOwnerCanonicalUserID\"\\n},\\n\"Grants\": [\\n{\\n\"Grantee\": {\\n\"Type\": \"CanonicalUser\",\\n\"DisplayName\": \"DOC-EXAMPLE-ACCOUNT-OWNER\",\\n\"ID\":\\n\"852b113e7a2f25102679df27bb0ae12b3f85be6BucketOwnerCanonicalUserID\"\\n},\\n\"Permission\": \"FULL_CONTROL\"\\n},\\n{\\n\"Grantee\": {\\n\"Type\": \"Group\",\\n\"URI\": \"http://acs.amazonaws.com/groups/s3/LogDelivery\"\\n},\\n\"Permission\": \"WRITE\"\\n},\\n{\\n\"Grantee\": {\\n\"Type\": \"Group\",\\n\"URI\": \"http://acs.amazonaws.com/groups/s3/LogDelivery\"\\n},\\n\"Permission\": \"READ_ACP\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-policy --bucket DOC-EXAMPLE-BUCKET --policy file://policy.json\\npolicy.json: {\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"S3ServerAccessLogsPolicy\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"logging.s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/EXAMPLE-LOGGING-PREFIX*\",\\n\"Condition\": {\\n\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:::SOURCE-BUCKET-NAME\"\\n},\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"SOURCE-AWS-ACCOUNT-ID\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-acl --bucket DOC-EXAMPLE-BUCKET --acl private', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Owner\": {\\n\"DisplayName\": \"DOC-EXAMPLE-ACCOUNT-OWNER\",\\n\"ID\": \"852b113e7a2f25102679df27bb0ae12b3f85be6BucketOwnerCanonicalUserID\"\\n},\\n\"Grants\": [\\n{\\n\"Grantee\": {\\n\"DisplayName\": \"DOC-EXAMPLE-ACCOUNT-OWNER\",\\n\"ID\":\\n\"852b113e7a2f25102679df27bb0ae12b3f85be6BucketOwnerCanonicalUserID\",\\n\"Type\": \"CanonicalUser\"\\n},\\n\"Permission\": \"FULL_CONTROL\"\\n},\\n{\\n\"Grantee\": {\\n\"Type\": \"Group\",\\n\"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\"\\n},\\n\"Permission\": \"READ\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-policy --bucket DOC-EXAMPLE-BUCKET --policy file://policy.json\\npolicy.json:\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [', '']]]\n",
      "[[['', '{\\n\"Sid\": \"PublicReadGetObject\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": [\\n\"s3:GetObject\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/OBJECT-KEY\"', ''], ['', '', '']], [['', '', ''], ['', '\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/PREFIX/*\"', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Owner\": {\\n\"DisplayName\": \"DOC-EXAMPLE-ACCOUNT-OWNER\",', '']]]\n",
      "[[['', '\"ID\": \"852b113e7a2f25102679df27bb0ae12b3f85be6BucketOwnerCanonicalUserID\"\\n},\\n\"Grants\": [\\n{\\n\"Grantee\": {\\n\"DisplayName\": \"DOC-EXAMPLE-ACCOUNT-OWNER\",\\n\"ID\":\\n\"852b113e7a2f25102679df27bb0ae12b3f85be6BucketOwnerCanonicalUserID\",\\n\"Type\": \"CanonicalUser\"\\n},\\n\"Permission\": \"FULL_CONTROL\"\\n},\\n{\\n\"Grantee\": {\\n\"DisplayName\": \"aws-scs-s3-readonly\",\\n\"ID\":\\n\"540804c33a284a299d2547575ce1010f2312ef3da9b3a053c8bc45bf233e4353\",\\n\"Type\": \"CanonicalUser\"\\n},\\n\"Permission\": \"READ\"\\n},\\n{\\n\"Grantee\": {\\n\"DisplayName\": \"aws-scs-s3-readonly\",\\n\"ID\":\\n\"540804c33a284a299d2547575ce1010f2312ef3da9b3a053c8bc45bf233e4353\",\\n\"Type\": \"CanonicalUser\"\\n},\\n\"Permission\": \"WRITE\"\\n},\\n{\\n\"Grantee\": {\\n\"DisplayName\": \"aws-scs-s3-readonly\",\\n\"ID\":\\n\"540804c33a284a299d2547575ce1010f2312ef3da9b3a053c8bc45bf233e4353\",\\n\"Type\": \"CanonicalUser\"\\n},\\n\"Permission\": \"READ_ACP\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-policy --bucket DOC-EXAMPLE-BUCKET --policy file://policy.json\\npolicy.json:\\n\"Id\": \"Policy15397346\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Stmt15399483\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"Region.elasticache-snapshot.amazonaws.com\"\\n},\\n\"Action\": [\\n\"s3:PutObject\",\\n\"s3:GetObject\",\\n\"s3:ListBucket\",\\n\"s3:GetBucketAcl\",\\n\"s3:ListMultipartUploadParts\",\\n\"s3:ListBucketMultipartUploads\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\",\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-acl --bucket DOC-EXAMPLE-BUCKET --acl private', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nA majority of modern use cases in Amazon S3 no longer require the use of ACLs, and we\\nrecommend that you disable ACLs except in unusual circumstances where you need to\\ncontrol access for each object individually. With Object Ownership, you can disable ACLs\\nand rely on policies for access control. When you disable ACLs, you can easily maintain a', '']]]\n",
      "[[['', 'bucket with objects uploaded by different AWS accounts. You, as the bucket owner, own all\\nthe objects in the bucket and can manage access to them using policies.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo minimize latency and costs and address regulatory requirements, choose a Region\\nclose to you. Objects stored in a Region never leave that Region unless you explicitly\\ntransfer them to another Region. For a list of Amazon S3 AWS Regions, see AWS\\nservice endpoints in the Amazon Web Services General Reference.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAvoid including sensitive information, such as account numbers, in the bucket name.\\nThe bucket name is visible in the URLs that point to the objects in the bucket.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis option:\\n• Is not available in the AWS CLI and is only available in console\\n• Is not available for directory buckets\\n• Does not copy the bucket policy from the existing bucket to the new bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe default setting is Bucket owner enforced. To apply the default setting and keep\\nACLs disabled, only the s3:CreateBucket permission is needed. To enable ACLs, you\\nmust have the s3:PutBucketOwnershipControls permission.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo enable all Block Public Access settings, only the s3:CreateBucket permission\\nis required. To turn off any Block Public Access settings, you must have the\\ns3:PutBucketPublicAccessBlock permission.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nIf you use the SSE-KMS option for your default encryption configuration, you are\\nsubject to the requests per second (RPS) quota of AWS KMS. For more information\\nabout AWS KMS quotas and how to request a quota increase, see Quotas in the AWS\\nKey Management Service Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nYou can use only KMS keys that are available in the same AWS Region as the\\nbucket. The Amazon S3 console lists only the first 100 KMS keys in the same\\nRegion as the bucket. To use a KMS key that is not listed, you must enter your\\nKMS key ARN. If you want to use a KMS key that is owned by a different account,\\nyou must first have permission to use the key and then you must enter the KMS\\nkey ARN. For more information on cross account permissions for KMS keys, see\\nCreating KMS keys that other accounts can use in the AWS Key Management\\nService Developer Guide. For more information on SSE-KMS, see Specifying server-\\nside encryption with AWS KMS (SSE-KMS).\\nWhen you use an AWS KMS key for server-side encryption in Amazon S3, you must\\nchoose a symmetric encryption KMS key. Amazon S3 supports only symmetric\\nencryption KMS keys and not asymmetric KMS keys. For more information, see\\nIdentifying symmetric and asymmetric KMS keys in the AWS Key Management\\nService Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nEnabling Object Lock also enables versioning for the bucket. After enabling\\nyou must configure the Object Lock default retention and legal hold settings to\\nprotect new objects from being deleted or overwritten.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo create an Object Lock enabled bucket, you must have the following\\npermissions: s3:CreateBucket, s3:PutBucketVersioning and\\ns3:PutBucketObjectLockConfiguration.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api create-bucket --bucket DOC-EXAMPLE-BUCKET --region us-east-1 --object-\\nownership BucketOwnerEnforced', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nIf you don’t set Object Ownership when you create a bucket by using the AWS CLI, the\\ndefault setting will be ObjectWriter (ACLs enabled).', ''], ['', '', '']], [['', '', ''], ['', '// Build the ObjectOwnership for CreateBucket\\nCreateBucketRequest createBucketRequest = CreateBucketRequest.builder()\\n.bucket(bucketName)\\n.objectOwnership(ObjectOwnership.BucketOwnerEnforced)\\n.build()\\n// Send the request to Amazon S3\\ns3client.createBucket(createBucketRequest);', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-ownership-controls --bucket DOC-EXAMPLE-BUCKET --ownership-\\ncontrols=\"Rules=[{ObjectOwnership=BucketOwnerEnforced}]\"', ''], ['', '', '']], [['', '', ''], ['', '// Build the ObjectOwnership for BucketOwnerEnforced\\nOwnershipControlsRule rule = OwnershipControlsRule.builder()\\n.objectOwnership(ObjectOwnership.BucketOwnerEnforced)\\n.build();\\nOwnershipControls ownershipControls = OwnershipControls.builder()\\n.rules(rule)\\n.build()\\n// Build the PutBucketOwnershipControlsRequest\\nPutBucketOwnershipControlsRequest putBucketOwnershipControlsRequest =\\nPutBucketOwnershipControlsRequest.builder()\\n.bucket(BUCKET_NAME)\\n.ownershipControls(ownershipControls)\\n.build();\\n// Send the request to Amazon S3\\ns3client.putBucketOwnershipControls(putBucketOwnershipControlsRequest);', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-ownership-controls --bucket DOC-EXAMPLE-BUCKET', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"RequireBucketOwnerFullControl\",\\n\"Action\": \"s3:CreateBucket\",\\n\"Effect\": \"Deny\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"s3:x-amz-object-ownership\": \"BucketOwnerEnforced\"', '']]]\n",
      "[[['', '}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf you have ACLs disabled with the Bucket owner enforced setting, you, as the bucket\\nowner, automatically own and have full control over all the objects in your bucket. You\\ndon't need to use this section to update your bucket policy to enforce object ownership for\\nthe bucket owner.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Only allow writes to my bucket with bucket owner full control\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:user/ExampleUser\"\\n]\\n},', '']]]\n",
      "[[['', '\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3:x-amz-acl\": \"bucket-owner-full-control\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3 cp file.txt s3://DOC-EXAMPLE-BUCKET --acl bucket-owner-full-control', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf clients need access to objects after uploading, you must grant additional permissions to\\nthe uploading account. For information about granting accounts access to your resources,\\nsee Walkthroughs that use policies to manage access to your Amazon S3 resources.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-object --bucket DOC-EXAMPLE-BUCKET --key object-key-name --body doc-\\nexample-body --acl public-read', ''], ['', '', '']], [['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"Owner\": {\\n\"ID\": \"852b113e7a2f25102679df27bb0ae12b3f85be6BucketOwnerCanonicalUserID\"\\n},\\n\"Grants\": [\\n{\\n\"Grantee\": {\\n\"ID\":\\n\"852b113e7a2f25102679df27bb0ae12b3f85be6BucketOwnerCanonicalUserID\",\\n\"Type\": \"CanonicalUser\"\\n},\\n\"Permission\": \"FULL_CONTROL\"\\n},\\n{\\n\"Grantee\": {\\n\"Type\": \"Group\",\\n\"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\"\\n},\\n\"Permission\": \"READ\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-ownership-controls --bucket DOC-EXAMPLE-BUCKET --ownership-\\ncontrols Rules=[{ObjectOwnership=BucketOwnerEnforced}]', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'http://website.s3-website.us-east-1.amazonaws.com', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe ACLs and policies continue to apply when you enable CORS on the bucket.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nIn the S3 console, the CORS configuration must be JSON.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '[\\n{\\n\"AllowedHeaders\": [\\n\"*\"\\n],\\n\"AllowedMethods\": [\\n\"PUT\",\\n\"POST\",\\n\"DELETE\"\\n],', '']]]\n",
      "[[['', '\"AllowedOrigins\": [\\n\"http://www.example1.com\"\\n],\\n\"ExposeHeaders\": []\\n},\\n{\\n\"AllowedHeaders\": [\\n\"*\"\\n],\\n\"AllowedMethods\": [\\n\"PUT\",\\n\"POST\",\\n\"DELETE\"\\n],\\n\"AllowedOrigins\": [\\n\"http://www.example2.com\"\\n],\\n\"ExposeHeaders\": []\\n},\\n{\\n\"AllowedHeaders\": [],\\n\"AllowedMethods\": [\\n\"GET\"\\n],\\n\"AllowedOrigins\": [\\n\"*\"\\n],\\n\"ExposeHeaders\": []\\n}\\n]', ''], ['', '', '']], [['', '', ''], ['', '<CORSConfiguration>\\n<CORSRule>\\n<AllowedOrigin>http://www.example1.com</AllowedOrigin>\\n<AllowedMethod>PUT</AllowedMethod>\\n<AllowedMethod>POST</AllowedMethod>\\n<AllowedMethod>DELETE</AllowedMethod>\\n<AllowedHeader>*</AllowedHeader>\\n</CORSRule>', '']]]\n",
      "[[['', '<CORSRule>\\n<AllowedOrigin>http://www.example2.com</AllowedOrigin>\\n<AllowedMethod>PUT</AllowedMethod>\\n<AllowedMethod>POST</AllowedMethod>\\n<AllowedMethod>DELETE</AllowedMethod>\\n<AllowedHeader>*</AllowedHeader>\\n</CORSRule>\\n<CORSRule>\\n<AllowedOrigin>*</AllowedOrigin>\\n<AllowedMethod>GET</AllowedMethod>\\n</CORSRule>\\n</CORSConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '[\\n{\\n\"AllowedHeaders\": [\\n\"*\"\\n],\\n\"AllowedMethods\": [\\n\"PUT\",\\n\"POST\",\\n\"DELETE\"\\n],\\n\"AllowedOrigins\": [\\n\"http://www.example.com\"\\n],\\n\"ExposeHeaders\": [\\n\"x-amz-server-side-encryption\",\\n\"x-amz-request-id\",\\n\"x-amz-id-2\"\\n],\\n\"MaxAgeSeconds\": 3000', '']]]\n",
      "[[['', '}\\n]', ''], ['', '', '']], [['', '', ''], ['', '<CORSConfiguration>\\n<CORSRule>\\n<AllowedOrigin>http://www.example.com</AllowedOrigin>\\n<AllowedMethod>PUT</AllowedMethod>\\n<AllowedMethod>POST</AllowedMethod>\\n<AllowedMethod>DELETE</AllowedMethod>\\n<AllowedHeader>*</AllowedHeader>\\n<MaxAgeSeconds>3000</MaxAgeSeconds>\\n<ExposeHeader>x-amz-server-side-encryption</\\nExposeHeader>\\n<ExposeHeader>x-amz-request-id</\\nExposeHeader>\\n<ExposeHeader>x-amz-id-2</ExposeHeader>\\n</CORSRule>\\n</CORSConfiguration>', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nIn the new S3 console, the CORS configuration must be JSON. For examples CORS\\nconfigurations in JSON and XML, see CORS configuration.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAmazon S3 displays the Amazon Resource Name (ARN) for the bucket next to the\\nCORS configuration editor title. For more information about ARNs, see Amazon\\nResource Names (ARNs) and AWS Service Namespaces in the Amazon Web Services\\nGeneral Reference.', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.BucketCrossOriginConfiguration;\\nimport com.amazonaws.services.s3.model.CORSRule;', '']]]\n",
      "[[['', 'import java.io.IOException;\\nimport java.util.ArrayList;\\nimport java.util.Arrays;\\nimport java.util.List;\\npublic class CORS {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\n// Create two CORS rules.\\nList<CORSRule.AllowedMethods> rule1AM = new\\nArrayList<CORSRule.AllowedMethods>();\\nrule1AM.add(CORSRule.AllowedMethods.PUT);\\nrule1AM.add(CORSRule.AllowedMethods.POST);\\nrule1AM.add(CORSRule.AllowedMethods.DELETE);\\nCORSRule rule1 = new\\nCORSRule().withId(\"CORSRule1\").withAllowedMethods(rule1AM)\\n.withAllowedOrigins(Arrays.asList(\"http://*.example.com\"));\\nList<CORSRule.AllowedMethods> rule2AM = new\\nArrayList<CORSRule.AllowedMethods>();\\nrule2AM.add(CORSRule.AllowedMethods.GET);\\nCORSRule rule2 = new\\nCORSRule().withId(\"CORSRule2\").withAllowedMethods(rule2AM)\\n.withAllowedOrigins(Arrays.asList(\"*\")).withMaxAgeSeconds(3000)\\n.withExposedHeaders(Arrays.asList(\"x-amz-server-side-encryption\"));\\nList<CORSRule> rules = new ArrayList<CORSRule>();\\nrules.add(rule1);\\nrules.add(rule2);\\n// Add the rules to a new CORS configuration.\\nBucketCrossOriginConfiguration configuration = new\\nBucketCrossOriginConfiguration();\\nconfiguration.setRules(rules);\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();', '']]]\n",
      "[[['', '// Add the configuration to the bucket.\\ns3Client.setBucketCrossOriginConfiguration(bucketName, configuration);\\n// Retrieve and display the configuration.\\nconfiguration = s3Client.getBucketCrossOriginConfiguration(bucketName);\\nprintCORSConfiguration(configuration);\\n// Add another new rule.\\nList<CORSRule.AllowedMethods> rule3AM = new\\nArrayList<CORSRule.AllowedMethods>();\\nrule3AM.add(CORSRule.AllowedMethods.HEAD);\\nCORSRule rule3 = new\\nCORSRule().withId(\"CORSRule3\").withAllowedMethods(rule3AM)\\n.withAllowedOrigins(Arrays.asList(\"http://www.example.com\"));\\nrules = configuration.getRules();\\nrules.add(rule3);\\nconfiguration.setRules(rules);\\ns3Client.setBucketCrossOriginConfiguration(bucketName, configuration);\\n// Verify that the new rule was added by checking the number of rules in\\nthe\\n// configuration.\\nconfiguration = s3Client.getBucketCrossOriginConfiguration(bucketName);\\nSystem.out.println(\"Expected # of rules = 3, found \" +\\nconfiguration.getRules().size());\\n// Delete the configuration.\\ns3Client.deleteBucketCrossOriginConfiguration(bucketName);\\nSystem.out.println(\"Removed CORS configuration.\");\\n// Retrieve and display the configuration to verify that it was\\n// successfully deleted.\\nconfiguration = s3Client.getBucketCrossOriginConfiguration(bucketName);\\nprintCORSConfiguration(configuration);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}', '']]]\n",
      "[[['', '}\\nprivate static void printCORSConfiguration(BucketCrossOriginConfiguration\\nconfiguration) {\\nif (configuration == null) {\\nSystem.out.println(\"Configuration is null.\");\\n} else {\\nSystem.out.println(\"Configuration has \" +\\nconfiguration.getRules().size() + \" rules\\\\n\");\\nfor (CORSRule rule : configuration.getRules()) {\\nSystem.out.println(\"Rule ID: \" + rule.getId());\\nSystem.out.println(\"MaxAgeSeconds: \" + rule.getMaxAgeSeconds());\\nSystem.out.println(\"AllowedMethod: \" + rule.getAllowedMethods());\\nSystem.out.println(\"AllowedOrigins: \" + rule.getAllowedOrigins());\\nSystem.out.println(\"AllowedHeaders: \" + rule.getAllowedHeaders());\\nSystem.out.println(\"ExposeHeader: \" + rule.getExposedHeaders());\\nSystem.out.println();\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass CORSTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\n// Specify your bucket region (an example region is shown).', '']]]\n",
      "[[['', 'private static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\npublic static void Main()\\n{\\ns3Client = new AmazonS3Client(bucketRegion);\\nCORSConfigTestAsync().Wait();\\n}\\nprivate static async Task CORSConfigTestAsync()\\n{\\ntry\\n{\\n// Create a new configuration request and add two rules\\nCORSConfiguration configuration = new CORSConfiguration\\n{\\nRules = new System.Collections.Generic.List<CORSRule>\\n{\\nnew CORSRule\\n{\\nId = \"CORSRule1\",\\nAllowedMethods = new List<string> {\"PUT\", \"POST\",\\n\"DELETE\"},\\nAllowedOrigins = new List<string> {\"http://\\n*.example.com\"}\\n},\\nnew CORSRule\\n{\\nId = \"CORSRule2\",\\nAllowedMethods = new List<string> {\"GET\"},\\nAllowedOrigins = new List<string> {\"*\"},\\nMaxAgeSeconds = 3000,\\nExposeHeaders = new List<string> {\"x-amz-server-side-\\nencryption\"}\\n}\\n}\\n};\\n// Add the configuration to the bucket.\\nawait PutCORSConfigurationAsync(configuration);\\n// Retrieve an existing configuration.\\nconfiguration = await RetrieveCORSConfigurationAsync();', '']]]\n",
      "[[['', '// Add a new rule.\\nconfiguration.Rules.Add(new CORSRule\\n{\\nId = \"CORSRule3\",\\nAllowedMethods = new List<string> { \"HEAD\" },\\nAllowedOrigins = new List<string> { \"http://www.example.com\" }\\n});\\n// Add the configuration to the bucket.\\nawait PutCORSConfigurationAsync(configuration);\\n// Verify that there are now three rules.\\nconfiguration = await RetrieveCORSConfigurationAsync();\\nConsole.WriteLine();\\nConsole.WriteLine(\"Expected # of rulest=3; found:{0}\",\\nconfiguration.Rules.Count);\\nConsole.WriteLine();\\nConsole.WriteLine(\"Pause before configuration delete. To continue,\\nclick Enter...\");\\nConsole.ReadKey();\\n// Delete the configuration.\\nawait DeleteCORSConfigurationAsync();\\n// Retrieve a nonexistent configuration.\\nconfiguration = await RetrieveCORSConfigurationAsync();\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\nstatic async Task PutCORSConfigurationAsync(CORSConfiguration configuration)\\n{\\nPutCORSConfigurationRequest request = new PutCORSConfigurationRequest\\n{', '']]]\n",
      "[[['', 'BucketName = bucketName,\\nConfiguration = configuration\\n};\\nvar response = await s3Client.PutCORSConfigurationAsync(request);\\n}\\nstatic async Task<CORSConfiguration> RetrieveCORSConfigurationAsync()\\n{\\nGetCORSConfigurationRequest request = new GetCORSConfigurationRequest\\n{\\nBucketName = bucketName\\n};\\nvar response = await s3Client.GetCORSConfigurationAsync(request);\\nvar configuration = response.Configuration;\\nPrintCORSRules(configuration);\\nreturn configuration;\\n}\\nstatic async Task DeleteCORSConfigurationAsync()\\n{\\nDeleteCORSConfigurationRequest request = new\\nDeleteCORSConfigurationRequest\\n{\\nBucketName = bucketName\\n};\\nawait s3Client.DeleteCORSConfigurationAsync(request);\\n}\\nstatic void PrintCORSRules(CORSConfiguration configuration)\\n{\\nConsole.WriteLine();\\nif (configuration == null)\\n{\\nConsole.WriteLine(\"\\\\nConfiguration is null\");\\nreturn;\\n}\\nConsole.WriteLine(\"Configuration has {0} rules:\",\\nconfiguration.Rules.Count);\\nforeach (CORSRule rule in configuration.Rules)\\n{', '']]]\n",
      "[[['', 'Console.WriteLine(\"Rule ID: {0}\", rule.Id);\\nConsole.WriteLine(\"MaxAgeSeconds: {0}\", rule.MaxAgeSeconds);\\nConsole.WriteLine(\"AllowedMethod: {0}\", string.Join(\", \",\\nrule.AllowedMethods.ToArray()));\\nConsole.WriteLine(\"AllowedOrigins: {0}\", string.Join(\", \",\\nrule.AllowedOrigins.ToArray()));\\nConsole.WriteLine(\"AllowedHeaders: {0}\", string.Join(\", \",\\nrule.AllowedHeaders.ToArray()));\\nConsole.WriteLine(\"ExposeHeader: {0}\", string.Join(\", \",\\nrule.ExposeHeaders.ToArray()));\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nWe recommend that your application not pin Amazon S3 TLS certificates as AWS\\ndoesn’t support pinning of publicly-trusted certificates. S3 automatically renews\\ncertificates and renewal can happen any time before certificate expiry. Renewing a\\ncertificate generates a new public-private key pair. If you’ve pinned an S3 certificate\\nwhich has been recently renewed with a new public key, you won’t be able to connect to\\nS3 until your application uses the new certificate.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nCRR requires both the source and destination S3 buckets to have versioning enabled.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nAWS Config managed rules only supports general purpose buckets when evaluating\\nAmazon S3 resources. AWS Config doesn’t record configuration changes for directory\\nbuckets. For more information, see AWS Config Managed Rules and List of AWS Config\\nManaged Rules in the AWS Config Developer Guide.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• The SOAP API does not support S3 Versioning. SOAP support over HTTP is deprecated,\\nbut it is still available over HTTPS. New Amazon S3 features are not supported for SOAP.\\n• Normal Amazon S3 rates apply for every version of an object stored and transferred.\\nEach version of an object is the entire object; it is not just a diff from the previous\\nversion. Thus, if you have three versions of an object stored, you are charged for three\\nobjects.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf you have an object expiration lifecycle configuration in your unversioned bucket and\\nyou want to maintain the same permanent delete behavior when you enable versioning,\\nyou must add a noncurrent expiration configuration. The noncurrent expiration lifecycle\\nconfiguration manages the deletes of the noncurrent object versions in the version-enabled\\nbucket. (A version-enabled bucket maintains one current, and zero or more noncurrent,\\nobject versions.) For more information, see Setting a lifecycle configuration on a bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nNormal Amazon S3 rates apply for every version of an object that is stored and transferred.\\nEach version of an object is the entire object; it is not a diff from the previous version. Thus,\\nif you have three versions of an object stored, you are charged for three objects.', ''], ['', '', '']], [['', '', ''], ['', '<VersioningConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n</VersioningConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<VersioningConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Status>Enabled</Status>', '']]]\n",
      "[[['', '</VersioningConfiguration>', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen you enable versioning on a bucket for the first time, it might take a short amount of\\ntime for the change to be fully propagated. We recommend that you wait for 15 minutes\\nafter enabling versioning before issuing write operations (PUT or DELETE) on objects in the\\nbucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nFor simplicity, the other examples in this topic use much shorter IDs.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nIf you notice a significant increase in the number of HTTP 503 (Service Unavailable)\\nresponses received for Amazon S3 PUT or DELETE object requests to a bucket that has\\nS3 Versioning enabled, you might have one or more objects in the bucket for which\\nthere are millions of versions. For more information, see the S3 Versioning section of\\nTroubleshooting.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you enable versioning on a bucket for the first time, it might take up to 15 minutes\\nfor the change to be fully propagated. We recommend that you wait for 15 minutes\\nafter enabling versioning before issuing write operations (PUT or DELETE) on objects\\nin the bucket. Write operations issued before this conversion is complete may apply to\\nunversioned objects.', ''], ['', '', '']], [['', '', ''], ['', '<VersioningConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n</VersioningConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<VersioningConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Status>Enabled</Status>\\n</VersioningConfiguration>', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nYou can use AWS multi-factor authentication (MFA) with versioning. When you use MFA\\nwith versioning, you must provide your AWS account’s access keys and a valid code from\\nthe account’s MFA device to permanently delete an object version or suspend or reactivate\\nversioning.\\nTo use MFA with versioning, you enable MFA Delete. However, you can't enable MFA\\nDelete using the AWS Management Console. You must use the AWS Command Line\\nInterface (AWS CLI) or the API. For more information, see Configuring MFA delete.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-versioning --bucket example-s3-bucket1 --versioning-configuration\\nStatus=Enabled', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-versioning --bucket example-s3-bucket1 --versioning-configuration\\nStatus=Enabled,MFADelete=Enabled --mfa \"SERIAL 123456\"', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nUsing MFA delete requires an approved physical or virtual authentication device. For more\\ninformation about using MFA delete in Amazon S3, see Configuring MFA delete.', ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;', '']]]\n",
      "[[['', 'namespace s3.amazon.com.docsamples\\n{\\nclass BucketVersioningConfiguration\\n{\\nstatic string bucketName = \"*** bucket name ***\";\\npublic static void Main(string[] args)\\n{\\nusing (var client = new AmazonS3Client(Amazon.RegionEndpoint.USEast1))\\n{\\ntry\\n{\\nEnableVersioningOnBucket(client);\\nstring bucketVersioningStatus =\\nRetrieveBucketVersioningConfiguration(client);\\n}\\ncatch (AmazonS3Exception amazonS3Exception)\\n{\\nif (amazonS3Exception.ErrorCode != null &&\\n(amazonS3Exception.ErrorCode.Equals(\"InvalidAccessKeyId\")\\n||\\namazonS3Exception.ErrorCode.Equals(\"InvalidSecurity\")))\\n{\\nConsole.WriteLine(\"Check the provided AWS Credentials.\");\\nConsole.WriteLine(\\n\"To sign up for service, go to http://aws.amazon.com/s3\");\\n}\\nelse\\n{\\nConsole.WriteLine(\\n\"Error occurred. Message:\\'{0}\\' when listing objects\",\\namazonS3Exception.Message);\\n}\\n}\\n}\\nConsole.WriteLine(\"Press any key to continue...\");\\nConsole.ReadKey();\\n}\\nstatic void EnableVersioningOnBucket(IAmazonS3 client)\\n{\\nPutBucketVersioningRequest request = new PutBucketVersioningRequest', '']]]\n",
      "[[['', '{\\nBucketName = bucketName,\\nVersioningConfig = new S3BucketVersioningConfig\\n{\\nStatus = VersionStatus.Enabled\\n}\\n};\\nPutBucketVersioningResponse response =\\nclient.PutBucketVersioning(request);\\n}\\nstatic string RetrieveBucketVersioningConfiguration(IAmazonS3 client)\\n{\\nGetBucketVersioningRequest request = new GetBucketVersioningRequest\\n{\\nBucketName = bucketName\\n};\\nGetBucketVersioningResponse response =\\nclient.GetBucketVersioning(request);\\nreturn response.VersioningConfig.Status;\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import java.io.IOException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Region;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3Client;\\nimport com.amazonaws.services.s3.model.AmazonS3Exception;\\nimport com.amazonaws.services.s3.model.BucketVersioningConfiguration;\\nimport com.amazonaws.services.s3.model.SetBucketVersioningConfigurationRequest;\\npublic class BucketVersioningConfigurationExample {', '']]]\n",
      "[[['', 'public static String bucketName = \"*** bucket name ***\";\\npublic static AmazonS3Client s3Client;\\npublic static void main(String[] args) throws IOException {\\ns3Client = new AmazonS3Client(new ProfileCredentialsProvider());\\ns3Client.setRegion(Region.getRegion(Regions.US_EAST_1));\\ntry {\\n// 1. Enable versioning on the bucket.\\nBucketVersioningConfiguration configuration =\\nnew BucketVersioningConfiguration().withStatus(\"Enabled\");\\nSetBucketVersioningConfigurationRequest setBucketVersioningConfigurationRequest\\n=\\nnew SetBucketVersioningConfigurationRequest(bucketName,configuration);\\ns3Client.setBucketVersioningConfiguration(setBucketVersioningConfigurationRequest);\\n// 2. Get bucket versioning configuration information.\\nBucketVersioningConfiguration conf =\\ns3Client.getBucketVersioningConfiguration(bucketName);\\nSystem.out.println(\"bucket versioning configuration status: \" +\\nconf.getStatus());\\n} catch (AmazonS3Exception amazonS3Exception) {\\nSystem.out.format(\"An Amazon S3 error occurred. Exception: %s\",\\namazonS3Exception.toString());\\n} catch (Exception ex) {\\nSystem.out.format(\"Exception: %s\", ex.toString());\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'def create_versioned_bucket(bucket_name, prefix):\\n\"\"\"\\nCreates an Amazon S3 bucket, enables it for versioning, and configures a\\nlifecycle', '']]]\n",
      "[[['', 'that expires noncurrent object versions after 7 days.\\nAdding a lifecycle configuration to a versioned bucket is a best practice.\\nIt helps prevent objects in the bucket from accumulating a large number of\\nnoncurrent versions, which can slow down request performance.\\nUsage is shown in the usage_demo_single_object function at the end of this\\nmodule.\\n:param bucket_name: The name of the bucket to create.\\n:param prefix: Identifies which objects are automatically expired under the\\nconfigured lifecycle rules.\\n:return: The newly created bucket.\\n\"\"\"\\ntry:\\nbucket = s3.create_bucket(\\nBucket=bucket_name,\\nCreateBucketConfiguration={\\n\"LocationConstraint\": s3.meta.client.meta.region_name\\n},\\n)\\nlogger.info(\"Created bucket %s.\", bucket.name)\\nexcept ClientError as error:\\nif error.response[\"Error\"][\"Code\"] == \"BucketAlreadyOwnedByYou\":\\nlogger.warning(\"Bucket %s already exists! Using it.\", bucket_name)\\nbucket = s3.Bucket(bucket_name)\\nelse:\\nlogger.exception(\"Couldn\\'t create bucket %s.\", bucket_name)\\nraise\\ntry:\\nbucket.Versioning().enable()\\nlogger.info(\"Enabled versioning on bucket %s.\", bucket.name)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t enable versioning on bucket %s.\", bucket.name)\\nraise\\ntry:\\nexpiration = 7\\nbucket.LifecycleConfiguration().put(\\nLifecycleConfiguration={\\n\"Rules\": [\\n{\\n\"Status\": \"Enabled\",', '']]]\n",
      "[[['', '\"Prefix\": prefix,\\n\"NoncurrentVersionExpiration\": {\"NoncurrentDays\":\\nexpiration},\\n}\\n]\\n}\\n)\\nlogger.info(\\n\"Configured lifecycle to expire noncurrent versions after %s days \"\\n\"on bucket %s.\",\\nexpiration,\\nbucket.name,\\n)\\nexcept ClientError as error:\\nlogger.warning(\\n\"Couldn\\'t configure lifecycle on bucket %s because %s. \"\\n\"Continuing anyway.\",\\nbucket.name,\\nerror,\\n)\\nreturn bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo use MFA delete with versioning, you enable MFA Delete. However, you cannot enable\\nMFA Delete using the AWS Management Console. You must use the AWS Command Line\\nInterface (AWS CLI) or the API.\\nFor examples of using MFA delete with versioning, see the examples section in the topic\\nEnabling versioning on buckets.\\nYou cannot use MFA delete with lifecycle configurations. For more information about\\nlifecycle configurations and how they interact with other configurations, see Lifecycle and\\nother bucket configurations.', ''], ['', '', '']], [['', '', ''], ['', '<VersioningConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Status>VersioningState</Status>\\n<MfaDelete>MfaDeleteState</MfaDelete>\\n</VersioningConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-versioning --bucket example-s3-bucket1 --versioning-configuration\\nStatus=Enabled,MFADelete=Enabled --mfa \"SERIAL 123456\"', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe version ID values that Amazon S3 assigns are URL safe (can be included as part of a\\nURI).', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'x-amz-version-id: 3/L4kqtJlcpXroDTDmJ+rmSpXd3dIbrHY', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nTo access object versions older than 300 versions, you must use the AWS CLI or the object's\\nURL.\", ''], ['', '', '']], [['', '', ''], ['', \"Important\\nYou can undelete an object only if it was deleted as the latest (current) version. You can't\\nundelete a previous version of an object that was deleted. For more information, see Using\\nversioning in S3 buckets.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThe following examples also work with a bucket that isn't versioning-enabled, or for\\nobjects that don't have individual versions. In those cases, Amazon S3 returns the object\\nlisting with a version ID of null.\", ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.ListVersionsRequest;\\nimport com.amazonaws.services.s3.model.S3VersionSummary;\\nimport com.amazonaws.services.s3.model.VersionListing;\\npublic class ListKeysVersioningEnabledBucket {\\npublic static void main(String[] args) {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\n// Retrieve the list of versions. If the bucket contains more versions', '']]]\n",
      "[[['', '// than the specified maximum number of results, Amazon S3 returns\\n// one page of results per request.\\nListVersionsRequest request = new ListVersionsRequest()\\n.withBucketName(bucketName)\\n.withMaxResults(2);\\nVersionListing versionListing = s3Client.listVersions(request);\\nint numVersions = 0, numPages = 0;\\nwhile (true) {\\nnumPages++;\\nfor (S3VersionSummary objectSummary :\\nversionListing.getVersionSummaries()) {\\nSystem.out.printf(\"Retrieved object %s, version %s\\\\n\",\\nobjectSummary.getKey(),\\nobjectSummary.getVersionId());\\nnumVersions++;\\n}\\n// Check whether there are more pages of versions to retrieve. If\\n// there are, retrieve them. Otherwise, exit the loop.\\nif (versionListing.isTruncated()) {\\nversionListing =\\ns3Client.listNextBatchOfVersions(versionListing);\\n} else {\\nbreak;\\n}\\n}\\nSystem.out.println(numVersions + \" object versions retrieved in \" +\\nnumPages + \" pages\");\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass ListObjectsVersioningEnabledBucketTest\\n{\\nstatic string bucketName = \"*** bucket name ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\npublic static void Main(string[] args)\\n{\\ns3Client = new AmazonS3Client(bucketRegion);\\nGetObjectListWithAllVersionsAsync().Wait();\\n}\\nstatic async Task GetObjectListWithAllVersionsAsync()\\n{\\ntry\\n{\\nListVersionsRequest request = new ListVersionsRequest()\\n{\\nBucketName = bucketName,\\n// You can optionally specify key name prefix in the request\\n// if you want list of object versions of a specific object.\\n// For this example we limit response to return list of 2\\nversions.\\nMaxKeys = 2\\n};\\ndo\\n{\\nListVersionsResponse response = await\\ns3Client.ListVersionsAsync(request);\\n// Process response.\\nforeach (S3ObjectVersion entry in response.Versions)\\n{', '']]]\n",
      "[[['', 'Console.WriteLine(\"key = {0} size = {1}\",\\nentry.Key, entry.Size);\\n}\\n// If response is truncated, set the marker to get the next\\n// set of keys.\\nif (response.IsTruncated)\\n{\\nrequest.KeyMarker = response.NextKeyMarker;\\nrequest.VersionIdMarker = response.NextVersionIdMarker;\\n}\\nelse\\n{\\nrequest = null;\\n}\\n} while (request != null);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'GET /?versions HTTP/1.1\\nHost: bucketName.s3.amazonaws.com\\nDate: Wed, 28 Oct 2009 22:32:00 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=', ''], ['', '', '']], [['', '', ''], ['', 'GET /?versions&prefix=myObject HTTP/1.1\\nHost: bucket.s3.amazonaws.com\\nDate: Wed, 28 Oct 2009 22:32:00 GMT\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'GET /?versions&key-marker=myObject&version-id-marker=298459348571 HTTP/1.1\\nHost: bucket.s3.amazonaws.com\\nDate: Wed, 28 Oct 2009 22:32:00 GMT\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-object-versions --bucket example-s3-bucket1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nTo access object versions older than 300 versions, you must use the AWS CLI or the object's\\nURL.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nYou can undelete an object only if it was deleted as the latest (current) version. You can't\\nundelete a previous version of an object that was deleted. For more information, see Using\\nversioning in S3 buckets.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'GET /my-image.jpg?versionId=L4kqtJlcpXroDTDmpUMLUo HTTP/1.1\\nHost: bucket.s3.amazonaws.com\\nDate: Wed, 28 Oct 2009 22:32:00 GMT\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=', ''], ['', '', '']], [['', '', ''], ['', 'HEAD /my-image.jpg?versionId=3HL4kqCxf3vjVBH40Nrjfkd HTTP/1.1\\nHost: bucket.s3.amazonaws.com\\nDate: Wed, 28 Oct 2009 22:32:00 GMT\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=', ''], ['', '', '']], [['', '', ''], ['', 'HTTP/1.1 200 OK', '']]]\n",
      "[[['', 'x-amz-id-2: ef8yU9AS1ed4OpIszj7UDNEHGran\\nx-amz-request-id: 318BC8BC143432E5\\nx-amz-version-id: 3HL4kqtJlcpXroDTDmjVBH40Nrjfkd\\nDate: Wed, 28 Oct 2009 22:32:00 GMT\\nLast-Modified: Sun, 1 Jan 2006 12:00:00 GMT\\nETag: \"fba9dede5f27731c9771645a39863328\"\\nContent-Length: 434234\\nContent-Type: text/plain\\nConnection: close\\nServer: AmazonS3', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nTo restore object versions in batches, you can use the CopyObject operation. The\\nCopyObject operation copies each object that is specified in the manifest. However,\\nbe aware that objects aren't necessarily copied in the same order as they appear in\\nthe manifest. For versioned buckets, if preserving current/non-current version order\", '']]]\n",
      "[[['', 'is important, you should copy all non-current versions first. Then, after the first job is\\ncomplete, copy the current versions in a subsequent job.', ''], ['', '', '']], [['', '', ''], ['', \"Important\\nYou can undelete an object only if it was deleted as the latest (current) version. You can't\\nundelete a previous version of an object that was deleted. For more information, see Using\\nversioning in S3 buckets.\", ''], ['', '', '']], [['', '', ''], ['', 'def rollback_object(bucket, object_key, version_id):\\n\"\"\"', '']]]\n",
      "[[['', 'Rolls back an object to an earlier version by deleting all versions that\\noccurred after the specified rollback version.\\nUsage is shown in the usage_demo_single_object function at the end of this\\nmodule.\\n:param bucket: The bucket that holds the object to roll back.\\n:param object_key: The object to roll back.\\n:param version_id: The version ID to roll back to.\\n\"\"\"\\n# Versions must be sorted by last_modified date because delete markers are\\n# at the end of the list even when they are interspersed in time.\\nversions = sorted(\\nbucket.object_versions.filter(Prefix=object_key),\\nkey=attrgetter(\"last_modified\"),\\nreverse=True,\\n)\\nlogger.debug(\\n\"Got versions:\\\\n%s\",\\n\"\\\\n\".join(\\n[\\nf\"\\\\t{version.version_id}, last modified {version.last_modified}\"\\nfor version in versions\\n]\\n),\\n)\\nif version_id in [ver.version_id for ver in versions]:\\nprint(f\"Rolling back to version {version_id}\")\\nfor version in versions:\\nif version.version_id != version_id:\\nversion.delete()\\nprint(f\"Deleted version {version.version_id}\")\\nelse:\\nbreak\\nprint(f\"Active version is now {bucket.Object(object_key).version_id}\")\\nelse:\\nraise KeyError(\\nf\"{version_id} was not found in the list of versions for \"\\nf\"{object_key}.\"\\n)', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nNormal Amazon S3 rates apply for every version of an object that is stored and transferred,\\nincluding noncurrent object versions. For more information, see Amazon S3 pricing.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Warning\\nWhen you permanently delete an object version, the action cannot be undone.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'def permanently_delete_object(bucket, object_key):\\n\"\"\"\\nPermanently deletes a versioned object by deleting all of its versions.\\nUsage is shown in the usage_demo_single_object function at the end of this\\nmodule.\\n:param bucket: The bucket that contains the object.\\n:param object_key: The object to delete.\\n\"\"\"\\ntry:\\nbucket.object_versions.filter(Prefix=object_key).delete()\\nlogger.info(\"Permanently deleted all versions of object %s.\", object_key)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t delete all versions of %s.\", object_key)\\nraise', ''], ['', '', '']], [['', '', ''], ['', 'DELETE /photo.gif?versionId=UIORUnfnd89493jJFJ HTTP/1.1\\nHost: bucket.s3.amazonaws.com\\nDate: Wed, 12 Oct 2009 17:50:00 GMT\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:xQE0diMbLRepdf3YB+FIEXAMPLE=\\nContent-Type: text/plain\\nContent-Length: 0', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api delete-object --bucket example-s3-bucket1 --key test.txt --version-\\nid versionID', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf you use a DeleteObject request where the current version is a delete marker (without\\nspecifying the version ID of the delete marker), Amazon S3 does not delete the delete\\nmarker, but instead PUTs another delete marker.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't use the Amazon S3 console to undelete folders. You must use the AWS CLI or\\nSDK. For examples, see How can I retrieve an Amazon S3 object that was deleted in a\\nversioning-enabled bucket? in the AWS Knowledge Center.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'DELETE /photo.gif?versionId=4857693 HTTP/1.1\\nHost: bucket.s3.amazonaws.com\\nDate: Wed, 28 Oct 2009 22:32:00 GMT\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=', ''], ['', '', '']], [['', '', ''], ['', '204 NoContent\\nx-amz-version-id: versionID\\nx-amz-delete-marker: true', ''], ['', '', '']], [['', '', ''], ['', 'def revive_object(bucket, object_key):\\n\"\"\"\\nRevives a versioned object that was deleted by removing the object\\'s active\\ndelete marker.\\nA versioned object presents as deleted when its latest version is a delete\\nmarker.\\nBy removing the delete marker, we make the previous version the latest version\\nand the object then presents as *not* deleted.\\nUsage is shown in the usage_demo_single_object function at the end of this\\nmodule.', '']]]\n",
      "[[['', ':param bucket: The bucket that contains the object.\\n:param object_key: The object to revive.\\n\"\"\"\\n# Get the latest version for the object.\\nresponse = s3.meta.client.list_object_versions(\\nBucket=bucket.name, Prefix=object_key, MaxKeys=1\\n)\\nif \"DeleteMarkers\" in response:\\nlatest_version = response[\"DeleteMarkers\"][0]\\nif latest_version[\"IsLatest\"]:\\nlogger.info(\\n\"Object %s was indeed deleted on %s. Let\\'s revive it.\",\\nobject_key,\\nlatest_version[\"LastModified\"],\\n)\\nobj = bucket.Object(object_key)\\nobj.Version(latest_version[\"VersionId\"]).delete()\\nlogger.info(\\n\"Revived %s, active version is now %s with body \\'%s\\'\",\\nobject_key,\\nobj.version_id,\\nobj.get()[\"Body\"].read(),\\n)\\nelse:\\nlogger.warning(\\n\"Delete marker is not the latest version for %s!\", object_key\\n)\\nelif \"Versions\" in response:\\nlogger.warning(\"Got an active version for %s, nothing to do.\", object_key)\\nelse:\\nlogger.error(\"Couldn\\'t get any version info for %s.\", object_key)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'DELETE /my-image.jpg?versionId=3HL4kqCxf3vjVBH40Nrjfkd HTTPS/1.1\\nHost: bucketName.s3.amazonaws.com\\nx-amz-mfa: 20899872 301749\\nDate: Wed, 28 Oct 2009 22:32:00 GMT\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=', ''], ['', '', '']], [['', '', ''], ['', 'PUT /my-image.jpg?acl&versionId=3HL4kqtJvjVBH40Nrjfkd HTTP/1.1\\nHost: bucket.s3.amazonaws.com\\nDate: Wed, 28 Oct 2009 22:32:00 GMT\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU=\\nContent-Length: 124\\n<AccessControlPolicy>', '']]]\n",
      "[[['', '<Owner>\\n<ID>75cc57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a</ID>\\n<DisplayName>mtd@amazon.com</DisplayName>\\n</Owner>\\n<AccessControlList>\\n<Grant>\\n<Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\nxsi:type=\"CanonicalUser\">\\n<ID>a9a7b886d6fd24a52fe8ca5bef65f89a64e0193f23000e241bf9b1c61be666e9</ID>\\n<DisplayName>BucketOwner@amazon.com</DisplayName>\\n</Grantee>\\n<Permission>FULL_CONTROL</Permission>\\n</Grant>\\n</AccessControlList>\\n</AccessControlPolicy>', ''], ['', '', '']], [['', '', ''], ['', 'GET /my-image.jpg?versionId=DVBH40Nr8X8gUMLUo&acl HTTP/1.1\\nHost: bucket.s3.amazonaws.com\\nDate: Wed, 28 Oct 2009 22:32:00 GMT\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:0RQf4/cRonhpaBX5sCYVf1bNRuU', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nWe recommend that you set a lifecycle expiration rule for versioning-enabled buckets that\\nare being backed up. If you do not set a lifecycle expiration period, your Amazon S3 storage\\ncosts might increase because AWS Backup retains all versions of your Amazon S3 data.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nWhen you restore an archived object from S3 Glacier Flexible Retrieval or S3 Glacier Deep\\nArchive, you pay for both the archived object and the copy that you restored temporarily.\\nFor information about pricing, see Amazon S3 pricing.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nUnlike in the S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive storage classes,\\nrestore requests for S3 Intelligent-Tiering objects don't accept the Days value.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nExpedited retrievals are a premium feature and are charged at the Expedited request and\\nretrieval rate.\\nFor information about Amazon S3 pricing, see Amazon S3 Pricing.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• For objects stored in the S3 Glacier Flexible Retrieval storage class or the S3\\nIntelligent-Tiering Archive Access tier, Standard retrievals initiated by using the S3\\nBatch Operations restore operation typically start within minutes and finish within 3-5\\nhours.\\n• For objects in the S3 Glacier Deep Archive storage class or the S3 Intelligent-Tiering\\nDeep Archive Access tier, Standard retrievals initiated by using the Batch Operations\\nrestore operation typically start within 9 hours and finish within 12 hours.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\n• When you restore an archived object from the S3 Glacier Flexible Retrieval or S3 Glacier\\nDeep Archive storage classes, you pay for both the archived object and the copy that you\\nrestored temporarily.\\n• When you restore an object from S3 Intelligent-Tiering, there are no retrieval charges for\\nStandard or Bulk retrievals.\\n• Subsequent restore requests called on archived objects that have already been restored\\nare billed as GET requests. For information about pricing, see Amazon S3 pricing.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nObjects from the S3 Intelligent-Tiering Archive Access and Deep Archive Access tiers\\nare automatically restored to the Frequent Access tier.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api restore-object --bucket example-s3-bucket --key dir1/example.obj --restore-\\nrequest \\'{\"Days\":25,\"GlacierJobParameters\":{\"Tier\":\"Standard\"}}\\'', ''], ['', '', '']], [['', '', ''], ['', '--restore-request Days=25,GlacierJobParameters={\"Tier\"=\"Standard\"}', ''], ['', '', '']], [['', '', ''], ['', \"aws s3api restore-object --bucket example-s3-bucket --key dir1/example.obj --restore-\\nrequest '{}'\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nUnlike in the S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive storage classes,\\nrestore requests for S3 Intelligent-Tiering objects don't accept the Days value.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api head-object --bucket example-s3-bucket --key dir1/example.obj', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nBatch Operations jobs can operate either on S3 Glacier Flexible Retrieval and S3 Glacier\\nDeep Archive storage class objects or on S3 Intelligent-Tiering Archive Access and Deep\\nArchive Access storage tier objects. Batch Operations can't operate on both types of\\narchived objects in the same job. To restore objects of both types, you must create separate\\nBatch Operations jobs.\", '']]]\n",
      "[[['', 'For more information about using Batch Operations to restore archive objects, see the\\nsection called “Restore objects”.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nObjects restored from the S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive storage\\nclasses are stored only for the number of days that you specify. The following procedures\\nreturn the expiration date for these copies.\\nObjects restored from the S3 Intelligent-Tiering Archive Access and Deep Archive Access\\nstorage tiers don't have expiration dates and instead are moved back to the Frequent\\nAccess tier.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api head-object --bucket example-s3-bucket --key dir1/example.obj', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Restore\": \"ongoing-request=\\\\\"true\\\\\"\",\\n\"LastModified\": \"2020-06-16T21:55:22+00:00\",\\n\"ContentLength\": 405,\\n\"ETag\": \"\\\\\"b662d79adeb7c8d787ea7eafb9ef6207\\\\\"\",\\n\"VersionId\": \"wbYaE2vtOV0iIBXrOqGAJt3fP1cHB8Wi\",\\n\"ContentType\": \"binary/octet-stream\",\\n\"ServerSideEncryption\": \"AES256\",\\n\"Metadata\": {},', '']]]\n",
      "[[['', '\"StorageClass\": \"GLACIER\"\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Restore\": \"ongoing-request=\\\\\"false\\\\\", expiry-date=\\\\\"Wed, 12 Aug 2020 00:00:00 GMT\\n\\\\\"\",\\n\"LastModified\": \"2020-06-16T21:55:22+00:00\",\\n\"ContentLength\": 405,\\n\"ETag\": \"\\\\\"b662d79adeb7c8d787ea7eafb9ef6207\\\\\"\",\\n\"VersionId\": \"wbYaE2vtOV0iIBXrOqGAJt3fP1cHB8Wi\",\\n\"ContentType\": \"binary/octet-stream\",\\n\"ServerSideEncryption\": \"AES256\",\\n\"Metadata\": {},\\n\"StorageClass\": \"GLACIER\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nS3 Object Lock has been assessed by Cohasset Associates for use in environments that are\\nsubject to SEC 17a-4, CFTC, and FINRA regulations. For more information about how Object\\nLock relates to these regulations, see the Cohasset Associates Compliance Assessment.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nWhen you PUT an object version that has an explicit individual retention mode and period\\nin a bucket, the object version's individual Object Lock settings override any bucket\\nproperty retention settings.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe only way to delete an object under the compliance mode before its retention date\\nexpires is to delete the associated AWS account.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nBy default, the Amazon S3 console includes the x-amz-bypass-governance-\\nretention:true header. If you try to delete objects protected by governance mode and\\nhave the s3:BypassGovernanceRetention permission, the operation will succeed.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\n• After you enable Object Lock on a bucket, you can't disable Object Lock or suspend\\nversioning for that bucket.\\n• S3 buckets with Object Lock can't be used as destination buckets for server access logs.\\nFor more information, see the section called “Logging server access”.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nBypassing governance mode doesn't affect an object version's legal hold status. If an object\\nversion has a legal hold enabled, the legal hold remains and prevents requests to overwrite\\nor delete the object version.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo use Object Lock with replication, you must grant two additional permissions on the\\nsource S3 bucket in the AWS Identity and Access Management (IAM) role that you use', '']]]\n",
      "[[['', 'to set up replication. The two additional permissions are s3:GetObjectRetention\\nand s3:GetObjectLegalHold. If the role has an s3:Get* permission statement, that\\nstatement satisfies the requirement. For more information, see Setting up permissions for\\nlive replication.\\nFor general information about S3 Replication, see Replicating objects overview.\\nFor examples of setting up S3 Replication, see Examples for configuring live replication.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nDelete markers are not WORM-protected, regardless of any retention period or legal hold in\\nplace on the underlying object.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"SetRetentionLimits\",\\n\"Statement\": [\\n{\\n\"Sid\": \"SetRetentionPeriod\",\\n\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": [\\n\"s3:PutObjectRetention\"\\n],\\n\"Resource\": \"arn:aws:s3:::example-s3-bucket1/*\",\\n\"Condition\": {\\n\"NumericGreaterThan\": {\\n\"s3:object-lock-remaining-retention-days\": \"10\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf your bucket is the destination bucket for a replication configuration, you can set up\\nminimum and maximum allowable retention periods for object replicas that are created\\nby using replication. To do so, you must allow the s3:ReplicateObject action in your\\nbucket policy. For more information about replication permissions, see the section called\\n“Setting up permissions”.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\n• After you enable Object Lock on a bucket, you can't disable Object Lock or suspend\\nversioning for that bucket.\\n• S3 buckets with Object Lock can't be used as destination buckets for server access logs.\\nFor more information, see the section called “Logging server access”.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nAfter you create a bucket, you can't change its name. For more information about\\nnaming buckets, see Bucket naming rules.\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api create-bucket --bucket example-s3-bucket1 --object-lock-enabled-for-bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou can run AWS CLI commands from the console by using AWS CloudShell. AWS\\nCloudShell is a browser-based, pre-authenticated shell that you can launch directly from\\nthe AWS Management Console. For more information, see What is CloudShell? in the AWS\\nCloudShell User Guide.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nObject Lock works only with versioned buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-object-lock-configuration --bucket example-s3-bucket1 --object-lock-\\nconfiguration=\\'{ \"ObjectLockEnabled\": \"Enabled\", \"Rule\": { \"DefaultRetention\":\\n{ \"Mode\": \"COMPLIANCE\", \"Days\": 50 }}}\\'', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can run AWS CLI commands from the console by using AWS CloudShell. AWS\\nCloudShell is a browser-based, pre-authenticated shell that you can launch directly from\\nthe AWS Management Console. For more information, see What is CloudShell? in the AWS\\nCloudShell User Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\n• If you want to set a legal hold on an object, the object's bucket must already have Object\\nLock enabled.\\n• When you PUT an object version that has an explicit individual retention mode and\\nperiod in a bucket, the object version's individual Object Lock settings override any\\nbucket property retention settings.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-object-legal-hold --bucket example-s3-bucket1 --key my-image.fs --legal-\\nhold=\"Status=ON\"', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object-legal-hold --bucket example-s3-bucket1 --key my-image.fs --legal-\\nhold=\"Status=OFF\"', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can run AWS CLI commands from the console by using AWS CloudShell. AWS\\nCloudShell is a browser-based, pre-authenticated shell that you can launch directly from\\nthe AWS Management Console. For more information, see What is CloudShell? in the AWS\\nCloudShell User Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\n• If you want to set a retention period on an object, the object's bucket must already have\\nObject Lock enabled.\\n• When you PUT an object version that has an explicit individual retention mode and\\nperiod in a bucket, the object version's individual Object Lock settings override any\\nbucket property retention settings.\\n• The only way to delete an object under the compliance mode before its retention date\\nexpires is to delete the associated AWS account.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-object-retention --bucket example-s3-bucket1 --key my-image.fs --\\nretention=\\'{ \"Mode\": \"GOVERNANCE\", \"RetainUntilDate\": \"2025-01-01T00:00:00\" }\\'', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can run AWS CLI commands from the console by using AWS CloudShell. AWS\\nCloudShell is a browser-based, pre-authenticated shell that you can launch directly from\\nthe AWS Management Console. For more information, see What is CloudShell? in the AWS\\nCloudShell User Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\n• If you want to set a default retention period on a bucket, the bucket must already have\\nObject Lock enabled.\\n• When you PUT an object version that has an explicit individual retention mode and\\nperiod in a bucket, the object version's individual Object Lock settings override any\\nbucket property retention settings.\\n• The only way to delete an object under the compliance mode before its retention date\\nexpires is to delete the associated AWS account.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-object-lock-configuration --bucket example-s3-bucket1 --object-lock-\\nconfiguration=\\'{ \"ObjectLockEnabled\": \"Enabled\", \"Rule\": { \"DefaultRetention\":\\n{ \"Mode\": \"COMPLIANCE\", \"Days\": 50 }}}\\'', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object-lock-configuration --bucket example-s3-bucket1 --object-lock-\\nconfiguration=\\'{ \"ObjectLockEnabled\": \"Enabled\"}\\'', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can run AWS CLI commands from the console by using AWS CloudShell. AWS\\nCloudShell is a browser-based, pre-authenticated shell that you can launch directly from\\nthe AWS Management Console. For more information, see What is CloudShell? in the AWS\\nCloudShell User Guide.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nWe recommend not using this storage class. The S3 Standard storage class is more cost-\\neffective.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• Only activate the Archive Access tier for 90 days if you want to bypass the Archive Instant\\nAccess tier. The Archive Access tier delivers slightly lower-cost storage with minute-to-\\nhour retrieval times. The Archive Instant Access tier delivers millisecond access and high-\\nthroughput performance.\\n• Activate the Archive Access and Deep Archive Access tiers only if your objects can be\\naccessed asynchronously by your application. If the object that you are retrieving is\\nstored in the Archive Access or Deep Archive Access tiers, first restore the object by using\\nRestoreObject.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf the size of an object is less than 128 KB, it is not monitored and not eligible for auto-\\ntiering. Smaller objects are always stored in the Frequent Access tier. For more information\\nabout S3 Intelligent-Tiering, see S3 Intelligent-Tiering access tiers.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe S3 Standard-IA and S3 One Zone-IA storage classes are suitable for objects larger than\\n128 KB that you plan to store for at least 30 days. If an object is less than 128 KB, Amazon\\nS3 charges you for 128 KB. If you delete an object before the end of the 30-day minimum', '']]]\n",
      "[[['', 'storage duration period, you are charged for 30 days. Objects that are deleted, overwritten,\\nor transitioned to a different storage class before 30 days will incur the normal storage\\nusage charge plus a pro-rated charge for the remainder of the 30-day minimum. For pricing\\ninformation, see Amazon S3 pricing.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nWhen you use S3 Glacier storage classes, your objects remain in Amazon S3. You can't\\naccess them directly through the separate Amazon S3 Glacier service. For information\\nabout the Amazon S3 Glacier service, see the Amazon S3 Glacier Developer Guide.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nS3 on Outposts doesn't support server-side encryption with AWS Key Management Service\\n(AWS KMS) keys (SSE-KMS).\", ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nWe recommend using the S3 Glacier storage classes within the Amazon S3 service for all of\\nyour long-term data.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf the size of an object is less than 128 KB, it is not monitored and is not eligible for\\nautomatic tiering. Smaller objects are always stored in the Frequent Access tier.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nOnly activate the Archive Access tier for 90 days if you want to bypass the Archive\\nInstant Access tier. The Archive Access tier delivers slightly lower storage costs, with\\nminute-to-hour retrieval times. The Archive Instant Access tier delivers millisecond\\naccess and high-throughput performance.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nActivate the Archive Access and Deep Archive Access tiers only if your objects can be\\naccessed asynchronously by your application. If the object that you are retrieving is stored\\nin the Archive Access or Deep Archive Access tiers, you must first restore the object by using\\nthe RestoreObject operation.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'PUT /my-image.jpg HTTP/1.1\\nHost: myBucket.s3.<Region>.amazonaws.com (http://amazonaws.com/)\\nDate: Wed, 1 Sep 2021 17:50:00 GMT\\nAuthorization: authorization string\\nContent-Type: image/jpeg\\nContent-Length: 11434\\nExpect: 100-continue\\nx-amz-storage-class: INTELLIGENT_TIERING', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>ExampleRule</ID>\\n<Filter>\\n<Prefix>documents/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<Days>0</Days>\\n<StorageClass>INTELLIGENT_TIERING</StorageClass>\\n</Transition>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Id\": \"string\",\\n\"Filter\": {\\n\"Prefix\": \"string\",\\n\"Tag\": {\\n\"Key\": \"string\",\\n\"Value\": \"string\"', '']]]\n",
      "[[['', '},\\n\"And\": {\\n\"Prefix\": \"string\",\\n\"Tags\": [\\n{\\n\"Key\": \"string\",\\n\"Value\": \"string\"\\n}\\n...\\n]\\n}\\n},\\n\"Status\": \"Enabled\"|\"Disabled\",\\n\"Tierings\": [\\n{\\n\"Days\": integer,\\n\"AccessTier\": \"ARCHIVE_ACCESS\"|\"DEEP_ARCHIVE_ACCESS\"\\n}\\n...\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'PUT /?intelligent-tiering&id=Id HTTP/1.1\\nHost: Bucket.s3.amazonaws.com\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<IntelligentTieringConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Id>string</Id>\\n<Filter>\\n<And>\\n<Prefix>string</Prefix>\\n<Tag>\\n<Key>string</Key>\\n<Value>string</Value>\\n</Tag>\\n...\\n</And>\\n<Prefix>string</Prefix>\\n<Tag>\\n<Key>string</Key>\\n<Value>string</Value>\\n</Tag>', '']]]\n",
      "[[['', '</Filter>\\n<Status>string</Status>\\n<Tiering>\\n<AccessTier>string</AccessTier>\\n<Days>integer</Days>\\n</Tiering>\\n...\\n</IntelligentTieringConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Records\":[\\n{\\n\"eventVersion\":\"2.3\",\\n\"eventSource\":\"aws:s3\",\\n\"awsRegion\":\"us-west-2\",\\n\"eventTime\":\"1970-01-01T00:00:00.000Z\",\\n\"eventName\":\"IntelligentTiering\",\\n\"userIdentity\":{\\n\"principalId\":\"s3.amazonaws.com\"\\n},\\n\"requestParameters\":{\\n\"sourceIPAddress\":\"s3.amazonaws.com\"\\n},\\n\"responseElements\":{\\n\"x-amz-request-id\":\"C3D13FE58DE4C810\",\\n\"x-amz-id-2\":\"FMyUVURIY8/IgAtTv8xRjskZQpcIZ9KG4V5Wp6S7S/\\nJRWeUWerMUE5JgHvANOjpD\"\\n},\\n\"s3\":{', '']]]\n",
      "[[['', '\"s3SchemaVersion\":\"1.0\",\\n\"configurationId\":\"testConfigRule\",\\n\"bucket\":{\\n\"name\":\"mybucket\",\\n\"ownerIdentity\":{\\n\"principalId\":\"A3NL1KOZZKExample\"\\n},\\n\"arn\":\"arn:aws:s3:::mybucket\"\\n},\\n\"object\":{\\n\"key\":\"HappyFace.jpg\",\\n\"size\":1024,\\n\"eTag\":\"d41d8cd98f00b204e9800998ecf8427e\",\\n}\\n},\\n\"intelligentTieringEventData\":{\\n\"destinationAccessTier\": \"ARCHIVE_ACCESS\"\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'HEAD /my-image.jpg HTTP/1.1\\nHost: bucket.s3.region.amazonaws.com\\nDate: Wed, 28 Oct 2009 22:32:00 GMT\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:02236Q3V0RonhpaBX5sCYVf1bNRuU=', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'HTTP/1.1 200 OK\\nx-amz-id-2: FSVaTMjrmBp3Izs1NnwBZeu7M19iI8UbxMbi0A8AirHANJBo+hEftBuiESACOMJp\\nx-amz-request-id: E5CEFCB143EB505A\\nDate: Fri, 13 Nov 2020 00:28:38 GMT\\nLast-Modified: Mon, 15 Oct 2012 21:58:07 GMT\\nETag: \"1accb31fcf202eba0c0f41fa2f09b4d7\"\\nx-amz-storage-class: \\'INTELLIGENT_TIERING\\'\\nx-amz-archive-status: \\'ARCHIVE_ACCESS\\'\\nx-amz-restore: \\'ongoing-request=\"true\"\\'\\nx-amz-restore-request-date: \\'Fri, 13 Nov 2020 00:20:00 GMT\\'\\nAccept-Ranges: bytes\\nContent-Type: binary/octet-stream\\nContent-Length: 300\\nServer: AmazonS3', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nThere are no data retrieval charges for lifecycle transitions. However, there are per-request\\ningestion charges when using PUT, COPY, or lifecycle rules to move data into any S3 storage\\nclass. Consider the ingestion or transition cost before moving objects into any storage class.\\nFor more information about cost considerations, see Amazon S3 pricing.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou can filter lifecycle rules based on object size.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you have multiple rules in an S3 Lifecycle configuration, an object can become\\neligible for multiple S3 Lifecycle actions. In such cases, Amazon S3 follows these general\\nrules:\\n• Permanent deletion takes precedence over transition.\\n• Transition takes precedence over creation of delete markers.\\n• When an object is eligible for both a S3 Glacier Flexible Retrieval and S3 Standard-IA\\n(or S3 One Zone-IA) transition, Amazon S3 chooses the S3 Glacier Flexible Retrieval\\ntransition.\\nFor examples, see Example 5: Overlapping filters, conflicting lifecycle actions, and what\\nAmazon S3 does with nonversioned buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nThe Copy operation for restored objects isn't supported in the Amazon S3 console for\\nobjects in the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes. For\\nthis type of Copy operation, use the AWS Command Line Interface (AWS CLI), the AWS\\nSDKs, or the REST API.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nWhen you restore an archive, you are paying for both the archive (S3 Glacier Flexible\\nRetrieval or S3 Glacier Deep Archive rate) and a copy that you restored temporarily (S3\\nStandard storage rate). For information about pricing, see Amazon S3 pricing.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nWhen you have multiple rules in an S3 Lifecycle configuration, an object can become\\neligible for multiple S3 Lifecycle actions. In such cases, Amazon S3 follows these general\\nrules:', '']]]\n",
      "[[['', '• Permanent deletion takes precedence over transition.\\n• Transition takes precedence over creation of delete markers.\\n• When an object is eligible for both a S3 Glacier Flexible Retrieval and S3 Standard-IA\\n(or S3 One Zone-IA) transition, Amazon S3 chooses the S3 Glacier Flexible Retrieval\\ntransition.\\nFor examples, see Example 5: Overlapping filters, conflicting lifecycle actions, and what\\nAmazon S3 does with nonversioned buckets.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\n• There may be a delay between the expiration date and the date at which Amazon S3\\nremoves an object. You are not charged for expiration or the storage time associated with\\nan object that has expired.\\n• Before updating, disabling, or deleting Lifecycle rules, use the LIST API operations (such\\nas ListObjectsV2, ListObjectVersions, and ListMultipartUploads) or Amazon S3 Inventory\\nto verify that Amazon S3 has transitioned and expired eligible objects based on your use\\ncases.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nPrior to updating, disabling, or deleting Lifecycle rules, use the LIST API operations (such\\nas ListObjectsV2, ListObjectVersions, and ListMultipartUploads) or Amazon S3 Inventory', '']]]\n",
      "[[['', \"to verify that Amazon S3 has transitioned and expired eligible objects based on your use\\ncases. If you're experiencing any issues with updating, disabling, or deleting Lifecycle rules,\\nsee Troubleshoot Amazon S3 Lifecycle issues.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nWhen you choose the S3 Glacier Flexible Retrieval or Glacier Deep Archive storage\\nclass, your objects remain in Amazon S3. You cannot access them directly through the\\nseparate Amazon S3 Glacier service. For more information, see Transitioning objects\\nusing Amazon S3 Lifecycle.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIn a nonversioned bucket the expiration action results in Amazon S3 permanently\\nremoving the object. For more information about lifecycle actions, see Elements to\\ndescribe lifecycle actions.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>ExampleRule</ID>\\n<Filter>\\n<Prefix>documents/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<Days>365</Days>\\n<StorageClass>GLACIER</StorageClass>\\n</Transition>\\n<Expiration>\\n<Days>3650</Days>\\n</Expiration>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Rules\": [\\n{\\n\"Filter\": {\\n\"Prefix\": \"documents/\"', '']]]\n",
      "[[['', '},\\n\"Status\": \"Enabled\",\\n\"Transitions\": [\\n{\\n\"Days\": 365,\\n\"StorageClass\": \"GLACIER\"\\n}\\n],\\n\"Expiration\": {\\n\"Days\": 3650\\n},\\n\"ID\": \"ExampleRule\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Rule>\\n<ID>id-1</ID>\\n<Expiration>\\n<Days>1</Days>\\n</Expiration>\\n<Filter>\\n<And>\\n<Prefix>myprefix</Prefix>\\n<Tag>\\n<Key>mytagkey1</Key>\\n<Value>mytagvalue1</Value>\\n</Tag>\\n<Tag>\\n<Key>mytagkey2</Key>\\n<Value>mytagvalue2</Value>\\n</Tag>\\n</And>\\n</Filter>\\n<Status>Enabled</Status>\\n</Rule>', '']]]\n",
      "[[['', '</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Rules\": [\\n{\\n\"ID\": \"id-1\",\\n\"Filter\": {\\n\"And\": {\\n\"Prefix\": \"myprefix\",\\n\"Tags\": [\\n{\\n\"Value\": \"mytagvalue1\",\\n\"Key\": \"mytagkey1\"\\n},\\n{\\n\"Value\": \"mytagvalue2\",\\n\"Key\": \"mytagkey2\"\\n}\\n]\\n}\\n},\\n\"Status\": \"Enabled\",\\n\"Expiration\": {\\n\"Days\": 1\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '$ aws s3api put-bucket-lifecycle-configuration \\\\', '']]]\n",
      "[[['', '--bucket DOC-EXAMPLE-BUCKET \\\\\\n--lifecycle-configuration file://lifecycle.json', ''], ['', '', '']], [['', '', ''], ['', '$ aws s3api get-bucket-lifecycle-configuration \\\\\\n--bucket DOC-EXAMPLE-BUCKET', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-bucket-lifecycle \\\\\\n--bucket DOC-EXAMPLE-BUCKET', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nWhen you add S3 Lifecycle configuration to a bucket, Amazon S3 replaces the bucket's\\ncurrent Lifecycle configuration, if there is one. To update a configuration, you retrieve it,\\nmake the desired changes, and then add the revised configuration to the bucket.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.BucketLifecycleConfiguration;\\nimport com.amazonaws.services.s3.model.BucketLifecycleConfiguration.Transition;\\nimport com.amazonaws.services.s3.model.StorageClass;\\nimport com.amazonaws.services.s3.model.Tag;\\nimport com.amazonaws.services.s3.model.lifecycle.LifecycleAndOperator;\\nimport com.amazonaws.services.s3.model.lifecycle.LifecycleFilter;\\nimport com.amazonaws.services.s3.model.lifecycle.LifecyclePrefixPredicate;\\nimport com.amazonaws.services.s3.model.lifecycle.LifecycleTagPredicate;\\nimport java.io.IOException;\\nimport java.util.Arrays;\\npublic class LifecycleConfiguration {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\n// Create a rule to archive objects with the \"glacierobjects/\"\\nprefix to Glacier\\n// immediately.\\nBucketLifecycleConfiguration.Rule rule1 = new\\nBucketLifecycleConfiguration.Rule()\\n.withId(\"Archive immediately rule\")\\n.withFilter(new LifecycleFilter(new\\nLifecyclePrefixPredicate(\"glacierobjects/\")))', '']]]\n",
      "[[['', '.addTransition(new\\nTransition().withDays(0).withStorageClass(StorageClass.Glacier))\\n.withStatus(BucketLifecycleConfiguration.ENABLED);\\n// Create a rule to transition objects to the Standard-Infrequent\\nAccess storage\\n// class\\n// after 30 days, then to Glacier after 365 days. Amazon S3 will\\ndelete the\\n// objects after 3650 days.\\n// The rule applies to all objects with the tag \"archive\" set to\\n\"true\".\\nBucketLifecycleConfiguration.Rule rule2 = new\\nBucketLifecycleConfiguration.Rule()\\n.withId(\"Archive and then delete rule\")\\n.withFilter(new LifecycleFilter(new\\nLifecycleTagPredicate(new Tag(\"archive\", \"true\"))))\\n.addTransition(new Transition().withDays(30)\\n.withStorageClass(StorageClass.StandardInfrequentAccess))\\n.addTransition(new\\nTransition().withDays(365).withStorageClass(StorageClass.Glacier))\\n.withExpirationInDays(3650)\\n.withStatus(BucketLifecycleConfiguration.ENABLED);\\n// Add the rules to a new BucketLifecycleConfiguration.\\nBucketLifecycleConfiguration configuration = new\\nBucketLifecycleConfiguration()\\n.withRules(Arrays.asList(rule1, rule2));\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new\\nProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\n// Save the configuration.\\ns3Client.setBucketLifecycleConfiguration(bucketName,\\nconfiguration);\\n// Retrieve the configuration.\\nconfiguration =\\ns3Client.getBucketLifecycleConfiguration(bucketName);', '']]]\n",
      "[[['', '// Add a new rule with both a prefix predicate and a tag\\npredicate.\\nconfiguration.getRules().add(new\\nBucketLifecycleConfiguration.Rule().withId(\"NewRule\")\\n.withFilter(new LifecycleFilter(new\\nLifecycleAndOperator(\\nArrays.asList(new\\nLifecyclePrefixPredicate(\"YearlyDocuments/\"),\\nnew\\nLifecycleTagPredicate(new Tag(\\n\"expire_after\",\\n\"ten_years\"))))))\\n.withExpirationInDays(3650)\\n.withStatus(BucketLifecycleConfiguration.ENABLED));\\n// Save the configuration.\\ns3Client.setBucketLifecycleConfiguration(bucketName,\\nconfiguration);\\n// Retrieve the configuration.\\nconfiguration =\\ns3Client.getBucketLifecycleConfiguration(bucketName);\\n// Verify that the configuration now has three rules.\\nconfiguration =\\ns3Client.getBucketLifecycleConfiguration(bucketName);\\nSystem.out.println(\"Expected # of rules = 3; found: \" +\\nconfiguration.getRules().size());\\n// Delete the configuration.\\ns3Client.deleteBucketLifecycleConfiguration(bucketName);\\n// Verify that the configuration has been deleted by\\nattempting to retrieve it.\\nconfiguration =\\ns3Client.getBucketLifecycleConfiguration(bucketName);\\nString s = (configuration == null) ? \"No configuration\\nfound.\" : \"Configuration found.\";\\nSystem.out.println(s);\\n} catch (AmazonServiceException e) {', '']]]\n",
      "[[['', \"// The call was transmitted successfully, but Amazon S3\\ncouldn't process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the\\nclient\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen you add a Lifecycle configuration, Amazon S3 replaces the existing configuration\\non the specified bucket. To update a configuration, you must first retrieve the Lifecycle\\nconfiguration, make the changes, and then add the revised Lifecycle configuration to\\nthe bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass LifecycleTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nclient = new AmazonS3Client(bucketRegion);\\nAddUpdateDeleteLifecycleConfigAsync().Wait();\\n}\\nprivate static async Task AddUpdateDeleteLifecycleConfigAsync()\\n{\\ntry\\n{\\nvar lifeCycleConfiguration = new LifecycleConfiguration()\\n{\\nRules = new List<LifecycleRule>\\n{\\nnew LifecycleRule\\n{\\nId = \"Archive immediately rule\",\\nFilter = new LifecycleFilter()\\n{\\nLifecycleFilterPredicate = new\\nLifecyclePrefixPredicate()\\n{\\nPrefix = \"glacierobjects/\"\\n}', '']]]\n",
      "[[['', '},\\nStatus = LifecycleRuleStatus.Enabled,\\nTransitions = new List<LifecycleTransition>\\n{\\nnew LifecycleTransition\\n{\\nDays = 0,\\nStorageClass = S3StorageClass.Glacier\\n}\\n},\\n},\\nnew LifecycleRule\\n{\\nId = \"Archive and then delete rule\",\\nFilter = new LifecycleFilter()\\n{\\nLifecycleFilterPredicate = new\\nLifecyclePrefixPredicate()\\n{\\nPrefix = \"projectdocs/\"\\n}\\n},\\nStatus = LifecycleRuleStatus.Enabled,\\nTransitions = new List<LifecycleTransition>\\n{\\nnew LifecycleTransition\\n{\\nDays = 30,\\nStorageClass =\\nS3StorageClass.StandardInfrequentAccess\\n},\\nnew LifecycleTransition\\n{\\nDays = 365,\\nStorageClass = S3StorageClass.Glacier\\n}\\n},\\nExpiration = new LifecycleRuleExpiration()\\n{\\nDays = 3650\\n}\\n}\\n}\\n};', '']]]\n",
      "[[['', '// Add the configuration to the bucket.\\nawait AddExampleLifecycleConfigAsync(client,\\nlifeCycleConfiguration);\\n// Retrieve an existing configuration.\\nlifeCycleConfiguration = await RetrieveLifecycleConfigAsync(client);\\n// Add a new rule.\\nlifeCycleConfiguration.Rules.Add(new LifecycleRule\\n{\\nId = \"NewRule\",\\nFilter = new LifecycleFilter()\\n{\\nLifecycleFilterPredicate = new LifecyclePrefixPredicate()\\n{\\nPrefix = \"YearlyDocuments/\"\\n}\\n},\\nExpiration = new LifecycleRuleExpiration()\\n{\\nDays = 3650\\n}\\n});\\n// Add the configuration to the bucket.\\nawait AddExampleLifecycleConfigAsync(client,\\nlifeCycleConfiguration);\\n// Verify that there are now three rules.\\nlifeCycleConfiguration = await RetrieveLifecycleConfigAsync(client);\\nConsole.WriteLine(\"Expected # of rulest=3; found:{0}\",\\nlifeCycleConfiguration.Rules.Count);\\n// Delete the configuration.\\nawait RemoveLifecycleConfigAsync(client);\\n// Retrieve a nonexistent configuration.\\nlifeCycleConfiguration = await RetrieveLifecycleConfigAsync(client);\\n}\\ncatch (AmazonS3Exception e)\\n{', '']]]\n",
      "[[['', 'Console.WriteLine(\"Error encountered ***. Message:\\'{0}\\' when writing\\nan object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\nstatic async Task AddExampleLifecycleConfigAsync(IAmazonS3 client,\\nLifecycleConfiguration configuration)\\n{\\nPutLifecycleConfigurationRequest request = new\\nPutLifecycleConfigurationRequest\\n{\\nBucketName = bucketName,\\nConfiguration = configuration\\n};\\nvar response = await client.PutLifecycleConfigurationAsync(request);\\n}\\nstatic async Task<LifecycleConfiguration>\\nRetrieveLifecycleConfigAsync(IAmazonS3 client)\\n{\\nGetLifecycleConfigurationRequest request = new\\nGetLifecycleConfigurationRequest\\n{\\nBucketName = bucketName\\n};\\nvar response = await client.GetLifecycleConfigurationAsync(request);\\nvar configuration = response.Configuration;\\nreturn configuration;\\n}\\nstatic async Task RemoveLifecycleConfigAsync(IAmazonS3 client)\\n{\\nDeleteLifecycleConfigurationRequest request = new\\nDeleteLifecycleConfigurationRequest\\n{\\nBucketName = bucketName\\n};\\nawait client.DeleteLifecycleConfigurationAsync(request);', '']]]\n",
      "[[['', '}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you have multiple rules in an S3 Lifecycle configuration, an object can become\\neligible for multiple S3 Lifecycle actions. In such cases, Amazon S3 follows these general\\nrules:', '']]]\n",
      "[[['', '• Permanent deletion takes precedence over transition.\\n• Transition takes precedence over creation of delete markers.\\n• When an object is eligible for both a S3 Glacier Flexible Retrieval and S3 Standard-IA\\n(or S3 One Zone-IA) transition, Amazon S3 chooses the S3 Glacier Flexible Retrieval\\ntransition.\\nFor examples, see Example 5: Overlapping filters, conflicting lifecycle actions, and what\\nAmazon S3 does with nonversioned buckets.', ''], ['', '', '']], [['Operation log', 'Description'], ['S3.EXPIRE.OBJECT', 'Amazon S3 permanently deletes the object\\nbecause of the Lifecycle expiration action.'], ['S3.CREATE.DELETEMARKER', 'Amazon S3 logically deletes the current\\nversion and adds a delete marker in a\\nversioning-enabled bucket.'], ['S3.TRANSITION_SIA.OBJECT', 'Amazon S3 transitions the object to the S3\\nStandard-IA storage class.']]]\n",
      "[[['Operation log', 'Description'], ['S3.TRANSITION_ZIA.OBJECT', 'Amazon S3 transitions the object to the S3\\nOne Zone-IA storage class.'], ['S3.TRANSITION_INT.OBJECT', 'Amazon S3 transitions the object to the S3\\nIntelligent-Tiering storage class.'], ['S3.TRANSITION_GIR.OBJECT', 'Amazon S3 initiates the transition of the\\nobject to the S3 Glacier Instant Retrieval\\nstorage class.'], ['S3.TRANSITION.OBJECT', 'Amazon S3 initiates the transition of the\\nobject to the S3 Glacier Flexible Retrieval\\nstorage class.'], ['S3.TRANSITION_GDA.OBJECT', 'Amazon S3 initiates the transition of the\\nobject to the S3 Glacier Deep Archive storage\\nclass.'], ['S3.DELETE.UPLOAD', 'Amazon S3 aborts an incomplete multipart\\nupload.']], [['', '', ''], ['', 'Note\\nAmazon S3 server access log records are generally delivered on a best-effort basis and\\ncannot be used for complete accounting of all Amazon S3 requests.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Records\":[\\n{\\n\"eventVersion\":\"2.3\",\\n\"eventSource\":\"aws:s3\",\\n\"awsRegion\":\"us-west-2\",\\n\"eventTime\":\"1970-01-01T00:00:00.000Z\",\\n\"eventName\":\"LifecycleExpiration:Delete\",\\n\"userIdentity\":{', '']]]\n",
      "[[['', '\"principalId\":\"s3.amazonaws.com\"\\n},\\n\"requestParameters\":{\\n\"sourceIPAddress\":\"s3.amazonaws.com\"\\n},\\n\"responseElements\":{\\n\"x-amz-request-id\":\"C3D13FE58DE4C810\",\\n\"x-amz-id-2\":\"FMyUVURIY8/IgAtTv8xRjskZQpcIZ9KG4V5Wp6S7S/\\nJRWeUWerMUE5JgHvANOjpD\"\\n},\\n\"s3\":{\\n\"s3SchemaVersion\":\"1.0\",\\n\"configurationId\":\"testConfigRule\",\\n\"bucket\":{\\n\"name\":\"example-s3-bucket\",\\n\"ownerIdentity\":{\\n\"principalId\":\"A3NL1KOZZKExample\"\\n},\\n\"arn\":\"arn:aws:s3:::example-s3-bucket\"\\n},\\n\"object\":{\\n\"key\":\"expiration/delete\",\\n\"sequencer\":\"0055AED6DCD90281E5\",\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"lifecycleEventData\":{\\n\"transitionEventData\": {\\n\"destinationStorageClass\": the destination storage class for the object\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n...\\n</Rule>\\n<Rule>\\n...\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<Filter>\\n<Prefix>logs/</Prefix>\\n</Filter>\\ntransition/expiration actions\\n...\\n</Rule>\\n...\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<Filter>\\n<Prefix>projectA/</Prefix>\\n</Filter>\\ntransition/expiration actions\\n...\\n</Rule>\\n<Rule>\\n<Filter>\\n<Prefix>projectB/</Prefix>', '']]]\n",
      "[[['', '</Filter>\\ntransition/expiration actions\\n...\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<Filter>\\n<Tag>\\n<Key>key</Key>\\n<Value>value</Value>\\n</Tag>\\n</Filter>\\ntransition/expiration actions\\n...\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<Filter>\\n<And>\\n<Tag>\\n<Key>key1</Key>\\n<Value>value1</Value>\\n</Tag>\\n<Tag>\\n<Key>key2</Key>\\n<Value>value2</Value>\\n</Tag>\\n...', '']]]\n",
      "[[['', '</And>\\n</Filter>\\ntransition/expiration actions\\n</Rule>\\n</Lifecycle>', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen you specify multiple tags in a filter, each tag key must be unique.', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<Filter>\\n<And>\\n<Prefix>key-prefix</Prefix>\\n<Tag>\\n<Key>key1</Key>\\n<Value>value1</Value>\\n</Tag>\\n<Tag>\\n<Key>key2</Key>\\n<Value>value2</Value>\\n</Tag>\\n...\\n</And>\\n</Filter>\\n<Status>Enabled</Status>\\ntransition/expiration actions', '']]]\n",
      "[[['', '</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<Filter>\\n</Filter>\\n<Status>Enabled</Status>\\ntransition/expiration actions\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<Filter>\\n<ObjectSizeGreaterThan>500</ObjectSizeGreaterThan>\\n</Filter>\\n<Status>Enabled</Status>\\ntransition/expiration actions\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>', '']]]\n",
      "[[['', '<Filter>\\n<And>\\n<Prefix>key-prefix</Prefix>\\n<ObjectSizeGreaterThan>500</ObjectSizeGreaterThan>\\n<ObjectSizeLessThan>64000</ObjectSizeLessThan>\\n</And>\\n</Filter>\\n<Status>Enabled</Status>\\ntransition/expiration actions\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nObject expiration lifecycle configurations don't remove incomplete multipart\\nuploads. To remove incomplete multipart uploads, you must use the\", '']]]\n",
      "[[['', \"AbortIncompleteMultipartUpload Lifecycle configuration action that's described\\nlater in this section.\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nYou can't specify this lifecycle action in a rule that has a filter that uses object tags.\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nYou can't specify this lifecycle action in a rule that has a filter that uses object tags.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Restoring previous versions of an object when using S3 Lifecycle configurations\\nAs explained in Restoring previous versions, you can use either of the following two\\nmethods to retrieve previous versions of an object:\\n• Method 1 – Copy a noncurrent version of the object into the same bucket. The copied\\nobject becomes the current version of that object, and all object versions are preserved.\\n• Method 2 – Permanently delete the current version of the object. When you delete the\\ncurrent object version, you, in effect, turn the noncurrent version into the current version\\nof that object.\\nWhen you're using S3 Lifecycle configuration rules with versioning-enabled buckets, we\\nrecommend as a best practice that you use Method 1.\\nS3 Lifecycle operates under an eventually consistent model. A current version that you\\npermanently deleted might not disappear until the changes propagate to all of the Amazon\\nS3 systems. (Therefore, Amazon S3 might be temporarily unaware of this deletion.) In\\nthe meantime, the lifecycle rule that you configured to expire noncurrent objects might\\npermanently remove noncurrent objects, including the one that you want to restore. So,\\ncopying the old version, as recommended in Method 1, is the safer alternative.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nAmazon S3 maintains only the last modified date for each object. For example, the\\nAmazon S3 console shows the Last modified date in the object's Properties pane. When\\nyou initially create a new object, this date reflects the date that the object is created.\\nIf you replace the object, the date changes accordingly. Therefore, the creation date is\\nsynonymous with the Last modified date.\", ''], ['', '', '']], [['', '', ''], ['', 'Important\\nThe date-based action is not a one-time action. Amazon S3 continues to apply the date-\\nbased action even after the date has passed, as long as the rule status is Enabled.\\nFor example, suppose that you specify a date-based Expiration action to delete all\\nobjects (assume that no filter is specified in the rule). On the specified date, Amazon\\nS3 expires all the objects in the bucket. Amazon S3 also continues to expire any new', '']]]\n",
      "[[['', 'objects that you create in the bucket. To stop the lifecycle action, you must either remove\\nthe action from the lifecycle rule, disable the rule, or delete the rule from the lifecycle\\nconfiguration.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nYou can't create date-based Lifecycle rules by using the Amazon S3 console, but you can\\nview, disable, or delete such rules.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>Transition and Expiration Rule</ID>\\n<Filter>\\n<Prefix>tax/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<Days>365</Days>\\n<StorageClass>GLACIER</StorageClass>\\n</Transition>\\n<Expiration>\\n<Days>3650</Days>\\n</Expiration>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>Archive all object same-day upon creation</ID>\\n<Filter>\\n<Prefix></Prefix>\\n</Filter>\\n<Status>Enabled</Status>', '']]]\n",
      "[[['', '<Transition>\\n<Days>0</Days>\\n<StorageClass>GLACIER</StorageClass>\\n</Transition>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '...\\n<Filter>\\n<And>\\n<Prefix>tax/</Prefix>\\n<Tag>\\n<Key>key1</Key>\\n<Value>value1</Value>\\n</Tag>\\n<Tag>\\n<Key>key2</Key>\\n<Value>value2</Value>\\n</Tag>\\n</And>\\n</Filter>\\n...', ''], ['', '', '']], [['', '', ''], ['', '...\\n<Filter>\\n<And>\\n<Tag>\\n<Key>key1</Key>\\n<Value>value1</Value>\\n</Tag>\\n<Tag>\\n<Key>key2</Key>\\n<Value>value2</Value>', '']]]\n",
      "[[['', '</Tag>\\n</And>\\n</Filter>\\n...', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you have multiple rules in an S3 Lifecycle configuration, an object can become\\neligible for multiple S3 Lifecycle actions. In such cases, Amazon S3 follows these general\\nrules:\\n• Permanent deletion takes precedence over transition.\\n• Transition takes precedence over creation of delete markers.\\n• When an object is eligible for both an S3 Glacier Flexible Retrieval and S3 Standard-\\nIA (or S3 One Zone-IA) transition, Amazon S3 chooses the S3 Glacier Flexible Retrieval\\ntransition.\\nFor examples, see Example 5: Overlapping filters, conflicting lifecycle actions, and what\\nAmazon S3 does with nonversioned buckets.', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>Rule1</ID>', '']]]\n",
      "[[['', '<Filter>\\n<Prefix>logs/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<Days>0</Days>\\n<StorageClass>GLACIER</StorageClass>\\n</Transition>\\n</Rule>\\n<Rule>\\n<ID>Rule2</ID>\\n<Filter>\\n<Prefix>documents/</Prefix>\\n</Filter>\\n<Status>Disabled</Status>\\n<Transition>\\n<Days>0</Days>\\n<StorageClass>GLACIER</StorageClass>\\n</Transition>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>example-id</ID>\\n<Filter>', '']]]\n",
      "[[['', '<Prefix>logs/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<Days>30</Days>\\n<StorageClass>STANDARD_IA</StorageClass>\\n</Transition>\\n<Transition>\\n<Days>90</Days>\\n<StorageClass>GLACIER</StorageClass>\\n</Transition>\\n<Expiration>\\n<Days>365</Days>\\n</Expiration>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can use one rule to describe all S3 Lifecycle actions if all actions apply to the same\\nset of objects (identified by the filter). Otherwise, you can add multiple rules with each\\nspecifying a different filter.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you have multiple rules in an S3 Lifecycle configuration, an object can become\\neligible for multiple S3 Lifecycle actions. In such cases, Amazon S3 follows these general\\nrules:\\n• Permanent deletion takes precedence over transition.\\n• Transition takes precedence over creation of delete markers.\\n• When an object is eligible for both an S3 Glacier Flexible Retrieval and S3 Standard-\\nIA (or S3 One Zone-IA) transition, Amazon S3 chooses the S3 Glacier Flexible Retrieval\\ntransition.\\nFor examples, see Example 5: Overlapping filters, conflicting lifecycle actions, and what\\nAmazon S3 does with nonversioned buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>ClassADocRule</ID>\\n<Filter>\\n<Prefix>classA/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<Days>365</Days>\\n<StorageClass>GLACIER</StorageClass>\\n</Transition>\\n<Expiration>\\n<Days>3650</Days>\\n</Expiration>\\n</Rule>\\n<Rule>\\n<ID>ClassBDocRule</ID>\\n<Filter>\\n<Prefix>classB/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<Days>90</Days>\\n<StorageClass>STANDARD_IA</StorageClass>\\n</Transition>\\n<Expiration>\\n<Days>365</Days>\\n</Expiration>', '']]]\n",
      "[[['', '</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you have multiple rules in an S3 Lifecycle configuration, an object can become\\neligible for multiple S3 Lifecycle actions. In such cases, Amazon S3 follows these general\\nrules:\\n• Permanent deletion takes precedence over transition.\\n• Transition takes precedence over creation of delete markers.\\n• When an object is eligible for both an S3 Glacier Flexible Retrieval and S3 Standard-\\nIA (or S3 One Zone-IA) transition, Amazon S3 chooses the S3 Glacier Flexible Retrieval\\ntransition.\\nFor examples, see Example 5: Overlapping filters, conflicting lifecycle actions, and what\\nAmazon S3 does with nonversioned buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>Rule 1</ID>\\n<Filter>\\n</Filter>\\n<Status>Enabled</Status>\\n<Expiration>\\n<Days>365</Days>\\n</Expiration>\\n</Rule>\\n<Rule>\\n<ID>Rule 2</ID>\\n<Filter>\\n<Prefix>logs/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<StorageClass>STANDARD_IA<StorageClass>\\n<Days>30</Days>\\n</Transition>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>Rule 1</ID>\\n<Filter>\\n<Prefix>logs/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Expiration>\\n<Days>365</Days>\\n</Expiration>\\n</Rule>\\n<Rule>\\n<ID>Rule 2</ID>\\n<Filter>\\n<Prefix>logs/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<StorageClass>STANDARD_IA<StorageClass>\\n<Days>365</Days>\\n</Transition>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>Rule 1</ID>\\n<Filter>\\n<Prefix></Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<StorageClass>STANDARD_IA<StorageClass>\\n<Days>10</Days>\\n</Transition>\\n</Rule>\\n<Rule>\\n<ID>Rule 2</ID>\\n<Filter>\\n<Prefix>logs/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<StorageClass>STANDARD_IA<StorageClass>\\n<Days>365</Days>\\n</Transition>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>', '']]]\n",
      "[[['', '<ID>Rule 1</ID>\\n<Filter>\\n<Tag>\\n<Key>tag1</Key>\\n<Value>value1</Value>\\n</Tag>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<StorageClass>GLACIER<StorageClass>\\n<Days>365</Days>\\n</Transition>\\n</Rule>\\n<Rule>\\n<ID>Rule 2</ID>\\n<Filter>\\n<Tag>\\n<Key>tag2</Key>\\n<Value>value2</Value>\\n</Tag>\\n</Filter>\\n<Status>Enabled</Status>\\n<Expiration>\\n<Days>14</Days>\\n</Expiration>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you have multiple rules in an S3 Lifecycle configuration, an object can become\\neligible for multiple S3 Lifecycle actions. In such cases, Amazon S3 follows these general\\nrules:\\n• Permanent deletion takes precedence over transition.\\n• Transition takes precedence over creation of delete markers.', '']]]\n",
      "[[['', '• When an object is eligible for both an S3 Glacier Flexible Retrieval and S3 Standard-\\nIA (or S3 One Zone-IA) transition, Amazon S3 chooses the S3 Glacier Flexible Retrieval\\ntransition.\\nFor examples, see Example 5: Overlapping filters, conflicting lifecycle actions, and what\\nAmazon S3 does with nonversioned buckets.', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>sample-rule</ID>\\n<Filter>\\n<Prefix></Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<Days>90</Days>\\n<StorageClass>STANDARD_IA</StorageClass>\\n</Transition>\\n<NoncurrentVersionTransition>\\n<NoncurrentDays>30</NoncurrentDays>\\n<StorageClass>GLACIER</StorageClass>\\n</NoncurrentVersionTransition>\\n<NoncurrentVersionExpiration>\\n<NewerNoncurrentVersions>5</NewerNoncurrentVersions>\\n<NoncurrentDays>365</NoncurrentDays>', '']]]\n",
      "[[['', '</NoncurrentVersionExpiration>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n...\\n<NoncurrentVersionExpiration>\\n<NewerNoncurrentVersions>10</NewerNoncurrentVersions>\\n<NoncurrentDays>30</NoncurrentDays>\\n</NoncurrentVersionExpiration>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n...\\n<Expiration>\\n<Days>60</Days>\\n</Expiration>\\n<NoncurrentVersionExpiration>\\n<NewerNoncurrentVersions>10</NewerNoncurrentVersions>\\n<NoncurrentDays>30</NoncurrentDays>\\n</NoncurrentVersionExpiration>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou cannot specify both a Days and an ExpiredObjectDeleteMarker tag on\\nthe same rule. When you specify the Days tag, Amazon S3 automatically performs\\nExpiredObjectDeleteMarker cleanup when the delete markers are old enough to\\nsatisfy the age criteria. To clean up delete markers as soon as they become the only\\nversion, create a separate rule with only the ExpiredObjectDeleteMarker tag.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>Rule 1</ID>\\n<Filter>\\n<Prefix>logs/</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Expiration>\\n<ExpiredObjectDeleteMarker>true</ExpiredObjectDeleteMarker>\\n</Expiration>\\n<NoncurrentVersionExpiration>\\n<NewerNoncurrentVersions>10</NewerNoncurrentVersions>\\n<NoncurrentDays>30</NoncurrentDays>\\n</NoncurrentVersionExpiration>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen you use the ExpiredObjectDeleteMarker S3 Lifecycle action, the rule cannot\\nspecify a tag-based filter.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen you use the AbortIncompleteMultipartUpload S3 Lifecycle action, the rule\\ncannot specify a tag-based filter.', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>sample-rule</ID>\\n<Filter>\\n<Prefix>SomeKeyPrefix/</Prefix>\\n</Filter>\\n<Status>rule-status</Status>\\n<AbortIncompleteMultipartUpload>\\n<DaysAfterInitiation>7</DaysAfterInitiation>\\n</AbortIncompleteMultipartUpload>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>Transition with a prefix and based on size</ID>\\n<Filter>\\n<And>\\n<Prefix>tax/</Prefix>\\n<ObjectSizeGreaterThan>500</ObjectSizeGreaterThan>\\n</And>\\n</Filter>\\n<Status>Enabled</Status>\\n<Transition>\\n<Days>365</Days>\\n<StorageClass>GLACIER</StorageClass>\\n</Transition>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n...\\n<And>\\n<ObjectSizeGreaterThan>500</ObjectSizeGreaterThan>\\n<ObjectSizeLessThan>64000</ObjectSizeLessThan>\\n</And>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<LifecycleConfiguration>\\n<Rule>\\n<ID>Expire noncurrent with size less than 1 byte</ID>\\n<Filter>\\n<ObjectSizeLessThan>1</ObjectSizeLessThan>\\n</Filter>\\n<Status>Enabled</Status>\\n<NoncurrentVersionExpiration>\\n<NewerNoncurrentVersions>10</NewerNoncurrentVersions>\\n<NoncurrentDays>30</NoncurrentDays>\\n</NoncurrentVersionExpiration>\\n</Rule>\\n</LifecycleConfiguration>', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nObjects in Amazon S3 Inventory reports aren't guaranteed to be sorted in any order.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nThe Object ACL field is defined in JSON format. An inventory report displays the value for\\nthe Object ACL field as a base64-encoded string.\\nFor example, suppose that you have the following Object ACL field in JSON format:\\n{\\n\"version\": \"2022-11-10\",\\n\"status\": \"AVAILABLE\",\\n\"grants\": [{\\n\"canonicalId\": \"example-canonical-user-ID\",\\n\"type\": \"CanonicalUser\",\\n\"permission\": \"READ\"\\n}]\\n}\\nThe Object ACL field is encoded and shown as the following base64-encoded string:\\neyJ2ZXJzaW9uIjoiMjAyMi0xMS0xMCIsInN0YXR1cyI6IkFWQUlMQUJMRSIsImdyYW50cyI6W3siY2Fub\\nTo get the decoded value in JSON for the Object ACL field, you can query this field in\\nAmazon Athena. For query examples, see Querying Amazon S3 Inventory with Amazon\\nAthena.', '2'], ['', '', '']], [['', '', ''], ['', '{\\n\"version\": \"2022-11-10\",\\n\"status\": \"AVAILABLE\",\\n\"grants\": [{\\n\"canonicalId\": \"example-canonical-user-ID\",\\n\"type\": \"CanonicalUser\",\\n\"permission\": \"READ\"\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'eyJ2ZXJzaW9uIjoiMjAyMi0xMS0xMCIsInN0YXR1cyI6IkFWQUlMQUJMRSIsImdyYW50cyI6W3siY2F', 'u'], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nWhen an object reaches the end of its lifetime based on its lifecycle configuration, Amazon\\nS3 queues the object for removal and removes it asynchronously. Therefore, there might be\\na delay between the expiration date and the date when Amazon S3 removes an object. The\\ninventory report includes the objects that have expired but haven't been removed yet. For\\nmore information about expiration actions in S3 Lifecycle, see Expiring objects.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nThe AWS managed key (aws/s3) is not supported for SSE-KMS encryption with S3\\nInventory.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Sid\": \"Allow Amazon S3 use of the customer managed key\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"kms:GenerateDataKey\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\":{\\n\"StringEquals\":{\\n\"aws:SourceAccount\":\"source-account-id\"\\n},', '']]]\n",
      "[[['', '\"ArnLike\":{\\n\"aws:SourceARN\": \"arn:aws:s3:::example-s3-source-bucket\"\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIt might take up to 48 hours for Amazon S3 to deliver the first inventory report.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf the bucket policy for the specified destination requires objects to be\\nencrypted before storing them in Amazon S3, you must choose Specify an\\nencryption key. Otherwise, copying objects to the destination will fail.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo encrypt the inventory list file with SSE-KMS, you must grant Amazon S3\\npermission to use the customer managed key. For instructions, see Grant Amazon\\nS3 Permission to Encrypt Using Your KMS Keys.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf the destination bucket that stores the inventory list file is owned by a different\\nAWS account, make sure that you use a fully qualified KMS key ARN to specify your\\nKMS key.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nBoth the AWS managed key (aws/s3) and your customer managed keys appear\\nin the list. However, the AWS managed key (aws/s3) is not supported for SSE-\\nKMS encryption with S3 Inventory.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', '<NotificationConfiguration>\\n<QueueConfiguration>\\n<Id>1</Id>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>prefix</Name>\\n<Value>destination-prefix/source-bucket</Value>\\n</FilterRule>\\n<FilterRule>\\n<Name>suffix</Name>\\n<Value>checksum</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n<Cloudcode>arn:aws:lambda:us-west-2:222233334444:cloud-function-list-write</\\nCloudcode>\\n<Event>s3:ObjectCreated:*</Event>\\n</QueueConfiguration>\\n</NotificationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', 'destination-prefix/source-bucket/config-ID/YYYY-MM-DDTHH-MMZ/manifest.json\\ndestination-prefix/source-bucket/config-ID/YYYY-MM-DDTHH-MMZ/manifest.checksum\\ndestination-prefix/source-bucket/config-ID/hive/dt=YYYY-MM-DD-HH-MM/symlink.txt', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'destination-prefix/source-bucket/config-ID/data/example-file-name.csv.gz\\n...\\ndestination-prefix/source-bucket/config-ID/data/example-file-name-1.csv.gz', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"sourceBucket\": \"example-source-bucket\",\\n\"destinationBucket\": \"arn:aws:s3:::example-inventory-destination-bucket\",\\n\"version\": \"2016-11-30\",', '']]]\n",
      "[[['', '\"creationTimestamp\" : \"1514944800000\",\\n\"fileFormat\": \"CSV\",\\n\"fileSchema\": \"Bucket, Key, VersionId, IsLatest, IsDeleteMarker,\\nSize, LastModifiedDate, ETag, StorageClass, IsMultipartUploaded,\\nReplicationStatus, EncryptionStatus, ObjectLockRetainUntilDate, ObjectLockMode,\\nObjectLockLegalHoldStatus, IntelligentTieringAccessTier, BucketKeyStatus,\\nChecksumAlgorithm, ObjectAccessControlList, ObjectOwner\",\\n\"files\": [\\n{\\n\"key\": \"Inventory/example-source-bucket/2016-11-06T21-32Z/\\nfiles/939c6d46-85a9-4ba8-87bd-9db705a579ce.csv.gz\",\\n\"size\": 2147483647,\\n\"MD5checksum\": \"f11166069f1990abeb9c97ace9cdfabc\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"sourceBucket\": \"example-source-bucket\",\\n\"destinationBucket\": \"arn:aws:s3:::example-destination-bucket\",\\n\"version\": \"2016-11-30\",\\n\"creationTimestamp\" : \"1514944800000\",\\n\"fileFormat\": \"ORC\",\\n\"fileSchema\":\\n\"struct<bucket:string,key:string,version_id:string,is_latest:boolean,is_delete_marke\\n\"files\": [\\n{\\n\"key\": \"inventory/example-source-bucket/data/\\nd794c570-95bb-4271-9128-26023c8b4900.orc\",\\n\"size\": 56291,\\n\"MD5checksum\": \"5925f4e78e1695c2d020b9f6eexample\"\\n}\\n]\\n}', 'r'], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"sourceBucket\": \"example-source-bucket\",\\n\"destinationBucket\": \"arn:aws:s3:::example-destination-bucket\",\\n\"version\": \"2016-11-30\",\\n\"creationTimestamp\" : \"1514944800000\",\\n\"fileFormat\": \"Parquet\",\\n\"fileSchema\": \"message s3.inventory { required binary bucket (UTF8);\\nrequired binary key (UTF8); optional binary version_id (UTF8); optional boolean\\nis_latest; optional boolean is_delete_marker; optional int64 size; optional\\nint64 last_modified_date (TIMESTAMP_MILLIS); optional binary e_tag (UTF8);\\noptional binary storage_class (UTF8); optional boolean is_multipart_uploaded;\\noptional binary replication_status (UTF8); optional binary encryption_status\\n(UTF8); optional int64 object_lock_retain_until_date (TIMESTAMP_MILLIS); optional\\nbinary object_lock_mode (UTF8); optional binary object_lock_legal_hold_status\\n(UTF8); optional binary intelligent_tiering_access_tier (UTF8); optional binary\\nbucket_key_status (UTF8); optional binary checksum_algorithm (UTF8); optional\\nbinary object_access_control_list (UTF8); optional binary object_owner (UTF8);}\",\\n\"files\": [\\n{\\n\"key\": \"inventory/example-source-bucket/data/\\nd754c470-85bb-4255-9218-47023c8b4910.parquet\",\\n\"size\": 56291,\\n\"MD5checksum\": \"5825f2e18e1695c2d030b9f6eexample\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nThe symlink.txt Apache Hive-compatible manifest file does not currently work with AWS\\nGlue.', '']]]\n",
      "[[['', 'Reading the symlink.txt file with Apache Hive and Apache Spark is not supported for\\nORC and Parquet-formatted inventory files.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'CREATE EXTERNAL TABLE your_table_name(\\nbucket string,\\nkey string,\\nversion_id string,\\nis_latest boolean,\\nis_delete_marker boolean,\\nsize bigint,\\nlast_modified_date timestamp,\\ne_tag string,\\nstorage_class string,\\nis_multipart_uploaded boolean,\\nreplication_status string,\\nencryption_status string,\\nobject_lock_retain_until_date bigint,\\nobject_lock_mode string,\\nobject_lock_legal_hold_status string,\\nintelligent_tiering_access_tier string,\\nbucket_key_status string,\\nchecksum_algorithm string,\\nobject_access_control_list string,\\nobject_owner string\\n) PARTITIONED BY (\\ndt string\\n)\\nROW FORMAT SERDE \\'org.apache.hadoop.hive.ql.io.orc.OrcSerde\\'\\nSTORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\\'\\nOUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\\'\\nLOCATION \\'s3://source-bucket/config-ID/hive/\\'\\nTBLPROPERTIES (\\n\"projection.enabled\" = \"true\",\\n\"projection.dt.type\" = \"date\",\\n\"projection.dt.format\" = \"yyyy-MM-dd-HH-mm\",\\n\"projection.dt.range\" = \"2022-01-01-00-00,NOW\",\\n\"projection.dt.interval\" = \"1\",\\n\"projection.dt.interval.unit\" = \"HOURS\"\\n);', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\", ''], ['', '', '']], [['', '', ''], ['', 'CREATE EXTERNAL TABLE your_table_name(\\nbucket string,\\nkey string,\\nversion_id string,\\nis_latest boolean,\\nis_delete_marker boolean,\\nsize string,\\nlast_modified_date string,\\ne_tag string,\\nstorage_class string,\\nis_multipart_uploaded boolean,\\nreplication_status string,\\nencryption_status string,\\nobject_lock_retain_until_date string,\\nobject_lock_mode string,\\nobject_lock_legal_hold_status string,', '']]]\n",
      "[[['', 'intelligent_tiering_access_tier string,\\nbucket_key_status string,\\nchecksum_algorithm string,\\nobject_access_control_list string,\\nobject_owner string\\n) PARTITIONED BY (\\ndt string\\n)\\nROW FORMAT SERDE \\'org.apache.hadoop.hive.serde2.OpenCSVSerde\\'\\nSTORED AS INPUTFORMAT \\'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat\\'\\nOUTPUTFORMAT \\'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat\\'\\nLOCATION \\'s3://source-bucket/config-ID/hive/\\'\\nTBLPROPERTIES (\\n\"projection.enabled\" = \"true\",\\n\"projection.dt.type\" = \"date\",\\n\"projection.dt.format\" = \"yyyy-MM-dd-HH-mm\",\\n\"projection.dt.range\" = \"2022-01-01-00-00,NOW\",\\n\"projection.dt.interval\" = \"1\",\\n\"projection.dt.interval.unit\" = \"HOURS\"\\n);', ''], ['', '', '']], [['', '', ''], ['', \"# Get a list of the latest inventory report dates available.\\nSELECT DISTINCT dt FROM your_table_name ORDER BY 1 DESC limit 10;\\n# Get the encryption status for a provided report date.\\nSELECT encryption_status, count(*) FROM your_table_name WHERE dt = 'YYYY-MM-DD-HH-\\nMM' GROUP BY encryption_status;\\n# Get the encryption status for inventory report dates in the provided range.\\nSELECT dt, encryption_status, count(*) FROM your_table_name\\nWHERE dt > 'YYYY-MM-DD-HH-MM' AND dt < 'YYYY-MM-DD-HH-MM' GROUP BY dt,\\nencryption_status;\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"# Get the S3 keys that have Object ACL grants with public access.\\nWITH grants AS (\\nSELECT key,\\nCAST(\\njson_extract(from_utf8(from_base64(object_access_control_list)),\\n'$.grants') AS ARRAY(MAP(VARCHAR, VARCHAR))\\n) AS grants_array\\nFROM your_table_name\\n)\\nSELECT key,\\ngrants_array,\\ngrant\\nFROM grants, UNNEST(grants_array) AS t(grant)\\nWHERE element_at(grant, 'uri') = 'http://acs.amazonaws.com/groups/global/AllUsers'\", ''], ['', '', '']], [['', '', ''], ['', \"# Get the S3 keys that have Object ACL grantees in addition to the object owner.\\nWITH grants AS\\n(SELECT key,\\nfrom_utf8(from_base64(object_access_control_list)) AS\\nobject_access_control_list,\\nobject_owner,\\nCAST(json_extract(from_utf8(from_base64(object_access_control_list)),\\n'$.grants') AS ARRAY(MAP(VARCHAR, VARCHAR))) AS grants_array\\nFROM your_table_name)\\nSELECT key,\\ngrant,\\nobjectowner\\nFROM grants, UNNEST(grants_array) AS t(grant)\\nWHERE cardinality(grants_array) > 1 AND element_at(grant, 'canonicalId') !=\\nobject_owner;\", ''], ['', '', '']], [['', '', ''], ['', \"# Get the S3 keys with READ permission that is granted in the Object ACL.\\nWITH grants AS (\\nSELECT key,\\nCAST(\\njson_extract(from_utf8(from_base64(object_access_control_list)),\\n'$.grants') AS ARRAY(MAP(VARCHAR, VARCHAR))\\n) AS grants_array\\nFROM your_table_name\", '']]]\n",
      "[[['', \")\\nSELECT key,\\ngrants_array,\\ngrant\\nFROM grants, UNNEST(grants_array) AS t(grant)\\nWHERE element_at(grant, 'permission') = 'READ';\", ''], ['', '', '']], [['', '', ''], ['', \"# Get the S3 keys that have Object ACL grants to a specific canonical user ID.\\nWITH grants AS (\\nSELECT key,\\nCAST(\\njson_extract(from_utf8(from_base64(object_access_control_list)),\\n'$.grants') AS ARRAY(MAP(VARCHAR, VARCHAR))\\n) AS grants_array\\nFROM your_table_name\\n)\\nSELECT key,\\ngrants_array,\\ngrant\\nFROM grants, UNNEST(grants_array) AS t(grant)\\nWHERE element_at(grant, 'canonicalId') = 'user-canonical-id';\", ''], ['', '', '']], [['', '', ''], ['', \"# Get the number of grantees on the Object ACL.\\nSELECT key,\\nobject_access_control_list,\\njson_array_length(json_extract(object_access_control_list,'$.grants')) AS\\ngrants_count\\nFROM your_table_name;\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe following procedure applies only to Amazon S3 Inventory reports that include\\nall versions, and only if the \"all versions\" reports are used as manifests for S3 Batch\\nOperations on buckets that have S3 Versioning enabled. You are not required to convert\\nstrings for S3 Inventory reports that specify the current version only.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"CREATE EXTERNAL TABLE table_name(bucket string, key string,\\nversion_id string) PARTITIONED BY (dt string)ROW FORMAT SERDE\\n'org.apache.hadoop.hive.serde2.OpenCSVSerde'STORED AS INPUTFORMAT\\n'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat' OUTPUTFORMAT\\n'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat' LOCATION 'Copied S3 URI';\", ''], ['', '', '']], [['', '', ''], ['', 'MSCK REPAIR TABLE table_name;', ''], ['', '', '']], [['', '', ''], ['', \"SELECT bucket as Bucket, key as Key, CASE WHEN version_id = '' THEN 'null' ELSE\\nversion_id END as VersionId FROM table_name WHERE dt = 'YYYY-MM-DD-HH-MM';\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'tail -n +2 file.csv > tmp.csv && mv tmp.csv file.csv', ''], ['', '', '']], [['', '', ''], ['', '$ins = New-Object System.IO.StreamReader File-location\\\\file.csv\\n$outs = New-Object System.IO.StreamWriter File-location\\\\temp.csv\\ntry {\\n$skip = 0\\nwhile ( !$ins.EndOfStream ) {\\n$line = $ins.ReadLine();\\nif ( $skip -ne 0 ) {\\n$outs.WriteLine($line);\\n} else {\\n$skip = 1\\n}\\n}\\n} finally {\\n$outs.Close();\\n$ins.Close();\\n}\\nMove-Item File-location\\\\temp.csv File-location\\\\file.csv -Force', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf a grantee in an ACL is the email address of an AWS account, S3 Inventory uses the\\ncanonicalId of that AWS account and the CanonicalUser type to specify this\\ngrantee. For more information, see Grantees in access control lists.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"version\": \"2022-11-10\",\\n\"status\": \"AVAILABLE\",\\n\"grants\": [{\\n\"uri\": \"http://acs.amazonaws.com/groups/global/AllUsers\",\\n\"permission\": \"READ\",\\n\"type\": \"Group\"\\n}, {\\n\"canonicalId\": \"example-canonical-id\",\\n\"permission\": \"FULL_CONTROL\",\\n\"type\": \"CanonicalUser\"\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe Object ACL field is defined in JSON format. An inventory report displays the value for\\nthe Object ACL field as a base64-encoded string.\\nFor example, suppose that you have the following Object ACL field in JSON format:\\n{\\n\"version\": \"2022-11-10\",\\n\"status\": \"AVAILABLE\",\\n\"grants\": [{\\n\"canonicalId\": \"example-canonical-user-ID\",\\n\"type\": \"CanonicalUser\",\\n\"permission\": \"READ\"\\n}]\\n}\\nThe Object ACL field is encoded and shown as the following base64-encoded string:\\neyJ2ZXJzaW9uIjoiMjAyMi0xMS0xMCIsInN0YXR1cyI6IkFWQUlMQUJMRSIsImdyYW50cyI6W3siY2Fub25', 'p']], [['', '', ''], ['', '{\\n\"version\": \"2022-11-10\",\\n\"status\": \"AVAILABLE\",\\n\"grants\": [{\\n\"canonicalId\": \"example-canonical-user-ID\",\\n\"type\": \"CanonicalUser\",\\n\"permission\": \"READ\"\\n}]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'eyJ2ZXJzaW9uIjoiMjAyMi0xMS0xMCIsInN0YXR1cyI6IkFWQUlMQUJMRSIsImdyYW50cyI6W3siY2Fub', '2'], ['', '', '']]]\n",
      "[[['', 'To get the decoded value in JSON for the Object ACL field, you can query this field in\\nAmazon Athena. For query examples, see Querying Amazon S3 Inventory with Amazon\\nAthena.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nS3 RTC does not apply to Batch Replication. Batch Replication is an on-demand\\nreplication job, and can be tracked with S3 Batch Operations. For more information, see\\nTracking job status and completion reports.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['Workload requireme\\nnt', 'S3 RTC (15-minute\\nSLA)', 'Cross-Region\\nReplication (CRR)', 'Single-Region\\nReplication (SRR)'], ['Replicate objects\\nbetween different\\nAWS accounts', 'Yes', 'Yes', 'Yes'], ['Replicate objects\\nwithin the same\\nAWS Region within', 'No', 'No', 'Yes']]]\n",
      "[[['Workload requireme\\nnt', 'S3 RTC (15-minute\\nSLA)', 'Cross-Region\\nReplication (CRR)', 'Single-Region\\nReplication (SRR)'], ['24-48 hours (not SLA\\nbacked)', '', '', ''], ['Replicate objects\\nbetween different\\nAWS Regions within\\n24-48 hours (not SLA\\nbacked)', 'No', 'Yes', 'No'], ['Predictable replicati\\non time: Backed by\\nSLA to replicate 99.9\\npercent of objects\\nwithin 15 minutes', 'Yes', 'No', 'No']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nYou must grant two new permissions on the source S3 bucket in the AWS Identity\\nand Access Management (IAM) role that you use to set up replication. The two new\\npermissions are s3:GetObjectRetention and s3:GetObjectLegalHold. If the\\nrole has an s3:Get* permission, it satisfies the requirement. For more information, see\\nSetting up permissions for live replication.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nObjects that existed before you set up replication aren't replicated automatically. In other\\nwords, Amazon S3 doesn't replicate objects retroactively. To replicate objects that were\\ncreated before your replication configuration, use S3 Batch Replication. Learn more about\\nconfiguring Batch Replication at Replicating existing objects.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<ReplicationConfiguration>\\n<Role>IAM-role-ARN</Role>\\n<Rule>\\n...\\n</Rule>\\n<Rule>\\n...\\n</Rule>\\n...\\n</ReplicationConfiguration>', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '...\\n<Rule>\\n<ID>Rule-1</ID>\\n<Status>Enabled-or-Disabled</Status>\\n<Filter>\\n<Prefix></Prefix>\\n</Filter>\\n<Priority>integer</Priority>\\n<DeleteMarkerReplication>\\n<Status>Enabled-or-Disabled</Status>\\n</DeleteMarkerReplication>\\n<Destination>\\n<Bucket>arn:aws:s3:::example-s3-bucket</Bucket>\\n</Destination>\\n</Rule>\\n<Rule>\\n...\\n</Rule>\\n...\\n...', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<Rule>\\n...\\n<Filter>\\n<Prefix>key-prefix</Prefix>\\n</Filter>\\n...\\n</Rule>\\n...', ''], ['', '', '']], [['', '', ''], ['', '<Rule>\\n...\\n<Filter>\\n<And>\\n<Tag>\\n<Key>key1</Key>\\n<Value>value1</Value>\\n</Tag>\\n<Tag>\\n<Key>key2</Key>\\n<Value>value2</Value>\\n</Tag>\\n...\\n</And>\\n</Filter>\\n...\\n</Rule>\\n...', ''], ['', '', '']], [['', '', ''], ['', '<Rule>\\n...', '']]]\n",
      "[[['', '<Filter>\\n<And>\\n<Prefix>key-prefix</Prefix>\\n<Tag>\\n<Key>key1</Key>\\n<Value>value1</Value>\\n</Tag>\\n<Tag>\\n<Key>key2</Key>\\n<Value>value2</Value>\\n</Tag>\\n...\\n</Filter>\\n...\\n</Rule>\\n...', ''], ['', '', '']], [['', '', ''], ['', \"Note\\n• If you specify a rule with an empty <Filter> element, your rule applies to all objects in\\nyour bucket.\\n• When you're using tag-based replication rules with live replication, new objects must be\\ntagged with the matching replication rule tag in the PutObject operation. Otherwise,\\nthe objects won't be replicated. If objects are tagged after the PutObject operation,\\nthose objects also won't be replicated.\\nTo replicate objects that have been tagged after the PutObject operation, you must\\nuse S3 Batch Replication. For more information about Batch Replication, see Replicating\\nexisting objects.\", ''], ['', '', '']], [['', '', ''], ['', '...\\n<Destination>\\n<Bucket>arn:aws:s3:::example-s3-bucket</Bucket>', '']]]\n",
      "[[['', '</Destination>\\n...', ''], ['', '', '']], [['', '', ''], ['', '...\\n<Destination>\\n<Bucket>arn:aws:s3:::example-s3-bucket</Bucket>\\n<StorageClass>storage-class</StorageClass>\\n</Destination>\\n...', ''], ['', '', '']], [['', '', ''], ['', '...\\n<Rule>\\n<ID>Rule-1</ID>\\n<Status>Enabled-or-Disabled</Status>\\n<Priority>integer</Priority>\\n<DeleteMarkerReplication>\\n<Status>Enabled-or-Disabled</Status>\\n</DeleteMarkerReplication>\\n<Destination>\\n<Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket>\\n</Destination>', '']]]\n",
      "[[['', '</Rule>\\n<Rule>\\n<ID>Rule-2</ID>\\n<Status>Enabled-or-Disabled</Status>\\n<Priority>integer</Priority>\\n<DeleteMarkerReplication>\\n<Status>Enabled-or-Disabled</Status>\\n</DeleteMarkerReplication>\\n<Destination>\\n<Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET2</Bucket>\\n</Destination>\\n</Rule>\\n...', ''], ['', '', '']], [['', '', ''], ['', '...\\n<Rule>\\n<ID>Rule-1</ID>\\n<Status>Enabled-or-Disabled</Status>\\n<Priority>integer</Priority>\\n<DeleteMarkerReplication>\\n<Status>Disabled</Status>\\n</DeleteMarkerReplication>\\n<Metrics>\\n<Status>Enabled</Status>\\n<EventThreshold>\\n<Minutes>15</Minutes>\\n</EventThreshold>\\n</Metrics>\\n<Destination>\\n<Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket>\\n</Destination>\\n</Rule>\\n<Rule>\\n<ID>Rule-2</ID>\\n<Status>Enabled-or-Disabled</Status>\\n<Priority>integer</Priority>\\n<DeleteMarkerReplication>\\n<Status>Enabled</Status>', '']]]\n",
      "[[['', '</DeleteMarkerReplication>\\n<Metrics>\\n<Status>Enabled</Status>\\n<EventThreshold>\\n<Minutes>15</Minutes>\\n</EventThreshold>\\n</Metrics>\\n<ReplicationTime>\\n<Status>Enabled</Status>\\n<Time>\\n<Minutes>15</Minutes>\\n</Time>\\n</ReplicationTime>\\n<Destination>\\n<Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET2</Bucket>\\n</Destination>\\n</Rule>\\n...', ''], ['', '', '']], [['', '', ''], ['', '...\\n<Destination>\\n<Bucket>arn:aws:s3:::example-s3-bucket</Bucket>\\n<Account>destination-bucket-owner-account-id</Account>\\n<AccessControlTranslation>\\n<Owner>Destination</Owner>\\n</AccessControlTranslation>\\n</Destination>\\n...', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nOnly a value of <Minutes>15</Minutes> is accepted for EventThreshold and Time.', ''], ['', '', '']], [['', '', ''], ['', '...\\n<Destination>\\n<Bucket>arn:aws:s3:::example-s3-bucket</Bucket>\\n<Metrics>\\n<Status>Enabled</Status>\\n<EventThreshold>\\n<Minutes>15</Minutes>\\n</EventThreshold>\\n</Metrics>\\n<ReplicationTime>\\n<Status>Enabled</Status>\\n<Time>\\n<Minutes>15</Minutes>\\n</Time>\\n</ReplicationTime>\\n</Destination>\\n...', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '...\\n<SourceSelectionCriteria>\\n<SseKmsEncryptedObjects>\\n<Status>Enabled</Status>\\n</SseKmsEncryptedObjects>\\n</SourceSelectionCriteria>\\n<Destination>\\n<Bucket>arn:aws:s3:::example-s3-bucket</Bucket>\\n<EncryptionConfiguration>\\n<ReplicaKmsKeyID>AWS KMS key ID to use for encrypting object replicas</\\nReplicaKmsKeyID>\\n</EncryptionConfiguration>\\n</Destination>\\n...', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nTo add a replication configuration to a bucket, you must have the iam:PassRole\\npermission. This permission allows you to pass the IAM role that grants Amazon S3\\nreplication permissions. You specify the IAM role by providing the Amazon Resource Name\\n(ARN) that is used in the Role element in the replication configuration XML. For more\\ninformation, see Granting a User Permissions to Pass a Role to an AWS service in the IAM\\nUser Guide.', ''], ['', '', '']], [['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ReplicationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Role>arn:aws:iam::account-id:role/role-name</Role>', '']]]\n",
      "[[['', '<Rule>\\n<Status>Enabled</Status>\\n<Destination><Bucket>arn:aws:s3:::example-s3-bucket</Bucket></Destination>\\n</Rule>\\n</ReplicationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ReplicationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Role>arn:aws:iam::account-id:role/role-name</Role>\\n<Rule>\\n<Status>Enabled</Status>\\n<Priority>1</Priority>\\n<DeleteMarkerReplication>\\n<Status>string</Status>\\n</DeleteMarkerReplication>\\n<Filter>\\n<Prefix>Tax/</Prefix>\\n</Filter>\\n<Destination><Bucket>arn:aws:s3:::example-s3-bucket</Bucket></Destination>\\n</Rule>\\n</ReplicationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>', '']]]\n",
      "[[['', '<ReplicationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Role>arn:aws:iam::account-id:role/role-name</Role>\\n<Rule>\\n<Status>Enabled</Status>\\n<Priority>1</Priority>\\n<DeleteMarkerReplication>\\n<Status>string</Status>\\n</DeleteMarkerReplication>\\n<Filter>\\n<And>\\n<Prefix>Tax/</Prefix>\\n<Tag>\\n<Tag>\\n<Key>tagA</Key>\\n<Value>valueA</Value>\\n</Tag>\\n</Tag>\\n<Tag>\\n<Tag>\\n<Key>tagB</Key>\\n<Value>valueB</Value>\\n</Tag>\\n</Tag>\\n</And>\\n</Filter>\\n<Destination><Bucket>arn:aws:s3:::example-s3-bucket</Bucket></Destination>\\n</Rule>\\n</ReplicationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ReplicationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Role>arn:aws:iam::account-id:role/role-name</Role>\\n<Rule>\\n<Status>Enabled</Status>\\n<Destination>\\n<Bucket>arn:aws:s3:::example-s3-bucket</Bucket>', '']]]\n",
      "[[['', '<StorageClass>storage-class</StorageClass>\\n</Destination>\\n</Rule>\\n</ReplicationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ReplicationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Role>arn:aws:iam::account-id:role/role-name</Role>\\n<Rule>\\n<Status>Enabled</Status>\\n<Priority>1</Priority>\\n<DeleteMarkerReplication>\\n<Status>string</Status>\\n</DeleteMarkerReplication>\\n<Filter>\\n<Prefix>Tax</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Destination>\\n<Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket>\\n</Destination>\\n...', '']]]\n",
      "[[['', '</Rule>\\n<Rule>\\n<Status>Enabled</Status>\\n<Priority>2</Priority>\\n<DeleteMarkerReplication>\\n<Status>string</Status>\\n</DeleteMarkerReplication>\\n<Filter>\\n<Prefix>Project</Prefix>\\n</Filter>\\n<Status>Enabled</Status>\\n<Destination>\\n<Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket>\\n<StorageClass>STANDARD_IA</StorageClass>\\n</Destination>\\n...\\n</Rule>\\n</ReplicationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<ReplicationConfiguration>\\n<Role>arn:aws:iam::account-id:role/role-name</Role>\\n<Rule>\\n<Status>Enabled</Status>\\n<Priority>1</Priority>\\n<DeleteMarkerReplication>\\n<Status>string</Status>\\n</DeleteMarkerReplication>\\n<Filter>\\n<Prefix>star</Prefix>\\n</Filter>\\n<Destination>\\n<Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket>', '']]]\n",
      "[[['', '</Destination>\\n</Rule>\\n<Rule>\\n<Status>Enabled</Status>\\n<Priority>2</Priority>\\n<DeleteMarkerReplication>\\n<Status>string</Status>\\n</DeleteMarkerReplication>\\n<Filter>\\n<Prefix>starship</Prefix>\\n</Filter>\\n<Destination>\\n<Bucket>arn:aws:s3:::DOC-EXAMPLE-BUCKET1</Bucket>\\n</Destination>\\n</Rule>\\n</ReplicationConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ReplicationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Role>arn:aws:iam::account-id:role/role-name</Role>\\n<Rule>\\n<Status>Enabled</Status>\\n<Prefix>key-prefix</Prefix>\\n<Destination><Bucket>arn:aws:s3:::example-s3-bucket</Bucket></Destination>\\n</Rule>\\n</ReplicationConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\":\"s3.amazonaws.com\"\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[', '']]]\n",
      "[[['', '{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\": [\\n\"s3.amazonaws.com\",\\n\"batchoperations.s3.amazonaws.com\"\\n]\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetReplicationConfiguration\",\\n\"s3:ListBucket\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::example-s3-bucket1\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetObjectVersionForReplication\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:GetObjectVersionTagging\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::example-s3-bucket1/*\"\\n]', '']]]\n",
      "[[['', '},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:ReplicateObject\",\\n\"s3:ReplicateDelete\",\\n\"s3:ReplicateTags\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket2/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nPermissions for the s3:ReplicateObject action on the example-s3-bucket2\\nbucket (the destination bucket) also allow replication of metadata such as object\\ntags and ACLS. Therefore you do not need to explicitly grant permission for the\\ns3:ReplicateTags action.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nThe AWS account that owns the IAM role must have permissions for the actions that it\\ngrants to the IAM role.\\nFor example, suppose that the source bucket contains objects owned by another AWS\\naccount. The owner of the objects must explicitly grant the AWS account that owns\\nthe IAM role the required permissions through the object ACL. Otherwise, Amazon S3\\ncan't access the objects, and replication of the objects fails. For information about ACL\\npermissions, see Access control list (ACL) overview.\\nThe permissions described here are related to the minimum replication configuration.\\nIf you choose to add optional replication configurations, you must grant additional\\npermissions to Amazon S3.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe ARN format of the role might appear different. If the role was created by using\\nthe console, the ARN format is arn:aws:iam::account-ID:role/service-\\nrole/role-name. If the role was created by using the AWS CLI, the ARN format is\\narn:aws:iam::account-ID:role/role-name. For more information, see IAM roles in\\nthe IAM User Guide.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Id\":\"PolicyForDestinationBucket\",', '']]]\n",
      "[[['', '\"Statement\":[\\n{\\n\"Sid\":\"Permissions on objects\",\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"AWS\":\"arn:aws:iam::SourceBucket-account-ID:role/service-role/source-\\naccount-IAM-role\"\\n},\\n\"Action\":[\\n\"s3:ReplicateDelete\",\\n\"s3:ReplicateObject\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket2/*\"\\n},\\n{\\n\"Sid\":\"Permissions on bucket\",\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"AWS\":\"arn:aws:iam::SourceBucket-account-ID:role/service-role/source-\\naccount-IAM-role\"\\n},\\n\"Action\": [\\n\"s3:List*\",\\n\"s3:GetBucketVersioning\",\\n\"s3:PutBucketVersioning\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket2\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '...\\n\"Statement\":[\\n{\\n\"Effect\":\"Deny\",\\n\"Principal\":{\\n\"AWS\":\"arn:aws:iam::SourceBucket-account-id:role/service-role/source-\\naccount-IAM-role\"\\n},\\n\"Action\":\"s3:ReplicateTags\",\\n\"Resource\":\"arn:aws:s3:::example-s3-bucket2/*\"\\n}\\n]\\n...', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nLive replication refers to Same-Region Replication (SRR) and Cross-Region Replication\\n(CRR). Live replication doesn't replicate any objects that existed in the bucket before you\\nset up replication. To replicate objects that existed before you set up replication, use\\non-demand replication. To sync buckets and replicate existing objects on demand, see\\nReplicating existing objects.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Tip\\nFor a step-by-step tutorial that demonstrates how to use live replication to replicate data,\\nsee Tutorial: Replicating data within and between AWS Regions using S3 Replication.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf you enter a prefix that is the name of a folder, you must use / (forward slash) as\\nthe last character (for example, pictures/).', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe number of destination buckets is limited to the number of AWS Regions in a given\\npartition. A partition is a grouping of Regions. AWS currently has three partitions: aws\\n(Standard Regions), aws-cn (China Regions), and aws-us-gov (AWS GovCloud (US)\\nRegions). To request an increase in your destination bucket quota, you can use service\\nquotas.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf versioning is not enabled on the destination bucket, you get a warning that contains\\nan Enable versioning button. Choose this button to enable versioning on the bucket.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you add a replication rule to a bucket, you must have the iam:PassRole\\npermission to be able to pass the IAM role that grants Amazon S3 replication\\npermissions. For more information, see Granting a user permissions to pass a role to an\\nAWS service in the IAM User Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nWhen you replicate objects that are encrypted with AWS KMS, the AWS KMS request\\nrate doubles in the source Region and increases in the destination Region by the\\nsame amount. These increased call rates to AWS KMS are due to the way that data\\nis re-encrypted by using the KMS key that you define for the replication destination\\nRegion. AWS KMS has a request rate quota that is per calling account per Region. For\\ninformation about the quota defaults, see AWS KMS Quotas - Requests per Second:\\nVaries in the AWS Key Management Service Developer Guide.\\nIf your current Amazon S3 PUT object request rate during replication is more than half\\nthe default AWS KMS rate limit for your account, we recommend that you request an\\nincrease to your AWS KMS request rate quota. To request an increase, create a case in\\nthe AWS Support Center at Contact Us. For example, suppose that your current PUT\\nobject request rate is 1,000 requests per second and you use AWS KMS to encrypt\\nyour objects. In this case, we recommend that you ask AWS Support to increase your\\nAWS KMS rate limit to 2,500 requests per second, in both your source and destination\\nRegions (if different), to ensure that there is no throttling by AWS KMS.\\nTo see your PUT object request rate in the source bucket, view PutRequests in the\\nAmazon CloudWatch request metrics for Amazon S3. For information about viewing\\nCloudWatch metrics, see Using the S3 console.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nYou can only use KMS keys that are enabled in the same AWS Region as the\\nbucket. When you choose Choose from your KMS keys, the S3 console lists\\nonly 100 KMS keys per Region. If you have more than 100 KMS keys in the same\\nRegion, you can see only the first 100 KMS keys in the S3 console. To use a KMS\\nkey that is not listed in the console, choose Enter AWS KMS key ARN, and enter\\nyour KMS key ARN.\\nWhen you use an AWS KMS key for server-side encryption in Amazon S3, you must\\nchoose a symmetric encryption KMS key. Amazon S3 supports only symmetric\\nencryption KMS keys and not asymmetric KMS keys. For more information, see\\nIdentifying symmetric and asymmetric KMS keys in the AWS Key Management\\nService Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen you use S3 RTC or S3 Replication metrics, additional fees apply.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nThe profile you use for this exercise must have the necessary permissions. For example,\\nin the replication configuration, you specify the IAM role that Amazon S3 can assume.\\nYou can do this only if the profile you use has the iam:PassRole permission. For\\nmore information, see Granting a User Permissions to Pass a Role to an AWS Service in\\nthe IAM User Guide. If you use administrator credentials to create a named profile, you\\ncan perform all the tasks.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api create-bucket \\\\\\n--bucket source \\\\\\n--region us-east-1 \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-versioning \\\\\\n--bucket source \\\\\\n--versioning-configuration Status=Enabled \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo set up replication configuration when both source and destination buckets are in\\nthe same AWS account, you use the same profile. This example uses acctA. To test\\nreplication configuration when the buckets are owned by different AWS accounts,', '']]]\n",
      "[[['', 'you specify different profiles for each. This example uses the acctB profile for the\\ndestination bucket.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api create-bucket \\\\\\n--bucket destination \\\\\\n--region us-west-2 \\\\\\n--create-bucket-configuration LocationConstraint=us-west-2 \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-versioning \\\\\\n--bucket destination \\\\\\n--versioning-configuration Status=Enabled \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\":\"s3.amazonaws.com\"\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', '$ aws iam create-role \\\\\\n--role-name replicationRole \\\\\\n--assume-role-policy-document file://s3-role-trust-policy.json \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetObjectVersionForReplication\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:GetObjectVersionTagging\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::source-bucket/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:ListBucket\",\\n\"s3:GetReplicationConfiguration\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::source-bucket\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:ReplicateObject\",', '']]]\n",
      "[[['', '\"s3:ReplicateDelete\",\\n\"s3:ReplicateTags\"\\n],\\n\"Resource\":\"arn:aws:s3:::destination-bucket/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '$ aws iam put-role-policy \\\\\\n--role-name replicationRole \\\\\\n--policy-document file://s3-role-permissions-policy.json \\\\\\n--policy-name replicationRolePolicy \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Role\": \"IAM-role-ARN\",\\n\"Rules\": [\\n{\\n\"Status\": \"Enabled\",\\n\"Priority\": 1,\\n\"DeleteMarkerReplication\": { \"Status\": \"Disabled\" },\\n\"Filter\" : { \"Prefix\": \"Tax\"},\\n\"Destination\": {\\n\"Bucket\": \"arn:aws:s3:::destination-bucket\"\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '$ aws s3api put-bucket-replication \\\\\\n--replication-configuration file://replication.json \\\\\\n--bucket source \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', '$ aws s3api get-bucket-replication \\\\\\n--bucket source \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe amount of time it takes for Amazon S3 to replicate an object depends on the\\nsize of the object. For information about how to see the status of replication, see\\nGetting replication status information.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.identitymanagement.AmazonIdentityManagement;\\nimport\\ncom.amazonaws.services.identitymanagement.AmazonIdentityManagementClientBuilder;\\nimport com.amazonaws.services.identitymanagement.model.CreateRoleRequest;\\nimport com.amazonaws.services.identitymanagement.model.PutRolePolicyRequest;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3Client;\\nimport com.amazonaws.services.s3.model.BucketReplicationConfiguration;\\nimport com.amazonaws.services.s3.model.BucketVersioningConfiguration;\\nimport com.amazonaws.services.s3.model.CreateBucketRequest;', '']]]\n",
      "[[['', 'import com.amazonaws.services.s3.model.DeleteMarkerReplication;\\nimport com.amazonaws.services.s3.model.DeleteMarkerReplicationStatus;\\nimport com.amazonaws.services.s3.model.ReplicationDestinationConfig;\\nimport com.amazonaws.services.s3.model.ReplicationRule;\\nimport com.amazonaws.services.s3.model.ReplicationRuleStatus;\\nimport com.amazonaws.services.s3.model.SetBucketVersioningConfigurationRequest;\\nimport com.amazonaws.services.s3.model.StorageClass;\\nimport com.amazonaws.services.s3.model.replication.ReplicationFilter;\\nimport com.amazonaws.services.s3.model.replication.ReplicationFilterPredicate;\\nimport com.amazonaws.services.s3.model.replication.ReplicationPrefixPredicate;\\nimport java.io.IOException;\\nimport java.util.ArrayList;\\nimport java.util.HashMap;\\nimport java.util.List;\\nimport java.util.Map;\\npublic class CrossRegionReplication {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString accountId = \"*** Account ID ***\";\\nString roleName = \"*** Role name ***\";\\nString sourceBucketName = \"*** Source bucket name ***\";\\nString destBucketName = \"*** Destination bucket name ***\";\\nString prefix = \"Tax/\";\\nString roleARN = String.format(\"arn:aws:iam::%s:%s\", accountId,\\nroleName);\\nString destinationBucketARN = \"arn:aws:s3:::\" + destBucketName;\\nAmazonS3 s3Client = AmazonS3Client.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\ncreateBucket(s3Client, clientRegion, sourceBucketName);\\ncreateBucket(s3Client, clientRegion, destBucketName);\\nassignRole(roleName, clientRegion, sourceBucketName,\\ndestBucketName);\\ntry {\\n// Create the replication rule.', '']]]\n",
      "[[['', 'List<ReplicationFilterPredicate> andOperands = new\\nArrayList<ReplicationFilterPredicate>();\\nandOperands.add(new ReplicationPrefixPredicate(prefix));\\nMap<String, ReplicationRule> replicationRules = new\\nHashMap<String, ReplicationRule>();\\nreplicationRules.put(\"ReplicationRule1\",\\nnew ReplicationRule()\\n.withPriority(0)\\n.withStatus(ReplicationRuleStatus.Enabled)\\n.withDeleteMarkerReplication(\\nnew\\nDeleteMarkerReplication().withStatus(\\nDeleteMarkerReplicationStatus.DISABLED))\\n.withFilter(new\\nReplicationFilter().withPredicate(\\nnew\\nReplicationPrefixPredicate(prefix)))\\n.withDestinationConfig(new\\nReplicationDestinationConfig()\\n.withBucketARN(destinationBucketARN)\\n.withStorageClass(StorageClass.Standard)));\\n// Save the replication rule to the source bucket.\\ns3Client.setBucketReplicationConfiguration(sourceBucketName,\\nnew BucketReplicationConfiguration()\\n.withRoleARN(roleARN)\\n.withRules(replicationRules));\\n// Retrieve the replication configuration and verify that\\nthe configuration\\n// matches the rule we just set.\\nBucketReplicationConfiguration replicationConfig = s3Client\\n.getBucketReplicationConfiguration(sourceBucketName);\\nReplicationRule rule =\\nreplicationConfig.getRule(\"ReplicationRule1\");\\nSystem.out.println(\"Retrieved destination bucket ARN: \"', '']]]\n",
      "[[['', '+\\nrule.getDestinationConfig().getBucketARN());\\nSystem.out.println(\"Retrieved priority: \" +\\nrule.getPriority());\\nSystem.out.println(\"Retrieved source-bucket replication rule\\nstatus: \" + rule.getStatus());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3\\ncouldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the\\nclient\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\nprivate static void createBucket(AmazonS3 s3Client, Regions region, String\\nbucketName) {\\nCreateBucketRequest request = new CreateBucketRequest(bucketName,\\nregion.getName());\\ns3Client.createBucket(request);\\nBucketVersioningConfiguration configuration = new\\nBucketVersioningConfiguration()\\n.withStatus(BucketVersioningConfiguration.ENABLED);\\nSetBucketVersioningConfigurationRequest enableVersioningRequest =\\nnew SetBucketVersioningConfigurationRequest(\\nbucketName, configuration);\\ns3Client.setBucketVersioningConfiguration(enableVersioningRequest);\\n}\\nprivate static void assignRole(String roleName, Regions region, String\\nsourceBucket, String destinationBucket) {\\nAmazonIdentityManagement iamClient =\\nAmazonIdentityManagementClientBuilder.standard()\\n.withRegion(region)\\n.withCredentials(new ProfileCredentialsProvider())\\n.build();\\nStringBuilder trustPolicy = new StringBuilder();\\ntrustPolicy.append(\"{\\\\\\\\r\\\\\\\\n \");', '']]]\n",
      "[[['', 'trustPolicy.append(\"\\\\\\\\\\\\\"Version\\\\\\\\\\\\\":\\\\\\\\\\\\\"2012-10-17\\\\\\\\\\\\\",\\\\\\\\r\\\\\\\\n \");\\ntrustPolicy.append(\"\\\\\\\\\\\\\"Statement\\\\\\\\\\\\\":[\\\\\\\\r\\\\\\\\n {\\\\\\\\r\\\\\\\\n\\n\");\\ntrustPolicy.append(\"\\\\\\\\\\\\\"Effect\\\\\\\\\\\\\":\\\\\\\\\\\\\"Allow\\\\\\\\\\\\\",\\\\\\\\r\\\\\\\\n \\\\\\\\\\n\\\\\"Principal\\\\\\\\\\\\\":{\\\\\\\\r\\\\\\\\n \");\\ntrustPolicy.append(\"\\\\\\\\\\\\\"Service\\\\\\\\\\\\\":\\\\\\\\\\\\\"s3.amazonaws.com\\\\\\\\\\\\\"\\\\\\\\r\\\\\\\\n\\n},\\\\\\\\r\\\\\\\\n \");\\ntrustPolicy.append(\"\\\\\\\\\\\\\"Action\\\\\\\\\\\\\":\\\\\\\\\\\\\"sts:AssumeRole\\\\\\\\\\\\\"\\\\\\\\r\\\\\\\\n\\n}\\\\\\\\r\\\\\\\\n ]\\\\\\\\r\\\\\\\\n}\");\\nCreateRoleRequest createRoleRequest = new CreateRoleRequest()\\n.withRoleName(roleName)\\n.withAssumeRolePolicyDocument(trustPolicy.toString());\\niamClient.createRole(createRoleRequest);\\nStringBuilder permissionPolicy = new StringBuilder();\\npermissionPolicy.append(\\n\"{\\\\\\\\r\\\\\\\\n \\\\\\\\\\\\\"Version\\\\\\\\\\\\\":\\\\\\\\\\\\\"2012-10-17\\\\\\\\\\\\\",\\\\\\\\r\\\\\\\\n\\n\\\\\\\\\\\\\"Statement\\\\\\\\\\\\\":[\\\\\\\\r\\\\\\\\n {\\\\\\\\r\\\\\\\\n \");\\npermissionPolicy.append(\\n\"\\\\\\\\\\\\\"Effect\\\\\\\\\\\\\":\\\\\\\\\\\\\"Allow\\\\\\\\\\\\\",\\\\\\\\r\\\\\\\\n \\\\\\\\\\n\\\\\"Action\\\\\\\\\\\\\":[\\\\\\\\r\\\\\\\\n \");\\npermissionPolicy.append(\"\\\\\\\\\\\\\"s3:GetObjectVersionForReplication\\\\\\\\\\\\\",\\\\\\n\\\\r\\\\\\\\n \");\\npermissionPolicy.append(\\n\"\\\\\\\\\\\\\"s3:GetObjectVersionAcl\\\\\\\\\\\\\"\\\\\\\\r\\\\\\\\n ],\\\\\\\\r\\\\\\n\\\\n \\\\\\\\\\\\\"Resource\\\\\\\\\\\\\":[\\\\\\\\r\\\\\\\\n \");\\npermissionPolicy.append(\"\\\\\\\\\\\\\"arn:aws:s3:::\");\\npermissionPolicy.append(sourceBucket);\\npermissionPolicy.append(\"/*\\\\\\\\\\\\\"\\\\\\\\r\\\\\\\\n ]\\\\\\\\r\\\\\\\\n },\\\\\\\\r\\\\\\\\n\\n{\\\\\\\\r\\\\\\\\n \");\\npermissionPolicy.append(\\n\"\\\\\\\\\\\\\"Effect\\\\\\\\\\\\\":\\\\\\\\\\\\\"Allow\\\\\\\\\\\\\",\\\\\\\\r\\\\\\\\n \\\\\\\\\\n\\\\\"Action\\\\\\\\\\\\\":[\\\\\\\\r\\\\\\\\n \");\\npermissionPolicy.append(\\n\"\\\\\\\\\\\\\"s3:ListBucket\\\\\\\\\\\\\",\\\\\\\\r\\\\\\\\n \\\\\\\\\\n\\\\\"s3:GetReplicationConfiguration\\\\\\\\\\\\\"\\\\\\\\r\\\\\\\\n \");\\npermissionPolicy.append(\"],\\\\\\\\r\\\\\\\\n \\\\\\\\\\\\\"Resource\\\\\\\\\\\\\":[\\\\\\\\r\\\\\\\\n\\n\\\\\\\\\\\\\"arn:aws:s3:::\");\\npermissionPolicy.append(sourceBucket);\\npermissionPolicy.append(\"\\\\\\\\r\\\\\\\\n \");\\npermissionPolicy', '']]]\n",
      "[[['', '.append(\"]\\\\\\\\r\\\\\\\\n },\\\\\\\\r\\\\\\\\n {\\\\\\\\r\\\\\\\\n\\n\\\\\\\\\\\\\"Effect\\\\\\\\\\\\\":\\\\\\\\\\\\\"Allow\\\\\\\\\\\\\",\\\\\\\\r\\\\\\\\n \");\\npermissionPolicy.append(\\n\"\\\\\\\\\\\\\"Action\\\\\\\\\\\\\":[\\\\\\\\r\\\\\\\\n \\\\\\\\\\n\\\\\"s3:ReplicateObject\\\\\\\\\\\\\",\\\\\\\\r\\\\\\\\n \");\\npermissionPolicy\\n.append(\"\\\\\\\\\\\\\"s3:ReplicateDelete\\\\\\\\\\\\\",\\\\\\\\r\\\\\\\\n\\n\\\\\\\\\\\\\"s3:ReplicateTags\\\\\\\\\\\\\",\\\\\\\\r\\\\\\\\n \");\\npermissionPolicy.append(\"\\\\\\\\\\\\\"s3:GetObjectVersionTagging\\\\\\\\\\\\\"\\\\\\\\r\\\\\\\\n\\\\\\\\r\\n\\\\\\\\n ],\\\\\\\\r\\\\\\\\n \");\\npermissionPolicy.append(\"\\\\\\\\\\\\\"Resource\\\\\\\\\\\\\":\\\\\\\\\\\\\"arn:aws:s3:::\");\\npermissionPolicy.append(destinationBucket);\\npermissionPolicy.append(\"/*\\\\\\\\\\\\\"\\\\\\\\r\\\\\\\\n }\\\\\\\\r\\\\\\\\n ]\\\\\\\\r\\\\\\\\n}\");\\nPutRolePolicyRequest putRolePolicyRequest = new\\nPutRolePolicyRequest()\\n.withRoleName(roleName)\\n.withPolicyDocument(permissionPolicy.toString())\\n.withPolicyName(\"crrRolePolicy\");\\niamClient.putRolePolicy(putRolePolicyRequest);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass CrossRegionReplicationTest', '']]]\n",
      "[[['', '{\\nprivate const string sourceBucket = \"*** source bucket ***\";\\n// Bucket ARN example - arn:aws:s3:::destinationbucket\\nprivate const string destinationBucketArn = \"*** destination bucket ARN\\n***\";\\nprivate const string roleArn = \"*** IAM Role ARN ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint sourceBucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\npublic static void Main()\\n{\\ns3Client = new AmazonS3Client(sourceBucketRegion);\\nEnableReplicationAsync().Wait();\\n}\\nstatic async Task EnableReplicationAsync()\\n{\\ntry\\n{\\nReplicationConfiguration replConfig = new ReplicationConfiguration\\n{\\nRole = roleArn,\\nRules =\\n{\\nnew ReplicationRule\\n{\\nPrefix = \"Tax\",\\nStatus = ReplicationRuleStatus.Enabled,\\nDestination = new ReplicationDestination\\n{\\nBucketArn = destinationBucketArn\\n}\\n}\\n}\\n};\\nPutBucketReplicationRequest putRequest = new\\nPutBucketReplicationRequest\\n{\\nBucketName = sourceBucket,\\nConfiguration = replConfig\\n};', '']]]\n",
      "[[['', 'PutBucketReplicationResponse putResponse = await\\ns3Client.PutBucketReplicationAsync(putRequest);\\n// Verify configuration by retrieving it.\\nawait RetrieveReplicationConfigurationAsync(s3Client);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\nprivate static async Task RetrieveReplicationConfigurationAsync(IAmazonS3\\nclient)\\n{\\n// Retrieve the configuration.\\nGetBucketReplicationRequest getRequest = new GetBucketReplicationRequest\\n{\\nBucketName = sourceBucket\\n};\\nGetBucketReplicationResponse getResponse = await\\nclient.GetBucketReplicationAsync(getRequest);\\n// Print.\\nConsole.WriteLine(\"Printing replication configuration information...\");\\nConsole.WriteLine(\"Role ARN: {0}\", getResponse.Configuration.Role);\\nforeach (var rule in getResponse.Configuration.Rules)\\n{\\nConsole.WriteLine(\"ID: {0}\", rule.Id);\\nConsole.WriteLine(\"Prefix: {0}\", rule.Prefix);\\nConsole.WriteLine(\"Status: {0}\", rule.Status);\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo use the following example, replace the user input placeholders with your\\nown information. Replace DOC-EXAMPLE-BUCKET with your destination bucket name.\\nReplace source-bucket-acct-ID:role/service-role/source-acct-IAM-\\nrole with the role you are using for this replication configuration.', '']]]\n",
      "[[['', 'If you created the IAM service role manually, set the role path as role/service-\\nrole/, as shown in the below policy example. For more information, see IAM ARNs in\\nthe IAM User Guide.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Id\":\"\",\\n\"Statement\":[\\n{\\n\"Sid\":\"Set-permissions-for-objects\",\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"AWS\":\"arn:aws:iam::source-bucket-acct-ID:role/service-role/source-\\nacct-IAM-role\"\\n},\\n\"Action\":[\"s3:ReplicateObject\", \"s3:ReplicateDelete\"],\\n\"Resource\":\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n},\\n{\\n\"Sid\":\"Set permissions on bucket\",\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"AWS\":\"arn:aws:iam::source-bucket-acct-ID:role/service-role/source-\\nacct-IAM-role\"\\n},\\n\"Action\":[\"s3:GetBucketVersioning\", \"s3:PutBucketVersioning\"],\\n\"Resource\":\"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Warning\\nAdd the owner override option only when the source and destination buckets are owned\\nby different AWS accounts. Amazon S3 doesn't check if the buckets are owned by same or\", '']]]\n",
      "[[['', \"different accounts. If you add the owner override when both buckets are owned by same\\nAWS account, Amazon S3 applies the owner override. It grants full permissions to the\\nowner of the destination bucket and doesn't replicate subsequent updates to the source\\nobject access control list (ACL). The replica owner can directly change the ACL associated\\nwith a replica with a PUT ACL request, but not through replication.\", ''], ['', '', '']], [['', '', ''], ['', '<ReplicationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n...\\n<Destination>\\n...\\n<AccessControlTranslation>\\n<Owner>Destination</Owner>\\n</AccessControlTranslation>\\n<Account>destination-bucket-owner-account-id</Account>\\n</Destination>\\n</Rule>\\n</ReplicationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ReplicationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<Role>arn:aws:iam::account-id:role/role-name</Role>\\n<Rule>\\n<ID>Rule-1</ID>\\n<Priority>1</Priority>\\n<Status>Enabled</Status>\\n<DeleteMarkerReplication>\\n<Status>Disabled</Status>\\n</DeleteMarkerReplication>\\n<Filter>\\n<Prefix>Tax</Prefix>', '']]]\n",
      "[[['', '</Filter>\\n<Destination>\\n<Bucket>arn:aws:s3:::destination-bucket</Bucket>\\n<Account>destination-bucket-owner-account-id</Account>\\n<AccessControlTranslation>\\n<Owner>Destination</Owner>\\n</AccessControlTranslation>\\n</Destination>\\n</Rule>\\n</ReplicationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '...\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:ObjectOwnerOverrideToBucketOwner\"\\n],\\n\"Resource\":\"arn:aws:s3:::destination-bucket/*\"\\n}\\n...', ''], ['', '', '']], [['', '', ''], ['', '...\\n{\\n\"Sid\":\"1\",\\n\"Effect\":\"Allow\",\\n\"Principal\":{\"AWS\":\"source-bucket-account-id\"},\\n\"Action\":[\"s3:ObjectOwnerOverrideToBucketOwner\"],', '']]]\n",
      "[[['', '\"Resource\":\"arn:aws:s3:::destination-bucket/*\"\\n}\\n...', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen you use S3 replication and the source and destination buckets are owned by different\\nAWS accounts, the bucket owner of the destination bucket can disable ACLs (with the\\nbucket owner enforced setting for Object Ownership) to change replica ownership to the\\nAWS account that owns the destination bucket. This setting mimics the existing owner\\noverride behavior without the need of s3:ObjectOwnerOverrideToBucketOwner\\npermission. This means that all objects that are replicated to the destination bucket with\\nthe bucket owner enforced setting are owned by the destination bucket owner. For more\\ninformation about Object Ownership, see Controlling ownership of objects and disabling\\nACLs for your bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nThe profiles you use for this exercise must have the necessary permissions. For\\nexample, in the replication configuration, you specify the IAM role that Amazon S3 can\\nassume. You can do this only if the profile you use has the iam:PassRole permission.\\nIf you use administrator user credentials to create a named profile then you can\\nperform all the tasks. For more information, see Granting a User Permissions to Pass a\\nRole to an AWS Service in the IAM User Guide.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api create-bucket \\\\\\n--bucket source \\\\\\n--region us-east-1 \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-versioning \\\\\\n--bucket source \\\\\\n--versioning-configuration Status=Enabled \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api create-bucket \\\\\\n--bucket destination \\\\\\n--region us-west-2 \\\\\\n--create-bucket-configuration LocationConstraint=us-west-2 \\\\', '']]]\n",
      "[[['', '--profile acctB', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-versioning \\\\\\n--bucket destination \\\\\\n--versioning-configuration Status=Enabled \\\\\\n--profile acctB', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"destination_bucket_policy_sid\",\\n\"Principal\": {\\n\"AWS\": \"source-bucket-owner-account-id\"\\n},\\n\"Action\": [\\n\"s3:ReplicateObject\",\\n\"s3:ReplicateDelete\",\\n\"s3:ObjectOwnerOverrideToBucketOwner\",\\n\"s3:ReplicateTags\",\\n\"s3:GetObjectVersionTagging\"\\n],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\\n\"arn:aws:s3:::destination/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-policy --region $ {destination_region} --\\nbucket $ {destination} --policy file://destination_bucket_policy.json', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\":\"s3.amazonaws.com\"\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '$ aws iam create-role \\\\\\n--role-name replicationRole \\\\\\n--assume-role-policy-document file://s3-role-trust-policy.json \\\\\\n--profile acctA', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetObjectVersionForReplication\",\\n\"s3:GetObjectVersionAcl\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::source/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:ListBucket\",\\n\"s3:GetReplicationConfiguration\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::source\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:ReplicateObject\",\\n\"s3:ReplicateDelete\",\\n\"s3:ObjectOwnerOverrideToBucketOwner\",\\n\"s3:ReplicateTags\",\\n\"s3:GetObjectVersionTagging\"\\n],\\n\"Resource\":\"arn:aws:s3:::destination/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '$ aws iam put-role-policy \\\\\\n--role-name replicationRole \\\\\\n--policy-document file://s3-role-perm-pol-changeowner.json \\\\', '']]]\n",
      "[[['', '--policy-name replicationRolechangeownerPolicy \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Role\":\"IAM-role-ARN\",\\n\"Rules\":[\\n{\\n\"Status\":\"Enabled\",\\n\"Priority\":1,\\n\"DeleteMarkerReplication\":{\\n\"Status\":\"Disabled\"\\n},\\n\"Filter\":{\\n},\\n\"Status\":\"Enabled\",\\n\"Destination\":{\\n\"Bucket\":\"arn:aws:s3:::destination\",\\n\"Account\":\"destination-bucket-owner-account-id\",\\n\"AccessControlTranslation\":{\\n\"Owner\":\"Destination\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '$ aws s3api put-bucket-replication \\\\\\n--replication-configuration file://replication.json \\\\\\n--bucket source \\\\', '']]]\n",
      "[[['', '--profile acctA', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nReplication metrics are billed at the same rate as Amazon CloudWatch custom metrics. For\\ninformation, see Amazon CloudWatch pricing.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nYou incur costs for only one PUT request per object replicated. For more information, see\\nthe pricing information in the Amazon S3 FAQ on replication.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Rules\": [\\n{\\n\"Status\": \"Enabled\",\\n\"Filter\": {\\n\"Prefix\": \"Tax\"\\n},\\n\"DeleteMarkerReplication\": {', '']]]\n",
      "[[['', '\"Status\": \"Disabled\"\\n},\\n\"Destination\": {\\n\"Bucket\": \"arn:aws:s3:::destination\",\\n\"Metrics\": {\\n\"Status\": \"Enabled\",\\n\"EventThreshold\": {\\n\"Minutes\": 15\\n}\\n},\\n\"ReplicationTime\": {\\n\"Status\": \"Enabled\",\\n\"Time\": {\\n\"Minutes\": 15\\n}\\n}\\n},\\n\"Priority\": 1\\n}\\n],\\n\"Role\": \"IAM-Role-ARN\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nMetrics:EventThreshold:Minutes and ReplicationTime:Time:Minutes can\\nonly have 15 as a valid value.', ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.model.DeleteMarkerReplication;\\nimport software.amazon.awssdk.services.s3.model.Destination;\\nimport software.amazon.awssdk.services.s3.model.Metrics;\\nimport software.amazon.awssdk.services.s3.model.MetricsStatus;\\nimport software.amazon.awssdk.services.s3.model.PutBucketReplicationRequest;\\nimport software.amazon.awssdk.services.s3.model.ReplicationConfiguration;', '']]]\n",
      "[[['', 'import software.amazon.awssdk.services.s3.model.ReplicationRule;\\nimport software.amazon.awssdk.services.s3.model.ReplicationRuleFilter;\\nimport software.amazon.awssdk.services.s3.model.ReplicationTime;\\nimport software.amazon.awssdk.services.s3.model.ReplicationTimeStatus;\\nimport software.amazon.awssdk.services.s3.model.ReplicationTimeValue;\\npublic class Main {\\npublic static void main(String[] args) {\\nS3Client s3 = S3Client.builder()\\n.region(Region.US_EAST_1)\\n.credentialsProvider(() -> AwsBasicCredentials.create(\\n\"AWS_ACCESS_KEY_ID\",\\n\"AWS_SECRET_ACCESS_KEY\")\\n)\\n.build();\\nReplicationConfiguration replicationConfig = ReplicationConfiguration\\n.builder()\\n.rules(\\nReplicationRule\\n.builder()\\n.status(\"Enabled\")\\n.priority(1)\\n.deleteMarkerReplication(\\nDeleteMarkerReplication\\n.builder()\\n.status(\"Disabled\")\\n.build()\\n)\\n.destination(\\nDestination\\n.builder()\\n.bucket(\"destination_bucket_arn\")\\n.replicationTime(\\nReplicationTime.builder().time(\\nReplicationTimeValue.builder().minutes(15).build()\\n).status(\\nReplicationTimeStatus.ENABLED\\n).build()\\n)\\n.metrics(\\nMetrics.builder().eventThreshold(\\nReplicationTimeValue.builder().minutes(15).build()', '']]]\n",
      "[[['', ').status(\\nMetricsStatus.ENABLED\\n).build()\\n)\\n.build()\\n)\\n.filter(\\nReplicationRuleFilter\\n.builder()\\n.prefix(\"testtest\")\\n.build()\\n)\\n.build())\\n.role(\"role_arn\")\\n.build();\\n// Put replication configuration\\nPutBucketReplicationRequest putBucketReplicationRequest =\\nPutBucketReplicationRequest\\n.builder()\\n.bucket(\"source_bucket\")\\n.replicationConfiguration(replicationConfig)\\n.build();\\ns3.putBucketReplication(putBucketReplicationRequest);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API', '']]]\n",
      "[[['', 'response header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can use multi-Region AWS KMS keys in Amazon S3. However, Amazon S3 currently\\ntreats multi-Region keys as though they were single-Region keys, and does not use the\\nmulti-Region features of the key. For more information, see Using multi-Region keys in\\nAWS Key Management Service Developer Guide.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"<ReplicationConfiguration>\\n<Rule>\\n...\\n<SourceSelectionCriteria>\\n<SseKmsEncryptedObjects>\\n<Status>Enabled</Status>\\n</SseKmsEncryptedObjects>\\n</SourceSelectionCriteria>\\n<Destination>\\n...\\n<EncryptionConfiguration>\\n<ReplicaKmsKeyID>AWS KMS key ARN or Key Alias ARN that's in the same AWS\\nRegion as the destination bucket.</ReplicaKmsKeyID>\\n</EncryptionConfiguration>\\n</Destination>\\n...\\n</Rule>\\n</ReplicationConfiguration>\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nThe KMS key must have been created in the same AWS Region as the destination buckets.\\nThe KMS key must be valid. The PutBucketReplication API operation doesn't check the\\nvalidity of KMS keys. If you use a KMS key that isn't valid, you will receive the HTTP 200 OK\\nstatus code in response, but replication fails.\", ''], ['', '', '']], [['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ReplicationConfiguration>\\n<Role>arn:aws:iam::account-id:role/role-name</Role>\\n<Rule>\\n<ID>Rule-1</ID>\\n<Priority>1</Priority>\\n<Status>Enabled</Status>\\n<DeleteMarkerReplication>\\n<Status>Disabled</Status>\\n</DeleteMarkerReplication>\\n<Filter>\\n<Prefix>Tax</Prefix>\\n</Filter>\\n<Destination>\\n<Bucket>arn:aws:s3:::example-s3-destination-bucket</Bucket>\\n<EncryptionConfiguration>\\n<ReplicaKmsKeyID>AWS KMS key ARN or Key Alias ARN that\\'s in the same AWS\\nRegion as the destination bucket. (S3 uses this key to encrypt object replicas.)</\\nReplicaKmsKeyID>\\n</EncryptionConfiguration>\\n</Destination>\\n<SourceSelectionCriteria>\\n<SseKmsEncryptedObjects>\\n<Status>Enabled</Status>\\n</SseKmsEncryptedObjects>\\n</SourceSelectionCriteria>\\n</Rule>\\n</ReplicationConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWe recommend that you use the s3:GetObjectVersionForReplication\\naction instead of the s3:GetObjectVersion action because\\ns3:GetObjectVersionForReplication provides Amazon S3 with only the minimum\\npermissions necessary for replication. In addition, the s3:GetObjectVersion action\\nallows replication of unencrypted and SSE-S3-encrypted objects, but not of objects that\\nare encrypted by using KMS keys (SSE-KMS or DSSE-KMS).', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '\"kms:EncryptionContext:aws:s3:arn\": [\\n\"arn:aws:s3:::bucket_ARN\"\\n]', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Action\": [\"kms:Decrypt\"],\\n\"Effect\": \"Allow\",\\n\"Condition\": {\\n\"StringLike\": {\\n\"kms:ViaService\": \"s3.source-bucket-region.amazonaws.com\",\\n\"kms:EncryptionContext:aws:s3:arn\": [\\n\"arn:aws:s3:::example-s3-source-bucket/key-prefix1*\"\\n]\\n}', '']]]\n",
      "[[['', '},\\n\"Resource\": [\\n\"List of AWS KMS key ARNs that are used to encrypt source objects.\"\\n]\\n},\\n{\\n\"Action\": [\"kms:Encrypt\"],\\n\"Effect\": \"Allow\",\\n\"Condition\": {\\n\"StringLike\": {\\n\"kms:ViaService\": \"s3.destination-bucket-1-region.amazonaws.com\",\\n\"kms:EncryptionContext:aws:s3:arn\": [\\n\"arn:aws:s3:::example-s3-destination-bucket1/key-prefix1*\"\\n]\\n}\\n},\\n\"Resource\": [\\n\"AWS KMS key ARNs (in the same AWS Region as destination bucket 1). Used to\\nencrypt object replicas created in destination bucket 1.\"\\n]\\n},\\n{\\n\"Action\": [\"kms:Encrypt\"],\\n\"Effect\": \"Allow\",\\n\"Condition\": {\\n\"StringLike\": {\\n\"kms:ViaService\": \"s3.destination-bucket-2-region.amazonaws.com\",\\n\"kms:EncryptionContext:aws:s3:arn\": [\\n\"arn:aws:s3:::example-s3-destination-bucket2/key-prefix1*\"\\n]\\n}\\n},\\n\"Resource\": [\\n\"AWS KMS key ARNs (in the same AWS Region as destination bucket 2). Used to\\nencrypt object replicas created in destination bucket 2.\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetReplicationConfiguration\",\\n\"s3:ListBucket\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::example-s3-source-bucket\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetObjectVersionForReplication\",\\n\"s3:GetObjectVersionAcl\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::example-s3-source-bucket/key-prefix1*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:ReplicateObject\",\\n\"s3:ReplicateDelete\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-destination-bucket/key-prefix1*\"\\n},\\n{\\n\"Action\":[\\n\"kms:Decrypt\"\\n],\\n\"Effect\":\"Allow\",\\n\"Condition\":{\\n\"StringLike\":{\\n\"kms:ViaService\":\"s3.source-bucket-region.amazonaws.com\",', '']]]\n",
      "[[['', '\"kms:EncryptionContext:aws:s3:arn\":[\\n\"arn:aws:s3:::example-s3-source-bucket/key-prefix1*\"\\n]\\n}\\n},\\n\"Resource\":[\\n\"List of the AWS KMS key ARNs that are used to encrypt source objects.\"\\n]\\n},\\n{\\n\"Action\":[\\n\"kms:Encrypt\"\\n],\\n\"Effect\":\"Allow\",\\n\"Condition\":{\\n\"StringLike\":{\\n\"kms:ViaService\":\"s3.destination-bucket-region.amazonaws.com\",\\n\"kms:EncryptionContext:aws:s3:arn\":[\\n\"arn:aws:s3:::example-s3-destination-bucket/prefix1*\"\\n]\\n}\\n},\\n\"Resource\":[\\n\"AWS KMS key ARNs (in the same AWS Region as the destination bucket) to use\\nfor encrypting object replicas\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetReplicationConfiguration\",\\n\"s3:ListBucket\"', '']]]\n",
      "[[['', '],\\n\"Resource\":[\\n\"arn:aws:s3:::example-s3-source-bucket\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:GetObjectVersionForReplication\",\\n\"s3:GetObjectVersionAcl\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::example-s3-source-bucket/key-prefix1*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:ReplicateObject\",\\n\"s3:ReplicateDelete\"\\n],\\n\"Resource\":\"arn:aws:s3:::example-s3-destination-bucket/key-prefix1*\"\\n},\\n{\\n\"Action\":[\\n\"kms:Decrypt\"\\n],\\n\"Effect\":\"Allow\",\\n\"Condition\":{\\n\"StringLike\":{\\n\"kms:ViaService\":\"s3.source-bucket-region.amazonaws.com\",\\n\"kms:EncryptionContext:aws:s3:arn\":[\\n\"arn:aws:s3:::example-s3-source-bucket\"\\n]\\n}\\n},\\n\"Resource\":[\\n\"List of the AWS KMS key ARNs that are used to encrypt source objects.\"\\n]\\n},\\n{\\n\"Action\":[\\n\"kms:Encrypt\"\\n],', '']]]\n",
      "[[['', '\"Effect\":\"Allow\",\\n\"Condition\":{\\n\"StringLike\":{\\n\"kms:ViaService\":\"s3.destination-bucket-region.amazonaws.com\",\\n\"kms:EncryptionContext:aws:s3:arn\":[\\n\"arn:aws:s3:::example-s3-destination-bucket\"\\n]\\n}\\n},\\n\"Resource\":[\\n\"AWS KMS key ARNs (in the same AWS Region as the destination bucket) to use\\nfor encrypting object replicas\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf you need to replicate SSE-KMS data cross-account, then your replication rule must\\nspecify a customer managed key from AWS KMS for the destination account. AWS\\nmanaged keys don't allow cross-account use, and therefore can't be used to perform cross-\\naccount replication.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nWhen an S3 Bucket Key is enabled for the source or destination bucket, the encryption\\ncontext will be the bucket's Amazon Resource Name (ARN), not the object's ARN. You\\nmust update your IAM policies to use the bucket ARN for the encryption context. For more\\ninformation, see S3 Bucket Keys and replication.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can use multi-Region AWS KMS keys in Amazon S3. However, Amazon S3 currently\\ntreats multi-Region keys as though they were single-Region keys, and does not use the\\nmulti-Region features of the key. For more information, see Using multi-Region keys in\\nAWS Key Management Service Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api create-bucket \\\\\\n--bucket DOC-EXAMPLE-SOURCE-BUCKET \\\\\\n--region us-east-1 \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-versioning \\\\\\n--bucket DOC-EXAMPLE-SOURCE-BUCKET \\\\\\n--versioning-configuration Status=Enabled \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo set up a replication configuration when both DOC-EXAMPLE-SOURCE-BUCKET and\\nDOC-EXAMPLE-DESTINATION-BUCKET buckets are in the same AWS account, you use\\nthe same profile. In this example, we use acctA. To configure replication when the\\nbuckets are owned by different AWS accounts, you specify different profiles for each.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api create-bucket \\\\\\n--bucket DOC-EXAMPLE-DESTINATION-BUCKET \\\\\\n--region us-west-2 \\\\', '']]]\n",
      "[[['', '--create-bucket-configuration LocationConstraint=us-west-2 \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-versioning \\\\\\n--bucket DOC-EXAMPLE-DESTINATION-BUCKET \\\\\\n--versioning-configuration Status=Enabled \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\":\"s3.amazonaws.com\"\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '$ aws iam create-role \\\\\\n--role-name replicationRolekmsobj \\\\\\n--assume-role-policy-document file://s3-role-trust-policy-kmsobj.json \\\\', '']]]\n",
      "[[['', '--profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nIn the permissions policy, you specify the AWS KMS key IDs that will be used\\nfor encryption of the example-s3-source-bucket and example-s3-\\ndestination-bucket buckets. You must create two separate KMS keys\\nfor the example-s3-source-bucket and example-s3-destination-\\nbucket buckets. AWS KMS keys are not shared outside the AWS Region in\\nwhich they were created.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Action\":[\\n\"s3:ListBucket\",\\n\"s3:GetReplicationConfiguration\",\\n\"s3:GetObjectVersionForReplication\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:GetObjectVersionTagging\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\\n\"arn:aws:s3:::example-s3-source-bucket\",\\n\"arn:aws:s3:::example-s3-source-bucket/*\"\\n]\\n},\\n{\\n\"Action\":[\\n\"s3:ReplicateObject\",\\n\"s3:ReplicateDelete\",\\n\"s3:ReplicateTags\"', '']]]\n",
      "[[['', '],\\n\"Effect\":\"Allow\",\\n\"Condition\":{\\n\"StringLikeIfExists\":{\\n\"s3:x-amz-server-side-encryption\":[\\n\"aws:kms\",\\n\"AES256\",\\n\"aws:kms:dsse\"\\n],\\n\"s3:x-amz-server-side-encryption-aws-kms-key-id\":[\\n\"AWS KMS key IDs(in ARN format) to use for encrypting\\nobject replicas\"\\n]\\n}\\n},\\n\"Resource\":\"arn:aws:s3:::example-s3-destination-bucket/*\"\\n},\\n{\\n\"Action\":[\\n\"kms:Decrypt\"\\n],\\n\"Effect\":\"Allow\",\\n\"Condition\":{\\n\"StringLike\":{\\n\"kms:ViaService\":\"s3.us-east-1.amazonaws.com\",\\n\"kms:EncryptionContext:aws:s3:arn\":[\\n\"arn:aws:s3:::example-s3-source-bucket/*\"\\n]\\n}\\n},\\n\"Resource\":[\\n\"AWS KMS key IDs(in ARN format) used to encrypt source\\nobjects.\"\\n]\\n},\\n{\\n\"Action\":[\\n\"kms:Encrypt\"\\n],\\n\"Effect\":\"Allow\",\\n\"Condition\":{\\n\"StringLike\":{\\n\"kms:ViaService\":\"s3.us-west-2.amazonaws.com\",\\n\"kms:EncryptionContext:aws:s3:arn\":[', '']]]\n",
      "[[['', '\"arn:aws:s3:::example-s3-destination-bucket/*\"\\n]\\n}\\n},\\n\"Resource\":[\\n\"AWS KMS key IDs(in ARN format) to use for encrypting object\\nreplicas\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '$ aws iam put-role-policy \\\\\\n--role-name replicationRolekmsobj \\\\\\n--policy-document file://s3-role-permissions-policykmsobj.json \\\\\\n--policy-name replicationRolechangeownerPolicy \\\\\\n--profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nIn the replication configuration, you specify the IAM role that Amazon S3 can assume.\\nYou can do this only if you have the iam:PassRole permission. The profile that you\\nspecify in the CLI command must have this permission. For more information, see\\nGranting a user permissions to pass a role to an AWS service in the IAM User Guide.', ''], ['', '', '']], [['', '', ''], ['', '<ReplicationConfiguration>\\n<Role>IAM-Role-ARN</Role>\\n<Rule>\\n<Priority>1</Priority>\\n<DeleteMarkerReplication>\\n<Status>Disabled</Status>\\n</DeleteMarkerReplication>\\n<Filter>\\n<Prefix>Tax</Prefix>', '']]]\n",
      "[[['', '</Filter>\\n<Status>Enabled</Status>\\n<SourceSelectionCriteria>\\n<SseKmsEncryptedObjects>\\n<Status>Enabled</Status>\\n</SseKmsEncryptedObjects>\\n</SourceSelectionCriteria>\\n<Destination>\\n<Bucket>arn:aws:s3:::example-s3-destination-bucket</Bucket>\\n<EncryptionConfiguration>\\n<ReplicaKmsKeyID>AWS KMS key IDs to use for encrypting object replicas</\\nReplicaKmsKeyID>\\n</EncryptionConfiguration>\\n</Destination>\\n</Rule>\\n</ReplicationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Role\":\"IAM-Role-ARN\",\\n\"Rules\":[\\n{\\n\"Status\":\"Enabled\",\\n\"Priority\":1,\\n\"DeleteMarkerReplication\":{\\n\"Status\":\"Disabled\"\\n},\\n\"Filter\":{\\n\"Prefix\":\"Tax\"\\n},\\n\"Destination\":{\\n\"Bucket\":\"arn:aws:s3:::example-s3-destination-bucket\",\\n\"EncryptionConfiguration\":{\\n\"ReplicaKmsKeyID\":\"AWS KMS key IDs (in ARN format) to use for\\nencrypting object replicas\"\\n}', '']]]\n",
      "[[['', '},\\n\"SourceSelectionCriteria\":{\\n\"SseKmsEncryptedObjects\":{\\n\"Status\":\"Enabled\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '$ aws s3api put-bucket-replication \\\\\\n--replication-configuration file://replication.json \\\\\\n--bucket example-s3-source-bucket \\\\\\n--profile acctA', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou must enable replica modification sync on both buckets to replicate replica metadata\\nchanges like object access control lists (ACLs), object tags, or Object Lock settings on the\\nreplicated objects. Like all replication rules, these rules can either be applied to the entire\\nAmazon S3 bucket or a subset of Amazon S3 objects filtered by prefix or object tags.', ''], ['', '', '']], [['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"Rules\": [\\n{\\n\"Status\": \"Enabled\",\\n\"Filter\": {\\n\"Prefix\": \"Tax\"\\n},\\n\"SourceSelectionCriteria\": {\\n\"ReplicaModifications\":{\\n\"Status\": \"Enabled\"\\n}\\n},\\n\"Destination\": {\\n\"Bucket\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"\\n},\\n\"Priority\": 1\\n}\\n],\\n\"Role\": \"IAM-Role-ARN\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nDelete marker replication is not supported for tag-based replication rules. Delete marker\\nreplication also does not adhere to the 15-minute SLA granted when using S3 Replication\\nTime Control.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Rules\": [\\n{\\n\"Status\": \"Enabled\",\\n\"Filter\": {\\n\"Prefix\": \"Tax\"\\n},\\n\"DeleteMarkerReplication\": {\\n\"Status\": \"Enabled\"\\n},\\n\"Destination\": {\\n\"Bucket\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET\"\\n},\\n\"Priority\": 1\\n}\\n],\\n\"Role\": \"IAM-Role-ARN\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf you disable a replication rule and then later re-enable the rule, any new or\\nchanged objects that weren't replicated while the rule was disabled are not\\nautomatically replicated when the rule is re-enabled. To replicate those objects,\\nyou must use S3 Batch Replication. For more information, see the section called\\n“Replicating existing objects”.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nOperations Failed Replication tracks S3 Replication failures aggregated at a per-minute\\ninterval. To identify the specific objects that have failed replication and their failure\\nreasons, subscribe to the OperationFailedReplication event in Amazon S3 Event\\nNotifications. For more information, see Receiving replication failure events with Amazon\\nS3 Event Notifications.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Rules\": [\\n{\\n\"Status\": \"Enabled\",\\n\"Filter\": {\\n\"Prefix\": \"Tax\"\\n},\\n\"Destination\": {\\n\"Bucket\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET\",\\n\"Metrics\": {\\n\"Status\": \"Enabled\"\\n}\\n},\\n\"Priority\": 1\\n}\\n],\\n\"Role\": \"IAM-Role-ARN\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nS3 Replication metrics are billed at the same rate as Amazon CloudWatch custom metrics.\\nFor more information, see Amazon CloudWatch pricing.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou can also view detailed metrics for S3 Replication in the Amazon S3 console by using\\nAmazon S3 Storage Lens. S3 Storage Lens is a cloud-storage analytics feature that you can\\nuse to gain organization-wide visibility into object-storage usage and activity. For more\\ninformation, see Using S3 Storage Lens to protect your data. For a complete list of metrics,\\nsee the S3 Storage Lens metrics glossary.', ''], ['', '', '']]]\n",
      "[[['Replication failure reason', 'Description'], ['AssumeRoleNotPermitted', \"Amazon S3 can't assume the AWS Identity\\nand Access Management (IAM) role that's\\nspecified in the replication configuration or\\nin the Batch Operations job.\"], ['DstBucketInvalidRegion', 'The destination bucket is not in the same\\nAWS Region as specified by the Batch\\nOperations job. This error is specific to Batch\\nReplication.'], ['DstBucketNotFound', \"Amazon S3 is unable to find the destinati\\non bucket that's specified in the replication\\nconfiguration.\"], ['DstBucketObjectLockConfigMissing', 'To replicate objects from a source bucket\\nwith Object Lock enabled, the destination\\nbucket must also have Object Lock enabled.\\nThis error indicates that Object Lock might\\nnot be enabled in the destination bucket. For\\nmore information, see Object Lock considera\\ntions.'], ['DstBucketUnversioned', 'Versioning is not enabled for the S3\\ndestination bucket. To replicate objects with\\nS3 Replication, enable versioning for the\\ndestination bucket.'], ['DstDelObjNotPermitted', 'Amazon S3 is unable to replicate delete\\nmarkers to the destination bucket. The']]]\n",
      "[[['Replication failure reason', 'Description'], ['', 's3:ReplicateDelete permission might\\nbe missing for the destination bucket.'], ['DstKmsKeyInvalidState', \"The AWS Key Management Service (AWS\\nKMS) key for the destination bucket isn't in a\\nvalid state. Review and enable the required\\nAWS KMS key. For more information about\\nmanaging AWS KMS keys, see Key states of\\nAWS KMS keys in the AWS Key Management\\nService Developer Guide.\"], ['DstKmsKeyNotFound', \"The AWS KMS key that's configured for\\nthe destination bucket in the replication\\nconfiguration doesn't exist.\"], ['DstMultipartCompleteNotPermitted', 'Amazon S3 is unable to complete multipart\\nuploads of objects in the destination bucket.\\nThe s3:ReplicateObject permission\\nmight be missing for the destination bucket.'], ['DstMultipartInitNotPermitted', 'Amazon S3 is unable to initiate multipart\\nuploads of objects to the destination bucket.\\nThe s3:ReplicateObject permission\\nmight be missing for the destination bucket.'], ['DstMultipartPartUploadNotPe\\nrmitted', 'Amazon S3 is unable to upload multipart\\nobjects to the destination bucket. The\\ns3:ReplicateObject permission might\\nbe missing for the destination bucket.'], ['DstObjectHardDeleted', 'S3 Batch Replication does not support re-\\nreplicating objects deleted with the version\\nID of the object from the destination bucket.\\nThis error is specific to Batch Replication.']]]\n",
      "[[['Replication failure reason', 'Description'], ['DstPutAclNotPermitted', 'Amazon S3 is unable to replicate object\\naccess control lists (ACLs) to the destinati\\non bucket. The s3:ReplicateObject\\npermission might be missing for the\\ndestination bucket.'], ['DstPutLegalHoldNotPermitted', 'Amazon S3 is unable to put an Object Lock\\nlegal hold on the destination objects when\\nit is replicating immutable objects. The\\ns3:PutObjectLegalHold permission\\nmight be missing for the destination bucket.\\nFor more information, see Legal holds.'], ['DstPutObjectNotPermitted', 'Amazon S3 is unable to replicate objects\\nto the destination bucket. The s3:Replic\\nateObject or s3:ObjectOwnerOver\\nrideToBucketOwner permissions might\\nbe missing for the destination bucket.'], ['DstPutTaggingNotPermitted', 'Amazon S3 is unable to replicate object tags\\nto the destination bucket. The s3:Replic\\nateObject permission might be missing\\nfor the destination bucket.'], ['DstVersionNotFound', 'Amazon S3 is unable to find the required\\nobject version in the destination bucket for\\nwhich metadata needs to be replicated.'], ['InitiateReplicationNotPermitted', 'Amazon S3 is unable to initiate replication\\non objects. The s3:InitiateReplica\\ntion permission might be missing for the\\nBatch Operations job. This error is specific to\\nBatch Replication.']]]\n",
      "[[['Replication failure reason', 'Description'], ['SrcBucketInvalidRegion', 'The source bucket is not in the same AWS\\nRegion as specified by the Batch Operation\\ns job. This error is specific to Batch Replicati\\non.'], ['SrcBucketNotFound', 'Amazon S3 is unable to find the source\\nbucket.'], ['SrcBucketReplicationConfigMissing', \"Amazon S3 couldn't find a replication\\nconfiguration for the source bucket.\"], ['SrcGetAclNotPermitted', 'Amazon S3 is unable to access the object\\nin the source bucket for replication. The\\ns3:GetObjectVersionAcl permissio\\nn might be missing for the source bucket\\nobject.\\nThe objects in the source bucket must be\\nowned by the bucket owner. If ACLs are\\nenabled, then verify if Object Ownership\\nis set to Bucket owner preferred or Object\\nwriter. If Object Ownership is set to Bucket\\nowner preferred, then the source bucket\\nobjects must have the bucket-owner-\\nfull-control ACL for the bucket owner\\nto become the object owner. The source\\naccount can take ownership of all objects in\\ntheir bucket by setting Object Ownership to\\nBucket owner enforced and disabling ACLs.'], ['SrcGetLegalHoldNotPermitted', 'Amazon S3 is unable to access the S3 Object\\nLock legal hold information.']]]\n",
      "[[['Replication failure reason', 'Description'], ['SrcGetObjectNotPermitted', 'Amazon S3 is unable to access the object\\nin the source bucket for replication. The\\ns3:GetObjectVersionForRepli\\ncation permission might be missing for\\nthe source bucket.'], ['SrcGetRetentionNotPermitted', 'Amazon S3 is unable to access the S3 Object\\nLock retention period information.'], ['SrcGetTaggingNotPermitted', 'Amazon S3 is unable to access object tag\\ninformation from the source bucket. The\\ns3:GetObjectVersionTagging\\npermission might be missing for the source\\nbucket.'], ['SrcHeadObjectNotPermitted', 'Amazon S3 is unable to retrieve object\\nmetadata from the source bucket. The\\ns3:GetObjectVersionForRepli\\ncation permission might be missing for\\nthe source bucket.'], ['SrcKeyNotFound', 'Amazon S3 is unable to find the source\\nobject key to replicate. Source object may\\nhave been deleted before replication was\\ncomplete.'], ['SrcKmsKeyInvalidState', \"The AWS KMS key for the source bucket\\nisn't in a valid state. Review and enable the\\nrequired AWS KMS key. For more informati\\non about managing AWS KMS keys, see\\nKey states of AWS KMS keys in the AWS Key\\nManagement Service Developer Guide.\"]]]\n",
      "[[['Replication failure reason', 'Description'], ['SrcObjectNotEligible', \"Some objects aren't eligible for replication.\\nThis may be due to the object's storage class\\nor the object tags don't match the replicati\\non configuration.\"], ['SrcObjectNotFound', 'Source object does not exist.'], ['SrcReplicationNotPending', 'Amazon S3 has already replicated this\\nobject. This object is no longer pending\\nreplication.'], ['SrcVersionNotFound', 'Amazon S3 is unable to find the source\\nobject version to replicate. Source object\\nversion may have been deleted before\\nreplication was complete.']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf object replication fails after you upload an object, you can't retry replication. You must\\nupload the object again. Objects transition to a FAILED state for issues such as missing\\nreplication role permissions, AWS KMS permissions, or bucket permissions. For temporary\\nfailures, such as if a bucket or Region is unavailable, replication status will not transition\\nto FAILED, but will remain PENDING. After the resource is back online, S3 will resume\\nreplicating those objects.\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nBefore deleting an object from a source bucket that has replication enabled, check the\\nobject's replication status to ensure that the object has been replicated.\\nIf lifecycle configuration is enabled on the source bucket, Amazon S3 suspends lifecycle\\nactions until it marks the objects status as either COMPLETED or FAILED.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3api head-object --bucket source-bucket --key object-key --version-id object-\\nversion-id', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"AcceptRanges\":\"bytes\",\\n\"ContentType\":\"image/jpeg\",\\n\"LastModified\":\"Mon, 23 Mar 2015 21:02:29 GMT\",\\n\"ContentLength\":3191,\\n\"ReplicationStatus\":\"COMPLETED\",\\n\"VersionId\":\"jfnW.HIMOfYiD_9rGbSkmroXsFj3fqZ.\",\\n\"ETag\":\"\\\\\"6805f2cfc46c0f04559748bb039d69ae\\\\\"\",\\n\"Metadata\":{\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'GetObjectMetadataRequest metadataRequest = new GetObjectMetadataRequest(bucketName,\\nkey);\\nObjectMetadata metadata = s3Client.getObjectMetadata(metadataRequest);\\nSystem.out.println(\"Replication Status : \" +\\nmetadata.getRawMetadataValue(Headers.OBJECT_REPLICATION_STATUS));', ''], ['', '', '']], [['', '', ''], ['', 'GetObjectMetadataRequest getmetadataRequest = new GetObjectMetadataRequest\\n{', '']]]\n",
      "[[['', 'BucketName = sourceBucket,\\nKey = objectKey\\n};\\nGetObjectMetadataResponse getmetadataResponse =\\nclient.GetObjectMetadata(getmetadataRequest);\\nConsole.WriteLine(\"Object replication status: {0}\",\\ngetmetadataResponse.ReplicationStatus);', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf you choose to have Amazon S3 generate the manifest, the manifest must be stored in the\\nsame AWS Region as the source bucket.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nDifferent permission are needed if you are generating a manifest or supplying one. For\\nmore information see, Specifying a manifest for a Batch Replication job.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Action\":[\\n\"s3:InitiateReplication\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\\n\"arn:aws:s3:::*** replication source bucket ***/*\"', '']]]\n",
      "[[['', ']\\n},\\n{\\n\"Action\":[\\n\"s3:GetReplicationConfiguration\",\\n\"s3:PutInventoryConfiguration\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\\n\"arn:aws:s3:::*** replication source bucket ***\"\\n]\\n},\\n{\\n\"Action\":[\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\\n\"arn:aws:s3:::*** manifest bucket ***/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::*** completion report bucket ****/*\",\\n\"arn:aws:s3:::*** manifest bucket ****/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Action\":[\\n\"s3:InitiateReplication\"', '']]]\n",
      "[[['', '],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\\n\"arn:aws:s3:::*** replication source bucket ***/*\"\\n]\\n},\\n{\\n\"Action\":[\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\\n\"arn:aws:s3:::*** manifest bucket ***/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::*** completion report bucket ****/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\":\"batchoperations.s3.amazonaws.com\"\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nFor examples on how to set up a basic replication rule see, Examples for configuring\\nlive replication.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe manifest is a list of all of the objects that you want to run the specified action\\non. To learn more about Batch Operations manifests, see Specifying a manifest. If\\nyou have a manifest prepared, choose S3 inventory report (manifest.json) or CSV. If', '']]]\n",
      "[[['', 'the objects in your manifest are in a versioned bucket, you should specify the version\\nIDs for the objects. For more information about creating a manifest see, Specifying a\\nmanifest.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe generated manifest must be stored in the same AWS Region as the source\\nbucket.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor more information about creating a IAM role, see Configuring IAM policies for Batch\\nReplication.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-job --account-id 111122223333 --operation\\n\\'{\"S3ReplicateObject\":{}}\\' --report \\'{\"Bucket\":\"arn:aws:s3:::***\\ncompletion report bucket ****\",\"Prefix\":\"batch-replication-report\",\\n\"Format\":\"Report_CSV_20180820\",\"Enabled\":true,\"ReportScope\":\"AllTasks\"}\\'\\n--manifest-generator \\'{\"S3JobManifestGenerator\": {\"ExpectedBucketOwner\":\\n\"111122223333\", \"SourceBucket\": \"arn:aws:s3:::*** replication source bucket\\n***\", \"EnableManifestOutput\": false, \"Filter\": {\"EligibleForReplication\": true,\\n\"ObjectReplicationStatuses\": [\"NONE\",\"FAILED\"]}}}\\' --priority 1 --role-arn\\narn:aws:iam::111122223333:role/batch-Replication-IAM-policy --no-confirmation-required\\n--region source-bucket-region', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe job must be initiated from the same AWS Region replication source bucket. The IAM\\nrole role/batch-Replication-IAM-policy was previously created. See Configuring\\nIAM policies for Batch Replication.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control describe-job --account-id 111122223333 --job-id job-id --region source-\\nbucket-region', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-job --account-id 111122223333 --operation\\n\\'{\"S3ReplicateObject\":{}}\\' --report \\'{\"Bucket\":\"arn:aws:s3:::***\\ncompletion report bucket ****\",\"Prefix\":\"batch-replication-report\",\\n\"Format\":\"Report_CSV_20180820\",\"Enabled\":true,\"ReportScope\":\"AllTasks\"}\\'\\n--manifest \\'{\"Spec\":{\"Format\":\"S3BatchOperations_CSV_20180820\",\"Fields\":\\n[\"Bucket\",\"Key\",\"VersionId\"]},\"Location\":{\"ObjectArn\":\"arn:aws:s3:::*** completion\\nreport bucket ****/manifest.csv\",\"ETag\":\"Manifest Etag\"}}\\' --priority 1 --role-arn\\narn:aws:iam::111122223333:role/batch-Replication-IAM-policy --no-confirmation-required\\n--region source-bucket-region', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe job must be initiated from the same AWS Region replication source bucket. The IAM\\nrole role/batch-Replication-IAM-policy was previously created. See Configuring\\nIAM policies for Batch Replication.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control describe-job --account-id 111122223333 --job-id job-id --region source-\\nbucket-region', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'PHI=True', ''], ['', '', '']], [['', '', ''], ['', 'Classification=PHI', ''], ['', '', '']], [['', '', ''], ['', 'Project=Blue', ''], ['', '', '']], [['', '', ''], ['', 'Project=x\\nClassification=confidential', ''], ['', '', '']], [['', '', ''], ['', 'photos/photo1.jpg\\nproject/projectx/document.pdf\\nproject/projecty/document2.pdf', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nIt is acceptable to use tags to label objects containing confidential data, such as personally\\nidentifiable information (PII) or protected health information (PHI). However, the tags\\nthemselves shouldn't contain any confidential information.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf you send this request with an empty tag set, Amazon S3 deletes the existing tag set\\non the object. If you use this method, you will be charged for a Tier 1 Request (PUT).\\nFor more information, see Amazon S3 Pricing.\\nThe DELETE Object tagging request is preferred because it achieves the same result\\nwithout incurring charges.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'phototype=raw\\nor\\nphototype=finished', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen granting permissions for the PUT Object and DELETE Object operations, this\\ncondition key is not supported. That is, you cannot create a policy to grant or deny a user\\npermissions to delete or overwrite an object based on its existing tags.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::111122223333:role/JohnDoe\"\\n]\\n},\\n\"Effect\": \"Allow\",\\n\"Action\": [\"s3:GetObject\", \"s3:GetObjectVersion\"],\\n\"Resource\": \"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\",\\n\"Condition\": {\\n\"StringEquals\":\\n{\"s3:ExistingObjectTag/environment\": \"production\"}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\"Principal\":{\"AWS\":[\\n\"arn:aws:iam::111122223333:role/JohnDoe\"\\n]\\n},\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObjectTagging\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n],\\n\"Condition\": {\"ForAnyValue:StringEquals\": {\"s3:RequestObjectTagKeys\": [\\n\"Owner\",\\n\"CreationDate\"\\n]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\"Principal\":{\"AWS\":[\\n\"arn:aws:iam::111122223333:user/JohnDoe\"\\n]\\n},\\n\"Effect\": \"Allow\",', '']]]\n",
      "[[['', '\"Action\": [\\n\"s3:PutObjectTagging\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"\\n],\\n\"Condition\": {\"StringEquals\": {\"s3:RequestObjectTag/Project\": \"X\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.*;\\nimport java.io.File;\\nimport java.util.ArrayList;', '']]]\n",
      "[[['', 'import java.util.List;\\npublic class ManagingObjectTags {\\npublic static void main(String[] args) {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\nString keyName = \"*** Object key ***\";\\nString filePath = \"*** File path ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\n// Create an object, add two new tags, and upload the object to Amazon\\nS3.\\nPutObjectRequest putRequest = new PutObjectRequest(bucketName, keyName,\\nnew File(filePath));\\nList<Tag> tags = new ArrayList<Tag>();\\ntags.add(new Tag(\"Tag 1\", \"This is tag 1\"));\\ntags.add(new Tag(\"Tag 2\", \"This is tag 2\"));\\nputRequest.setTagging(new ObjectTagging(tags));\\nPutObjectResult putResult = s3Client.putObject(putRequest);\\n// Retrieve the object\\'s tags.\\nGetObjectTaggingRequest getTaggingRequest = new\\nGetObjectTaggingRequest(bucketName, keyName);\\nGetObjectTaggingResult getTagsResult =\\ns3Client.getObjectTagging(getTaggingRequest);\\n// Replace the object\\'s tags with two new tags.\\nList<Tag> newTags = new ArrayList<Tag>();\\nnewTags.add(new Tag(\"Tag 3\", \"This is tag 3\"));\\nnewTags.add(new Tag(\"Tag 4\", \"This is tag 4\"));\\ns3Client.setObjectTagging(new SetObjectTaggingRequest(bucketName,\\nkeyName, new ObjectTagging(newTags)));\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client', '']]]\n",
      "[[['', \"// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\npublic class ObjectTagsTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\nprivate const string keyName = \"*** key name for the new object ***\";\\nprivate const string filePath = @\"*** file path ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nclient = new AmazonS3Client(bucketRegion);\\nPutObjectWithTagsTestAsync().Wait();\\n}\\nstatic async Task PutObjectWithTagsTestAsync()\\n{', '']]]\n",
      "[[['', 'try\\n{\\n// 1. Put an object with tags.\\nvar putRequest = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nFilePath = filePath,\\nTagSet = new List<Tag>{\\nnew Tag { Key = \"Keyx1\", Value = \"Value1\"},\\nnew Tag { Key = \"Keyx2\", Value = \"Value2\" }\\n}\\n};\\nPutObjectResponse response = await\\nclient.PutObjectAsync(putRequest);\\n// 2. Retrieve the object\\'s tags.\\nGetObjectTaggingRequest getTagsRequest = new GetObjectTaggingRequest\\n{\\nBucketName = bucketName,\\nKey = keyName\\n};\\nGetObjectTaggingResponse objectTags = await\\nclient.GetObjectTaggingAsync(getTagsRequest);\\nfor (int i = 0; i < objectTags.Tagging.Count; i++)\\nConsole.WriteLine(\"Key: {0}, Value: {1}\",\\nobjectTags.Tagging[i].Key, objectTags.Tagging[i].Value);\\n// 3. Replace the tagset.\\nTagging newTagSet = new Tagging();\\nnewTagSet.TagSet = new List<Tag>{\\nnew Tag { Key = \"Key3\", Value = \"Value3\"},\\nnew Tag { Key = \"Key4\", Value = \"Value4\" }\\n};\\nPutObjectTaggingRequest putObjTagsRequest = new\\nPutObjectTaggingRequest()\\n{\\nBucketName = bucketName,\\nKey = keyName,', '']]]\n",
      "[[['', 'Tagging = newTagSet\\n};\\nPutObjectTaggingResponse response2 = await\\nclient.PutObjectTaggingAsync(putObjTagsRequest);\\n// 4. Retrieve the object\\'s tags.\\nGetObjectTaggingRequest getTagsRequest2 = new\\nGetObjectTaggingRequest();\\ngetTagsRequest2.BucketName = bucketName;\\ngetTagsRequest2.Key = keyName;\\nGetObjectTaggingResponse objectTags2 = await\\nclient.GetObjectTaggingAsync(getTagsRequest2);\\nfor (int i = 0; i < objectTags2.Tagging.Count; i++)\\nConsole.WriteLine(\"Key: {0}, Value: {1}\",\\nobjectTags2.Tagging[i].Key, objectTags2.Tagging[i].Value);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\\n\"Error encountered ***. Message:\\'{0}\\' when writing an\\nobject\"\\n, e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\\n\"Encountered an error. Message:\\'{0}\\' when writing an object\"\\n, e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\nOn May 13, 2024, we started deploying a change to eliminate charges for unauthorized\\nrequests that aren't initiated by the bucket owner. After the deployment of this change is\\ncompleted, bucket owners will never incur request or bandwidth charges for requests that\\nreturn AccessDenied (HTTP 403 Forbidden) errors when these requests are initiated\\nfrom outside of their individual AWS account or AWS organization. For more information\\non a full list of HTTP 3XX and 4XX status codes that won't be billed, see Billing for Amazon\\nS3 error responses. This billing change requires no updates to your applications and applies\\nto all S3 buckets. When deployment of this change is completed in all AWS Regions, we’ll\\nupdate our documentation.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['Charge', 'Comments'], ['Storage', \"You pay for storing objects in your S3 buckets.\\nThe rate you're charged depends on your\\nobjects' size, how long you stored the objects\\nduring the month, and the storage class.\\nAmazon S3 offers the following storage\\nclasses: S3 Standard, S3 Express One Zone,\\nS3 Intelligent-Tiering, S3 Standard-IA (IA\\nfor infrequent access), S3 One Zone-IA, S3\\nGlacier Instant Retrieval, S3 Glacier Flexible\\nRetrieval, S3 Glacier Deep Archive, or Reduced\\nRedundancy Storage (RRS). For more informati\\non about storage classes, see Using Amazon\\nS3 storage classes.\\nBe aware that if you have S3 Versioning\\nenabled, you're charged for each version of\\nan object that is retained. For more informati\\non about versioning, see How S3 Versioning\\nworks.\"], ['Monitoring and automation', 'You pay a monthly monitoring and automatio\\nn fee per object stored in the S3 Intellige\\nnt-Tiering storage class to monitor access\\npatterns and move objects between access\\ntiers in S3 Intelligent-Tiering.'], ['Requests', \"You pay for requests, for example, GET\\nrequests, made against your S3 buckets and\\nobjects. This includes lifecycle requests. The\\nrates for requests depend on what kind of\\nrequest you're making. For information about\\nrequest pricing, see Amazon S3 Pricing.\"]]]\n",
      "[[['Charge', 'Comments'], ['Retrievals', 'You pay for retrieving objects that are stored\\nin S3 Standard-IA, S3 One Zone-IA, S3 Glacier\\nInstant Retrieval, S3 Glacier Flexible Retrieval,\\nand S3 Glacier Deep Archive storage.'], ['Early deletes', 'If you delete an object stored in S3 Standard-\\nIA, S3 One Zone-IA, S3 Glacier Instant\\nRetrieval, S3 Glacier Flexible Retrieval, or\\nS3 Glacier Deep Archive storage before the\\nminimum storage commitment has passed,\\nyou pay an early deletion fee for that object.'], ['Storage management', \"You pay for the storage management features\\n(Amazon S3 Inventory, analytics, and object\\ntagging) that are enabled on your account's\\nbuckets.\"], ['Bandwidth', 'You pay for all bandwidth into and out of\\nAmazon S3, except for the following:\\n•\\nData transferred in from the internet\\n•\\nData transferred out to an Amazon Elastic\\nCompute Cloud (Amazon EC2) instance,\\nwhen the instance is in the same AWS\\nRegion as the S3 bucket\\n•\\nData transferred out to Amazon CloudFront\\n(CloudFront)\\nYou also pay a fee for any data transferred by\\nusing Amazon S3 Transfer Acceleration.']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Tip\\nFor detailed information about every request that Amazon S3 receives for your objects,\\nturn on server access logging for your buckets. For more information, see Logging requests\\nwith server access logging.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nOn May 13, 2024, we started deploying a change to eliminate charges for unauthorized\\nrequests that aren't initiated by the bucket owner. After the deployment of this change is\\ncompleted, bucket owners will never incur request or bandwidth charges for requests that\\nreturn AccessDenied (HTTP 403 Forbidden) errors when these requests are initiated\\nfrom outside of their individual AWS account or AWS organization. For more information\", '']]]\n",
      "[[['', \"on a full list of HTTP 3XX and 4XX status codes that won't be billed, see Billing for Amazon\\nS3 error responses. This billing change requires no updates to your applications and applies\\nto all S3 buckets. When deployment of this change is completed in all AWS Regions, we’ll\\nupdate our documentation.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region1-region2-AWS-In-A\\nBytes', 'GB', 'Hourly', 'The amount of accelerat\\ned data transferred to\\nregion1 from r egion2'], ['region1-region2-AWS-In-A\\nBytes-T1', 'GB', 'Hourly', 'The amount of T1\\naccelerated data\\ntransferred to region1\\nfrom region2, where\\nT1 refers to CloudFron\\nt requests to points of\\npresence (POPs) in the\\nUnited States, Europe,\\nand Japan'], ['region1-region2-AWS-In-A\\nBytes-T2', 'GB', 'Hourly', 'The amount of T2\\naccelerated data\\ntransferred to region1\\nfrom region2, where\\nT2 refers to CloudFron\\nt requests to POPs in all\\nother AWS edge locati\\nons'], ['region1-region2-AWS-In-Bytes', 'GB', 'Hourly', 'The amount of data\\ntransferred to region1\\nfrom region2'], ['region1-region2-AWS-Out-\\nABytes', 'GB', 'Hourly', 'The amount of accelerat\\ned data transferred from\\nregion1 to r egion2'], ['region1-region2-AWS-Out-\\nABytes-T1', 'GB', 'Hourly', 'The amount of T1\\naccelerated data\\ntransferred from']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['', '', '', 'region1 to r egion2,\\nwhere T1 refers to\\nCloudFront requests\\nto POPs in the United\\nStates, Europe, and\\nJapan'], ['region1-region2-AWS-Out-\\nABytes-T2', 'GB', 'Hourly', 'The amount of T2\\naccelerated data\\ntransferred from\\nregion1 to r egion2,\\nwhere T2 refers to\\nCloudFront requests to\\nPOPs in all other AWS\\nedge locations'], ['region1-region2-AWS-Out-\\nBytes', 'GB', 'Hourly', 'The amount of data\\ntransferred from\\nregion1 to r egion2'], ['region-BatchOperations-J\\nobs', 'Count', 'Hourly', 'The number of S3\\nBatch Operations jobs\\nperformed'], ['region-BatchOperations-O\\nbjects', 'Count', 'Hourly', 'The number of object\\noperations performed by\\nS3 Batch Operations'], ['region-Bulk-Retrieval-Bytes', 'GB', 'Hourly', 'The amount of data\\nretrieved with Bulk S3\\nGlacier Flexible Retrieval\\nor S3 Glacier Deep\\nArchive requests']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-BytesDeleted-GDA', 'GB', 'Monthly', 'The amount of data\\ndeleted by a DeleteObj\\nect operation from\\nS3 Glacier Deep Archive\\nstorage'], ['region-BytesDeleted-GIR', 'GB', 'Monthly', 'The amount of data\\ndeleted by a DeleteObj\\nect operation from S3\\nGlacier Instant Retrieval\\nstorage.'], ['region-BytesDeleted-GLACIER', 'GB', 'Monthly', 'The amount of data\\ndeleted by a DeleteObj\\nect operation from S3\\nGlacier Flexible Retrieval\\nstorage'], ['region-BytesDeleted-INT', 'GB', 'Monthly', 'The amount of data\\ndeleted by a DeleteObj\\nect operation from\\nS3 Intelligent-Tiering\\nstorage'], ['region-BytesDeleted-RRS', 'GB', 'Monthly', 'The amount of data\\ndeleted by a DeleteObj\\nect operation from\\nReduced Redundancy\\nStorage (RRS) storage'], ['region-BytesDeleted-SIA', 'GB', 'Monthly', 'The amount of data\\ndeleted by a DeleteObj\\nect operation from S3\\nStandard-IA storage']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-BytesDeleted-STAN\\nDARD', 'GB', 'Monthly', 'The amount of data\\ndeleted by a DeleteObj\\nect operation from S3\\nStandard storage'], ['region-BytesDeleted-ZIA', 'GB', 'Monthly', 'The amount of data\\ndeleted by a DeleteObj\\nect operation from S3\\nOne Zone-IA storage'], ['region-C3DataTransfer-In-\\nBytes', 'GB', 'Hourly', 'The amount of data\\ntransferred into Amazon\\nS3 from Amazon EC2\\nwithin the same AWS\\nRegion'], ['region-C3DataTransfer-Out-\\nBytes', 'GB', 'Hourly', 'The amount of data\\ntransferred from Amazon\\nS3 to Amazon EC2 within\\nthe same AWS Region'], ['region-CloudFront-In-Bytes', 'GB', 'Hourly', 'The amount of data\\ntransferred into an AWS\\nRegion from a CloudFron\\nt distribution'], ['region-CloudFront-Out-Bytes', 'GB', 'Hourly', 'The amount of data\\ntransferred from an AWS\\nRegion to a CloudFront\\ndistribution'], ['region-DataTransfer-In-B\\nytes', 'GB', 'Hourly', 'The amount of data\\ntransferred into Amazon\\nS3 from the internet']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-DataTransfer-Out-\\nBytes', 'GB', 'Hourly', 'The amount of data\\ntransferred from Amazon\\n1\\nS3 to the internet'], ['region-DataTransfer-Regi\\nonal-Bytes', 'GB', 'Hourly', 'The amount of data\\ntransferred from Amazon\\nS3 to AWS resources\\nwithin the same AWS\\nRegion'], ['region-EarlyDelete-ByteHrs', 'GB-Hours', 'Hourly', 'Prorated storage usage\\nfor objects deleted from,\\nS3 Glacier Flexible\\nRetrieval storage before\\nthe 90-day minimum co\\n2\\nmmitment ended'], ['region-EarlyDelete-GDA', 'GB-Hours', 'Hourly', 'Prorated storage usage\\nfor objects deleted\\nfrom S3 Glacier Deep\\nArchive storage before\\nthe 180-day minimum\\n2\\ncommitment ended'], ['region-EarlyDelete-GIR', 'GB-Hours', 'Hourly', 'Prorated storage usage\\nfor objects deleted\\nfrom S3 Glacier Instant\\nRetrieval before the\\n90-day minimum\\ncommitment ended.']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-EarlyDelete-GIR-S\\nmObjects', 'GB-Hours', 'Hourly', 'Prorated storage usage\\nfor small objects (smaller\\nthan 128 KB) that were\\ndeleted from S3 Glacier\\nInstant Retrieval before\\nthe 90-day minimum\\ncommitment ended.'], ['region-EarlyDelete-SIA', 'GB-Hours', 'Hourly', 'Prorated storage usage\\nfor objects deleted from\\nS3 Standard-IA before\\nthe 30-day minimum co\\n3\\nmmitment ended'], ['region-EarlyDelete-SIA-S\\nmObjects', 'GB-Hours', 'Hourly', 'Prorated storage usage\\nfor small objects (smaller\\nthan 128 KB) that\\nwere deleted from S3\\nStandard-IA before\\nthe 30-day minimum\\n3\\ncommitment ended'], ['region-EarlyDelete-ZIA', 'GB-Hours', 'Hourly', 'Prorated storage usage\\nfor objects deleted from\\nS3 One Zone-IA before\\nthe 30-day minimum\\n3\\ncommitment ended']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-EarlyDelete-ZIA-S\\nmObjects', 'GB-Hours', 'Hourly', 'Prorated storage usage\\nfor small objects (smaller\\nthan 128 KB) that were\\ndeleted from S3 One\\nZone-IA before the\\n30-day minimum co\\n3\\nmmitment ended'], ['region-Expedited-Retrieval-\\nBytes', 'GB', 'Hourly', 'The amount of data\\nretrieved with Expedited\\nS3 Glacier Flexible\\nRetrieval requests'], ['region-Inventory-Objects\\nListed', 'Objects', 'Hourly', 'The number of objects\\nlisted for an object group\\n(objects are grouped by\\nbucket or prefix) with an\\ninventory list'], ['region-Monitoring-Automa\\ntion-INT', 'Objects', 'Hourly', 'The number of unique\\nobjects monitored\\nand auto-tiered in the\\nS3 Intelligent-Tiering\\nstorage class'], ['region-MRAP-Out-Bytes', 'GB', 'Hourly', 'The amount of data\\ntransferred through an\\nS3 Multi-Region Access\\nPoints endpoint out of\\nbuckets in a Region\\n(MRAP data routing\\npricing).']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-MRAP-In-Bytes', 'GB', 'Hourly', 'The amount of data\\ntransferred through an\\nS3 Multi-Region Access\\nPoints endpoint out of\\nbuckets in a Region\\n(MRAP data routing\\npricing).'], ['regiongroup1-regiongroup2- -\\nMRAP-Out-Bytes', 'GB', 'Hourly', 'The amount of data\\ntransferred through an\\nS3 Multi-Region Access\\nPoints endpoint from a\\nbucket in r egiongro\\nup1 to a client in\\nregiongroup2 located\\noutside of the AWS\\nnetwork.'], ['regiongroup1-regiongroup2- -\\nMRAP-In-Bytes', 'GB', 'Hourly', 'The amount of data\\ntransferred through an\\nS3 Multi-Region Access\\nPoints endpoint to a\\nbucket in r egiongro\\nup1 from a client in\\nregiongroup2 located\\noutside of the AWS\\nnetwork.'], ['region-OverwriteBytes-Copy-\\nGDA', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nCopyObject operation\\nfrom S3 Glacier Deep\\nArchive storage']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-OverwriteBytes-Copy-\\nGIR', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nCopyObject operation\\nfrom S3 Glacier Instant\\nRetrieval storage.'], ['region-OverwriteBytes-Copy-\\nGLACIER', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nCopyObject operation\\nfrom S3 Glacier Flexible\\nRetrieval storage'], ['region-OverwriteBytes-Copy-\\nINT', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nCopyObject operation\\nfrom S3 Intelligent-\\nTiering storage'], ['region-OverwriteBytes-Copy-\\nRRS', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nCopyObject operation\\nfrom Reduced Re\\ndundancy Storage (RRS)\\nstorage'], ['region-OverwriteBytes-Copy-\\nSIA', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nCopyObject operation\\nfrom S3 Standard-IA\\nstorage']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-OverwriteBytes-Copy-\\nSTANDARD', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nCopyObject operation\\nfrom S3 Standard s\\ntorage'], ['region-OverwriteBytes-Copy-\\nZIA', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nCopyObject operation\\nfrom S3 One Zone-IA st\\norage'], ['region-OverwriteBytes-Put-\\nGDA', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nPutObject operation\\nfrom S3 Glacier Deep\\nArchive storage'], ['region-OverwriteBytes-Put-\\nGIR', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nPutObject operation\\nfrom S3 Glacier Instant\\nRetrieval storage.'], ['region-OverwriteBytes-Put-\\nGLACIER', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nPutObject operation\\nfrom S3 Glacier Flexible\\nRetrieval storage'], ['region-OverwriteBytes-Put-\\nINT', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nPutObject operation\\nfrom S3 Intelligent-\\nTiering storage']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-OverwriteBytes-Put-\\nRRS', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nPutObject operation\\nfrom Reduced Re\\ndundancy Storage (RRS)\\nstorage'], ['region-OverwriteBytes-Put-\\nSIA', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nPutObject operation\\nfrom S3 Standard-IA\\nstorage'], ['region-OverwriteBytes-Put-\\nSTANDARD', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nPutObject operation\\nfrom S3 Standard s\\ntorage'], ['region-OverwriteBytes-Put-\\nZIA', 'GB', 'Monthly', 'The amount of data\\noverwritten by a\\nPutObject operation\\nfrom S3 One Zone-IA st\\norage']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region1-region2-S3RTC-In-\\nBytes', 'GB', 'Monthly', 'The amount of data\\ntransferred for S3\\nReplication Time Control\\n(S3 RTC) from region2\\nto r egion1 by the\\nPutObjectReplTime ,\\nGetObjectReplTime ,\\nInitiateMultipartU\\nploadRepl\\nTime , U ploadPar\\ntReplTime ,\\nCompleteMultipartU\\nploadReplTime , and\\nWriteACLReplTime\\noperations'], ['region1-region2-S3RTC-Out-\\nBytes', 'GB', 'Monthly', 'The amount of data\\ntransferred for S3\\nReplication Time Control\\n(S3 RTC) from region1\\nto r egion2 by the\\nPutObjectReplTime ,\\nGetObjectReplTime ,\\nInitiateMultipartU\\nploadRepl\\nTime , U ploadPar\\ntReplTime ,\\nCompleteMultipartU\\nploadReplTime , and\\nWriteACLReplTime\\noperations']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-Requests-GDA-Tier1', 'Count', 'Hourly', 'The number of PUT,\\nCOPY, POST, C reateMul\\ntipartUpload ,\\nUploadPart , or\\nCompleteMultipartU\\npload requests on\\nS3 Glacier Deep Archive\\n6\\nobjects'], ['region-Requests-GDA-Tier2', 'Count', 'Hourly', 'The number of GET and\\nHEAD requests on S3\\nGlacier Deep Archive\\nobjects'], ['region-Requests-GDA-Tier3', 'Count', 'Hourly', 'The number of S3 Glacier\\nDeep Archive standard\\nrestore requests'], ['region-Requests-GDA-Tier5', 'Count', 'Hourly', 'The number of Bulk S3\\nGlacier Deep Archive\\nrestore requests'], ['region-Requests-GIR-Tier1', 'Count', 'Hourly', 'The number of PUT,\\nCOPY, or POST requests\\non S3 Glacier Instant\\nRetrieval objects.'], ['region-Requests-GIR-Tier2', 'Count', 'Hourly', 'The number of GET and\\nall other non-S3 Glacier\\nInstant Retrieval-Tier1\\nrequests on S3 Glacier\\nInstant Retrieval objects.']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-Requests-GLACIER-\\nTier1', 'Count', 'Hourly', 'The number of PUT,\\nCOPY, POST, C reateMul\\ntipartUpload ,\\nUploadPart , or\\nCompleteMultipartU\\npload requests on S3\\nGlacier Flexible Retrieval\\n6\\nobjects'], ['region-Requests-GLACIER-\\nTier2', 'Count', 'Hourly', 'The number of GET and\\nall other requests not\\nlisted on S3 Glacier\\nFlexible Retrieval objects'], ['region-Requests-INT-Tier1', 'Count', 'Hourly', 'The number of PUT,\\nCOPY, or POST requests\\non S3 Intelligent-Tiering\\nobjects'], ['region-Requests-INT-Tier2', 'Count', 'Hourly', 'The number of GET\\nand all other non-Tier1\\nrequests for S3 Intellige\\nnt-Tiering objects'], ['region-Requests-SIA-Tier1', 'Count', 'Hourly', 'The number of PUT,\\nCOPY, or POST requests\\non S3 Standard-IA\\nobjects'], ['region-Requests-SIA-Tier2', 'Count', 'Hourly', 'The number of GET\\nand all other non-S3\\nGlacier Instant Retrieval\\n-Tier1 requests on S3\\nStandard-IA objects']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-Requests-Tier1', 'Count', 'Hourly', 'The number of PUT,\\nCOPY, or POST requests\\nfor S3 Standard, RRS,\\nand tags, plus LIST\\nrequests for all buckets\\nand objects'], ['region-Requests-Tier2', 'Count', 'Hourly', 'The number of GET\\nand all other non-Tier1\\nrequests'], ['region-Requests-Tier3', 'Count', 'Hourly', 'The number of lifecycle\\nrequests to S3 Glacier\\nFlexible Retrieval or S3\\nGlacier Deep Archive\\nand standard S3 Glacier\\nFlexible Retrieval restore\\nrequests'], ['region-Requests-Tier4', 'Count', 'Hourly', 'The number of lifecycle\\ntransitions to S3 Glacier\\nInstant Retrieval, S3\\nIntelligent-Tiering, S3\\nStandard-IA, or S3 One\\nZone-IA storage'], ['region-Requests-Tier5', 'Count', 'Hourly', 'The number of Bulk S3\\nGlacier Flexible Retrieval\\nrestore requests'], ['region-Requests-Tier6', 'Count', 'Hourly', 'The number of Expedited\\nS3 Glacier Flexible\\nRetrieval restore re\\nquests']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-Requests-Tier8', 'Count', 'Hourly', 'The number of S3 Access\\nGrants requests'], ['region-Requests-XZ-Tier1', 'Count', 'Hourly', 'The number of PUT or\\nCOPY requests on S3\\nExpress One Zone objec\\nts'], ['region-Requests-XZ-Tier2', 'Count', 'Hourly', 'The number of GET and\\nall other non-S3 Express\\nOne Zone-Tier1 requests\\non S3 Express One Zone\\nobjects'], ['region-Requests-ZIA-Tier1', 'Count', 'Hourly', 'The number of PUT,\\nCOPY, or POST requests\\non S3 One Zone-IA\\nobjects'], ['region-Requests-ZIA-Tier2', 'Count', 'Hourly', 'The number of GET and\\nall other non-S3 One\\nZone-IA-Tier1 requests\\non S3 One Zone-IA\\nobjects'], ['region-Retrieval-GIR', 'GB', 'Hourly', 'The amount of data\\nretrieved from S3 Glacier\\nInstant Retrieval storage.'], ['region-Retrieval-SIA', 'GB', 'Hourly', 'The amount of data\\nretrieved from S3\\nStandard-IA storage']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-Retrieval-XZ', 'GB', 'Hourly', 'The portion of the data\\nthat exceeds 512 KB in a\\ngiven retrieval request\\n(PUT or COPY) with S3\\nExpress One Zone stora\\nge'], ['region-Retrieval-ZIA', 'GB', 'Hourly', 'The amount of data\\nretrieved from S3 One\\nZone-IA storage'], ['region-S3DSSE-In-Bytes', 'GB', 'Monthly', 'The amount of data\\ndual-encrypted by\\nAmazon S3'], ['region-S3DSSE-Out-Bytes', 'GB', 'Monthly', 'The amount of dual-encr\\nypted data decrypted by\\nAmazon S3'], ['region-S3G-DataTransfer-In-\\nBytes', 'GB', 'Hourly', 'The amount of data\\ntransferred into Amazon\\nS3 to restore objects\\nfrom S3 Glacier Flexible\\nRetrieval or S3 Glacier\\nDeep Archive storage'], ['region-S3G-DataTransfer-\\nOut-Bytes', 'GB', 'Hourly', 'The amount of data\\ntransferred from Amazon\\nS3 to transition objects\\nto S3 Glacier Flexible\\nRetrieval or S3 Glacier\\nDeep Archive storage']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-Select-Returned-B\\nytes', 'GB', 'Hourly', 'The amount of data\\nreturned with Select\\nrequests from S3\\nStandard storage'], ['region-Select-Returned-GIR-\\nBytes', 'GB', 'Hourly', 'The amount of data\\nreturned with Select\\nrequests from S3 Glacier\\nInstant Retrieval storage.'], ['region-Select-Returned-INT-\\nBytes', 'GB', 'Hourly', 'The amount of data\\nreturned with Select\\nrequests from S3\\nIntelligent-Tiering\\nstorage'], ['region-Select-Returned-SIA-\\nBytes', 'GB', 'Hourly', 'The amount of data\\nreturned with Select\\nrequests from S3\\nStandard-IA storage'], ['region-Select-Returned-ZIA-\\nBytes', 'GB', 'Hourly', 'The amount of data\\nreturned with Select\\nrequests from S3 One\\nZone-IA storage'], ['region-Select-Scanned-Bytes', 'GB', 'Hourly', 'The amount of data\\nscanned with Select\\nrequests from S3\\nStandard storage'], ['region-Select-Scanned-GIR-\\nBytes', 'GB', 'Hourly', 'The amount of data\\nscanned with Select\\nrequests from S3 Glacier\\nInstant Retrieval storage.']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-Select-Scanned-INT-\\nBytes', 'GB', 'Hourly', 'The amount of data\\nscanned with Select\\nrequests from S3\\nIntelligent-Tiering\\nstorage'], ['region-Select-Scanned-SIA-\\nBytes', 'GB', 'Hourly', 'The amount of data\\nscanned with Select\\nrequests from S3\\nStandard-IA storage'], ['region-Select-Scanned-ZIA-\\nBytes', 'GB', 'Hourly', 'The amount of data\\nscanned with Select\\nrequests from S3 One\\nZone-IA storage'], ['region-Standard-Retrieval-\\nBytes', 'GB', 'Hourly', 'The amount of data\\nretrieved with standard\\nS3 Glacier Flexible\\nRetrieval or S3 Glacier\\nDeep Archive requests'], ['region-StorageAnalytics-\\nObjCount', 'Objects', 'Hourly', 'The number of unique\\nobjects monitored in\\neach Storage Class Anal\\nysis configuration.'], ['region-StorageLens-ObjCount', 'Objects', 'Daily', 'The number of unique\\nobjects in each S3\\nStorage Lens dashboard\\nthat are tracked by S3\\nStorage Lens advanced\\nmetrics and recommend\\nations.']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-StorageLensFreeTier-\\nObjCount', 'Objects', 'Daily', 'The number of unique\\nobjects in each S3\\nStorage Lens dashboard\\nthat are tracked by\\nS3 Storage Lens usage\\nmetrics.'], ['StorageObjectCount', 'Count', 'Daily', 'The number of objects\\nstored within a given\\nbucket'], ['region-TagStorage-TagHrs', 'Tag-Hours', 'Daily', 'The total of tags on all\\nobjects in the bucket\\nreported by hour'], ['region-TimedStorage-ByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in S3 Standard s\\ntorage'], ['region-TimedStorage-GDA-\\nByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in S3 Glacier\\nDeep Archive storage'], ['region-TimedStorage-GDA-\\nStaging', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in S3 Glacier\\nDeep Archive staging\\nstorage'], ['region-TimedStorage-GIR-\\nByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in S3 Glacier\\nInstant Retrieval storage.']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-TimedStorage-GIR-\\nSmObjects', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that small\\nobjects (smaller than 128\\nKB) were stored in S3\\nGlacier Instant Retrieval\\nstorage.'], ['region-TimedStorage-Glac\\nierByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in S3 Glacier\\nFlexible Retrieval storage'], ['region-TimedStorage-Glac\\nierStaging', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in S3 Glacier\\nFlexible Retrieval staging\\nstorage'], ['region-TimedStorage-INT-FA-\\nByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in the Frequent\\nAccess tier of S3\\nIntelligent-Tiering\\n5\\nstorage'], ['region-TimedStorage-INT-IA-\\nByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in the Infrequen\\nt Access tier of S3\\nIntelligent-Tiering\\nstorage']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-TimedStorage-INT-AA-\\nByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in the Archive\\nAccess tier of S3\\nIntelligent-Tiering\\nstorage'], ['region-TimedStorage-INT-\\nAIA-ByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in the Archive\\nInstant Access tier of\\nS3 Intelligent-Tiering\\nstorage'], ['region-TimedStorage-INT-\\nDAA-ByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in the Deep\\nArchive Access tier of\\nS3 Intelligent-Tiering\\nstorage'], ['region-TimedStorage-RRS-\\nByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in Reduced\\nRedundancy Storage\\n(RRS) storage'], ['region-TimedStorage-SIA-\\nByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in S3 Standard-IA\\nstorage']]]\n",
      "[[['Usage Type', 'Units', 'Granulari\\nty', 'Description'], ['region-TimedStorage-SIA-\\nSmObjects', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that small\\nobjects (smaller than 128\\nKB) were stored in S3\\n4\\nStandard-IA storage'], ['region-TimedStorage-XZ-B\\nyteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in S3 Express One\\nZone storage'], ['region-TimedStorage-ZIA-\\nByteHrs', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that data was\\nstored in S3 One Zone-\\nIA storage'], ['region-TimedStorage-ZIA-\\nSmObjects', 'GB-Month', 'Daily', 'The number of GB-\\nmonths that small\\nobjects (smaller than 128\\nKB) were stored in S3\\nOne Zone-IA storage'], ['region-Upload-XZ', 'GB', 'Hourly', 'The amount of data\\nthat exceeds 512 KB in\\na given upload request\\n(PUT or COPY) with S3\\nExpress One Zone']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nIn general, S3 bucket owners are billed for requests with HTTP 200 OK successful\\nresponses and HTTP 4XX client error responses. Bucket owners aren't billed for HTTP 5XX\\nserver error responses, such as HTTP 503 Slow Down errors. For more information on S3\\nerror codes under HTTP 3XX and 4XX status codes that aren't billed, see Billing for Amazon\\nS3 error responses. For more information about billing charges if your bucket is configured\\nas a Requester Pays bucket, see How Requester Pays charges work.\", ''], ['', '', '']], [['', '', ''], ['', \"Important\\nOn May 13, 2024, we started deploying a change to eliminate charges for unauthorized\\nrequests that aren't initiated by the bucket owner. After the deployment of this change is\\ncompleted, bucket owners will never incur request or bandwidth charges for requests that\\nreturn AccessDenied (HTTP 403 Forbidden) errors when these requests are initiated\\nfrom outside of their individual AWS account or AWS organization. The current page\\nshows a full list of HTTP 3XX and 4XX status codes that won't be billed. This billing change\", '']]]\n",
      "[[['', 'requires no updates to your applications and applies to all S3 buckets. When deployment of\\nthis change is completed in all AWS Regions, we’ll update our documentation.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nFor AccessDenied (HTTP 403 Forbidden), S3 doesn't charge the bucket owner when\\nthe request is initiated outside of the bucket owner's individual AWS account or the bucket\\nowner's AWS organization.\", ''], ['', '', '']], [['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['301 Moved\\nPermanent\\nly', 'PermanentRedirect', 'The bucket that you are\\nattempting to access\\nmust be addressed using\\nthe specified endpoint.\\nSend all future requests\\nto this endpoint.', ''], [None, 'PermanentRedirectC\\nontrolError', 'The API operation you\\nare attempting to access\\nmust be addressed using\\nthe specified endpoint.\\nSend all future requests\\nto this endpoint.', '']]]\n",
      "[[['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['307\\nTemporary\\nRedirect', 'TemporaryRedirect', 'You are being redirected\\nto the bucket while the\\nDomain Name System\\n(DNS) server is being\\nupdated.', ''], ['400 Bad\\nRequest', 'AuthorizationHeade\\nrMalformed', 'The authorization header\\nthat you provided is not\\nvalid.', ''], [None, 'AuthorizationQuery\\nParametersError', 'The authorization query\\nparameters that you\\nprovided are not valid.', ''], [None, 'ExpiredToken', 'The provided token has\\nexpired.', ''], [None, 'IllegalLocationCon\\nstraintException', 'You are trying to access\\na bucket from a different\\nRegion than where the\\nbucket exists. To avoid\\nthis error, use the --\\nregion option. For\\nexample: aws s3 cp\\nawsexample.txt s\\n3:// example-s3-\\nbucket/ --region\\nap-east-1 .', '']]]\n",
      "[[['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['', 'InvalidArgument', 'This error might occur\\nfor the following\\nreasons:\\n•\\nThe specified\\nargument was not\\nvalid.\\n•\\nThe request was\\nmissing a required\\nheader.\\n•\\nThe specified\\nargument was\\nincomplete or in the\\nwrong format.\\n•\\nThe specified\\nargument must have a\\nlength greater than or\\nequal to 3.', ''], [None, 'InvalidDigest', 'The Content-MD5 or\\nchecksum value that you\\nspecified is not valid.', ''], [None, 'InvalidEncryptionA\\nlgorithmError', 'The encryption request\\nthat you specified is not\\nvalid. The valid value is\\nAES256.', '']]]\n",
      "[[['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['', 'InvalidRequest', 'This error might occur\\nfor the following\\nreasons:\\n•\\nThe request is using\\nthe wrong signature\\nversion. Use AWS4-\\nHMAC-SHA256\\n(Signature Version 4).\\n•\\nAn access point can\\nbe created only for an\\nexisting bucket.\\n•\\nThe access point is not\\nin a state where it can\\nbe deleted.\\n•\\nAn access point can\\nbe listed only for an\\nexisting bucket.\\n•\\nThe next token is not\\nvalid.\\n•\\nAt least one action\\nmust be specified in a\\nlifecycle rule.\\n•\\nAt least one lifecycle\\nrule must be specified.\\n•', None]]]\n",
      "[[['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['', '', 'The number of\\nlifecycle rules must\\nnot exceed the allowed\\nlimit of 1000 rules.\\n•\\nThe range for the\\nMaxResults\\nparameter is not valid.\\n•\\nSOAP requests must\\nbe made over an\\nHTTPS connection.\\n•\\nAmazon S3 Transfer\\nAcceleration is\\nnot supported for\\nbuckets with non-DNS\\ncompliant names.\\n•\\nAmazon S3 Transfer\\nAcceleration is not\\nsupported for buckets\\nwith periods (.) in their\\nnames.\\n•\\nThe Amazon S3\\nTransfer Acceleration\\nendpoint supports\\nonly virtual style\\nrequests.\\n•\\nAmazon S3 Transfer\\nAcceleration is not', None]]]\n",
      "[[['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['', '', 'configured on this\\nbucket.\\n•\\nAmazon S3 Transfer\\nAcceleration is\\ndisabled on this\\nbucket.\\n•\\nAmazon S3 Transfer\\nAcceleration is not\\nsupported on this\\nbucket. For assistance,\\ncontact AWS Support.\\n•\\nAmazon S3 Transfer\\nAcceleration cannot be\\nenabled on this bucket.\\nFor assistance, contact\\nAWS Support.\\n•\\nConflicting values\\nprovided in HTTP\\nheaders and query par\\nameters.\\n•\\nConflicting values\\nprovided in HTTP\\nheaders and POST\\nform fields.\\n•\\nCopyObject request\\nmade on objects larger\\nthan 5GB in size.', '']]]\n",
      "[[['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['', 'InvalidSOAPRequest', 'The SOAP request body\\nis not valid.', ''], [None, 'InvalidStorageClass', 'The storage class that\\nyou specified is not valid.', ''], [None, 'InvalidTag', 'Your request contains\\ntag input that is not\\nvalid. For example, your\\nrequest might contain\\nduplicate keys, keys or\\nvalues that are too long,\\nor system tags.', ''], [None, 'InvalidToken', 'The provided token is\\nmalformed or otherwise\\nnot valid.', ''], [None, 'InvalidURI', \"The specified URI\\ncouldn't be parsed.\", ''], [None, 'KeyTooLongError', 'Your key is too long.', ''], [None, 'MalformedACLError', 'The ACL that you\\nprovided was not well\\nformed or did not\\nvalidate against our\\npublished schema.', ''], [None, 'MalformedPOSTRequest', 'The body of your POST\\nrequest is not well-form\\ned multipart/form-data.', '']]]\n",
      "[[['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['', 'MalformedXML', 'The XML that you\\nprovided was not well\\nformed or did not\\nvalidate against our\\npublished schema.', ''], [None, 'MaxPostPreDataLeng\\nthExceededError', 'Your POST request fields\\npreceding the upload file\\nwere too large.', ''], [None, 'MetadataTooLarge', 'Your metadata headers\\nexceed the maximum\\nallowed metadata size.', ''], [None, 'MissingRequestBodyError', 'You sent an empty XML\\ndocument as a request.', ''], [None, 'MissingSecurityHeader', 'Your request is missing a\\nrequired header.', ''], [None, 'NoLoggingStatusForKey', 'There is no such thing\\nas a logging status\\nsubresource for a key.', ''], [None, 'RequestHeaderSecti\\nonTooLarge', 'The request header and\\nquery parameters used\\nto make the request\\nexceed the maximum\\nallowed sizes', ''], [None, 'UnexpectedContent', 'This request contains\\nunsupported content.', '']]]\n",
      "[[['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['', 'UserKeyMustBeSpecified', 'The bucket POST\\nrequest must contain the\\nspecified field name. If\\nit is specified, check the\\norder of the fields.', ''], [None, 'IncorrectEndpoint', 'The specified bucket\\nexists in another Region.\\nDirect requests to the\\ncorrect endpoint.', ''], ['403\\nForbidden', 'RequestTimeTooSkewed', \"The difference between\\nthe request time and the\\nserver's time is too large.\", ''], [None, 'SignatureDoesNotMatch', 'The request signature\\nthat the server calculate\\nd does not match the\\nsignature that you\\nprovided. Check your\\nAWS secret access key\\nand signing method. For\\nmore information, see\\nREST Authentication and\\nSOAP Authentication.', '']]]\n",
      "[[['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['', 'NotSignedUp', 'Your account is not\\nsigned up for the\\nAmazon S3 service. You\\nmust sign up before you\\ncan use Amazon S3.\\nYou can sign up at the\\nfollowing URL: https://a\\nws.amazon.com/s3', ''], [None, 'InvalidSecurity', 'The provided security\\ncredentials are not valid.', ''], [None, 'InvalidPayer', 'All access to this object\\nhas been disabled. For\\nfurther assistance, see\\nContact Us.', ''], [None, 'InvalidAccessKeyId', 'The AWS access key ID\\nthat you provided does\\nnot exist in our records.', ''], [None, 'AccountProblem', 'There is a problem with\\nyour AWS account that\\nprevents the operation\\nfrom completing s\\nuccessfully. For further\\nassistance, see Contact\\nUs.', '']]]\n",
      "[[['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['', 'UnauthorizedAccessError', \"Applicable in China\\nRegions only. Returned\\nwhen a request is made\\nto a bucket that doesn't\\nhave an ICP license. For\\nmore information, see\\nICP Recordal.\", ''], ['404 Not\\nFound', 'NoSuchUpload', 'The specified multipart\\nupload does not exist.\\nThe upload ID might not\\nbe valid, or the multipart\\nupload might have been\\naborted or completed.', ''], [None, 'NoSuchWebsiteConfi\\nguration', 'The specified bucket\\ndoes not have a website\\nconfiguration.', ''], ['405\\nMethod\\nNot\\nAllowed', 'MethodNotAllowed', 'The specified method is\\nnot allowed against this\\nresource.', ''], ['409\\nConflict', 'BucketAlreadyExists', 'The requested bucket\\nname is not available.\\nThe bucket namespace\\nis shared by all users of\\nthe system. Specify a\\ndifferent name and try\\nagain.', '']]]\n",
      "[[['HTTP\\nstatus\\ncode', 'Error code', 'Description of error\\ncode', ''], ['', 'InvalidBucketState', 'The request is not valid\\nfor the current state of\\nthe bucket.', ''], [None, 'OperationAborted', 'A conflicting condition\\nal operation is currently\\nin progress against this\\nresource. Try again.', ''], ['411\\nLength\\nRequired', 'MissingContentLength', 'You must provide the\\nContent-Length HTTP\\nheader.', ''], ['412\\nPrecondit\\nion Failed', 'RequestIsNotMultiP\\nartContent', 'A bucket POST request\\nmust be of the enclosure\\n-type multipart/form-dat\\na.', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe Amazon S3 console limits the amount of data returned to 40 MB. To retrieve more\\ndata, use the AWS CLI or the API.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'A,B\\nC,D\\nD,E\\nE,F\\nG,H\\nI,J', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'package com.amazonaws;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.CSVInput;\\nimport com.amazonaws.services.s3.model.CSVOutput;\\nimport com.amazonaws.services.s3.model.CompressionType;\\nimport com.amazonaws.services.s3.model.ExpressionType;\\nimport com.amazonaws.services.s3.model.InputSerialization;\\nimport com.amazonaws.services.s3.model.OutputSerialization;\\nimport com.amazonaws.services.s3.model.SelectObjectContentEvent;', '']]]\n",
      "[[['', 'import com.amazonaws.services.s3.model.SelectObjectContentEventVisitor;\\nimport com.amazonaws.services.s3.model.SelectObjectContentRequest;\\nimport com.amazonaws.services.s3.model.SelectObjectContentResult;\\nimport java.io.File;\\nimport java.io.FileOutputStream;\\nimport java.io.InputStream;\\nimport java.io.OutputStream;\\nimport java.util.concurrent.atomic.AtomicBoolean;\\nimport static com.amazonaws.util.IOUtils.copy;\\n/**\\n* This example shows how to query data from S3Select and consume the response in\\nthe form of an\\n* InputStream of records and write it to a file.\\n*/\\npublic class RecordInputStreamExample {\\nprivate static final String BUCKET_NAME = \"${my-s3-bucket}\";\\nprivate static final String CSV_OBJECT_KEY = \"${my-csv-object-key}\";\\nprivate static final String S3_SELECT_RESULTS_PATH = \"${my-s3-select-results-\\npath}\";\\nprivate static final String QUERY = \"select s._1 from S3Object s\";\\npublic static void main(String[] args) throws Exception {\\nfinal AmazonS3 s3Client = AmazonS3ClientBuilder.defaultClient();\\nSelectObjectContentRequest request = generateBaseCSVRequest(BUCKET_NAME,\\nCSV_OBJECT_KEY, QUERY);\\nfinal AtomicBoolean isResultComplete = new AtomicBoolean(false);\\ntry (OutputStream fileOutputStream = new FileOutputStream(new File\\n(S3_SELECT_RESULTS_PATH));\\nSelectObjectContentResult result =\\ns3Client.selectObjectContent(request)) {\\nInputStream resultInputStream =\\nresult.getPayload().getRecordsInputStream(\\nnew SelectObjectContentEventVisitor() {\\n@Override\\npublic void visit(SelectObjectContentEvent.StatsEvent event)\\n{\\nSystem.out.println(', '']]]\n",
      "[[['', '\"Received Stats, Bytes Scanned: \" +\\nevent.getDetails().getBytesScanned()\\n+ \" Bytes Processed: \" +\\nevent.getDetails().getBytesProcessed());\\n}\\n/*\\n* An End Event informs that the request has finished\\nsuccessfully.\\n*/\\n@Override\\npublic void visit(SelectObjectContentEvent.EndEvent event)\\n{\\nisResultComplete.set(true);\\nSystem.out.println(\"Received End Event. Result is\\ncomplete.\");\\n}\\n}\\n);\\ncopy(resultInputStream, fileOutputStream);\\n}\\n/*\\n* The End Event indicates all matching records have been transmitted.\\n* If the End Event is not received, the results may be incomplete.\\n*/\\nif (!isResultComplete.get()) {\\nthrow new Exception(\"S3 Select request was incomplete as End Event was\\nnot received.\");\\n}\\n}\\nprivate static SelectObjectContentRequest generateBaseCSVRequest(String bucket,\\nString key, String query) {\\nSelectObjectContentRequest request = new SelectObjectContentRequest();\\nrequest.setBucketName(bucket);\\nrequest.setKey(key);\\nrequest.setExpression(query);\\nrequest.setExpressionType(ExpressionType.SQL);\\nInputSerialization inputSerialization = new InputSerialization();\\ninputSerialization.setCsv(new CSVInput());\\ninputSerialization.setCompressionType(CompressionType.NONE);', '']]]\n",
      "[[['', 'request.setInputSerialization(inputSerialization);\\nOutputSerialization outputSerialization = new OutputSerialization();\\noutputSerialization.setCsv(new CSVOutput());\\nrequest.setOutputSerialization(outputSerialization);\\nreturn request;\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAmazon S3 Select queries currently do not support subqueries or joins.', ''], ['', '', '']], [['', '', ''], ['', 'SELECT *\\nSELECT projection1 AS column_alias_1, projection2 AS column_alias_2', ''], ['', '', '']], [['', '', ''], ['', 'FROM table_name\\nFROM table_name alias\\nFROM table_name AS alias', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'FROM S3Object[*].path\\nFROM S3Object[*].path alias\\nFROM S3Object[*].path AS alias', ''], ['', '', '']], [['', '', ''], ['', \"Note\\n• This form of the FROM clause works only with JSON objects.\\n• Wildcard characters always emit at least one record. If no record matches, then Amazon\\nS3 Select emits the value MISSING. During output serialization (after the query finishes\\nrunning), Amazon S3 Select replaces MISSING values with empty records.\\n• Aggregate functions (AVG, COUNT, MAX, MIN, and SUM) skip MISSING values.\\n• If you don't provide an alias when using a wildcard character, you can refer to the row\\nby using the last element in the path. For example, you could select all prices from a list\\nof books by using the query SELECT price FROM S3Object[*].books[*].price.\\nIf the path ends in a wildcard character instead of a name, then you can use\\nthe value _1 to refer to the row. For example, instead of SELECT price FROM\\nS3Object[*].books[*].price, you could use the query SELECT _1.price FROM\\nS3Object[*].books[*].\\n• Amazon S3 Select always treats a JSON document as an array of root-level values. Thus,\\neven if the JSON object that you are querying has only one root element, the FROM\\nclause must begin with S3Object[*]. However, for compatibility reasons, Amazon S3\\nSelect allows you to omit the wildcard character if you don't include a path. Thus, the\\ncomplete clause FROM S3Object is equivalent to FROM S3Object[*] as S3Object.\", '']]]\n",
      "[[['', 'If you include a path, you must also use the wildcard character. So, FROM S3Object and\\nFROM S3Object[*].path are both valid clauses, but FROM S3Object.path is not.', ''], ['', '', '']], [['', '', ''], ['', '{ \"Rules\": [ {\"id\": \"1\"}, {\"expr\": \"y > x\"}, {\"id\": \"2\", \"expr\": \"z = DEBUG\"} ]}\\n{ \"created\": \"June 27\", \"modified\": \"July 6\" }', ''], ['', '', '']], [['', '', ''], ['', 'SELECT id FROM S3Object[*].Rules[*].id', ''], ['', '', '']], [['', '', ''], ['', '{\"id\":\"1\"}\\n{}\\n{\"id\":\"2\"}\\n{}', ''], ['', '', '']], [['', '', ''], ['', 'SELECT id FROM S3Object[*].Rules[*].id WHERE id IS NOT MISSING', ''], ['', '', '']], [['', '', ''], ['', '{\"id\":\"1\"}', '']]]\n",
      "[[['', '{\"id\":\"2\"}', ''], ['', '', '']], [['', '', ''], ['', '{ \"created\": \"936864000\", \"dir_name\": \"important_docs\", \"files\": [ { \"name\": \".\" },\\n{ \"name\": \"..\" }, { \"name\": \".aws\" }, { \"name\": \"downloads\" } ], \"owner\": \"Amazon\\nS3\" }\\n{ \"created\": \"936864000\", \"dir_name\": \"other_docs\", \"files\": [ { \"name\": \".\" },\\n{ \"name\": \"..\" }, { \"name\": \"my stuff\" }, { \"name\": \"backup\" } ], \"owner\": \"User\" }', ''], ['', '', '']], [['', '', ''], ['', 'SELECT d.dir_name, d.files FROM S3Object[*] d', ''], ['', '', '']], [['', '', ''], ['', '{\"dir_name\":\"important_docs\",\"files\":[{\"name\":\".\"},{\"name\":\"..\"},{\"name\":\".aws\"},\\n{\"name\":\"downloads\"}]}\\n{\"dir_name\":\"other_docs\",\"files\":[{\"name\":\".\"},{\"name\":\"..\"},{\"name\":\"my stuff\"},\\n{\"name\":\"backup\"}]}', ''], ['', '', '']], [['', '', ''], ['', 'SELECT _1.dir_name, _1.owner FROM S3Object[*]', ''], ['', '', '']], [['', '', ''], ['', '{\"dir_name\":\"important_docs\",\"owner\":\"Amazon S3\"}\\n{\"dir_name\":\"other_docs\",\"owner\":\"User\"}', ''], ['', '', '']], [['', '', ''], ['', 'WHERE condition', ''], ['', '', '']], [['', '', ''], ['', 'LIMIT number', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\"name\": \"Susan Smith\",\\n\"org\": \"engineering\",\\n\"projects\":\\n[\\n{\"project_name\":\"project1\", \"completed\":false},\\n{\"project_name\":\"project2\", \"completed\":true}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Select s.name from S3Object s', ''], ['', '', '']], [['', '', ''], ['', '{\"name\":\"Susan Smith\"}', ''], ['', '', '']], [['', '', ''], ['', 'Select s.projects[0].project_name from S3Object s', ''], ['', '', '']], [['', '', ''], ['', '{\"project_name\":\"project1\"}', ''], ['', '', '']], [['', '', ''], ['', 'SELECT s.name from S3Object s', ''], ['', '', '']], [['', '', ''], ['', 'SELECT s.\"name\" from S3Object s', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'SELECT s.name from S3Object s', ''], ['', '', '']], [['', '', ''], ['', 'SELECT s.\"NAME\" from S3Object s', ''], ['', '', '']], [['', '', ''], ['', 'SELECT s.\"CAST\" from S3Object s', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'SELECT s.CAST from S3Object s', ''], ['', '', '']]]\n",
      "[[['Name', 'Description', 'Examples'], ['bool', 'A Boolean value, either TRUE or FALSE.', 'FALSE'], ['int, integer', 'An 8-byte signed integer in the range -9,223,372,036,854\\n,775,808 to 9,223,372,036,854,775,807.', '100000'], ['string', 'A UTF8-encoded variable-length string. The default limit is\\n1 character. The maximum character limit is 2,147,483,647.', \"'xyz'\"], ['float', 'An 8-byte floating point number.', 'CAST(0.456\\nAS FLOAT)'], ['decimal,\\nnumeric', 'A base-10 number, with a maximum precision of 38 (that\\nis, the maximum number of significant digits), and with\\n31 31\\na scale within the range of -2 to 2 -1 (that is, the\\nbase-10 exponent).\\nNote\\nAmazon S3 Select ignores scale and precision when\\nyou provide both at the same time.', '123.456'], ['timestamp', 'Timestamps represent a specific moment in time, always\\ninclude a local offset, and are capable of arbitrary precision\\n.\\nIn the text format, timestamps follow the W3C note on\\ndate and time formats, but they must end with the literal\\nT if the timestamps are not at least whole-day precision.\\nFractional seconds are allowed, with at least one digit of\\nprecision, and an unlimited maximum. Local-time offsets c\\nan be represented as either hour:minute offsets from UTC,', \"CAST('200\\n7-04-05T1\\n4:30Z' AS\\nTIMESTAMP)\"]], [['', '', ''], ['', 'Note\\nAmazon S3 Select ignores scale and precision when\\nyou provide both at the same time.', ''], ['', '', '']]]\n",
      "[[['Name', 'Description', 'Examples'], ['', 'or as the literal Z to denote a local time of UTC. Local-time\\noffsets are required on timestamps with time and are not\\nallowed on date values.', '']], [['', '', ''], ['', 'Note\\nFor LIST Parquet type output, Amazon S3 Select supports only JSON format. However, if\\nthe query limits the data to simple values, the LIST Parquet type can also be queried in\\nCSV format.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTimestamps saved as an INT(96) are unsupported.\\nBecause of the range of the INT(64) type, timestamps that are using the NANOS\\nunit can represent only values between 1677-09-21 00:12:43 and 2262-04-11\\n23:47:16. Values outside of this range cannot be represented with the NANOS unit.', ''], ['', '', '']]]\n",
      "[[['Parquet types', 'Supported data types'], ['DATE', 'timestamp'], ['DECIMAL', 'decimal, numeric'], ['ENUM', 'string'], ['INT(8)', 'int, integer'], ['INT(16)', 'int, integer'], ['INT(32)', 'int, integer'], ['INT(64)', 'decimal, numeric'], ['LIST', 'Each Parquet type in list is mapped to the corresponding data type.'], ['STRING', 'string'], ['TIMESTAMP', 'timestamp']]]\n",
      "[]\n",
      "[[['Operator or\\nelement', 'Associativity', 'Required'], ['-', 'right', 'unary minus'], ['*, /, %', 'left', 'multiplic\\nation, division,\\nmodulo'], ['+, -', 'left', 'addition,\\nsubtraction'], ['IN', '', 'set membershi\\np'], ['BETWEEN', '', 'range\\ncontainment'], ['LIKE', '', 'string pattern\\nmatching'], ['<>', '', 'less than,\\ngreater than'], ['=', 'right', 'equality,\\nassignment'], ['NOT', 'right', 'logical\\nnegation'], ['AND', 'left', 'logical\\nconjunction'], ['OR', 'left', 'logical\\ndisjunction']]]\n",
      "[[['', '', ''], ['', 'absolute\\naction\\nadd\\nall\\nallocate\\nalter\\nand\\nany\\nare\\nas\\nasc\\nassertion\\nat\\nauthorization\\navg\\nbag\\nbegin\\nbetween\\nbit\\nbit_length\\nblob\\nbool\\nboolean\\nboth\\nby\\ncascade\\ncascaded\\ncase\\ncast\\ncatalog\\nchar\\nchar_length\\ncharacter\\ncharacter_length\\ncheck\\nclob\\nclose', '']]]\n",
      "[[['', 'coalesce\\ncollate\\ncollation\\ncolumn\\ncommit\\nconnect\\nconnection\\nconstraint\\nconstraints\\ncontinue\\nconvert\\ncorresponding\\ncount\\ncreate\\ncross\\ncurrent\\ncurrent_date\\ncurrent_time\\ncurrent_timestamp\\ncurrent_user\\ncursor\\ndate\\nday\\ndeallocate\\ndec\\ndecimal\\ndeclare\\ndefault\\ndeferrable\\ndeferred\\ndelete\\ndesc\\ndescribe\\ndescriptor\\ndiagnostics\\ndisconnect\\ndistinct\\ndomain\\ndouble\\ndrop\\nelse\\nend\\nend-exec\\nescape', '']]]\n",
      "[[['', 'except\\nexception\\nexec\\nexecute\\nexists\\nexternal\\nextract\\nfalse\\nfetch\\nfirst\\nfloat\\nfor\\nforeign\\nfound\\nfrom\\nfull\\nget\\nglobal\\ngo\\ngoto\\ngrant\\ngroup\\nhaving\\nhour\\nidentity\\nimmediate\\nin\\nindicator\\ninitially\\ninner\\ninput\\ninsensitive\\ninsert\\nint\\ninteger\\nintersect\\ninterval\\ninto\\nis\\nisolation\\njoin\\nkey\\nlanguage\\nlast', '']]]\n",
      "[[['', 'leading\\nleft\\nlevel\\nlike\\nlimit\\nlist\\nlocal\\nlower\\nmatch\\nmax\\nmin\\nminute\\nmissing\\nmodule\\nmonth\\nnames\\nnational\\nnatural\\nnchar\\nnext\\nno\\nnot\\nnull\\nnullif\\nnumeric\\noctet_length\\nof\\non\\nonly\\nopen\\noption\\nor\\norder\\nouter\\noutput\\noverlaps\\npad\\npartial\\npivot\\nposition\\nprecision\\nprepare\\npreserve\\nprimary', '']]]\n",
      "[[['', 'prior\\nprivileges\\nprocedure\\npublic\\nread\\nreal\\nreferences\\nrelative\\nrestrict\\nrevoke\\nright\\nrollback\\nrows\\nschema\\nscroll\\nsecond\\nsection\\nselect\\nsession\\nsession_user\\nset\\nsexp\\nsize\\nsmallint\\nsome\\nspace\\nsql\\nsqlcode\\nsqlerror\\nsqlstate\\nstring\\nstruct\\nsubstring\\nsum\\nsymbol\\nsystem_user\\ntable\\ntemporary\\nthen\\ntime\\ntimestamp\\ntimezone_hour\\ntimezone_minute\\nto', '']]]\n",
      "[[['', 'trailing\\ntransaction\\ntranslate\\ntranslation\\ntrim\\ntrue\\ntuple\\nunion\\nunique\\nunknown\\nunpivot\\nupdate\\nupper\\nusage\\nuser\\nusing\\nvalue\\nvalues\\nvarchar\\nvarying\\nview\\nwhen\\nwhenever\\nwhere\\nwith\\nwork\\nwrite\\nyear\\nzone', ''], ['', '', '']]]\n",
      "[[['Function', 'Argument type', 'Return type'], ['AVG(expressio\\nn )', 'INT, FLOAT, DECIMAL', 'DECIMAL\\nfor an INT\\nargument,\\nFLOAT for a\\nfloating-point\\nargument;\\notherwise the\\nsame as the\\nargument data\\ntype.'], ['COUNT', '-', 'INT'], ['MAX(expressio\\nn )', 'INT, DECIMAL', 'Same as the\\nargument\\ntype.'], ['MIN(expressio\\nn )', 'INT, DECIMAL', 'Same as the\\nargument\\ntype.'], ['SUM(expressio\\nn )', 'INT, FLOAT, DOUBLE, DECIMAL', 'INT for an\\nINT argument,\\nFLOAT for a\\nfloating-point\\nargument;\\notherwise, the\\nsame as the\\nargument data\\ntype.']]]\n",
      "[[['', '', ''], ['', '\"DOC-EXAMPLE-BUCKET\",\"example-folder/\",\"0\"\\n\"DOC-EXAMPLE-BUCKET\",\"example-folder/object1\",\"2011267\"\\n\"DOC-EXAMPLE-BUCKET\",\"example-folder/object2\",\"1570024\"', ''], ['', '', '']], [['', '', ''], ['', \"SELECT SUM(CAST(_3 as INT)) FROM s3object s WHERE _2 LIKE 'example-folder/%' AND _2 !=\\n'example-folder/';\", ''], ['', '', '']], [['', '', ''], ['', '3581291', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nCurrently, Amazon S3 Select doesn't support ORDER BY or queries that contain new lines.\\nMake sure that you use queries with no line breaks.\", ''], ['', '', '']], [['', '', ''], ['', 'CASE expression WHEN value THEN result [WHEN...] [ELSE result] END', ''], ['', '', '']], [['', '', ''], ['', 'CASE WHEN boolean condition THEN result [WHEN ...] [ELSE result] END', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf you use the Amazon S3 console to run the following examples and your CSV file contains\\na header row, choose Exclude the first line of CSV data.', ''], ['', '', '']], [['', '', ''], ['', \"SELECT venuecity, CASE venuecity WHEN 'New York City' THEN 'Big Apple' ELSE 'other' END\\nFROM S3Object;\", ''], ['', '', '']], [['', '', ''], ['', 'venuecity | case\\n-----------------+-----------\\nLos Angeles | other\\nNew York City | Big Apple\\nSan Francisco | other\\nBaltimore | other\\n...', ''], ['', '', '']], [['', '', ''], ['', \"SELECT pricepaid, CASE WHEN CAST(pricepaid as FLOAT) < 10000 THEN 'group 1' WHEN\\nCAST(pricepaid as FLOAT) > 10000 THEN 'group 2' ELSE 'group 3' END FROM S3Object;\", ''], ['', '', '']], [['', '', ''], ['', 'pricepaid | case\\n-----------+---------\\n12624.00 | group 2\\n10000.00 | group 3\\n10000.00 | group 3\\n9996.00 | group 1\\n9988.00 | group 1\\n...', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'COALESCE ( expression, expression, ... )', ''], ['', '', '']], [['', '', ''], ['', \"COALESCE(1) -- 1\\nCOALESCE(null) -- null\\nCOALESCE(null, null) -- null\\nCOALESCE(missing) -- null\\nCOALESCE(missing, missing) -- null\\nCOALESCE(1, null) -- 1\\nCOALESCE(null, null, 1) -- 1\\nCOALESCE(null, 'string') -- 'string'\\nCOALESCE(missing, 1) -- 1\", ''], ['', '', '']], [['', '', ''], ['', 'NULLIF ( expression1, expression2 )', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"NULLIF(1, 1) -- null\\nNULLIF(1, 2) -- 1\\nNULLIF(1.0, 1) -- null\\nNULLIF(1, '1') -- 1\\nNULLIF([1], [1]) -- null\\nNULLIF(1, NULL) -- 1\\nNULLIF(NULL, 1) -- null\\nNULLIF(null, null) -- null\\nNULLIF(missing, null) -- null\\nNULLIF(missing, missing) -- null\", ''], ['', '', '']], [['', '', ''], ['', 'CAST ( expression AS data_type )', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"CAST('2007-04-05T14:30Z' AS TIMESTAMP)\\nCAST(0.456 AS FLOAT)\", ''], ['', '', '']], [['', '', ''], ['', 'DATE_ADD( date_part, quantity, timestamp )', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'DATE_ADD(year, 5, `2010-01-01T`) -- 2015-01-01 (equivalent to\\n2015-01-01T)\\nDATE_ADD(month, 1, `2010T`) -- 2010-02T (result will add precision\\nas necessary)\\nDATE_ADD(month, 13, `2010T`) -- 2011-02T\\nDATE_ADD(day, -1, `2017-01-10T`) -- 2017-01-09 (equivalent to\\n2017-01-09T)\\nDATE_ADD(hour, 1, `2017T`) -- 2017-01-01T01:00-00:00\\nDATE_ADD(hour, 1, `2017-01-02T03:04Z`) -- 2017-01-02T04:04Z\\nDATE_ADD(minute, 1, `2017-01-02T03:04:05.006Z`) -- 2017-01-02T03:05:05.006Z\\nDATE_ADD(second, 1, `2017-01-02T03:04:05.006Z`) -- 2017-01-02T03:04:06.006Z', ''], ['', '', '']], [['', '', ''], ['', 'DATE_DIFF( date_part, timestamp1, timestamp2 )', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'DATE_DIFF(year, `2010-01-01T`, `2011-01-01T`) -- 1\\nDATE_DIFF(year, `2010T`, `2010-05T`) -- 4 (2010T is equivalent to\\n2010-01-01T00:00:00.000Z)\\nDATE_DIFF(month, `2010T`, `2011T`) -- 12\\nDATE_DIFF(month, `2011T`, `2010T`) -- -12\\nDATE_DIFF(day, `2010-01-01T23:00`, `2010-01-02T01:00`) -- 0 (need to be at least 24h\\napart to be 1 day apart)', ''], ['', '', '']], [['', '', ''], ['', 'EXTRACT( date_part FROM timestamp )', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'EXTRACT(YEAR FROM `2010-01-01T`) -- 2010\\nEXTRACT(MONTH FROM `2010T`) -- 1 (equivalent to\\n2010-01-01T00:00:00.000Z)\\nEXTRACT(MONTH FROM `2010-10T`) -- 10\\nEXTRACT(HOUR FROM `2017-01-02T03:04:05+07:08`) -- 3\\nEXTRACT(MINUTE FROM `2017-01-02T03:04:05+07:08`) -- 4\\nEXTRACT(TIMEZONE_HOUR FROM `2017-01-02T03:04:05+07:08`) -- 7\\nEXTRACT(TIMEZONE_MINUTE FROM `2017-01-02T03:04:05+07:08`) -- 8', ''], ['', '', '']], [['', '', ''], ['', 'TO_STRING ( timestamp time_format_pattern )', ''], ['', '', '']], [['Format', 'Example', 'Description'], ['yy', '69', '2-digit year'], ['y', '1969', '4-digit year'], ['yyyy', '1969', 'Zero-padded\\n4-digit year']]]\n",
      "[[['Format', 'Example', 'Description'], ['M', '1', 'Month of year'], ['MM', '01', 'Zero-padded\\nmonth of yea\\nr'], ['MMM', 'Jan', 'Abbreviated\\nmonth year n\\name'], ['MMMM', 'January', 'Full month of\\nyear name'], ['MMMMM', 'J', 'Month of year\\nfirst letter\\n(NOTE: This\\nformat is\\nnot valid for\\nuse with the\\nTO_TIMEST\\nAMP\\nfunction.)'], ['d', '2', 'Day of month\\n(1-31)'], ['dd', '02', 'Zero-padded\\nday of month\\n(01-31)'], ['a', 'AM', 'AM or PM of\\nday'], ['h', '3', 'Hour of day\\n(1-12)']]]\n",
      "[[['Format', 'Example', 'Description'], ['hh', '03', 'Zero-padded\\nhour of day (0\\n1-12)'], ['H', '3', 'Hour of day\\n(0-23)'], ['HH', '03', 'Zero-padded\\nhour of day (0\\n0-23)'], ['m', '4', 'Minute of\\nhour (0-59)'], ['mm', '04', 'Zero-padd\\ned minute of\\nhour (00-59)'], ['s', '5', 'Second of\\nminute (0-59)'], ['ss', '05', 'Zero-padd\\ned second\\nof minute\\n(00-59)'], ['S', '0', 'Fraction of\\na second\\n(precision: 0.1,\\nrange: 0.0-0.9)'], ['SS', '6', 'Fraction of\\na second\\n(precision:\\n0.01, range:\\n0.0-0.99)']]]\n",
      "[[['Format', 'Example', 'Description'], ['SSS', '60', 'Fraction of\\na second\\n(precision:\\n0.001, range:\\n0.0-0.999)'], ['…', '…', '…'], ['SSSSSSSSS', '60000000', 'Fraction of\\na second\\n(maximum\\nprecision: 1\\nnanosecon\\nd, range:\\n0.0-0.999\\n999999)'], ['n', '60000000', 'Nano of a\\nsecond'], ['X', '+07 or Z', 'Offset in\\nhours, or Z if\\nthe offset is 0'], ['XX or XXXX', '+0700 or Z', 'Offset in\\nhours and\\nminutes, or Z\\nif the offset is\\n0'], ['XXX or XXXXX', '+07:00 or Z', 'Offset in\\nhours and\\nminutes, or Z\\nif the offset is\\n0']]]\n",
      "[[['Format', 'Example', 'Description'], ['x', '7', 'Offset in\\nhours'], ['xx or xxxx', '700', 'Offset in\\nhours and mi\\nnutes'], ['xxx or xxxxx', '+07:00', 'Offset in\\nhours and mi\\nnutes']], [['', '', ''], ['', 'TO_STRING(`1969-07-20T20:18Z`, \\'MMMM d, y\\') -- \"July 20, 1969\"\\nTO_STRING(`1969-07-20T20:18Z`, \\'MMM d, yyyy\\') -- \"Jul 20, 1969\"\\nTO_STRING(`1969-07-20T20:18Z`, \\'M-d-yy\\') -- \"7-20-69\"\\nTO_STRING(`1969-07-20T20:18Z`, \\'MM-d-y\\') -- \"07-20-1969\"\\nTO_STRING(`1969-07-20T20:18Z`, \\'MMMM d, y h:m a\\') -- \"July 20, 1969 8:18\\nPM\"\\nTO_STRING(`1969-07-20T20:18Z`, \\'y-MM-dd\\'\\'T\\'\\'H:m:ssX\\') --\\n\"1969-07-20T20:18:00Z\"\\nTO_STRING(`1969-07-20T20:18+08:00Z`, \\'y-MM-dd\\'\\'T\\'\\'H:m:ssX\\') --\\n\"1969-07-20T20:18:00Z\"\\nTO_STRING(`1969-07-20T20:18+08:00`, \\'y-MM-dd\\'\\'T\\'\\'H:m:ssXXXX\\') --\\n\"1969-07-20T20:18:00+0800\"\\nTO_STRING(`1969-07-20T20:18+08:00`, \\'y-MM-dd\\'\\'T\\'\\'H:m:ssXXXXX\\') --\\n\"1969-07-20T20:18:00+08:00\"', ''], ['', '', '']], [['', '', ''], ['', 'TO_TIMESTAMP ( string )', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"TO_TIMESTAMP('2007T') -- `2007T`\\nTO_TIMESTAMP('2007-02-23T12:14:33.079-08:00') -- `2007-02-23T12:14:33.079-08:00`\", ''], ['', '', '']], [['', '', ''], ['', 'UTCNOW()', ''], ['', '', '']], [['', '', ''], ['', 'UTCNOW() -- 2017-10-13T16:02:11.123Z', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nCHAR_LENGTH and CHARACTER_LENGTH are synonyms.', ''], ['', '', '']], [['', '', ''], ['', 'CHAR_LENGTH ( string )', ''], ['', '', '']], [['', '', ''], ['', \"CHAR_LENGTH('') -- 0\\nCHAR_LENGTH('abcdefg') -- 7\", ''], ['', '', '']], [['', '', ''], ['', 'LOWER ( string )', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"LOWER('AbCdEfG!@#$') -- 'abcdefg!@#$'\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe first character of the input string has an index position of 1.\\n• If start is < 1, with no length specified, then the index position is set to 1.\\n• If start is < 1, with a length specified, then the index position is set to start +\\nlength -1.\\n• If start + length -1 < 0, then an empty string is returned.\\n• If start + length -1 > = 0, then the substring starting at index position 1 with the\\nlength start + length - 1 is returned.', ''], ['', '', '']], [['', '', ''], ['', 'SUBSTRING( string FROM start [ FOR length ] )', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'SUBSTRING(\"123456789\", 0) -- \"123456789\"\\nSUBSTRING(\"123456789\", 1) -- \"123456789\"\\nSUBSTRING(\"123456789\", 2) -- \"23456789\"\\nSUBSTRING(\"123456789\", -4) -- \"123456789\"\\nSUBSTRING(\"123456789\", 0, 999) -- \"123456789\"\\nSUBSTRING(\"123456789\", 1, 5) -- \"12345\"', ''], ['', '', '']], [['', '', ''], ['', 'TRIM ( [[LEADING | TRAILING | BOTH remove_chars] FROM] string )', ''], ['', '', '']], [['', '', ''], ['', \"TRIM(' foobar ') -- 'foobar'\\nTRIM(' \\\\tfoobar\\\\t ') -- '\\\\tfoobar\\\\t'\\nTRIM(LEADING FROM ' foobar ') -- 'foobar '\\nTRIM(TRAILING FROM ' foobar ') -- ' foobar'\\nTRIM(BOTH FROM ' foobar ') -- 'foobar'\\nTRIM(BOTH '12' FROM '1112211foobar22211122') -- 'foobar'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'UPPER ( string )', ''], ['', '', '']], [['', '', ''], ['', \"UPPER('AbCdEfG!@#$') -- 'ABCDEFG!@#$'\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets. For more', '']]]\n",
      "[[['', 'information about using Batch Operations with S3 Express One Zone and directory buckets,\\nsee Using Batch Operations with S3 Express One Zone.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\":\"batchoperations.s3.amazonaws.com\"\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Action\": [\\n\"s3:PutObject\",\\n\"s3:PutObjectAcl\",\\n\"s3:PutObjectTagging\"\\n],\\n\"Effect\": \"Allow\",\\n\"Resource\": \"arn:aws:s3:::DestinationBucket/*\"\\n},\\n{\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectAcl\",\\n\"s3:GetObjectTagging\",\\n\"s3:ListBucket\"\\n],\\n\"Effect\": \"Allow\",\\n\"Resource\": [\\n\"arn:aws:s3:::SourceBucket\",\\n\"arn:aws:s3:::SourceBucket/*\"\\n]\\n},', '']]]\n",
      "[[['', '{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ManifestBucket/*\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ReportBucket/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObjectTagging\",\\n\"s3:PutObjectVersionTagging\"\\n],\\n\"Resource\": \"arn:aws:s3:::TargetResource/*\"\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ManifestBucket/*\"', '']]]\n",
      "[[['', ']\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::ReportBucket/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:DeleteObjectTagging\",\\n\"s3:DeleteObjectVersionTagging\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::TargetResource/*\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ManifestBucket/*\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\"', '']]]\n",
      "[[['', '],\\n\"Resource\": [\\n\"arn:aws:s3:::ReportBucket/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObjectAcl\",\\n\"s3:PutObjectVersionAcl\"\\n],\\n\"Resource\": \"arn:aws:s3:::TargetResource/*\"\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ManifestBucket/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::ReportBucket/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:RestoreObject\"\\n],\\n\"Resource\": \"arn:aws:s3:::TargetResource/*\"\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ManifestBucket/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::ReportBucket/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:GetBucketObjectLockConfiguration\",\\n\"Resource\": [', '']]]\n",
      "[[['', '\"arn:aws:s3:::TargetResource\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObjectRetention\",\\n\"s3:BypassGovernanceRetention\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::TargetResource/*\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ManifestBucket/*\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ReportBucket/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:GetBucketObjectLockConfiguration\",', '']]]\n",
      "[[['', '\"Resource\": [\\n\"arn:aws:s3:::TargetResource\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:PutObjectLegalHold\",\\n\"Resource\": [\\n\"arn:aws:s3:::TargetResource/*\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ManifestBucket/*\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ReportBucket/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{', '']]]\n",
      "[[['', '\"Action\":[\\n\"s3:InitiateReplication\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\\n\"arn:aws:s3:::*** replication source bucket ***/*\"\\n]\\n},\\n{\\n\"Action\":[\\n\"s3:GetReplicationConfiguration\",\\n\"s3:PutInventoryConfiguration\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\\n\"arn:aws:s3:::*** replication source bucket ***\"\\n]\\n},\\n{\\n\"Action\":[\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\\n\"arn:aws:s3:::*** manifest bucket ***/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::*** completion report bucket ****/*\",\\n\"arn:aws:s3:::*** manifest bucket ****/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Action\":[\\n\"s3:InitiateReplication\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\\n\"arn:aws:s3:::*** replication source bucket ***/*\"\\n]\\n},\\n{\\n\"Action\":[\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":[\\n\"arn:aws:s3:::*** manifest bucket ***/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::*** completion report bucket ****/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\n• Regardless of how you specify your manifest, the list itself must be stored in a general\\npurpose bucket. Batch Operations can't import existing manifests from, or save\\ngenerated manifests to directory buckets. Objects described within the manifest,\\nhowever, can be stored in directory buckets. For more information, see Directory\\nbuckets.\\n• If the objects in your manifest are in a versioned bucket, specifying the version IDs for\\nthe objects directs Batch Operations to perform the operation on a specific version.\\nIf no version IDs are specified, Batch Operations performs the operation on the latest\\nversion of the objects. If your manifest includes a version ID field, you must provide a\\nversion ID for all objects in the manifest.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nBatch Operations jobs that perform actions on directory buckets require specific\\npermissions. For more information, see AWS Identity and Access Management (IAM) for\\nS3 Express One Zone.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThe report must be stored in a general purpose bucket. Batch Operations can't save\\nreports to directory buckets. For more information, see Directory buckets.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nCompletion reports are always encrypted with Amazon S3 managed keys (SSE-S3).', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nRegardless of how you specify your manifest, the list itself must be stored in a general\\npurpose bucket. Batch Operations can't import existing manifests from, or save generated\\nmanifests to directory buckets. Objects described within the manifest, however, can be\\nstored in directory buckets. For more information, see Directory buckets.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf the objects in your manifest are in a versioned bucket, specifying the version IDs for\\nthe objects directs Batch Operations to perform the operation on a specific version. If no\\nversion IDs are specified, Batch Operations performs the operation on the latest version of\\nthe objects. If your manifest includes a version ID field, you must provide a version ID for all\\nobjects in the manifest.', ''], ['', '', '']], [['', '', ''], ['', 'Examplebucket,objectkey1\\nExamplebucket,objectkey2\\nExamplebucket,objectkey3\\nExamplebucket,photos/jpgs/objectkey4\\nExamplebucket,photos/jpgs/newjersey/objectkey5\\nExamplebucket,object%20key%20with%20spaces', ''], ['', '', '']], [['', '', ''], ['', 'Examplebucket,objectkey1,PZ9ibn9D5lP6p298B7S9_ceqx1n5EJ0p\\nExamplebucket,objectkey2,YY_ouuAJByNW1LRBfFMfxMge7XQWxMBF\\nExamplebucket,objectkey3,jbo9_jhdPEyB4RrmOxWS0kU0EoNrU_oI\\nExamplebucket,photos/jpgs/objectkey4,6EqlikJJxLTsHsnbZbSRffn24_eh5Ny4\\nExamplebucket,photos/jpgs/newjersey/objectkey5,imHf3FAiRsvBW_EHB8GOu.NHunHO1gVs', '']]]\n",
      "[[['', 'Examplebucket,object%20key%20with%20spaces,9HkPvDaZY5MVbMhn6TMn1YTb5ArQAo3w', ''], ['', '', '']], [['', '', ''], ['', 'Note\\n• S3 Batch Operations supports CSV inventory reports that are encrypted with SSE-KMS.\\n• If you submit an inventory report manifest that\\'s encrypted with SSE-KMS, your IAM\\npolicy must include the permissions \"kms:Decrypt\" and \"kms:GenerateDataKey\"\\nfor the manifest.json object and all associated CSV data files.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nS3 Batch Operations doesn't support CSV manifest files that are encrypted with SSE-\\nKMS.\", ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you\\'re using a manually created manifest and a versioned bucket, we recommend\\nthat you specify the version IDs for the objects. When you create a job, S3 Batch Operations\\nparses the entire manifest before running the job. However, it doesn\\'t take a \"snapshot\" of\\nthe state of the bucket.\\nBecause manifests can contain billions of objects, jobs might take a long time to run, which\\ncan affect which version of an object that the job acts upon. Suppose that you overwrite', '']]]\n",
      "[[['', \"an object with a new version while a job is running and you didn't specify a version ID for\\nthat object. In this case, Amazon S3 performs the operation on the latest version of the\\nobject, not on the version that existed when you created the job. The only way to avoid this\\nbehavior is to specify version IDs for the objects that are listed in the manifest.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't clone most jobs that have automatically generated manifests. Batch\\nreplication jobs can be cloned, except when they use the KeyNameConstraint,\", '']]]\n",
      "[[['', 'MatchAnyStorageClass, ObjectSizeGreaterThanBytes, or\\nObjectSizeLessThanBytes manifest filter criteria.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor copy operations, you must create the job in the same Region as the destination\\nbucket. For all other operations, you must create the job in the same Region as the\\nobjects in the manifest.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe Amazon S3 console supports automatic manifest generation for batch replication\\njobs only. For all other job types, if you want Amazon S3 to generate a manifest\\nautomatically based on filter criteria that you specify, you must configure your job\\nusing the AWS CLI, AWS SDKs, or Amazon S3 REST API.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws iam create-role \\\\\\n--role-name S3BatchJobRole \\\\\\n--assume-role-policy-document \\'{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\":\"batchoperations.s3.amazonaws.com\"\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]\\n}\\'', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nBatch Operations jobs that perform actions on directory buckets require\\nspecific permissions. For more information, see AWS Identity and Access\\nManagement (IAM) for S3 Express One Zone.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws iam put-role-policy \\\\\\n--role-name S3BatchJobRole \\\\\\n--policy-name PutObjectTaggingBatchJobPolicy \\\\\\n--policy-document \\'{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObjectTagging\",\\n\"s3:PutObjectVersionTagging\"\\n],\\n\"Resource\": \"arn:aws:s3:::example-s3-destination-bucket/*\"\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:GetBucketLocation\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-MANIFEST-BUCKET\",\\n\"arn:aws:s3:::DOC-EXAMPLE-MANIFEST-BUCKET/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\",', '']]]\n",
      "[[['', '\"s3:GetBucketLocation\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::DOC-EXAMPLE-REPORT-BUCKET\",\\n\"arn:aws:s3:::DOC-EXAMPLE-REPORT-BUCKET/*\"\\n]\\n}\\n]\\n}\\'', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-job \\\\\\n--region us-west-2 \\\\\\n--account-id acct-id \\\\\\n--operation \\'{\"S3PutObjectTagging\": { \"TagSet\": [{\"Key\":\"keyOne\",\\n\"Value\":\"ValueOne\"}] }}\\' \\\\\\n--manifest \\'{\"Spec\":{\"Format\":\"S3BatchOperations_CSV_20180820\",\"Fields\":\\n[\"Bucket\",\"Key\"]},\"Location\":\\n{\"ObjectArn\":\"arn:aws:s3:::my_manifests/\\nmanifest.csv\",\"ETag\":\"60e460c9d1046e73f7dde5043ac3ae85\"}}\\' \\\\\\n--report \\'{\"Bucket\":\"arn:aws:s3:::DOC-EXAMPLE-REPORT-\\nBUCKET\",\"Prefix\":\"final-reports\",\\n\"Format\":\"Report_CSV_20180820\",\"Enabled\":true,\"ReportScope\":\"AllTasks\"}\\' \\\\\\n--priority 42 \\\\\\n--role-arn IAM-role \\\\\\n--client-request-token $(uuidgen) \\\\\\n--description \"job description\" \\\\\\n--no-confirmation-required', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws iam create-role \\\\\\n--role-name S3BatchJobRole \\\\\\n--assume-role-policy-document \\'{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\":\"batchoperations.s3.amazonaws.com\"\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]\\n}\\'', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nBatch Operations jobs that perform actions on directory buckets require\\nspecific permissions. For more information, see AWS Identity and Access\\nManagement (IAM) for S3 Express One Zone.', ''], ['', '', '']], [['', '', ''], ['', 'aws iam put-role-policy \\\\\\n--role-name S3BatchJobRole \\\\\\n--policy-name DeleteObjectTaggingBatchJobPolicy \\\\\\n--policy-document \\'{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:DeleteObjectTagging\",\\n\"s3:DeleteObjectVersionTagging\"\\n],\\n\"Resource\": \"arn:aws:s3:::example-s3-destination-bucket/*\"\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[', '']]]\n",
      "[[['', '\"s3:PutInventoryConfiguration\"\\n],\\n\"Resource\": \"arn:aws:s3:::example-s3-destination-bucket\"\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:ListBucket\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-MANIFEST-OUTPUT-BUCKET\",\\n\"arn:aws:s3:::DOC-EXAMPLE-MANIFEST-OUTPUT-BUCKET/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:PutObject\",\\n\"s3:ListBucket\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3:::DOC-EXAMPLE-REPORT-BUCKET\",\\n\"arn:aws:s3:::DOC-EXAMPLE-REPORT-BUCKET/*\",\\n\"arn:aws:s3:::DOC-EXAMPLE-MANIFEST-OUTPUT-BUCKET/*\"\\n]\\n}\\n]\\n}\\'', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-job \\\\\\n--account-id 012345678901 \\\\\\n--operation \\'{\\n\"S3DeleteObjectTagging\": {}\\n}\\' \\\\\\n--report \\'{\\n\"Bucket\":\"arn:aws:s3:::DOC-EXAMPLE-REPORT-BUCKET\",\\n\"Prefix\":\"reports\",\\n\"Format\":\"Report_CSV_20180820\",\\n\"Enabled\":true,\\n\"ReportScope\":\"AllTasks\"\\n}\\' \\\\\\n--manifest-generator \\'{\\n\"S3JobManifestGenerator\": {\\n\"ExpectedBucketOwner\": \"012345678901\",\\n\"SourceBucket\": \"arn:aws:s3:::DOC-EXAMPLE-SOURCE-BUCKET\",\\n\"EnableManifestOutput\": true,\\n\"ManifestOutputLocation\": {\\n\"ExpectedManifestBucketOwner\": \"012345678901\",\\n\"Bucket\": \"arn:aws:s3:::DOC-EXAMPLE-MANIFEST-OUTPUT-BUCKET\",\\n\"ManifestPrefix\": \"prefix\",\\n\"ManifestFormat\": \"S3InventoryReport_CSV_20211130\"\\n},\\n\"Filter\": {\\n\"CreatedAfter\": \"2023-09-01\",\\n\"CreatedBefore\": \"2023-10-01\",\\n\"KeyNameConstraint\": {\\n\"MatchAnyPrefix\": [\\n\"prefix\"\\n],\\n\"MatchAnySuffix\": [\\n\"suffix\"\\n]\\n},\\n\"ObjectSizeGreaterThanBytes\": 100,\\n\"ObjectSizeLessThanBytes\": 200,\\n\"MatchAnyStorageClass\": [\\n\"STANDARD\",\\n\"STANDARD_IA\"', '']]]\n",
      "[[['', \"]\\n}\\n}\\n}' \\\\\\n--priority 2 \\\\\\n--role-arn IAM-role \\\\\\n--region us-east-1\", ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.*;\\nimport java.util.UUID;\\nimport java.util.ArrayList;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class CreateJob {\\npublic static void main(String[] args) {\\nString accountId = \"Account ID\";\\nString iamRoleArn = \"IAM Role ARN\";', '']]]\n",
      "[[['', 'String reportBucketName = \"arn:aws:s3:::DOC-EXAMPLE-REPORT-BUCKET\";\\nString uuid = UUID.randomUUID().toString();\\nArrayList tagSet = new ArrayList<S3Tag>();\\ntagSet.add(new S3Tag().withKey(\"keyOne\").withValue(\"ValueOne\"));\\ntry {\\nJobOperation jobOperation = new JobOperation()\\n.withS3PutObjectTagging(new S3SetObjectTaggingOperation()\\n.withTagSet(tagSet)\\n);\\nJobManifest manifest = new JobManifest()\\n.withSpec(new JobManifestSpec()\\n.withFormat(\"S3BatchOperations_CSV_20180820\")\\n.withFields(new String[]{\\n\"Bucket\", \"Key\"\\n}))\\n.withLocation(new JobManifestLocation()\\n.withObjectArn(\"arn:aws:s3:::my_manifests/manifest.csv\")\\n.withETag(\"60e460c9d1046e73f7dde5043ac3ae85\"));\\nJobReport jobReport = new JobReport()\\n.withBucket(reportBucketName)\\n.withPrefix(\"reports\")\\n.withFormat(\"Report_CSV_20180820\")\\n.withEnabled(true)\\n.withReportScope(\"AllTasks\");\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\ns3ControlClient.createJob(new CreateJobRequest()\\n.withAccountId(accountId)\\n.withOperation(jobOperation)\\n.withManifest(manifest)\\n.withReport(jobReport)\\n.withPriority(42)\\n.withRoleArn(iamRoleArn)\\n.withClientRequestToken(uuid)\\n.withDescription(\"job description\")\\n.withConfirmationRequired(false)', '']]]\n",
      "[[['', \");\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.CreateJobRequest;\\nimport com.amazonaws.services.s3control.model.CreateJobResult;\\nimport com.amazonaws.services.s3control.model.JobManifestGenerator;\\nimport com.amazonaws.services.s3control.model.JobManifestGeneratorFilter;\\nimport com.amazonaws.services.s3control.model.JobOperation;\\nimport com.amazonaws.services.s3control.model.JobReport;\\nimport com.amazonaws.services.s3control.model.KeyNameConstraint;\\nimport com.amazonaws.services.s3control.model.S3JobManifestGenerator;\\nimport com.amazonaws.services.s3control.model.S3ManifestOutputLocation;\\nimport com.amazonaws.services.s3control.model.S3SetObjectTaggingOperation;\\nimport com.amazonaws.services.s3control.model.S3Tag;', '']]]\n",
      "[[['', 'import java.time.Instant;\\nimport java.util.Date;\\nimport java.util.UUID;\\nimport java.util.ArrayList;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class test {\\npublic static void main(String[] args) {\\nString accountId = \"012345678901\";\\nString iamRoleArn = \"arn:aws:iam::012345678901:role/ROLE\";\\nString sourceBucketName = \"arn:aws:s3:::DOC-EXAMPLE-SOURCE-BUCKET\";\\nString reportBucketName = \"arn:aws:s3:::DOC-EXAMPLE-REPORT-BUCKET\";\\nString manifestOutputBucketName = \"arn:aws:s3:::DOC-EXAMPLE-MANIFEST-\\nOUTPUT-BUCKET\";\\nString uuid = UUID.randomUUID().toString();\\nlong minimumObjectSize = 100L;\\nArrayList<S3Tag> tagSet = new ArrayList<>();\\ntagSet.add(new S3Tag().withKey(\"keyOne\").withValue(\"ValueOne\"));\\nArrayList<String> prefixes = new ArrayList<>();\\nprefixes.add(\"s3KeyStartsWith\");\\ntry {\\nJobOperation jobOperation = new JobOperation()\\n.withS3PutObjectTagging(new S3SetObjectTaggingOperation()\\n.withTagSet(tagSet)\\n);\\nS3ManifestOutputLocation manifestOutputLocation = new\\nS3ManifestOutputLocation()\\n.withBucket(manifestOutputBucketName)\\n.withManifestPrefix(\"manifests\")\\n.withExpectedManifestBucketOwner(accountId)\\n.withManifestFormat(\"S3InventoryReport_CSV_20211130\");\\nJobManifestGeneratorFilter jobManifestGeneratorFilter = new\\nJobManifestGeneratorFilter()\\n.withEligibleForReplication(true)\\n.withKeyNameConstraint(\\nnew KeyNameConstraint()\\n.withMatchAnyPrefix(prefixes))\\n.withCreatedBefore(Date.from(Instant.now()))\\n.withObjectSizeGreaterThanBytes(minimumObjectSize);', '']]]\n",
      "[[['', 'S3JobManifestGenerator s3JobManifestGenerator = new\\nS3JobManifestGenerator()\\n.withEnableManifestOutput(true)\\n.withManifestOutputLocation(manifestOutputLocation)\\n.withFilter(jobManifestGeneratorFilter)\\n.withSourceBucket(sourceBucketName);\\nJobManifestGenerator jobManifestGenerator = new\\nJobManifestGenerator()\\n.withS3JobManifestGenerator(s3JobManifestGenerator);\\nJobReport jobReport = new JobReport()\\n.withBucket(reportBucketName)\\n.withPrefix(\"reports\")\\n.withFormat(\"Report_CSV_20180820\")\\n.withEnabled(true)\\n.withReportScope(\"AllTasks\");\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\nCreateJobResult createJobResult = s3ControlClient.createJob(new\\nCreateJobRequest()\\n.withAccountId(accountId)\\n.withOperation(jobOperation)\\n.withManifestGenerator(jobManifestGenerator)\\n.withReport(jobReport)\\n.withPriority(42)\\n.withRoleArn(iamRoleArn)\\n.withClientRequestToken(uuid)\\n.withDescription(\"job description\")\\n.withConfirmationRequired(true)\\n);\\nSystem.out.println(\"Created job \" + createJobResult.getJobId());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t\\nprocess\\n// it and returned an error response.\\ne.printStackTrace();', '']]]\n",
      "[[['', \"} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowBatchOperationsDestinationObjectCOPY\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\",\\n\"s3:PutObjectVersionAcl\",\\n\"s3:PutObjectAcl\",\\n\"s3:PutObjectVersionTagging\",\\n\"s3:PutObjectTagging\",\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:GetObjectAcl\",\\n\"s3:GetObjectTagging\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:GetObjectVersionTagging\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ObjectDestinationBucket/*\",\\n\"arn:aws:s3:::ObjectSourceBucket/*\",\\n\"arn:aws:s3:::ObjectDestinationManifestBucket/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowBatchOperationsSourceObjectCOPY\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::DestinationAccountNumber:role/\\nBatchOperationsDestinationRoleCOPY\"\\n},\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:GetObjectAcl\",\\n\"s3:GetObjectTagging\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:GetObjectVersionTagging\"\\n],\\n\"Resource\": \"arn:aws:s3:::ObjectSourceBucket/*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowBatchOperationsDestinationObjectCOPY\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\",\\n\"s3:PutObjectVersionAcl\",\\n\"s3:PutObjectAcl\",\\n\"s3:PutObjectVersionTagging\",', '']]]\n",
      "[[['', '\"s3:PutObjectTagging\",\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:GetObjectAcl\",\\n\"s3:GetObjectTagging\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:GetObjectVersionTagging\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ObjectDestinationBucket/*\",\\n\"arn:aws:s3:::ObjectSourceBucket/*\",\\n\"arn:aws:s3:::ObjectSourceManifestBucket/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowBatchOperationsSourceManfiestRead\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": [\\n\"arn:aws:iam::DestinationAccountNumber:user/ConsoleUserCreatingJob\",\\n\"arn:aws:iam::DestinationAccountNumber:role/\\nBatchOperationsDestinationRoleCOPY\"', '']]]\n",
      "[[['', ']\\n},\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\": \"arn:aws:s3:::ObjectSourceManifestBucket/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowBatchOperationsSourceObjectCOPY\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::DestinationAccountNumber:role/\\nBatchOperationsDestinationRoleCOPY\"\\n},\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:GetObjectAcl\",\\n\"s3:GetObjectTagging\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:GetObjectVersionTagging\"\\n],\\n\"Resource\": \"arn:aws:s3:::ObjectSourceBucket/*\"\\n}\\n]', '']]]\n",
      "[[['', '}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"sourceBucket\": \"batchoperationsdemo\",\\n\"destinationBucket\": \"arn:aws:s3:::testbucket\",\\n\"version\": \"2021-05-22\",\\n\"creationTimestamp\": \"1558656000000\",\\n\"fileFormat\": \"CSV\",\\n\"fileSchema\": \"Bucket, Key, VersionId, IsLatest, IsDeleteMarker,\\nBucketKeyStatus\",\\n\"files\": [\\n{', '']]]\n",
      "[[['', '\"key\": \"demoinv/batchoperationsdemo/DemoInventory/data/009a40e4-\\nf053-4c16-8c75-6100f8892202.csv.gz\",\\n\"size\": 72691,\\n\"MD5checksum\": \"c24c831717a099f0ebe4a9d1c5d3935c\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'batchoperationsdemo,0100059%7Ethumb.jpg,lsrtIxksLu0R0ZkYPL.LhgD5caTYn6vu\\nbatchoperationsdemo,0100074%7Ethumb.jpg,sd2M60g6Fdazoi6D5kNARIE7KzUibmHR\\nbatchoperationsdemo,0100075%7Ethumb.jpg,TLYESLnl1mXD5c4BwiOIinqFrktddkoL\\nbatchoperationsdemo,0200147%7Ethumb.jpg,amufzfMi_fEw0Rs99rxR_HrDFlE.l3Y0\\nbatchoperationsdemo,0301420%7Ethumb.jpg,9qGU2SEscL.C.c_sK89trmXYIwooABSh\\nbatchoperationsdemo,0401524%7Ethumb.jpg,ORnEWNuB1QhHrrYAGFsZhbyvEYJ3DUor', '']]]\n",
      "[[['', 'batchoperationsdemo,200907200065HQ\\n%7Ethumb.jpg,d8LgvIVjbDR5mUVwW6pu9ahTfReyn5V4\\nbatchoperationsdemo,200907200076HQ\\n%7Ethumb.jpg,XUT25d7.gK40u_GmnupdaZg3BVx2jN40\\nbatchoperationsdemo,201103190002HQ\\n%7Ethumb.jpg,z.2sVRh0myqVi0BuIrngWlsRPQdb7qOS', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"CopyObjectsToEncrypt\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\",\\n\"s3:PutObjectTagging\",\\n\"s3:PutObjectAcl\",\\n\"s3:PutObjectVersionTagging\",\\n\"s3:PutObjectVersionAcl\",\\n\"s3:GetObject\",\\n\"s3:GetObjectAcl\",\\n\"s3:GetObjectTagging\",\\n\"s3:GetObjectVersion\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:GetObjectVersionTagging\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::SOURCE_BUCKET_FOR_COPY/*\",\\n\"arn:aws:s3:::DESTINATION_BUCKET_FOR_COPY/*\"', '']]]\n",
      "[[['', ']\\n},\\n{\\n\"Sid\": \"ReadManifest\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\"\\n],\\n\"Resource\": \"arn:aws:s3:::MANIFEST_KEY\"\\n},\\n{\\n\"Sid\": \"WriteReport\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": \"arn:aws:s3:::REPORT_BUCKET/*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Sid\": \"AddIamPermissions\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"iam:GetRole\",\\n\"iam:PassRole\"\\n],\\n\"Resource\": \"arn:aws:iam::ACCOUNT-ID:role/IAM_ROLE_NAME\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf the bucket policy for the specified destination requires objects to be\\nencrypted before storing them in Amazon S3, you must specify an encryption\\nkey. Otherwise, copying objects to the destination will fail.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['Response code', 'Description'], ['Succeeded', \"The task completed normally. If you requested\\na job completion report, the task's result string\\nis included in the report.\"], ['TemporaryFailure', 'The task suffered a temporary failure and\\nwill be redriven before the job completes.\\nThe result string is ignored. If this is the final\\nredrive, the error message is included in the\\nfinal report.'], ['PermanentFailure', 'The task suffered a permanent failure. If you\\nrequested a job-completion report, the task\\nis marked as Failed and includes the error\\nmessage string. Result strings from failed\\ntasks are ignored.']], [['', '', ''], ['', 'Important\\nAWS Lambda functions written in Java accept either RequestHandler or\\nRequestStreamHandler handler interfaces. However, to support S3 Batch Operations', '']]]\n",
      "[[['', 'request and response format, AWS Lambda requires the RequestStreamHandler\\ninterface for custom serialization and deserialization of a request and response.\\nThis interface allows Lambda to pass an InputStream and OutputStream to the Java\\nhandleRequest method.\\nBe sure to use the RequestStreamHandler interface when using Lambda functions with\\nS3 Batch Operations. If you use a RequestHandler interface, the batch job will fail with\\n\"Invalid JSON returned in Lambda payload\" in the completion report.\\nFor more information, see Handler interfaces in the AWS Lambda User Guide.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"batchoperations.s3.amazonaws.com\"\\n},\\n\"Action\": \"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",', '']]]\n",
      "[[['', '\"Statement\": [\\n{\\n\"Sid\": \"BatchOperationsLambdaPolicy\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:PutObject\",\\n\"lambda:InvokeFunction\"\\n],\\n\"Resource\": \"*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"invocationSchemaVersion\": \"1.0\",\\n\"invocationId\": \"YXNkbGZqYWRmaiBhc2RmdW9hZHNmZGpmaGFzbGtkaGZza2RmaAo\",\\n\"job\": {\\n\"id\": \"f3cc4f60-61f6-4a2b-8a21-d07600c373ce\"\\n},\\n\"tasks\": [\\n{\\n\"taskId\": \"dGFza2lkZ29lc2hlcmUK\",\\n\"s3Key\": \"customerImage1.jpg\",\\n\"s3VersionId\": \"1\",\\n\"s3BucketArn\": \"arn:aws:s3:us-east-1:0123456788:awsexamplebucket1\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"invocationSchemaVersion\": \"1.0\",\\n\"treatMissingKeysAs\" : \"PermanentFailure\",\\n\"invocationId\" : \"YXNkbGZqYWRmaiBhc2RmdW9hZHNmZGpmaGFzbGtkaGZza2RmaAo\",\\n\"results\": [\\n{\\n\"taskId\": \"dGFza2lkZ29lc2hlcmUK\",\\n\"resultCode\": \"Succeeded\",\\n\"resultString\": \"[\\\\\"Mary Major\", \\\\\"John Stiles\\\\\"]\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import logging\\nfrom urllib import parse\\nimport boto3\\nfrom botocore.exceptions import ClientError\\nlogger = logging.getLogger(__name__)\\nlogger.setLevel(\"INFO\")\\ns3 = boto3.client(\"s3\")\\ndef lambda_handler(event, context):\\n\"\"\"\\nRemoves a delete marker from the specified versioned object.\\n:param event: The S3 batch event that contains the ID of the delete marker\\nto remove.\\n:param context: Context about the event.\\n:return: A result structure that Amazon S3 uses to interpret the result of the\\noperation. When the result code is TemporaryFailure, S3 retries the\\noperation.\\n\"\"\"\\n# Parse job parameters from Amazon S3 batch operations', '']]]\n",
      "[[['', 'invocation_id = event[\"invocationId\"]\\ninvocation_schema_version = event[\"invocationSchemaVersion\"]\\nresults = []\\nresult_code = None\\nresult_string = None\\ntask = event[\"tasks\"][0]\\ntask_id = task[\"taskId\"]\\ntry:\\nobj_key = parse.unquote(task[\"s3Key\"], encoding=\"utf-8\")\\nobj_version_id = task[\"s3VersionId\"]\\nbucket_name = task[\"s3BucketArn\"].split(\":\")[-1]\\nlogger.info(\\n\"Got task: remove delete marker %s from object %s.\", obj_version_id,\\nobj_key\\n)\\ntry:\\n# If this call does not raise an error, the object version is not a delete\\n# marker and should not be deleted.\\nresponse = s3.head_object(\\nBucket=bucket_name, Key=obj_key, VersionId=obj_version_id\\n)\\nresult_code = \"PermanentFailure\"\\nresult_string = (\\nf\"Object {obj_key}, ID {obj_version_id} is not \" f\"a delete marker.\"\\n)\\nlogger.debug(response)\\nlogger.warning(result_string)\\nexcept ClientError as error:\\ndelete_marker = error.response[\"ResponseMetadata\"][\"HTTPHeaders\"].get(\\n\"x-amz-delete-marker\", \"false\"\\n)\\nif delete_marker == \"true\":\\nlogger.info(\\n\"Object %s, version %s is a delete marker.\", obj_key,\\nobj_version_id\\n)\\ntry:\\ns3.delete_object(', '']]]\n",
      "[[['', 'Bucket=bucket_name, Key=obj_key, VersionId=obj_version_id\\n)\\nresult_code = \"Succeeded\"\\nresult_string = (\\nf\"Successfully removed delete marker \"\\nf\"{obj_version_id} from object {obj_key}.\"\\n)\\nlogger.info(result_string)\\nexcept ClientError as error:\\n# Mark request timeout as a temporary failure so it will be\\nretried.\\nif error.response[\"Error\"][\"Code\"] == \"RequestTimeout\":\\nresult_code = \"TemporaryFailure\"\\nresult_string = (\\nf\"Attempt to remove delete marker from \"\\nf\"object {obj_key} timed out.\"\\n)\\nlogger.info(result_string)\\nelse:\\nraise\\nelse:\\nraise ValueError(\\nf\"The x-amz-delete-marker header is either not \"\\nf\"present or is not \\'true\\'.\"\\n)\\nexcept Exception as error:\\n# Mark all other exceptions as permanent failures.\\nresult_code = \"PermanentFailure\"\\nresult_string = str(error)\\nlogger.exception(error)\\nfinally:\\nresults.append(\\n{\\n\"taskId\": task_id,\\n\"resultCode\": result_code,\\n\"resultString\": result_string,\\n}\\n)\\nreturn {\\n\"invocationSchemaVersion\": invocation_schema_version,\\n\"treatMissingKeysAs\": \"PermanentFailure\",\\n\"invocationId\": invocation_id,\\n\"results\": results,\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-job\\n--account-id <AccountID>\\n--operation \\'{\"LambdaInvoke\": { \"FunctionArn\":\\n\"arn:aws:lambda:Region:AccountID:function:LambdaFunctionName\" } }\\'\\n--manifest \\'{\"Spec\":{\"Format\":\"S3BatchOperations_CSV_20180820\",\"Fields\":\\n[\"Bucket\",\"Key\"]},\"Location\":\\n{\"ObjectArn\":\"arn:aws:s3:::ManifestLocation\",\"ETag\":\"ManifestETag\"}}\\'\\n--report\\n\\'{\"Bucket\":\"arn:aws:s3:::awsexamplebucket1\",\"Format\":\"Report_CSV_20180820\",\"Enabled\":tr\\n--priority 2\\n--role-arn arn:aws:iam::AccountID:role/BatchOperationsRole\\n--region Region\\n--description “Lambda Function\"', 'u'], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe maximum number of characters for the key field in the manifest is 1,024.', ''], ['', '', '']], [['', '', ''], ['', 'my-bucket,{\"origKey\": \"object1key\", \"newKey\": \"newObject1Key\"}\\nmy-bucket,{\"origKey\": \"object2key\", \"newKey\": \"newObject2Key\"}\\nmy-bucket,{\"origKey\": \"object3key\", \"newKey\": \"newObject3Key\"}', ''], ['', '', '']], [['', '', ''], ['', 'my-bucket,%7B%22origKey%22%3A%20%22object1key%22%2C%20%22newKey%22%3A\\n%20%22newObject1Key%22%7D\\nmy-bucket,%7B%22origKey%22%3A%20%22object2key%22%2C%20%22newKey%22%3A\\n%20%22newObject2Key%22%7D\\nmy-bucket,%7B%22origKey%22%3A%20%22object3key%22%2C%20%22newKey%22%3A\\n%20%22newObject3Key%22%7D', ''], ['', '', '']], [['', '', ''], ['', 'my-bucket,object1key%7Clower', '']]]\n",
      "[[['', 'my-bucket,object2key%7Cupper\\nmy-bucket,object3key%7Creverse\\nmy-bucket,object4key%7Cdelete', ''], ['', '', '']], [['', '', ''], ['', 'import logging\\nfrom urllib import parse\\nimport boto3\\nfrom botocore.exceptions import ClientError\\nlogger = logging.getLogger(__name__)\\nlogger.setLevel(\"INFO\")\\ns3 = boto3.resource(\"s3\")\\ndef lambda_handler(event, context):\\n\"\"\"\\nApplies the specified revision to the specified object.\\n:param event: The Amazon S3 batch event that contains the ID of the object to\\nrevise and the revision type to apply.\\n:param context: Context about the event.\\n:return: A result structure that Amazon S3 uses to interpret the result of the\\noperation.\\n\"\"\"\\n# Parse job parameters from Amazon S3 batch operations\\ninvocation_id = event[\"invocationId\"]\\ninvocation_schema_version = event[\"invocationSchemaVersion\"]\\nresults = []\\nresult_code = None\\nresult_string = None\\ntask = event[\"tasks\"][0]\\ntask_id = task[\"taskId\"]\\n# The revision type is packed with the object key as a pipe-delimited string.\\nobj_key, revision = parse.unquote(task[\"s3Key\"], encoding=\"utf-8\").split(\"|\")\\nbucket_name = task[\"s3BucketArn\"].split(\":\")[-1]\\nlogger.info(\"Got task: apply revision %s to %s.\", revision, obj_key)', '']]]\n",
      "[[['', 'try:\\nstanza_obj = s3.Bucket(bucket_name).Object(obj_key)\\nstanza = stanza_obj.get()[\"Body\"].read().decode(\"utf-8\")\\nif revision == \"lower\":\\nstanza = stanza.lower()\\nelif revision == \"upper\":\\nstanza = stanza.upper()\\nelif revision == \"reverse\":\\nstanza = stanza[::-1]\\nelif revision == \"delete\":\\npass\\nelse:\\nraise TypeError(f\"Can\\'t handle revision type \\'{revision}\\'.\")\\nif revision == \"delete\":\\nstanza_obj.delete()\\nresult_string = f\"Deleted stanza {stanza_obj.key}.\"\\nelse:\\nstanza_obj.put(Body=bytes(stanza, \"utf-8\"))\\nresult_string = (\\nf\"Applied revision type \\'{revision}\\' to \" f\"stanza {stanza_obj.key}.\"\\n)\\nlogger.info(result_string)\\nresult_code = \"Succeeded\"\\nexcept ClientError as error:\\nif error.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\\nresult_code = \"Succeeded\"\\nresult_string = (\\nf\"Stanza {obj_key} not found, assuming it was deleted \"\\nf\"in an earlier revision.\"\\n)\\nlogger.info(result_string)\\nelse:\\nresult_code = \"PermanentFailure\"\\nresult_string = (\\nf\"Got exception when applying revision type \\'{revision}\\' \"\\nf\"to {obj_key}: {error}.\"\\n)\\nlogger.exception(result_string)\\nfinally:\\nresults.append(\\n{', '']]]\n",
      "[[['', '\"taskId\": task_id,\\n\"resultCode\": result_code,\\n\"resultString\": result_string,\\n}\\n)\\nreturn {\\n\"invocationSchemaVersion\": invocation_schema_version,\\n\"treatMissingKeysAs\": \"PermanentFailure\",\\n\"invocationId\": invocation_id,\\n\"results\": results,\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nRunning this job removes all object tag sets on every object listed in the manifest.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\nThe S3 Initiate Restore Object job only initiates the request to restore objects. S3 Batch\\nOperations reports the job as complete for each object after the request is initiated for\\nthat object. Amazon S3 doesn't update the job or otherwise notify you when the objects\\nhave been restored. However, you can use S3 Event Notifications to receive notifications\", '']]]\n",
      "[[['', 'when the objects are available in Amazon S3. For more information, see Amazon S3 Event\\nNotifications.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nDo not set ExpirationInDays when creating S3 Initiate Restore Object operation jobs\\nthat target S3 Intelligent-Tiering Archive Access and Deep Archive Access tier objects.\\nObjects in S3 Intelligent-Tiering archive access tiers are not subject to restore expiration,\\nso specifying ExpirationInDays results in a restore request failure.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3control list-jobs \\\\\\n--region us-west-2 \\\\\\n--account-id acct-id \\\\\\n--job-statuses \\'[\"Active\",\"Complete\"]\\' \\\\\\n--max-results 20', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control describe-job \\\\\\n--region us-west-2 \\\\\\n--account-id acct-id \\\\\\n--job-id 00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nS3 Batch Operations honor job priorities on a best-effort basis. Although jobs with higher\\npriorities generally take precedence over jobs with lower priorities, Amazon S3 does not\\nguarantee strict ordering of jobs.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control update-job-priority \\\\\\n--region us-west-2 \\\\\\n--account-id acct-id \\\\\\n--priority 98 \\\\\\n--job-id 00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.UpdateJobPriorityRequest;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class UpdateJobPriority {\\npublic static void main(String[] args) {\\nString accountId = \"Account ID\";\\nString jobId = \"00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c\";\\ntry {\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\ns3ControlClient.updateJobPriority(new UpdateJobPriorityRequest()\\n.withAccountId(accountId)\\n.withJobId(jobId)\\n.withPriority(98));\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client', '']]]\n",
      "[[['', \"// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['Status', 'Description', 'Transitions'], ['New', 'A job begins in the New state\\nwhen you create it.', 'A job automatically moves to\\nthe Preparing state when\\nAmazon S3 begins processing\\nthe manifest object.'], ['Preparing', 'Amazon S3 is processing the\\nmanifest object and other job\\nparameters to set up and run\\nthe job.', 'A job automatically moves\\nto the Ready state after\\nAmazon S3 finishes processin\\ng the manifest and other\\nparameters. It is then ready']]]\n",
      "[[['Status', 'Description', 'Transitions'], ['', '', 'to begin running the specified\\noperation on the objects\\nlisted in the manifest.\\nIf the job requires confirmat\\nion before running, such as\\nwhen you create a job using\\nthe Amazon S3 console,\\nthen the job transitions\\nfrom Preparing to\\nSuspended . It remains in\\nthe Suspended state until\\nyou confirm that you want to\\nrun it.'], ['Suspended', 'The job requires confirmat\\nion, but you have not yet\\nconfirmed that you want to\\nrun it. Only jobs that you\\ncreate using the Amazon S3\\nconsole require confirmat\\nion. A job that is created\\nusing the console enters the\\nSuspended state immediate\\nly after Preparing . After\\nyou confirm that you want\\nto run the job and the job\\nbecomes Ready, it never\\nreturns to the Suspended\\nstate.', 'After you confirm that you\\nwant to run the job, its status\\nchanges to Ready.']]]\n",
      "[[['Status', 'Description', 'Transitions'], ['Ready', 'Amazon S3 is ready to begin\\nrunning the requested object\\noperations.', 'A job automatically moves\\nto Active when Amazon S3\\nbegins to run it. The amount\\nof time that a job remains in\\nthe Ready state depends on\\nwhether you have higher-pr\\niority jobs running already\\nand how long those jobs take\\nto complete.'], ['Active', 'Amazon S3 is performing the\\nrequested operation on the\\nobjects listed in the manifest.\\nWhile a job is Active, you\\ncan monitor its progress using\\nthe Amazon S3 console or the\\nDescribeJob operation\\nthrough the REST API, AWS\\nCLI, or AWS SDKs.', 'A job moves out of the\\nActive state when it is no\\nlonger running operations\\non objects. This can happen\\nautomatically, such as when\\na job completes successfully\\nor fails. Or it can occur as a\\nresult of user actions, such as\\ncanceling a job. The state that\\nthe job moves to depends on\\nthe reason for the transition.'], ['Pausing', 'The job is transitioning to\\nPaused from another state.', 'A job automatically moves to\\nPaused when the Pausing\\nstage is finished.'], ['Paused', 'A job can become Paused if\\nyou submit another job with\\na higher priority while the\\ncurrent job is running.', \"A Paused job automatic\\nally returns to Active after\\nany higher-priority jobs\\nthat are blocking the job's'\\nexecution complete, fail, or\\nare suspended.\"]]]\n",
      "[[['Status', 'Description', 'Transitions'], ['Complete', 'The job has finished\\nperforming the requested\\noperation on all objects in\\nthe manifest. The operation\\nmight have succeeded or\\nfailed for every object. If\\nyou configured the job to\\ngenerate a completion report,\\nthe report is available as soon\\nas the job is Complete.', 'Complete is a terminal\\nstate. Once a job reaches\\nComplete, it does not\\ntransition to any other state.'], ['Cancelling', 'The job is transitioning to the\\nCancelled state.', 'A job automatically moves\\nto Cancelled when the\\nCancelling stage is\\nfinished.'], ['Cancelled', 'You requested that the job\\nbe canceled, and S3 Batch\\nOperations has successfully\\ncancelled the job. The job will\\nnot submit any new requests\\nto Amazon S3.', 'Cancelled is a terminal\\nstate. After a job reaches\\nCancelled , it will not\\ntransition to any other state.'], ['Failing', 'The job is transitioning to the\\nFailed state.', 'A job automatically moves to\\nFailed once the Failing\\nstage is finished.'], ['Failed', 'The job has failed and is\\nno longer running. For\\nmore information about job\\nfailures, see Tracking job\\nfailure.', 'Failed is a terminal state.\\nAfter a job reaches Failed,\\nit will not transition to any\\nother state.']]]\n",
      "[[['', '', ''], ['', \"aws s3control update-job-status \\\\\\n--region us-west-2 \\\\\\n--account-id 181572960644 \\\\\\n--job-id 00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c \\\\\\n--requested-job-status 'Ready'\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3control update-job-status \\\\\\n--region us-west-2 \\\\\\n--account-id 181572960644 \\\\\\n--job-id 00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c \\\\\\n--status-update-reason \"No longer needed\" \\\\\\n--requested-job-status Cancelled', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;', '']]]\n",
      "[[['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.UpdateJobStatusRequest;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class UpdateJobStatus {\\npublic static void main(String[] args) {\\nString accountId = \"Account ID\";\\nString jobId = \"00e123a4-c0d8-41f4-a0eb-b46f9ba5b07c\";\\ntry {\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\ns3ControlClient.updateJobStatus(new UpdateJobStatusRequest()\\n.withAccountId(accountId)\\n.withJobId(jobId)\\n.withRequestedJobStatus(\"Ready\"));\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAmazon S3 Batch Operations generates both management and data events in CloudTrail\\nduring job execution. The volume of these events scale with the number of keys in each\\njob’s manifest. Refer to the CloudTrail pricing page for details, which includes examples\\nof how pricing changes depending on the number of trails you have configured in your\\naccount. To learn how to configure and log events to fit your needs, see Create your first\\ntrail in the AWS CloudTrail User Guide.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nS3 Batch Operations operate asynchronously and the tasks don't necessarily run in the\\norder that the objects are listed in the manifest. Therefore, you can't use the manifest\\nordering to determine which objects' tasks succeeded and which ones failed. Instead, you\\ncan examine the job's completion report (if you requested one) or view your AWS CloudTrail\\nevent logs to help determine the source of the failures.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAmazon EventBridge is the preferred way to manage your events. Amazon CloudWatch\\nEvents and EventBridge are the same underlying service and API, but EventBridge provides\\nmore features. Changes that you make in either CloudWatch or EventBridge appear in each\\nconsole. For more information, see the Amazon EventBridge User Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nOnly S3 Batch Operations job status-change events are recorded in CloudTrail.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"eventVersion\": \"1.05\",\\n\"userIdentity\": {\\n\"accountId\": \"123456789012\",\\n\"invokedBy\": \"s3.amazonaws.com\"\\n},\\n\"eventTime\": \"2020-02-05T18:25:30Z\",\\n\"eventSource\": \"s3.amazonaws.com\",\\n\"eventName\": \"JobStatusChanged\",\\n\"awsRegion\": \"us-west-2\",\\n\"sourceIPAddress\": \"s3.amazonaws.com\",\\n\"userAgent\": \"s3.amazonaws.com\",\\n\"requestParameters\": null,\\n\"responseElements\": null,\\n\"eventID\": \"f907577b-bf3d-4c53-b9ed-8a83a118a554\",\\n\"readOnly\": false,\\n\"eventType\": \"AwsServiceEvent\",\\n\"recipientAccountId\": \"123412341234\",\\n\"serviceEventDetails\": {\\n\"jobId\": \"d6e58ec4-897a-4b6d-975f-10d7f0fb63ce\",\\n\"jobArn\": \"arn:aws:s3:us-west-2:181572960644:job/\\nd6e58ec4-897a-4b6d-975f-10d7f0fb63ce\",\\n\"status\": \"Complete\",\\n\"jobEventId\": \"b268784cf0a66749f1a05bce259804f5\",\\n\"failureCodes\": [],', '']]]\n",
      "[[['', '\"statusChangeReason\": []\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"source\": [\\n\"aws.s3\"\\n],\\n\"detail-type\": [\\n\"AWS Service Event via CloudTrail\"\\n],\\n\"detail\": {\\n\"eventSource\": [\\n\"s3.amazonaws.com\"\\n],\\n\"eventName\": [\\n\"JobCreated\",\\n\"JobStatusChanged\"\\n]\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"version\": \"0\",\\n\"id\": \"51dc8145-541c-5518-2349-56d7dffdf2d8\",\\n\"detail-type\": \"AWS Service Event via CloudTrail\",\\n\"source\": \"aws.s3\",\\n\"account\": \"123456789012\",\\n\"time\": \"2020-02-27T15:25:49Z\",\\n\"region\": \"us-east-1\",\\n\"resources\": [],\\n\"detail\": {\\n\"eventVersion\": \"1.05\",\\n\"userIdentity\": {\\n\"accountId\": \"11112223334444\",\\n\"invokedBy\": \"s3.amazonaws.com\"\\n},\\n\"eventTime\": \"2020-02-27T15:25:49Z\",\\n\"eventSource\": \"s3.amazonaws.com\",\\n\"eventName\": \"JobCreated\",\\n\"awsRegion\": \"us-east-1\",\\n\"sourceIPAddress\": \"s3.amazonaws.com\",\\n\"userAgent\": \"s3.amazonaws.com\",\\n\"eventID\": \"7c38220f-f80b-4239-8b78-2ed867b7d3fa\",\\n\"readOnly\": false,\\n\"eventType\": \"AwsServiceEvent\",\\n\"serviceEventDetails\": {\\n\"jobId\": \"e849b567-5232-44be-9a0c-40988f14e80c\",\\n\"jobArn\": \"arn:aws:s3:us-east-1:181572960644:job/\\ne849b567-5232-44be-9a0c-40988f14e80c\",\\n\"status\": \"New\",\\n\"jobEventId\": \"f177ff24f1f097b69768e327038f30ac\",\\n\"failureCodes\": [],\\n\"statusChangeReason\": []\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"version\": \"0\",\\n\"id\": \"c8791abf-2af8-c754-0435-fd869ce25233\",\\n\"detail-type\": \"AWS Service Event via CloudTrail\",\\n\"source\": \"aws.s3\",\\n\"account\": \"123456789012\",', '']]]\n",
      "[[['', '\"time\": \"2020-02-27T15:26:42Z\",\\n\"region\": \"us-east-1\",\\n\"resources\": [],\\n\"detail\": {\\n\"eventVersion\": \"1.05\",\\n\"userIdentity\": {\\n\"accountId\": \"1111222233334444\",\\n\"invokedBy\": \"s3.amazonaws.com\"\\n},\\n\"eventTime\": \"2020-02-27T15:26:42Z\",\\n\"eventSource\": \"s3.amazonaws.com\",\\n\"eventName\": \"JobStatusChanged\",\\n\"awsRegion\": \"us-east-1\",\\n\"sourceIPAddress\": \"s3.amazonaws.com\",\\n\"userAgent\": \"s3.amazonaws.com\",\\n\"eventID\": \"0238c1f7-c2b0-440b-8dbd-1ed5e5833afb\",\\n\"readOnly\": false,\\n\"eventType\": \"AwsServiceEvent\",\\n\"serviceEventDetails\": {\\n\"jobId\": \"e849b567-5232-44be-9a0c-40988f14e80c\",\\n\"jobArn\": \"arn:aws:s3:us-east-1:181572960644:job/\\ne849b567-5232-44be-9a0c-40988f14e80c\",\\n\"status\": \"Complete\",\\n\"jobEventId\": \"51f5ac17dba408301d56cd1b2c8d1e9e\",\\n\"failureCodes\": [],\\n\"statusChangeReason\": []\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nCompletion reports are always encrypted with Amazon S3 managed keys (SSE-S3).', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Format\": \"Report_CSV_20180820\",\\n\"ReportCreationDate\": \"2019-04-05T17:48:39.725Z\",\\n\"Results\": [\\n{\\n\"TaskExecutionStatus\": \"succeeded\",\\n\"Bucket\": \"my-job-reports\",\\n\"MD5Checksum\": \"83b1c4cbe93fc893f54053697e10fd6e\",\\n\"Key\": \"job-f8fb9d89-a3aa-461d-bddc-ea6a1b131955/\\nresults/6217b0fab0de85c408b4be96aeaca9b195a7daa5.csv\"\\n},\\n{\\n\"TaskExecutionStatus\": \"failed\",\\n\"Bucket\": \"my-job-reports\",\\n\"MD5Checksum\": \"22ee037f3515975f7719699e5c416eaa\",\\n\"Key\": \"job-f8fb9d89-a3aa-461d-bddc-ea6a1b131955/results/\\nb2ddad417e94331e9f37b44f1faf8c7ed5873f2e.csv\"\\n}\\n],\\n\"ReportSchema\": \"Bucket, Key, VersionId, TaskStatus, ErrorCode, HTTPStatusCode,\\nResultMessage\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'awsexamplebucket1,image_14975,,failed,200,PermanentFailure,\"Lambda returned\\nfunction error: {\"\"errorMessage\"\":\"\"2019-04-05T17:35:21.155Z 2845ca0d-38d9-4c4b-\\nabcf-379dc749c452 Task timed out after 3.00 seconds\"\"}\"\\nawsexamplebucket1,image_15897,,failed,200,PermanentFailure,\"Lambda returned\\nfunction error: {\"\"errorMessage\"\":\"\"2019-04-05T17:35:29.610Z 2d0a330b-de9b-425f-\\nb511-29232fde5fe4 Task timed out after 3.00 seconds\"\"}\"\\nawsexamplebucket1,image_14819,,failed,200,PermanentFailure,\"Lambda returned function\\nerror: {\"\"errorMessage\"\":\"\"2019-04-05T17:35:22.362Z fcf5efde-74d4-4e6d-b37a-\\nc7f18827f551 Task timed out after 3.00 seconds\"\"}\"\\nawsexamplebucket1,image_15930,,failed,200,PermanentFailure,\"Lambda returned function\\nerror: {\"\"errorMessage\"\":\"\"2019-04-05T17:35:29.809Z 3dd5b57c-4a4a-48aa-8a35-\\ncbf027b7957e Task timed out after 3.00 seconds\"\"}\"\\nawsexamplebucket1,image_17644,,failed,200,PermanentFailure,\"Lambda\\nreturned function error: {\"\"errorMessage\"\":\"\"2019-04-05T17:35:46.025Z\\n10a764e4-2b26-4d8c-9056-1e1072b4723f Task timed out after 3.00 seconds\"\"}\"\\nawsexamplebucket1,image_17398,,failed,200,PermanentFailure,\"Lambda returned\\nfunction error: {\"\"errorMessage\"\":\"\"2019-04-05T17:35:44.661Z 1e306352-4c54-4eba-\\naee8-4d02f8c0235c Task timed out after 3.00 seconds\"\"}\"', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'awsexamplebucket1,image_17775,,succeeded,200,,\"{u\\'CopySourceVersionId\\':\\n\\'xVR78haVKlRnurYofbTfYr3ufYbktF8h\\', u\\'CopyObjectResult\\': {u\\'LastModified\\':\\ndatetime.datetime(2019, 4, 5, 17, 35, 39, tzinfo=tzlocal()), u\\'ETag\\':\\n\\'\"\"fe66f4390c50f29798f040d7aae72784\"\"\\'}, \\'ResponseMetadata\\': {\\'HTTPStatusCode\\':\\n200, \\'RetryAttempts\\': 0, \\'HostId\\': \\'nXNaClIMxEJzWNmeMNQV2KpjbaCJLn0OGoXWZpuVOFS/\\niQYWxb3QtTvzX9SVfx2lA3oTKLwImKw=\\', \\'RequestId\\': \\'3ED5852152014362\\', \\'HTTPHeaders\\':\\n{\\'content-length\\': \\'234\\', \\'x-amz-id-2\\': \\'nXNaClIMxEJzWNmeMNQV2KpjbaCJLn0OGoXWZpuVOFS/\\niQYWxb3QtTvzX9SVfx2lA3oTKLwImKw=\\', \\'x-amz-copy-source-version-id\\':\\n\\'xVR78haVKlRnurYofbTfYr3ufYbktF8h\\', \\'server\\': \\'AmazonS3\\', \\'x-amz-request-id\\':\\n\\'3ED5852152014362\\', \\'date\\': \\'Fri, 05 Apr 2019 17:35:39 GMT\\', \\'content-type\\':\\n\\'application/xml\\'}}}\"\\nawsexamplebucket1,image_17763,,succeeded,200,,\"{u\\'CopySourceVersionId\\':\\n\\'6HjOUSim4Wj6BTcbxToXW44pSZ.40pwq\\', u\\'CopyObjectResult\\': {u\\'LastModified\\':\\ndatetime.datetime(2019, 4, 5, 17, 35, 39, tzinfo=tzlocal()),\\nu\\'ETag\\': \\'\"\"fe66f4390c50f29798f040d7aae72784\"\"\\'}, \\'ResponseMetadata\\':\\n{\\'HTTPStatusCode\\': 200, \\'RetryAttempts\\': 0, \\'HostId\\': \\'GiCZNYr8LHd/\\nThyk6beTRP96IGZk2sYxujLe13TuuLpq6U2RD3we0YoluuIdm1PRvkMwnEW1aFc=\\', \\'RequestId\\':\\n\\'1BC9F5B1B95D7000\\', \\'HTTPHeaders\\': {\\'content-length\\': \\'234\\', \\'x-amz-id-2\\':\\n\\'GiCZNYr8LHd/Thyk6beTRP96IGZk2sYxujLe13TuuLpq6U2RD3we0YoluuIdm1PRvkMwnEW1aFc=\\', \\'x-\\namz-copy-source-version-id\\': \\'6HjOUSim4Wj6BTcbxToXW44pSZ.40pwq\\', \\'server\\': \\'AmazonS3\\',\\n\\'x-amz-request-id\\': \\'1BC9F5B1B95D7000\\', \\'date\\': \\'Fri, 05 Apr 2019 17:35:39 GMT\\',\\n\\'content-type\\': \\'application/xml\\'}}}\"\\nawsexamplebucket1,image_17860,,succeeded,200,,\"{u\\'CopySourceVersionId\\':\\n\\'m.MDD0g_QsUnYZ8TBzVFrp.TmjN8PJyX\\', u\\'CopyObjectResult\\': {u\\'LastModified\\':\\ndatetime.datetime(2019, 4, 5, 17, 35, 40, tzinfo=tzlocal()), u\\'ETag\\':\\n\\'\"\"fe66f4390c50f29798f040d7aae72784\"\"\\'}, \\'ResponseMetadata\\': {\\'HTTPStatusCode\\':\\n200, \\'RetryAttempts\\': 0, \\'HostId\\': \\'F9ooZOgpE5g9sNgBZxjdiPHqB4+0DNWgj3qbsir\\n+sKai4fv7rQEcF2fBN1VeeFc2WH45a9ygb2g=\\', \\'RequestId\\': \\'8D9CA56A56813DF3\\', \\'HTTPHeaders\\':\\n{\\'content-length\\': \\'234\\', \\'x-amz-id-2\\': \\'F9ooZOgpE5g9sNgBZxjdiPHqB4+0DNWgj3qbsir\\n+sKai4fv7rQEcF2fBN1VeeFc2WH45a9ygb2g=\\', \\'x-amz-copy-source-version-id\\':\\n\\'m.MDD0g_QsUnYZ8TBzVFrp.TmjN8PJyX\\', \\'server\\': \\'AmazonS3\\', \\'x-amz-request-id\\':\\n\\'8D9CA56A56813DF3\\', \\'date\\': \\'Fri, 05 Apr 2019 17:35:40 GMT\\', \\'content-type\\':\\n\\'application/xml\\'}}}\"', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nJob tags should not contain any confidential information or personal data.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf you send this request with an empty tag set, S3 Batch Operations deletes the\\nexisting tag set on the object. If you use this method, you are charged for a Tier 1\\nRequest (PUT). For more information, see Amazon S3 pricing.\\nTo delete existing tags for your Batch Operations job, the DeleteJobTagging action\\nis preferred because it achieves the same result without incurring charges.', ''], ['', '', '']], [['', '', ''], ['', 'read -d \\'\\' OPERATION <<EOF\\n{\\n\"S3PutObjectCopy\": {\\n\"TargetResource\": \"arn:aws:s3:::destination-bucket\"\\n}\\n}', '']]]\n",
      "[[['', 'EOF', ''], ['', '', '']], [['', '', ''], ['', 'read -d \\'\\' TAGS <<EOF\\n[\\n{\\n\"Key\": \"department\",\\n\"Value\": \"Marketing\"\\n},\\n{\\n\"Key\": \"FiscalYear\",\\n\"Value\": \"2020\"\\n}\\n]\\nEOF', ''], ['', '', '']], [['', '', ''], ['', 'read -d \\'\\' MANIFEST <<EOF\\n{\\n\"Spec\": {\\n\"Format\": \"EXAMPLE_S3BatchOperations_CSV_20180820\",\\n\"Fields\": [\\n\"Bucket\",\\n\"Key\"\\n]\\n},\\n\"Location\": {\\n\"ObjectArn\": \"arn:aws:s3:::example-bucket/example_manifest.csv\",\\n\"ETag\": \"example-5dc7a8bfb90808fc5d546218\"\\n}\\n}\\nEOF', ''], ['', '', '']], [['', '', ''], ['', 'read -d \\'\\' REPORT <<EOF\\n{\\n\"Bucket\": \"arn:aws:s3:::example-report-bucket\",\\n\"Format\": \"Example_Report_CSV_20180820\",\\n\"Enabled\": true,', '']]]\n",
      "[[['', '\"Prefix\": \"reports/copy-with-replace-metadata\",\\n\"ReportScope\": \"AllTasks\"\\n}\\nEOF', ''], ['', '', '']], [['', '', ''], ['', 'aws \\\\\\ns3control create-job \\\\\\n--account-id 123456789012 \\\\\\n--manifest \"${MANIFEST//$\\'\\\\n\\'}\" \\\\\\n--operation \"${OPERATION//$\\'\\\\n\\'/}\" \\\\\\n--report \"${REPORT//$\\'\\\\n\\'}\" \\\\\\n--priority 10 \\\\\\n--role-arn arn:aws:iam::123456789012:role/batch-operations-role \\\\\\n--tags \"${TAGS//$\\'\\\\n\\'/}\" \\\\\\n--client-request-token \"$(uuidgen)\" \\\\\\n--region us-west-2 \\\\\\n--description \"Copy with Replace Metadata\";', ''], ['', '', '']], [['', '', ''], ['', 'public String createJob(final AWSS3ControlClient awss3ControlClient) {\\nfinal String manifestObjectArn = \"arn:aws:s3:::example-manifest-bucket/\\nmanifests/10_manifest.csv\";\\nfinal String manifestObjectVersionId = \"example-5dc7a8bfb90808fc5d546218\";\\nfinal JobManifestLocation manifestLocation = new JobManifestLocation()\\n.withObjectArn(manifestObjectArn)\\n.withETag(manifestObjectVersionId);\\nfinal JobManifestSpec manifestSpec =\\nnew\\nJobManifestSpec().withFormat(JobManifestFormat.S3InventoryReport_CSV_20161130);\\nfinal JobManifest manifestToPublicApi = new JobManifest()\\n.withLocation(manifestLocation)', '']]]\n",
      "[[['', '.withSpec(manifestSpec);\\nfinal String jobReportBucketArn = \"arn:aws:s3:::example-report-bucket\";\\nfinal String jobReportPrefix = \"example-job-reports\";\\nfinal JobReport jobReport = new JobReport()\\n.withEnabled(true)\\n.withReportScope(JobReportScope.AllTasks)\\n.withBucket(jobReportBucketArn)\\n.withPrefix(jobReportPrefix)\\n.withFormat(JobReportFormat.Report_CSV_20180820);\\nfinal String lambdaFunctionArn = \"arn:aws:lambda:us-\\nwest-2:123456789012:function:example-function\";\\nfinal JobOperation jobOperation = new JobOperation()\\n.withLambdaInvoke(new\\nLambdaInvokeOperation().withFunctionArn(lambdaFunctionArn));\\nfinal S3Tag departmentTag = new\\nS3Tag().withKey(\"department\").withValue(\"Marketing\");\\nfinal S3Tag fiscalYearTag = new S3Tag().withKey(\"FiscalYear\").withValue(\"2020\");\\nfinal String roleArn = \"arn:aws:iam::123456789012:role/example-batch-operations-\\nrole\";\\nfinal Boolean requiresConfirmation = true;\\nfinal int priority = 10;\\nfinal CreateJobRequest request = new CreateJobRequest()\\n.withAccountId(\"123456789012\")\\n.withDescription(\"Test lambda job\")\\n.withManifest(manifestToPublicApi)\\n.withOperation(jobOperation)\\n.withPriority(priority)\\n.withRoleArn(roleArn)\\n.withReport(jobReport)\\n.withTags(departmentTag, fiscalYearTag)\\n.withConfirmationRequired(requiresConfirmation);\\nfinal CreateJobResult result = awss3ControlClient.createJob(request);\\nreturn result.getJobId();\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws \\\\\\ns3control delete-job-tagging \\\\\\n--account-id 123456789012 \\\\\\n--job-id Example-e25a-4ed2-8bee-7f8ed7fc2f1c \\\\\\n--region us-east-1;', ''], ['', '', '']], [['', '', ''], ['', 'public void deleteJobTagging(final AWSS3ControlClient awss3ControlClient,\\nfinal String jobId) {\\nfinal DeleteJobTaggingRequest deleteJobTaggingRequest = new\\nDeleteJobTaggingRequest()\\n.withJobId(jobId);\\nfinal DeleteJobTaggingResult deleteJobTaggingResult =\\nawss3ControlClient.deleteJobTagging(deleteJobTaggingRequest);\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf you send this request with an empty tag set, S3 Batch Operations deletes the existing tag\\nset on the object. Also, if you use this method, you are charged for a Tier 1 Request (PUT).\\nFor more information, see Amazon S3 pricing.\\nTo delete existing tags for your Batch Operations job, the DeleteJobTagging action is\\npreferred because it achieves the same result without incurring charges.', ''], ['', '', '']], [['', '', ''], ['', 'read -d \\'\\' TAGS <<EOF\\n[\\n{\\n\"Key\": \"department\",\\n\"Value\": \"Marketing\"\\n},\\n{\\n\"Key\": \"FiscalYear\",\\n\"Value\": \"2020\"\\n}\\n]\\nEOF', ''], ['', '', '']], [['', '', ''], ['', 'aws \\\\\\ns3control put-job-tagging \\\\\\n--account-id 123456789012 \\\\\\n--tags \"${TAGS//$\\'\\\\n\\'/}\" \\\\\\n--job-id Example-e25a-4ed2-8bee-7f8ed7fc2f1c \\\\\\n--region us-east-1;', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public void putJobTagging(final AWSS3ControlClient awss3ControlClient,\\nfinal String jobId) {\\nfinal S3Tag departmentTag = new\\nS3Tag().withKey(\"department\").withValue(\"Marketing\");\\nfinal S3Tag fiscalYearTag = new S3Tag().withKey(\"FiscalYear\").withValue(\"2020\");\\nfinal PutJobTaggingRequest putJobTaggingRequest = new PutJobTaggingRequest()\\n.withJobId(jobId)\\n.withTags(departmentTag, fiscalYearTag);\\nfinal PutJobTaggingResult putJobTaggingResult =\\nawss3ControlClient.putJobTagging(putJobTaggingRequest);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws \\\\\\ns3control get-job-tagging \\\\\\n--account-id 123456789012 \\\\\\n--job-id Example-e25a-4ed2-8bee-7f8ed7fc2f1c \\\\\\n--region us-east-1;', ''], ['', '', '']], [['', '', ''], ['', 'public List<S3Tag> getJobTagging(final AWSS3ControlClient awss3ControlClient,\\nfinal String jobId) {\\nfinal GetJobTaggingRequest getJobTaggingRequest = new GetJobTaggingRequest()\\n.withJobId(jobId);\\nfinal GetJobTaggingResult getJobTaggingResult =\\nawss3ControlClient.getJobTagging(getJobTaggingRequest);', '']]]\n",
      "[[['', 'final List<S3Tag> tags = getJobTaggingResult.getTags();\\nreturn tags;\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nJob tag keys and values are case sensitive.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:CreateJob\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:RequestTag/department\": \"${aws:PrincipalTag/\\ndepartment}\"\\n}\\n}\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:UpdateJobPriority\",\\n\"s3:UpdateJobStatus\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:ResourceTag/department\": \"${aws:PrincipalTag/\\ndepartment}\"\\n}\\n}\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:PutJobTagging\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:RequestTag/department\": \"${aws:PrincipalTag/\\ndepartment}\",\\n\"aws:ResourceTag/department\": \"${aws:PrincipalTag/\\ndepartment}\"', '']]]\n",
      "[[['', '}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:CreateJob\",\\n\"Resource\": \"*\",', '']]]\n",
      "[[['', '\"Condition\": {\\n\"StringEquals\": {\\n\"aws:RequestTag/department\": \"${aws:PrincipalTag/department}\",\\n\"aws:RequestTag/stage\": \"QA\"\\n},\\n\"NumericLessThanEquals\": {\\n\"s3:RequestJobPriority\": 100\\n}\\n}\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:UpdateJobStatus\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:ResourceTag/department\": \"${aws:PrincipalTag/department}\"\\n}\\n}\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:UpdateJobPriority\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:ResourceTag/department\": \"${aws:PrincipalTag/department}\",\\n\"aws:ResourceTag/stage\": \"QA\"\\n},\\n\"NumericLessThanEquals\": {\\n\"s3:RequestJobPriority\": 100\\n}\\n}\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:PutJobTagging\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:RequestTag/department\" : \"${aws:PrincipalTag/department}\",\\n\"aws:ResourceTag/department\": \"${aws:PrincipalTag/department}\",', '']]]\n",
      "[[['', '\"aws:RequestTag/stage\": \"QA\",\\n\"aws:ResourceTag/stage\": \"QA\"\\n}\\n}\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:GetJobTagging\",\\n\"Resource\": \"*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:CreateJob\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"ForAnyValue:StringEquals\": {\\n\"aws:RequestTag/stage\": [\\n\"QA\",\\n\"Production\"\\n]\\n},\\n\"StringEquals\": {', '']]]\n",
      "[[['', '\"aws:RequestTag/department\": \"${aws:PrincipalTag/\\ndepartment}\"\\n},\\n\"NumericLessThanEquals\": {\\n\"s3:RequestJobPriority\": 300\\n}\\n}\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:UpdateJobStatus\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:ResourceTag/department\": \"${aws:PrincipalTag/\\ndepartment}\"\\n}\\n}\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:UpdateJobPriority\",\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"ForAnyValue:StringEquals\": {\\n\"aws:ResourceTag/stage\": [\\n\"QA\",\\n\"Production\"\\n]\\n},\\n\"StringEquals\": {\\n\"aws:ResourceTag/department\": \"${aws:PrincipalTag/\\ndepartment}\"\\n},\\n\"NumericLessThanEquals\": {\\n\"s3:RequestJobPriority\": 300\\n}\\n}\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:PutJobTagging\",', '']]]\n",
      "[[['', '\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:RequestTag/department\": \"${aws:PrincipalTag/\\ndepartment}\",\\n\"aws:ResourceTag/department\": \"${aws:PrincipalTag/\\ndepartment}\"\\n},\\n\"ForAnyValue:StringEquals\": {\\n\"aws:RequestTag/stage\": [\\n\"QA\",\\n\"Production\"\\n],\\n\"aws:ResourceTag/stage\": [\\n\"QA\",\\n\"Production\"\\n]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'export AWS_PROFILE=\\'aws-user\\'\\nread -d \\'\\' bops_trust_policy <<EOF\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": [\\n\"batchoperations.s3.amazonaws.com\"\\n]\\n},\\n\"Action\": \"sts:AssumeRole\"\\n}\\n]\\n}\\nEOF\\naws iam create-role --role-name bops-objectlock --assume-role-policy-document\\n\"${bops_trust_policy}\"', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'read -d \\'\\' bops_permissions <<EOF\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3:GetBucketObjectLockConfiguration\",\\n\"Resource\": [\\n\"arn:aws:s3:::{{ManifestBucket}}\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObject\",\\n\"s3:GetObjectVersion\",\\n\"s3:GetBucketLocation\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::{{ManifestBucket}}/*\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObject\",\\n\"s3:GetBucketLocation\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::{{ReportBucket}}/*\"\\n]\\n}\\n]\\n}', '']]]\n",
      "[[['', 'EOF\\naws iam put-role-policy --role-name bops-objectlock --policy-name object-lock-\\npermissions --policy-document \"${bops_permissions}\"', ''], ['', '', '']], [['', '', ''], ['', 'public void createObjectLockRole() {\\nfinal String roleName = \"bops-object-lock\";\\nfinal String trustPolicy = \"{\" +\\n\" \\\\\"Version\\\\\": \\\\\"2012-10-17\\\\\", \" +\\n\" \\\\\"Statement\\\\\": [ \" +\\n\" { \" +\\n\" \\\\\"Effect\\\\\": \\\\\"Allow\\\\\", \" +\\n\" \\\\\"Principal\\\\\": { \" +\\n\" \\\\\"Service\\\\\": [\" +\\n\" \\\\\"batchoperations.s3.amazonaws.com\\\\\"\" +\\n\" ]\" +\\n\" }, \" +', '']]]\n",
      "[[['', '\" \\\\\"Action\\\\\": \\\\\"sts:AssumeRole\\\\\" \" +\\n\" } \" +\\n\" ]\" +\\n\"}\";\\nfinal String bopsPermissions = \"{\" +\\n\" \\\\\"Version\\\\\": \\\\\"2012-10-17\\\\\",\" +\\n\" \\\\\"Statement\\\\\": [\" +\\n\" {\" +\\n\" \\\\\"Effect\\\\\": \\\\\"Allow\\\\\",\" +\\n\" \\\\\"Action\\\\\": \\\\\"s3:GetBucketObjectLockConfiguration\\\\\",\" +\\n\" \\\\\"Resource\\\\\": [\" +\\n\" \\\\\"arn:aws:s3:::ManifestBucket\\\\\"\" +\\n\" ]\" +\\n\" },\" +\\n\" {\" +\\n\" \\\\\"Effect\\\\\": \\\\\"Allow\\\\\",\" +\\n\" \\\\\"Action\\\\\": [\" +\\n\" \\\\\"s3:GetObject\\\\\",\" +\\n\" \\\\\"s3:GetObjectVersion\\\\\",\" +\\n\" \\\\\"s3:GetBucketLocation\\\\\"\" +\\n\" ],\" +\\n\" \\\\\"Resource\\\\\": [\" +\\n\" \\\\\"arn:aws:s3:::ManifestBucket/*\\\\\"\" +\\n\" ]\" +\\n\" },\" +\\n\" {\" +\\n\" \\\\\"Effect\\\\\": \\\\\"Allow\\\\\",\" +\\n\" \\\\\"Action\\\\\": [\" +\\n\" \\\\\"s3:PutObject\\\\\",\" +\\n\" \\\\\"s3:GetBucketLocation\\\\\"\" +\\n\" ],\" +\\n\" \\\\\"Resource\\\\\": [\" +\\n\" \\\\\"arn:aws:s3:::ReportBucket/*\\\\\"\" +\\n\" ]\" +\\n\" }\" +\\n\" ]\" +\\n\"}\";\\nfinal AmazonIdentityManagement iam =\\nAmazonIdentityManagementClientBuilder.defaultClient();\\nfinal CreateRoleRequest createRoleRequest = new CreateRoleRequest()\\n.withAssumeRolePolicyDocument(bopsPermissions)', '']]]\n",
      "[[['', '.withRoleName(roleName);\\nfinal CreateRoleResult createRoleResult = iam.createRole(createRoleRequest);\\nfinal PutRolePolicyRequest putRolePolicyRequest = new PutRolePolicyRequest()\\n.withPolicyDocument(bopsPermissions)\\n.withPolicyName(\"bops-permissions\")\\n.withRoleName(roleName);\\nfinal PutRolePolicyResult putRolePolicyResult =\\niam.putRolePolicy(putRolePolicyRequest);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'export AWS_PROFILE=\\'aws-user\\'\\nread -d \\'\\' retention_permissions <<EOF\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObjectRetention\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::{{ManifestBucket}}/*\"\\n]\\n}\\n]\\n}\\nEOF', '']]]\n",
      "[[['', 'aws iam put-role-policy --role-name bops-objectlock --policy-name retention-permissions\\n--policy-document \"${retention_permissions}\"', ''], ['', '', '']], [['', '', ''], ['', 'public void allowPutObjectRetention() {\\nfinal String roleName = \"bops-object-lock\";\\nfinal String retentionPermissions = \"{\" +\\n\" \\\\\"Version\\\\\": \\\\\"2012-10-17\\\\\",\" +\\n\" \\\\\"Statement\\\\\": [\" +\\n\" {\" +\\n\" \\\\\"Effect\\\\\": \\\\\"Allow\\\\\",\" +\\n\" \\\\\"Action\\\\\": [\" +\\n\" \\\\\"s3:PutObjectRetention\\\\\"\" +\\n\" ],\" +\\n\" \\\\\"Resource\\\\\": [\" +\\n\" \\\\\"arn:aws:s3:::ManifestBucket*\\\\\"\" +\\n\" ]\" +\\n\" }\" +\\n\" ]\" +\\n\"}\";\\nfinal AmazonIdentityManagement iam =\\nAmazonIdentityManagementClientBuilder.defaultClient();\\nfinal PutRolePolicyRequest putRolePolicyRequest = new PutRolePolicyRequest()\\n.withPolicyDocument(retentionPermissions)\\n.withPolicyName(\"retention-permissions\")\\n.withRoleName(roleName);\\nfinal PutRolePolicyResult putRolePolicyResult =\\niam.putRolePolicy(putRolePolicyRequest);\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'export AWS_PROFILE=\\'aws-user\\'\\nexport AWS_DEFAULT_REGION=\\'us-west-2\\'\\nexport ACCOUNT_ID=123456789012\\nexport ROLE_ARN=\\'arn:aws:iam::123456789012:role/bops-objectlock\\'\\nread -d \\'\\' OPERATION <<EOF\\n{\\n\"S3PutObjectRetention\": {\\n\"Retention\": {\\n\"RetainUntilDate\":\"2025-01-01T00:00:00\",\\n\"Mode\":\"COMPLIANCE\"\\n}\\n}\\n}\\nEOF\\nread -d \\'\\' MANIFEST <<EOF\\n{\\n\"Spec\": {\\n\"Format\": \"S3BatchOperations_CSV_20180820\",\\n\"Fields\": [\\n\"Bucket\",\\n\"Key\"\\n]\\n},\\n\"Location\": {\\n\"ObjectArn\": \"arn:aws:s3:::ManifestBucket/compliance-objects-manifest.csv\",\\n\"ETag\": \"Your-manifest-ETag\"\\n}\\n}\\nEOF\\nread -d \\'\\' REPORT <<EOF\\n{\\n\"Bucket\": \"arn:aws:s3:::ReportBucket\",\\n\"Format\": \"Report_CSV_20180820\",\\n\"Enabled\": true,\\n\"Prefix\": \"reports/compliance-objects-bops\",\\n\"ReportScope\": \"AllTasks\"\\n}', '']]]\n",
      "[[['', 'EOF\\naws \\\\\\ns3control create-job \\\\\\n--account-id \"${ACCOUNT_ID}\" \\\\\\n--manifest \"${MANIFEST//$\\'\\\\n\\'}\" \\\\\\n--operation \"${OPERATION//$\\'\\\\n\\'/}\" \\\\\\n--report \"${REPORT//$\\'\\\\n\\'}\" \\\\\\n--priority 10 \\\\\\n--role-arn \"${ROLE_ARN}\" \\\\\\n--client-request-token \"$(uuidgen)\" \\\\\\n--region \"${AWS_DEFAULT_REGION}\" \\\\\\n--description \"Set compliance retain-until to 1 Jul 2030\";', ''], ['', '', '']], [['', '', ''], ['', 'export AWS_PROFILE=\\'aws-user\\'\\nexport AWS_DEFAULT_REGION=\\'us-west-2\\'\\nexport ACCOUNT_ID=123456789012\\nexport ROLE_ARN=\\'arn:aws:iam::123456789012:role/bops-objectlock\\'\\nread -d \\'\\' OPERATION <<EOF\\n{\\n\"S3PutObjectRetention\": {\\n\"Retention\": {\\n\"RetainUntilDate\":\"2025-01-15T00:00:00\",\\n\"Mode\":\"COMPLIANCE\"\\n}\\n}\\n}\\nEOF\\nread -d \\'\\' MANIFEST <<EOF\\n{\\n\"Spec\": {\\n\"Format\": \"S3BatchOperations_CSV_20180820\",\\n\"Fields\": [\\n\"Bucket\",\\n\"Key\"\\n]', '']]]\n",
      "[[['', '},\\n\"Location\": {\\n\"ObjectArn\": \"arn:aws:s3:::ManifestBucket/compliance-objects-manifest.csv\",\\n\"ETag\": \"Your-manifest-ETag\"\\n}\\n}\\nEOF\\nread -d \\'\\' REPORT <<EOF\\n{\\n\"Bucket\": \"arn:aws:s3:::ReportBucket\",\\n\"Format\": \"Report_CSV_20180820\",\\n\"Enabled\": true,\\n\"Prefix\": \"reports/compliance-objects-bops\",\\n\"ReportScope\": \"AllTasks\"\\n}\\nEOF\\naws \\\\\\ns3control create-job \\\\\\n--account-id \"${ACCOUNT_ID}\" \\\\\\n--manifest \"${MANIFEST//$\\'\\\\n\\'}\" \\\\\\n--operation \"${OPERATION//$\\'\\\\n\\'/}\" \\\\\\n--report \"${REPORT//$\\'\\\\n\\'}\" \\\\\\n--priority 10 \\\\\\n--role-arn \"${ROLE_ARN}\" \\\\\\n--client-request-token \"$(uuidgen)\" \\\\\\n--region \"${AWS_DEFAULT_REGION}\" \\\\\\n--description \"Extend compliance retention to 15 Jan 2025\";', ''], ['', '', '']], [['', '', ''], ['', 'public String createComplianceRetentionJob(final AWSS3ControlClient awss3ControlClient)\\nthrows ParseException {\\nfinal String manifestObjectArn = \"arn:aws:s3:::ManifestBucket/compliance-objects-\\nmanifest.csv\";\\nfinal String manifestObjectVersionId = \"your-object-version-Id\";\\nfinal JobManifestLocation manifestLocation = new JobManifestLocation()\\n.withObjectArn(manifestObjectArn)\\n.withETag(manifestObjectVersionId);', '']]]\n",
      "[[['', 'final JobManifestSpec manifestSpec =\\nnew JobManifestSpec()\\n.withFormat(JobManifestFormat.S3BatchOperations_CSV_20180820)\\n.withFields(\"Bucket\", \"Key\");\\nfinal JobManifest manifestToPublicApi = new JobManifest()\\n.withLocation(manifestLocation)\\n.withSpec(manifestSpec);\\nfinal String jobReportBucketArn = \"arn:aws:s3:::ReportBucket\";\\nfinal String jobReportPrefix = \"reports/compliance-objects-bops\";\\nfinal JobReport jobReport = new JobReport()\\n.withEnabled(true)\\n.withReportScope(JobReportScope.AllTasks)\\n.withBucket(jobReportBucketArn)\\n.withPrefix(jobReportPrefix)\\n.withFormat(JobReportFormat.Report_CSV_20180820);\\nfinal SimpleDateFormat format = new SimpleDateFormat(\"dd/MM/yyyy\");\\nfinal Date janFirst = format.parse(\"01/01/2025\");\\nfinal JobOperation jobOperation = new JobOperation()\\n.withS3PutObjectRetention(new S3SetObjectRetentionOperation()\\n.withRetention(new S3Retention()\\n.withMode(S3ObjectLockRetentionMode.COMPLIANCE)\\n.withRetainUntilDate(janFirst)));\\nfinal String roleArn = \"arn:aws:iam::123456789012:role/bops-object-lock\";\\nfinal Boolean requiresConfirmation = true;\\nfinal int priority = 10;\\nfinal CreateJobRequest request = new CreateJobRequest()\\n.withAccountId(\"123456789012\")\\n.withDescription(\"Set compliance retain-until to 1 Jan 2025\")\\n.withManifest(manifestToPublicApi)\\n.withOperation(jobOperation)\\n.withPriority(priority)\\n.withRoleArn(roleArn)\\n.withReport(jobReport)\\n.withConfirmationRequired(requiresConfirmation);\\nfinal CreateJobResult result = awss3ControlClient.createJob(request);', '']]]\n",
      "[[['', 'return result.getJobId();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public String createExtendComplianceRetentionJob(final AWSS3ControlClient\\nawss3ControlClient) throws ParseException {\\nfinal String manifestObjectArn = \"arn:aws:s3:::ManifestBucket/compliance-objects-\\nmanifest.csv\";\\nfinal String manifestObjectVersionId = \"15ad5ba069e6bbc465c77bf83d541385\";\\nfinal JobManifestLocation manifestLocation = new JobManifestLocation()\\n.withObjectArn(manifestObjectArn)\\n.withETag(manifestObjectVersionId);\\nfinal JobManifestSpec manifestSpec =\\nnew JobManifestSpec()\\n.withFormat(JobManifestFormat.S3BatchOperations_CSV_20180820)\\n.withFields(\"Bucket\", \"Key\");\\nfinal JobManifest manifestToPublicApi = new JobManifest()\\n.withLocation(manifestLocation)\\n.withSpec(manifestSpec);\\nfinal String jobReportBucketArn = \"arn:aws:s3:::ReportBucket\";\\nfinal String jobReportPrefix = \"reports/compliance-objects-bops\";\\nfinal JobReport jobReport = new JobReport()\\n.withEnabled(true)\\n.withReportScope(JobReportScope.AllTasks)\\n.withBucket(jobReportBucketArn)\\n.withPrefix(jobReportPrefix)\\n.withFormat(JobReportFormat.Report_CSV_20180820);\\nfinal SimpleDateFormat format = new SimpleDateFormat(\"dd/MM/yyyy\");\\nfinal Date jan15th = format.parse(\"15/01/2025\");\\nfinal JobOperation jobOperation = new JobOperation()\\n.withS3PutObjectRetention(new S3SetObjectRetentionOperation()\\n.withRetention(new S3Retention()', '']]]\n",
      "[[['', '.withMode(S3ObjectLockRetentionMode.COMPLIANCE)\\n.withRetainUntilDate(jan15th)));\\nfinal String roleArn = \"arn:aws:iam::123456789012:role/bops-object-lock\";\\nfinal Boolean requiresConfirmation = true;\\nfinal int priority = 10;\\nfinal CreateJobRequest request = new CreateJobRequest()\\n.withAccountId(\"123456789012\")\\n.withDescription(\"Extend compliance retention to 15 Jan 2025\")\\n.withManifest(manifestToPublicApi)\\n.withOperation(jobOperation)\\n.withPriority(priority)\\n.withRoleArn(roleArn)\\n.withReport(jobReport)\\n.withConfirmationRequired(requiresConfirmation);\\nfinal CreateJobResult result = awss3ControlClient.createJob(request);\\nreturn result.getJobId();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'export AWS_PROFILE=\\'aws-user\\'\\nexport AWS_DEFAULT_REGION=\\'us-west-2\\'\\nexport ACCOUNT_ID=123456789012\\nexport ROLE_ARN=\\'arn:aws:iam::123456789012:role/bops-objectlock\\'\\nread -d \\'\\' OPERATION <<EOF\\n{\\n\"S3PutObjectRetention\": {', '']]]\n",
      "[[['', '\"Retention\": {\\n\"RetainUntilDate\":\"2025-01-30T00:00:00\",\\n\"Mode\":\"GOVERNANCE\"\\n}\\n}\\n}\\nEOF\\nread -d \\'\\' MANIFEST <<EOF\\n{\\n\"Spec\": {\\n\"Format\": \"S3BatchOperations_CSV_20180820\",\\n\"Fields\": [\\n\"Bucket\",\\n\"Key\"\\n]\\n},\\n\"Location\": {\\n\"ObjectArn\": \"arn:aws:s3:::ManifestBucket/governance-objects-manifest.csv\",\\n\"ETag\": \"Your-manifest-ETag\"\\n}\\n}\\nEOF\\nread -d \\'\\' REPORT <<EOF\\n{\\n\"Bucket\": \"arn:aws:s3:::ReportBucketT\",\\n\"Format\": \"Report_CSV_20180820\",\\n\"Enabled\": true,\\n\"Prefix\": \"reports/governance-objects\",\\n\"ReportScope\": \"AllTasks\"\\n}\\nEOF\\naws \\\\\\ns3control create-job \\\\\\n--account-id \"${ACCOUNT_ID}\" \\\\\\n--manifest \"${MANIFEST//$\\'\\\\n\\'}\" \\\\\\n--operation \"${OPERATION//$\\'\\\\n\\'/}\" \\\\\\n--report \"${REPORT//$\\'\\\\n\\'}\" \\\\\\n--priority 10 \\\\\\n--role-arn \"${ROLE_ARN}\" \\\\\\n--client-request-token \"$(uuidgen)\" \\\\\\n--region \"${AWS_DEFAULT_REGION}\" \\\\', '']]]\n",
      "[[['', '--description \"Put governance retention\";', ''], ['', '', '']], [['', '', ''], ['', 'export AWS_PROFILE=\\'aws-user\\'\\nread -d \\'\\' bypass_governance_permissions <<EOF\\n{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:BypassGovernanceRetention\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ManifestBucket/*\"\\n]\\n}\\n]\\n}\\nEOF\\naws iam put-role-policy --role-name bops-objectlock --policy-name bypass-governance-\\npermissions --policy-document \"${bypass_governance_permissions}\"\\nexport AWS_PROFILE=\\'aws-user\\'\\nexport AWS_DEFAULT_REGION=\\'us-west-2\\'\\nexport ACCOUNT_ID=123456789012\\nexport ROLE_ARN=\\'arn:aws:iam::123456789012:role/bops-objectlock\\'\\nread -d \\'\\' OPERATION <<EOF\\n{\\n\"S3PutObjectRetention\": {\\n\"BypassGovernanceRetention\": true,\\n\"Retention\": {\\n}\\n}', '']]]\n",
      "[[['', '}\\nEOF\\nread -d \\'\\' MANIFEST <<EOF\\n{\\n\"Spec\": {\\n\"Format\": \"S3BatchOperations_CSV_20180820\",\\n\"Fields\": [\\n\"Bucket\",\\n\"Key\"\\n]\\n},\\n\"Location\": {\\n\"ObjectArn\": \"arn:aws:s3:::ManifestBucket/governance-objects-manifest.csv\",\\n\"ETag\": \"Your-manifest-ETag\"\\n}\\n}\\nEOF\\nread -d \\'\\' REPORT <<EOF\\n{\\n\"Bucket\": \"arn:aws:s3:::REPORT_BUCKET\",\\n\"Format\": \"Report_CSV_20180820\",\\n\"Enabled\": true,\\n\"Prefix\": \"reports/bops-governance\",\\n\"ReportScope\": \"AllTasks\"\\n}\\nEOF\\naws \\\\\\ns3control create-job \\\\\\n--account-id \"${ACCOUNT_ID}\" \\\\\\n--manifest \"${MANIFEST//$\\'\\\\n\\'}\" \\\\\\n--operation \"${OPERATION//$\\'\\\\n\\'/}\" \\\\\\n--report \"${REPORT//$\\'\\\\n\\'}\" \\\\\\n--priority 10 \\\\\\n--role-arn \"${ROLE_ARN}\" \\\\\\n--client-request-token \"$(uuidgen)\" \\\\\\n--region \"${AWS_DEFAULT_REGION}\" \\\\\\n--description \"Remove governance retention\";', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public String createGovernanceRetentionJob(final AWSS3ControlClient awss3ControlClient)\\nthrows ParseException {\\nfinal String manifestObjectArn = \"arn:aws:s3:::ManifestBucket/governance-objects-\\nmanifest.csv\";\\nfinal String manifestObjectVersionId = \"15ad5ba069e6bbc465c77bf83d541385\";\\nfinal JobManifestLocation manifestLocation = new JobManifestLocation()\\n.withObjectArn(manifestObjectArn)\\n.withETag(manifestObjectVersionId);\\nfinal JobManifestSpec manifestSpec =\\nnew JobManifestSpec()\\n.withFormat(JobManifestFormat.S3BatchOperations_CSV_20180820)\\n.withFields(\"Bucket\", \"Key\");\\nfinal JobManifest manifestToPublicApi = new JobManifest()\\n.withLocation(manifestLocation)\\n.withSpec(manifestSpec);\\nfinal String jobReportBucketArn = \"arn:aws:s3:::ReportBucket\";\\nfinal String jobReportPrefix = \"reports/governance-objects\";\\nfinal JobReport jobReport = new JobReport()\\n.withEnabled(true)\\n.withReportScope(JobReportScope.AllTasks)\\n.withBucket(jobReportBucketArn)\\n.withPrefix(jobReportPrefix)\\n.withFormat(JobReportFormat.Report_CSV_20180820);\\nfinal SimpleDateFormat format = new SimpleDateFormat(\"dd/MM/yyyy\");\\nfinal Date jan30th = format.parse(\"30/01/2025\");', '']]]\n",
      "[[['', 'final JobOperation jobOperation = new JobOperation()\\n.withS3PutObjectRetention(new S3SetObjectRetentionOperation()\\n.withRetention(new S3Retention()\\n.withMode(S3ObjectLockRetentionMode.GOVERNANCE)\\n.withRetainUntilDate(jan30th)));\\nfinal String roleArn = \"arn:aws:iam::123456789012:role/bops-object-lock\";\\nfinal Boolean requiresConfirmation = true;\\nfinal int priority = 10;\\nfinal CreateJobRequest request = new CreateJobRequest()\\n.withAccountId(\"123456789012\")\\n.withDescription(\"Put governance retention\")\\n.withManifest(manifestToPublicApi)\\n.withOperation(jobOperation)\\n.withPriority(priority)\\n.withRoleArn(roleArn)\\n.withReport(jobReport)\\n.withConfirmationRequired(requiresConfirmation);\\nfinal CreateJobResult result = awss3ControlClient.createJob(request);\\nreturn result.getJobId();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public void allowBypassGovernance() {\\nfinal String roleName = \"bops-object-lock\";\\nfinal String bypassGovernancePermissions = \"{\" +\\n\" \\\\\"Version\\\\\": \\\\\"2012-10-17\\\\\",\" +\\n\" \\\\\"Statement\\\\\": [\" +\\n\" {\" +\\n\" \\\\\"Effect\\\\\": \\\\\"Allow\\\\\",\" +\\n\" \\\\\"Action\\\\\": [\" +\\n\" \\\\\"s3:BypassGovernanceRetention\\\\\"\" +\\n\" ],\" +', '']]]\n",
      "[[['', '\" \\\\\"Resource\\\\\": [\" +\\n\" \\\\\"arn:aws:s3:::ManifestBucket/*\\\\\"\" +\\n\" ]\" +\\n\" }\" +\\n\" ]\" +\\n\"}\";\\nfinal AmazonIdentityManagement iam =\\nAmazonIdentityManagementClientBuilder.defaultClient();\\nfinal PutRolePolicyRequest putRolePolicyRequest = new PutRolePolicyRequest()\\n.withPolicyDocument(bypassGovernancePermissions)\\n.withPolicyName(\"bypass-governance-permissions\")\\n.withRoleName(roleName);\\nfinal PutRolePolicyResult putRolePolicyResult =\\niam.putRolePolicy(putRolePolicyRequest);\\n}\\npublic String createRemoveGovernanceRetentionJob(final AWSS3ControlClient\\nawss3ControlClient) {\\nfinal String manifestObjectArn = \"arn:aws:s3:::ManifestBucket/governance-objects-\\nmanifest.csv\";\\nfinal String manifestObjectVersionId = \"15ad5ba069e6bbc465c77bf83d541385\";\\nfinal JobManifestLocation manifestLocation = new JobManifestLocation()\\n.withObjectArn(manifestObjectArn)\\n.withETag(manifestObjectVersionId);\\nfinal JobManifestSpec manifestSpec =\\nnew JobManifestSpec()\\n.withFormat(JobManifestFormat.S3BatchOperations_CSV_20180820)\\n.withFields(\"Bucket\", \"Key\");\\nfinal JobManifest manifestToPublicApi = new JobManifest()\\n.withLocation(manifestLocation)\\n.withSpec(manifestSpec);\\nfinal String jobReportBucketArn = \"arn:aws:s3:::ReportBucket\";\\nfinal String jobReportPrefix = \"reports/bops-governance\";\\nfinal JobReport jobReport = new JobReport()\\n.withEnabled(true)\\n.withReportScope(JobReportScope.AllTasks)\\n.withBucket(jobReportBucketArn)', '']]]\n",
      "[[['', '.withPrefix(jobReportPrefix)\\n.withFormat(JobReportFormat.Report_CSV_20180820);\\nfinal JobOperation jobOperation = new JobOperation()\\n.withS3PutObjectRetention(new S3SetObjectRetentionOperation()\\n.withRetention(new S3Retention()));\\nfinal String roleArn = \"arn:aws:iam::123456789012:role/bops-object-lock\";\\nfinal Boolean requiresConfirmation = true;\\nfinal int priority = 10;\\nfinal CreateJobRequest request = new CreateJobRequest()\\n.withAccountId(\"123456789012\")\\n.withDescription(\"Remove governance retention\")\\n.withManifest(manifestToPublicApi)\\n.withOperation(jobOperation)\\n.withPriority(priority)\\n.withRoleArn(roleArn)\\n.withReport(jobReport)\\n.withConfirmationRequired(requiresConfirmation);\\nfinal CreateJobResult result = awss3ControlClient.createJob(request);\\nreturn result.getJobId();\\n}', ''], ['', '', '']], [['', '', ''], ['', \"export AWS_PROFILE='aws-user'\\nread -d '' legal_hold_permissions <<EOF\", '']]]\n",
      "[[['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:PutObjectLegalHold\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::ManifestBucket/*\"\\n]\\n}\\n]\\nEOF\\naws iam put-role-policy --role-name bops-objectlock --policy-name legal-hold-\\npermissions --policy-document \"${legal_hold_permissions}\"', ''], ['', '', '']], [['', '', ''], ['', 'export AWS_PROFILE=\\'aws-user\\'\\nexport AWS_DEFAULT_REGION=\\'us-west-2\\'\\nexport ACCOUNT_ID=123456789012\\nexport ROLE_ARN=\\'arn:aws:iam::123456789012:role/bops-objectlock\\'\\nread -d \\'\\' OPERATION <<EOF\\n{\\n\"S3PutObjectLegalHold\": {\\n\"LegalHold\": {\\n\"Status\":\"OFF\"\\n}\\n}\\n}\\nEOF\\nread -d \\'\\' MANIFEST <<EOF\\n{\\n\"Spec\": {\\n\"Format\": \"S3BatchOperations_CSV_20180820\",\\n\"Fields\": [\\n\"Bucket\",', '']]]\n",
      "[[['', '\"Key\"\\n]\\n},\\n\"Location\": {\\n\"ObjectArn\": \"arn:aws:s3:::ManifestBucket/legalhold-object-manifest.csv\",\\n\"ETag\": \"Your-manifest-ETag\"\\n}\\n}\\nEOF\\nread -d \\'\\' REPORT <<EOF\\n{\\n\"Bucket\": \"arn:aws:s3:::ReportBucket\",\\n\"Format\": \"Report_CSV_20180820\",\\n\"Enabled\": true,\\n\"Prefix\": \"reports/legalhold-objects-bops\",\\n\"ReportScope\": \"AllTasks\"\\n}\\nEOF\\naws \\\\\\ns3control create-job \\\\\\n--account-id \"${ACCOUNT_ID}\" \\\\\\n--manifest \"${MANIFEST//$\\'\\\\n\\'}\" \\\\\\n--operation \"${OPERATION//$\\'\\\\n\\'/}\" \\\\\\n--report \"${REPORT//$\\'\\\\n\\'}\" \\\\\\n--priority 10 \\\\\\n--role-arn \"${ROLE_ARN}\" \\\\\\n--client-request-token \"$(uuidgen)\" \\\\\\n--region \"${AWS_DEFAULT_REGION}\" \\\\\\n--description \"Turn off legal hold\";', ''], ['', '', '']], [['', '', ''], ['', 'public void allowPutObjectLegalHold() {\\nfinal String roleName = \"bops-object-lock\";\\nfinal String legalHoldPermissions = \"{\" +\\n\" \\\\\"Version\\\\\": \\\\\"2012-10-17\\\\\",\" +\\n\" \\\\\"Statement\\\\\": [\" +\\n\" {\" +\\n\" \\\\\"Effect\\\\\": \\\\\"Allow\\\\\",\" +', '']]]\n",
      "[[['', '\" \\\\\"Action\\\\\": [\" +\\n\" \\\\\"s3:PutObjectLegalHold\\\\\"\" +\\n\" ],\" +\\n\" \\\\\"Resource\\\\\": [\" +\\n\" \\\\\"arn:aws:s3:::ManifestBucket/*\\\\\"\" +\\n\" ]\" +\\n\" }\" +\\n\" ]\" +\\n\"}\";\\nfinal AmazonIdentityManagement iam =\\nAmazonIdentityManagementClientBuilder.defaultClient();\\nfinal PutRolePolicyRequest putRolePolicyRequest = new PutRolePolicyRequest()\\n.withPolicyDocument(legalHoldPermissions)\\n.withPolicyName(\"legal-hold-permissions\")\\n.withRoleName(roleName);\\nfinal PutRolePolicyResult putRolePolicyResult =\\niam.putRolePolicy(putRolePolicyRequest);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public String createLegalHoldOffJob(final AWSS3ControlClient awss3ControlClient) {\\nfinal String manifestObjectArn = \"arn:aws:s3:::ManifestBucket/legalhold-object-\\nmanifest.csv\";\\nfinal String manifestObjectVersionId = \"15ad5ba069e6bbc465c77bf83d541385\";\\nfinal JobManifestLocation manifestLocation = new JobManifestLocation()\\n.withObjectArn(manifestObjectArn)\\n.withETag(manifestObjectVersionId);\\nfinal JobManifestSpec manifestSpec =\\nnew JobManifestSpec()\\n.withFormat(JobManifestFormat.S3BatchOperations_CSV_20180820)\\n.withFields(\"Bucket\", \"Key\");\\nfinal JobManifest manifestToPublicApi = new JobManifest()\\n.withLocation(manifestLocation)\\n.withSpec(manifestSpec);', '']]]\n",
      "[[['', 'final String jobReportBucketArn = \"arn:aws:s3:::ReportBucket\";\\nfinal String jobReportPrefix = \"reports/legalhold-objects-bops\";\\nfinal JobReport jobReport = new JobReport()\\n.withEnabled(true)\\n.withReportScope(JobReportScope.AllTasks)\\n.withBucket(jobReportBucketArn)\\n.withPrefix(jobReportPrefix)\\n.withFormat(JobReportFormat.Report_CSV_20180820);\\nfinal JobOperation jobOperation = new JobOperation()\\n.withS3PutObjectLegalHold(new S3SetObjectLegalHoldOperation()\\n.withLegalHold(new S3ObjectLockLegalHold()\\n.withStatus(S3ObjectLockLegalHoldStatus.OFF)));\\nfinal String roleArn = \"arn:aws:iam::123456789012:role/bops-object-lock\";\\nfinal Boolean requiresConfirmation = true;\\nfinal int priority = 10;\\nfinal CreateJobRequest request = new CreateJobRequest()\\n.withAccountId(\"123456789012\")\\n.withDescription(\"Turn off legal hold\")\\n.withManifest(manifestToPublicApi)\\n.withOperation(jobOperation)\\n.withPriority(priority)\\n.withRoleArn(roleArn)\\n.withReport(jobReport)\\n.withConfirmationRequired(requiresConfirmation);\\nfinal CreateJobResult result = awss3ControlClient.createJob(request);\\nreturn result.getJobId();\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nS3 does not support delivery of CloudTrail logs to the requester or the bucket owner for\\nVPC endpoint requests when the VPC endpoint policy denies them.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nNewer Amazon S3 features are not supported for SOAP. We recommend that you use either\\nthe REST API or the AWS SDKs.', ''], ['', '', '']], [['SOAP API name', 'API event name used in CloudTrail log'], ['ListAllMyBuckets', 'ListBuckets'], ['CreateBucket', 'CreateBucket']]]\n",
      "[[['SOAP API name', 'API event name used in CloudTrail log'], ['DeleteBucket', 'DeleteBucket'], ['GetBucketAccessControlPolicy', 'GetBucketAcl'], ['SetBucketAccessControlPolicy', 'PutBucketAcl'], ['GetBucketLoggingStatus', 'GetBucketLogging'], ['SetBucketLoggingStatus', 'PutBucketLogging']], [['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']]]\n",
      "[[['Data event type (console)', 'resources.type value', 'Data APIs logged to\\nCloudTrail'], ['S3', 'AWS::S3::Object', '• AbortMultipartUpload\\n• CompleteMultipartUpload\\n• CopyObject\\n• CreateMultipartUpload\\n• DeleteObject\\n• DeleteObjectTagging\\n• DeleteObjects\\n• GetObject\\n• GetObjectAcl\\n• GetObjectAttributes\\n• GetObjectLegalHold']]]\n",
      "[[['Data event type (console)', 'resources.type value', 'Data APIs logged to\\nCloudTrail'], ['', '', '• GetObjectRetention\\n• GetObjectTagging\\n• GetObjectTorrent\\n• HeadObject\\n• ListMultipartUploads\\n• ListObjectVersions\\n• ListObjects\\n• ListParts\\n• PutObject\\n• PutObjectAcl\\n• PutObjectLegalHold\\n• PutObjectRetention\\n• PutObjectTagging\\n• RestoreObject\\n• SelectObjectContent\\n• UploadPart\\n• UploadPartCopy']]]\n",
      "[[['Data event type (console)', 'resources.type value', 'Data APIs logged to\\nCloudTrail'], ['S3 Access Point', 'AWS::S3::Access Point', '• AbortMultipartUpload\\n• CompleteMultipartUpload\\n• CopyObject (same-region\\ncopies only)\\n• CreateMultipartUpload\\n• DeleteObject\\n• DeleteObjectTagging\\n• GetBucketAcl\\n• GetBucketCors\\n• GetBucketLocation\\n• GetBucketNotificat\\nionConfiguration\\n• GetBucketPolicy\\n• GetObject\\n• GetObjectAcl\\n• GetObjectAttributes\\n• GetObjectLegalHold\\n• GetObjectRetention\\n• GetObjectTagging\\n• HeadBucket\\n• HeadObject\\n• ListMultipartUploads\\n• ListObjects\\n• ListObjectsV2\\n• ListObjectVersions\\n• ListParts\\n• Presign\\n• PutObject']]]\n",
      "[[['Data event type (console)', 'resources.type value', 'Data APIs logged to\\nCloudTrail'], ['', '', '• PutObjectLegalHold\\n• PutObjectRetention\\n• PutObjectAcl\\n• PutObjectTagging\\n• RestoreObject\\n• UploadPart\\n• UploadPartCopy (same-reg\\nion copies only)']]]\n",
      "[[['Data event type (console)', 'resources.type value', 'Data APIs logged to\\nCloudTrail'], ['S3 Object Lambda', 'AWS::S3ObjectLambd\\na::AccessPoint', '• AbortMultipartUpload\\n• CompleteMultipartUpload\\n• CopyObject (same-region\\ncopies only)\\n• CreateMultipartUpload\\n• DeleteObject\\n• DeleteObjectTagging\\n• GetObject\\n• GetObjectAcl\\n• GetObjectLegalHold\\n• GetObjectRetention\\n• GetObjectTagging\\n• HeadObject\\n• ListMultipartUploads\\n• ListObjects\\n• ListObjectVersions\\n• ListParts\\n• PutObject\\n• PutObjectLegalHold\\n• PutObjectRetention\\n• PutObjectAcl\\n• PutObjectTagging\\n• RestoreObject\\n• UploadPart\\n• WriteGetObjectResponse']]]\n",
      "[[['Data event type (console)', 'resources.type value', 'Data APIs logged to\\nCloudTrail'], ['S3 Outposts', 'AWS::S3Outposts::O\\nbject', '• AbortMultipartUpload\\n• CompleteMultipartUpload\\n• CopyObject (same-region\\ncopies only)\\n• CreateMultipartUpload\\n• DeleteObject\\n• DeleteObjectTagging\\n• GetObject\\n• GetObjectTagging\\n• HeadObject\\n• ListMultipartUploads\\n• ListObjects\\n• ListObjectsV2\\n• ListParts\\n• PutObject\\n• PutObjectTagging\\n• UploadPart\\n• UploadPartCopy']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor S3 Express One Zone, CloudTrail logging of Zonal endpoint (object-level, or data plane)\\nAPI operations (for example, PutObject or GetObject) is not supported.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe examples assume that CloudTrail logs are appropriately configured.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nCloudTrail always delivers object-level API logs to the requester (Account B). In addition,\\nCloudTrail also delivers the same logs to the bucket owner (Account A) even when the\\nbucket owner does not own the object (Account C) or have permissions for those same API\\noperations on that object.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Records\": [\\n{\\n\"eventVersion\": \"1.03\",\\n\"userIdentity\": {\\n\"type\": \"IAMUser\",\\n\"principalId\": \"111122223333\",\\n\"arn\": \"arn:aws:iam::111122223333:user/myUserName\",\\n\"accountId\": \"111122223333\",', '']]]\n",
      "[[['', '\"accessKeyId\": \"AKIAIOSFODNN7EXAMPLE\",\\n\"userName\": \"myUserName\"\\n},\\n\"eventTime\": \"2019-02-01T03:18:19Z\",\\n\"eventSource\": \"s3.amazonaws.com\",\\n\"eventName\": \"ListBuckets\",\\n\"awsRegion\": \"us-west-2\",\\n\"sourceIPAddress\": \"127.0.0.1\",\\n\"userAgent\": \"[]\",\\n\"requestParameters\": {\\n\"host\": [\\n\"s3.us-west-2.amazonaws.com\"\\n]\\n},\\n\"responseElements\": null,\\n\"additionalEventData\": {\\n\"SignatureVersion\": \"SigV2\",\\n\"AuthenticationMethod\": \"QueryString\",\\n\"aclRequired\": \"Yes\"\\n},\\n\"requestID\": \"47B8E8D397DCE7A6\",\\n\"eventID\": \"cdc4b7ed-e171-4cef-975a-ad829d4123e8\",\\n\"eventType\": \"AwsApiCall\",\\n\"recipientAccountId\": \"444455556666\",\\n\"tlsDetails\": {\\n\"tlsVersion\": \"TLSv1.2\",\\n\"cipherSuite\": \"ECDHE-RSA-AES128-GCM-SHA256\",\\n\"clientProvidedHostHeader\": \"s3.amazonaws.com\"\\n}\\n},\\n{\\n\"eventVersion\": \"1.03\",\\n\"userIdentity\": {\\n\"type\": \"IAMUser\",\\n\"principalId\": \"111122223333\",\\n\"arn\": \"arn:aws:iam::111122223333:user/myUserName\",\\n\"accountId\": \"111122223333\",\\n\"accessKeyId\": \"AKIAIOSFODNN7EXAMPLE\",\\n\"userName\": \"myUserName\"\\n},\\n\"eventTime\": \"2019-02-01T03:22:33Z\",\\n\"eventSource\": \"s3.amazonaws.com\",\\n\"eventName\": \"PutBucketAcl\",\\n\"awsRegion\": \"us-west-2\",', '']]]\n",
      "[[['', '\"sourceIPAddress\": \"\",\\n\"userAgent\": \"[]\",\\n\"requestParameters\": {\\n\"bucketName\": \"\",\\n\"AccessControlPolicy\": {\\n\"AccessControlList\": {\\n\"Grant\": {\\n\"Grantee\": {\\n\"xsi:type\": \"CanonicalUser\",\\n\"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\\n\"ID\":\\n\"d25639fbe9c19cd30a4c0f43fbf00e2d3f96400a9aa8dabfbbebe1906Example\"\\n},\\n\"Permission\": \"FULL_CONTROL\"\\n}\\n},\\n\"xmlns\": \"http://s3.amazonaws.com/doc/2006-03-01/\",\\n\"Owner\": {\\n\"ID\":\\n\"d25639fbe9c19cd30a4c0f43fbf00e2d3f96400a9aa8dabfbbebe1906Example\"\\n}\\n},\\n\"host\": [\\n\"s3.us-west-2.amazonaws.com\"\\n],\\n\"acl\": [\\n\"\"\\n]\\n},\\n\"responseElements\": null,\\n\"additionalEventData\": {\\n\"SignatureVersion\": \"SigV4\",\\n\"CipherSuite\": \"ECDHE-RSA-AES128-SHA\",\\n\"AuthenticationMethod\": \"AuthHeader\"\\n},\\n\"requestID\": \"BD8798EACDD16751\",\\n\"eventID\": \"607b9532-1423-41c7-b048-ec2641693c47\",\\n\"eventType\": \"AwsApiCall\",\\n\"recipientAccountId\": \"111122223333\",\\n\"tlsDetails\": {\\n\"tlsVersion\": \"TLSv1.2\",\\n\"cipherSuite\": \"ECDHE-RSA-AES128-GCM-SHA256\",\\n\"clientProvidedHostHeader\": \"s3.amazonaws.com\"\\n}', '']]]\n",
      "[[['', '},\\n{\\n\"eventVersion\": \"1.03\",\\n\"userIdentity\": {\\n\"type\": \"IAMUser\",\\n\"principalId\": \"111122223333\",\\n\"arn\": \"arn:aws:iam::111122223333:user/myUserName\",\\n\"accountId\": \"111122223333\",\\n\"accessKeyId\": \"AKIAIOSFODNN7EXAMPLE\",\\n\"userName\": \"myUserName\"\\n},\\n\"eventTime\": \"2019-02-01T03:26:37Z\",\\n\"eventSource\": \"s3.amazonaws.com\",\\n\"eventName\": \"GetBucketVersioning\",\\n\"awsRegion\": \"us-west-2\",\\n\"sourceIPAddress\": \"\",\\n\"userAgent\": \"[]\",\\n\"requestParameters\": {\\n\"host\": [\\n\"s3.us-west-2.amazonaws.com\"\\n],\\n\"bucketName\": \"example-s3-bucket1\",\\n\"versioning\": [\\n\"\"\\n]\\n},\\n\"responseElements\": null,\\n\"additionalEventData\": {\\n\"SignatureVersion\": \"SigV4\",\\n\"CipherSuite\": \"ECDHE-RSA-AES128-SHA\",\\n\"AuthenticationMethod\": \"AuthHeader\"\\n},\\n\"requestID\": \"07D681279BD94AED\",\\n\"eventID\": \"f2b287f3-0df1-4961-a2f4-c4bdfed47657\",\\n\"eventType\": \"AwsApiCall\",\\n\"recipientAccountId\": \"111122223333\",\\n\"tlsDetails\": {\\n\"tlsVersion\": \"TLSv1.2\",\\n\"cipherSuite\": \"ECDHE-RSA-AES128-GCM-SHA256\",\\n\"clientProvidedHostHeader\": \"s3.amazonaws.com\"\\n}\\n}\\n]', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"eventVersion\": \"1.08\",\\n\"userIdentity\": {\\n\"type\": \"IAMUser\",\\n\"principalId\": \"111122223333\",\\n\"arn\": \"arn:aws:iam::111122223333:user/yourUserName\",\\n\"accountId\": \"222222222222\",\\n\"accessKeyId\": \"AKIAIOSFODNN7EXAMPLE\",\\n\"userName\": \"yourUserName\"\\n},\\n\"eventTime\": \"2020-11-30T15:44:33Z\",\\n\"eventSource\": \"s3-outposts.amazonaws.com\",\\n\"eventName\": \"PutObject\",\\n\"awsRegion\": \"us-east-1\",\\n\"sourceIPAddress\": \"26.29.66.20\",\\n\"userAgent\": \"aws-cli/1.18.39 Python/3.4.10 Darwin/18.7.0 botocore/1.15.39\",\\n\"requestParameters\": {\\n\"expires\": \"Wed, 21 Oct 2020 07:28:00 GMT\",\\n\"Content-Language\": \"english\",\\n\"x-amz-server-side-encryption-customer-key-MD5\": \"wJalrXUtnFEMI/K7MDENG/\\nbPxRfiCYEXAMPLEKEY\",\\n\"ObjectCannedACL\": \"BucketOwnerFullControl\",\\n\"x-amz-server-side-encryption\": \"Aes256\",', '']]]\n",
      "[[['', '\"Content-Encoding\": \"gzip\",\\n\"Content-Length\": \"10\",\\n\"Cache-Control\": \"no-cache\",\\n\"Content-Type\": \"text/html; charset=UTF-8\",\\n\"Content-Disposition\": \"attachment\",\\n\"Content-MD5\": \"je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEY\",\\n\"x-amz-storage-class\": \"Outposts\",\\n\"x-amz-server-side-encryption-customer-algorithm\": \"Aes256\",\\n\"bucketName\": \"example-s3-bucket1\",\\n\"Key\": \"path/upload.sh\"\\n},\\n\"responseElements\": {\\n\"x-amz-server-side-encryption-customer-key-MD5\": \"wJalrXUtnFEMI/K7MDENG/\\nbPxRfiCYEXAMPLEKEY\",\\n\"x-amz-server-side-encryption\": \"Aes256\",\\n\"x-amz-version-id\": \"001\",\\n\"x-amz-server-side-encryption-customer-algorithm\": \"Aes256\",\\n\"ETag\": \"d41d8cd98f00b204e9800998ecf8427f\"\\n},\\n\"additionalEventData\": {\\n\"CipherSuite\": \"ECDHE-RSA-AES128-SHA\",\\n\"bytesTransferredIn\": 10,\\n\"x-amz-id-2\": \"29xXQBV2O\\n+xOHKItvzY1suLv1i6A52E0zOX159fpfsItYd58JhXwKxXAXI4IQkp6\",\\n\"SignatureVersion\": \"SigV4\",\\n\"bytesTransferredOut\": 20,\\n\"AuthenticationMethod\": \"AuthHeader\"\\n},\\n\"requestID\": \"8E96D972160306FA\",\\n\"eventID\": \"ee3b4e0c-ab12-459b-9998-0a5a6f2e4015\",\\n\"readOnly\": false,\\n\"resources\": [\\n{\\n\"accountId\": \"222222222222\",\\n\"type\": \"AWS::S3Outposts::Object\",\\n\"ARN\": \"arn:aws:s3-outposts:us-east-1:YYY:outpost/op-01ac5d28a6a232904/\\nbucket/path/upload.sh\"\\n},\\n{\\n\"accountId\": \"222222222222\",\\n\"type\": \"AWS::S3Outposts::Bucket\",\\n\"ARN\": \"arn:aws:s3-outposts:us-east-1:YYY:outpost/op-01ac5d28a6a232904/\\nbucket/\"\\n}', '']]]\n",
      "[[['', '],\\n\"eventType\": \"AwsApiCall\",\\n\"managementEvent\": false,\\n\"recipientAccountId\": \"444455556666\",\\n\"sharedEventID\": \"02759a4c-c040-4758-b84b-7cbaaf17747a\",\\n\"edgeDeviceDetails\": {\\n\"type\": \"outposts\",\\n\"deviceId\": \"op-01ac5d28a6a232904\"\\n},\\n\"eventCategory\": \"Data\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\n• The default setting for CloudTrail is to find only management events. Check to ensure\\nthat you have the data events enabled for your account.\\n• With an S3 bucket that is generating a high workload, you could quickly generate\\nthousands of logs in a short amount of time. Be mindful of how long you choose to\\nenable CloudTrail data events for a busy bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\n• It's a best practice to create a lifecycle configuration for your AWS CloudTrail data event\\nbucket. Configure the lifecycle configuration to periodically remove log files after the\\nperiod of time you believe you need to audit them. Doing so reduces the amount of\\ndata that Athena analyzes for each query. For more information, see Setting a lifecycle\\nconfiguration on a bucket.\\n• For more information about logging format, see Logging Amazon S3 API calls using AWS\\nCloudTrail.\\n• For examples of how to query CloudTrail logs, see the AWS Big Data Blog post Analyze\\nSecurity, Compliance, and Operational Activity Using AWS CloudTrail and Amazon\\nAthena.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAdditional charges apply for data events. For more information, see AWS CloudTrail pricing.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you use the CloudTrail console or the Amazon S3 console to configure a trail to log\\ndata events for an S3 bucket, the Amazon S3 console shows that object-level logging is\\nenabled for the bucket.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nCloudTrail events for Amazon S3 include the signature version in the request details under\\nthe key name of 'additionalEventData. To find the signature version on requests\\nmade for objects in Amazon S3 such as GET, PUT, and DELETE requests, you must enable\\nCloudTrail data events. (This feature is turned off by default.)\", ''], ['', '', '']], [['', '', ''], ['', \"SELECT EventTime, EventName as S3_Action, requestParameters as Request_Parameters,\\nawsregion as AWS_Region, sourceipaddress as Source_IP, useragent as User_Agent\\nFROM s3_cloudtrail_events_db.cloudtrail_table\\nWHERE eventsource='s3.amazonaws.com'\", '']]]\n",
      "[[['', \"AND json_extract_scalar(additionalEventData, '$.SignatureVersion')='SigV2'\\nLIMIT 10;\", ''], ['', '', '']], [['', '', ''], ['', \"SELECT useridentity.arn, Count(requestid) as RequestCount\\nFROM s3_cloudtrail_events_db.cloudtrail_table\\nWHERE eventsource='s3.amazonaws.com'\\nand json_extract_scalar(additionalEventData, '$.SignatureVersion')='SigV2'\\nGroup by useridentity.arn\", ''], ['', '', '']], [['', '', ''], ['', 'CREATE EXTERNAL TABLE s3_cloudtrail_events_db.cloudtrail_table_partitioned(\\neventversion STRING,\\nuserIdentity STRUCT<\\ntype:STRING,\\nprincipalid:STRING,\\narn:STRING,\\naccountid:STRING,\\ninvokedby:STRING,\\naccesskeyid:STRING,\\nuserName:STRING,\\nsessioncontext:STRUCT<\\nattributes:STRUCT<\\nmfaauthenticated:STRING,\\ncreationdate:STRING>,\\nsessionIssuer:STRUCT<\\ntype:STRING,\\nprincipalId:STRING,\\narn:STRING,\\naccountId:STRING,\\nuserName:STRING>', '']]]\n",
      "[[['', \">\\n>,\\neventTime STRING,\\neventSource STRING,\\neventName STRING,\\nawsRegion STRING,\\nsourceIpAddress STRING,\\nuserAgent STRING,\\nerrorCode STRING,\\nerrorMessage STRING,\\nrequestParameters STRING,\\nresponseElements STRING,\\nadditionalEventData STRING,\\nrequestId STRING,\\neventId STRING,\\nresources ARRAY<STRUCT<ARN:STRING,accountId: STRING,type:STRING>>,\\neventType STRING,\\napiVersion STRING,\\nreadOnly STRING,\\nrecipientAccountId STRING,\\nserviceEventDetails STRING,\\nsharedEventID STRING,\\nvpcEndpointId STRING\\n)\\nPARTITIONED BY (region string, year string, month string, day string)\\nROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\\nLOCATION 's3://DOC-EXAMPLE-BUCKET1/AWSLogs/111122223333/';\", ''], ['', '', '']], [['', '', ''], ['', \"ALTER TABLE s3_cloudtrail_events_db.cloudtrail_table_partitioned ADD\\nPARTITION (region= 'us-east-1', year= '2019', month= '02', day= '19') LOCATION\\n's3://DOC-EXAMPLE-BUCKET1/AWSLogs/111122223333/CloudTrail/us-east-1/2019/02/19/'\\nPARTITION (region= 'us-west-1', year= '2019', month= '02', day= '19') LOCATION\\n's3://DOC-EXAMPLE-BUCKET1/AWSLogs/111122223333/CloudTrail/us-west-1/2019/02/19/'\\nPARTITION (region= 'us-west-2', year= '2019', month= '02', day= '19') LOCATION\\n's3://DOC-EXAMPLE-BUCKET1/AWSLogs/111122223333/CloudTrail/us-west-2/2019/02/19/'\\nPARTITION (region= 'ap-southeast-1', year= '2019', month= '02', day= '19') LOCATION\\n's3://DOC-EXAMPLE-BUCKET1/AWSLogs/111122223333/CloudTrail/ap-southeast-1/2019/02/19/'\", '']]]\n",
      "[[['', \"PARTITION (region= 'ap-southeast-2', year= '2019', month= '02', day= '19') LOCATION\\n's3://DOC-EXAMPLE-BUCKET1/AWSLogs/111122223333/CloudTrail/ap-southeast-2/2019/02/19/'\\nPARTITION (region= 'ap-northeast-1', year= '2019', month= '02', day= '19') LOCATION\\n's3://DOC-EXAMPLE-BUCKET1/AWSLogs/111122223333/CloudTrail/ap-northeast-1/2019/02/19/'\\nPARTITION (region= 'eu-west-1', year= '2019', month= '02', day= '19') LOCATION\\n's3://DOC-EXAMPLE-BUCKET1/AWSLogs/111122223333/CloudTrail/eu-west-1/2019/02/19/'\\nPARTITION (region= 'sa-east-1', year= '2019', month= '02', day= '19') LOCATION\\n's3://DOC-EXAMPLE-BUCKET1/AWSLogs/111122223333/CloudTrail/sa-east-1/2019/02/19/';\", ''], ['', '', '']], [['', '', ''], ['', \"SELECT useridentity.arn,\\nCount(requestid) AS RequestCount\\nFROM s3_cloudtrail_events_db.cloudtrail_table_partitioned\\nWHERE eventsource='s3.amazonaws.com'\\nAND json_extract_scalar(additionalEventData, '$.SignatureVersion')='SigV2'\\nAND region='us-east-1'\\nAND year='2019'\\nAND month='02'\\nAND day='19'\\nGroup by useridentity.arn\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"SELECT\\neventTime,\\neventName,\\neventSource,\\nsourceIpAddress,\\nuserAgent,\\njson_extract_scalar(requestParameters, '$.bucketName') as bucketName,\\njson_extract_scalar(requestParameters, '$.key') as object,\\nuserIdentity.arn as userArn\\nFROM\\ns3_cloudtrail_events_db.cloudtrail_table\\nWHERE\\neventName = 'PutObject'\\nAND eventTime BETWEEN '2019-07-05T00:00:00Z' and '2019-07-06T00:00:00Z'\", ''], ['', '', '']], [['', '', ''], ['', \"SELECT\\neventTime,\\neventName,\\neventSource,\\nsourceIpAddress,\\nuserAgent,\\njson_extract_scalar(requestParameters, '$.bucketName') as bucketName,\\njson_extract_scalar(requestParameters, '$.key') as object,\\nuserIdentity.arn as userArn\\nFROM\\ns3_cloudtrail_events_db.cloudtrail_table\\nWHERE\\neventName = 'GetObject'\\nAND eventTime BETWEEN '2019-07-05T00:00:00Z' and '2019-07-06T00:00:00Z'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"SELECT\\neventTime,\\neventName,\\neventSource,\\nsourceIpAddress,\\nuserAgent,\\njson_extract_scalar(requestParameters, '$.bucketName') as bucketName,\\nuserIdentity.arn as userArn,\\nuserIdentity.accountId\\nFROM\\ns3_cloudtrail_events_db.cloudtrail_table\\nWHERE\\nuserIdentity.accountId = 'anonymous'\\nAND eventTime BETWEEN '2019-07-05T00:00:00Z' and '2019-07-06T00:00:00Z'\", ''], ['', '', '']], [['', '', ''], ['', \"SELECT\\neventTime,\\neventName,\\neventSource,\\nsourceIpAddress,\\nuserAgent,\\nuserIdentity.arn as userArn,\\njson_extract_scalar(requestParameters, '$.bucketName') as bucketName,\\njson_extract_scalar(requestParameters, '$.key') as object,\\njson_extract_scalar(additionalEventData, '$.aclRequired') as aclRequired\\nFROM\\ns3_cloudtrail_events_db.cloudtrail_table\", '']]]\n",
      "[[['', \"WHERE\\njson_extract_scalar(additionalEventData, '$.aclRequired') = 'Yes'\\nAND eventTime BETWEEN '2022-05-10T00:00:00Z' and '2022-08-10T00:00:00Z'\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\n• These query examples can also be useful for security monitoring. You can review\\nthe results for PutObject or GetObject calls from unexpected or unauthorized IP\\naddresses or requesters and for identifying any anonymous requests to your buckets.\\n• This query only retrieves information from the time at which logging was enabled.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nServer access logs don't record information about wrong-Region redirect errors for Regions\\nthat launched after March 20, 2019. Wrong-Region redirect errors occur when a request for\\nan object or bucket is made outside the Region in which the bucket exists.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'logs/2013-11-01-21-32-16-E568B2907131C0C0', ''], ['', '', '']], [['', '', ''], ['', 'logs2013-11-01-21-32-16-E568B2907131C0C0', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '[DestinationPrefix][YYYY]-[MM]-[DD]-[hh]-[mm]-[ss]-[UniqueString]', ''], ['', '', '']], [['', '', ''], ['', 'logs/2013-11-01-21-32-16-E568B2907131C0C0', ''], ['', '', '']], [['', '', ''], ['', '[DestinationPrefix][SourceAccountId]/[SourceRegion]/[SourceBucket]/[YYYY]/[MM]/\\n[DD]/[YYYY]-[MM]-[DD]-[hh]-[mm]-[ss]-[UniqueString]', ''], ['', '', '']], [['', '', ''], ['', 'logs/123456789012/us-west-2/DOC-EXAMPLE-SOURCE-\\nBUCKET/2023/03/01/2023-03-01-21-32-16-E568B2907131C0C0', ''], ['', '', '']]]\n",
      "[[['', ''], ['[TargetPrefix][YYYY]-[MM]-[DD]-[hh]-[mm]-[ss]-[UniqueString]', ''], ['', '']], [['', ''], ['[TargetPrefix][SourceAccountId]/[SourceRegion]/[SourceBucket]/[YYYY]/[MM]/[DD]/\\n[YYYY]-[MM]-[DD]-[hh]-[mm]-[ss]-[UniqueString]', ''], ['', '']], [['', ''], ['[DestinationPrefix][YYYY]-[MM]-[DD]-[hh]-[mm]-[ss]-[UniqueString]', ''], ['', '']], [['', ''], ['[DestinationPrefix][SourceAccountId]/[SourceRegion]/[SourceBucket]/[YYYY]/[MM]/[DD]/\\n[YYYY]-[MM]-[DD]-[hh]-[mm]-[ss]-[UniqueString]', ''], ['', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nS3 does not support delivery of CloudTrail logs or server access logs to the requester or the\\nbucket owner for VPC endpoint requests when the VPC endpoint policy denies them or for\\nrequests that fail before the VPC policy is evaluated.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\n• There is no extra charge for enabling server access logging on an Amazon S3 bucket.\\nHowever, any log files that the system delivers to you will accrue the usual charges for\\nstorage. (You can delete the log files at any time.) We do not assess data-transfer charges\\nfor log file delivery, but we do charge the normal data-transfer rate for accessing the log\\nfiles.\\n• Your destination bucket should not have server access logging enabled. You can have\\nlogs delivered to any bucket that you own that is in the same Region as the source\\nbucket, including the source bucket itself. However, delivering logs to the source bucket\\nwill cause an infinite loop of logs and is not recommended. For simpler log management,\\nwe recommend that you save access logs in a different bucket. For more information, see\\nHow do I enable log delivery?\\n• S3 buckets that have S3 Object Lock enabled can't be used as destination buckets for\\nserver access logs. Your destination bucket must not have a default retention period\\nconfiguration.\\n• The destination bucket must not have Requester Pays enabled.\\n• You can use default bucket encryption on the destination bucket only if you use server-\\nside encryption with Amazon S3 managed keys (SSE-S3), which uses the 256-bit\", '']]]\n",
      "[[['', 'Advanced Encryption Standard (AES-256). Default server-side encryption with AWS Key\\nManagement Service (AWS KMS) keys (SSE-KMS) is not supported.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf there are Deny statements in your bucket policy, make sure that they don't prevent\\nAmazon S3 from delivering access logs.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"S3ServerAccessLogsPolicy\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"logging.s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"s3:PutObject\"\\n],', '']]]\n",
      "[[['', '\"Resource\": \"arn:aws:s3:::example-s3-destination-bucket/EXAMPLE-LOGGING-\\nPREFIX*\",\\n\"Condition\": {\\n\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:::example-s3-source-bucket\"\\n},\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"SOURCE-ACCOUNT-ID\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nAs a security best practice, Amazon S3 disables access control lists (ACLs) by default in all\\nnew buckets. For more information about ACL permissions in the Amazon S3 console, see\\nConfiguring ACLs.', ''], ['', '', '']], [['', '', ''], ['', 'http://acs.amazonaws.com/groups/s3/LogDelivery', ''], ['', '', '']], [['', '', ''], ['', '<Grant>\\n<Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"Group\">\\n<URI>http://acs.amazonaws.com/groups/s3/LogDelivery</URI>', '']]]\n",
      "[[['', '</Grantee>\\n<Permission>WRITE</Permission>\\n</Grant>\\n<Grant>\\n<Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:type=\"Group\">\\n<URI>http://acs.amazonaws.com/groups/s3/LogDelivery</URI>\\n</Grantee>\\n<Permission>READ_ACP</Permission>\\n</Grant>', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you enable Amazon S3 server access logging by using AWS CloudFormation on a\\nbucket and you\\'re using ACLs to grant access to the S3 log delivery group, you must also\\nadd \"AccessControl\": \"LogDeliveryWrite\" to your CloudFormation template. Doing\\nso is important because you can grant those permissions only by creating an ACL for the\\nbucket, but you can\\'t create custom ACLs for buckets in CloudFormation. You can use only\\ncanned ACLs with CloudFormation.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nSpecifying a prefix with a slash (/) makes it simpler for you to locate the log objects.\\nFor example, if you specify the prefix value logs/, each log object that Amazon S3\\ncreates begins with the logs/ prefix in its key, as follows:\\nlogs/2013-11-01-21-32-16-E568B2907131C0C0\\nIf you specify the prefix value logs, the log object appears as follows:\\nlogs2013-11-01-21-32-16-E568B2907131C0C0', ''], ['', '', '']], [['', '', ''], ['', 'logs/2013-11-01-21-32-16-E568B2907131C0C0', ''], ['', '', '']], [['', '', ''], ['', 'logs2013-11-01-21-32-16-E568B2907131C0C0', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<BucketLoggingStatus xmlns=\"http://doc.s3.amazonaws.com/2006-03-01\">\\n<LoggingEnabled>\\n<TargetBucket>example-s3-destination-bucket</TargetBucket>\\n<TargetPrefix>logs/</TargetPrefix>\\n</LoggingEnabled>\\n</BucketLoggingStatus>', ''], ['', '', '']], [['', '', ''], ['', '<BucketLoggingStatus xmlns=\"http://doc.s3.amazonaws.com/2006-03-01\">\\n<LoggingEnabled>\\n<TargetBucket>example-s3-destination-bucket</TargetBucket>\\n<TargetPrefix>logs/</TargetPrefix>\\n<TargetObjectKeyFormat>\\n<PartitionedPrefix>\\n<PartitionDateSource>EventTime</PartitionDateSource>\\n</PartitionedPrefix>\\n</TargetObjectKeyFormat>\\n</LoggingEnabled>\\n</BucketLoggingStatus>', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf the destination bucket uses the Bucket owner enforced setting for Object Ownership, you\\ncan't use destination grants to grant permissions to other users. To grant permissions to\", '']]]\n",
      "[[['', 'others, you can update the bucket policy on the destination bucket. For more information,\\nsee Permissions for log delivery.', ''], ['', '', '']], [['', '', ''], ['', '<BucketLoggingStatus xmlns=\"http://doc.s3.amazonaws.com/2006-03-01\">\\n</BucketLoggingStatus>', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using System;\\nusing System.IO;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing Microsoft.Extensions.Configuration;\\n/// <summary>\\n/// This example shows how to enable logging on an Amazon Simple Storage\\n/// Service (Amazon S3) bucket. You need to have two Amazon S3 buckets for\\n/// this example. The first is the bucket for which you wish to enable\\n/// logging, and the second is the location where you want to store the\\n/// logs.\\n/// </summary>\\npublic class ServerAccessLogging\\n{\\nprivate static IConfiguration _configuration = null!;\\npublic static async Task Main()\\n{\\nLoadConfig();\\nstring bucketName = _configuration[\"BucketName\"];\\nstring logBucketName = _configuration[\"LogBucketName\"];\\nstring logObjectKeyPrefix = _configuration[\"LogObjectKeyPrefix\"];\\nstring accountId = _configuration[\"AccountId\"];\\n// If the AWS Region defined for your default user is different\\n// from the Region where your Amazon S3 bucket is located,\\n// pass the Region name to the Amazon S3 client object\\'s constructor.\\n// For example: RegionEndpoint.USWest2 or RegionEndpoint.USEast2.\\nIAmazonS3 client = new AmazonS3Client();\\ntry\\n{\\n// Update bucket policy for target bucket to allow delivery of\\nlogs to it.\\nawait SetBucketPolicyToAllowLogDelivery(\\nclient,\\nbucketName,\\nlogBucketName,\\nlogObjectKeyPrefix,\\naccountId);', '']]]\n",
      "[[['', '// Enable logging on the source bucket.\\nawait EnableLoggingAsync(\\nclient,\\nbucketName,\\nlogBucketName,\\nlogObjectKeyPrefix);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine($\"Error: {e.Message}\");\\n}\\n}\\n/// <summary>\\n/// This method grants appropriate permissions for logging to the\\n/// Amazon S3 bucket where the logs will be stored.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client which will be\\nused\\n/// to apply the bucket policy.</param>\\n/// <param name=\"sourceBucketName\">The name of the source bucket.</param>\\n/// <param name=\"logBucketName\">The name of the bucket where logging\\n/// information will be stored.</param>\\n/// <param name=\"logPrefix\">The logging prefix where the logs should be\\ndelivered.</param>\\n/// <param name=\"accountId\">The account id of the account where the\\nsource bucket exists.</param>\\n/// <returns>Async task.</returns>\\npublic static async Task SetBucketPolicyToAllowLogDelivery(\\nIAmazonS3 client,\\nstring sourceBucketName,\\nstring logBucketName,\\nstring logPrefix,\\nstring accountId)\\n{\\nvar resourceArn = @\"\"\"arn:aws:s3:::\" + logBucketName + \"/\" +\\nlogPrefix + @\"*\"\"\";\\nvar newPolicy = @\"{\\n\"\"Statement\"\":[{\\n\"\"Sid\"\": \"\"S3ServerAccessLogsPolicy\"\",\\n\"\"Effect\"\": \"\"Allow\"\",', '']]]\n",
      "[[['', '\"\"Principal\"\": { \"\"Service\"\":\\n\"\"logging.s3.amazonaws.com\"\" },\\n\"\"Action\"\": [\"\"s3:PutObject\"\"],\\n\"\"Resource\"\": [\" + resourceArn + @\"],\\n\"\"Condition\"\": {\\n\"\"ArnLike\"\": { \"\"aws:SourceArn\"\":\\n\"\"arn:aws:s3:::\" + sourceBucketName + @\"\"\" },\\n\"\"StringEquals\"\": { \"\"aws:SourceAccount\"\": \"\"\" +\\naccountId + @\"\"\" }\\n}\\n}]\\n}\";\\nConsole.WriteLine($\"The policy to apply to bucket {logBucketName} to\\nenable logging:\");\\nConsole.WriteLine(newPolicy);\\nPutBucketPolicyRequest putRequest = new PutBucketPolicyRequest\\n{\\nBucketName = logBucketName,\\nPolicy = newPolicy,\\n};\\nawait client.PutBucketPolicyAsync(putRequest);\\nConsole.WriteLine(\"Policy applied.\");\\n}\\n/// <summary>\\n/// This method enables logging for an Amazon S3 bucket. Logs will be\\nstored\\n/// in the bucket you selected for logging. Selected prefix\\n/// will be prepended to each log object.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client which will be\\nused\\n/// to configure and apply logging to the selected Amazon S3 bucket.</\\nparam>\\n/// <param name=\"bucketName\">The name of the Amazon S3 bucket for which\\nyou\\n/// wish to enable logging.</param>\\n/// <param name=\"logBucketName\">The name of the Amazon S3 bucket where\\nlogging\\n/// information will be stored.</param>\\n/// <param name=\"logObjectKeyPrefix\">The prefix to prepend to each\\n/// object key.</param>\\n/// <returns>Async task.</returns>', '']]]\n",
      "[[['', 'public static async Task EnableLoggingAsync(\\nIAmazonS3 client,\\nstring bucketName,\\nstring logBucketName,\\nstring logObjectKeyPrefix)\\n{\\nConsole.WriteLine($\"Enabling logging for bucket {bucketName}.\");\\nvar loggingConfig = new S3BucketLoggingConfig\\n{\\nTargetBucketName = logBucketName,\\nTargetPrefix = logObjectKeyPrefix,\\n};\\nvar putBucketLoggingRequest = new PutBucketLoggingRequest\\n{\\nBucketName = bucketName,\\nLoggingConfig = loggingConfig,\\n};\\nawait client.PutBucketLoggingAsync(putBucketLoggingRequest);\\nConsole.WriteLine($\"Logging enabled.\");\\n}\\n/// <summary>\\n/// Loads configuration from settings files.\\n/// </summary>\\npublic static void LoadConfig()\\n{\\n_configuration = new ConfigurationBuilder()\\n.SetBasePath(Directory.GetCurrentDirectory())\\n.AddJsonFile(\"settings.json\") // Load settings from .json file.\\n.AddJsonFile(\"settings.local.json\", true) // Optionally, load\\nlocal settings.\\n.Build();\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;', '']]]\n",
      "[[['', 'import software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.BucketLoggingStatus;\\nimport software.amazon.awssdk.services.s3.model.LoggingEnabled;\\nimport software.amazon.awssdk.services.s3.model.PartitionedPrefix;\\nimport software.amazon.awssdk.services.s3.model.PutBucketLoggingRequest;\\nimport software.amazon.awssdk.services.s3.model.TargetObjectKeyFormat;\\n// Class to set a bucket policy on a target S3 bucket and enable server access\\nlogging on a source S3 bucket.\\npublic class ServerAccessLogging {\\nprivate static S3Client s3Client;\\npublic static void main(String[] args) {\\nString sourceBucketName = \"SOURCE-BUCKET\";\\nString targetBucketName = \"TARGET-BUCKET\";\\nString sourceAccountId = \"123456789012\";\\nString targetPrefix = \"logs/\";\\n// Create S3 Client.\\ns3Client = S3Client.builder().\\nregion(Region.US_EAST_2)\\n.build();\\n// Set a bucket policy on the target S3 bucket to enable server access\\nlogging by granting the\\n// logging.s3.amazonaws.com principal permission to use the PutObject\\noperation.\\nServerAccessLogging serverAccessLogging = new ServerAccessLogging();\\nserverAccessLogging.setTargetBucketPolicy(sourceAccountId, sourceBucketName,\\ntargetBucketName);\\n// Enable server access logging on the source S3 bucket.\\nserverAccessLogging.enableServerAccessLogging(sourceBucketName,\\ntargetBucketName,\\ntargetPrefix);\\n}\\n// Function to set a bucket policy on the target S3 bucket to enable server\\naccess logging by granting the\\n// logging.s3.amazonaws.com principal permission to use the PutObject operation.\\npublic void setTargetBucketPolicy(String sourceAccountId, String\\nsourceBucketName, String targetBucketName) {\\nString policy = \"{\\\\n\" +', '']]]\n",
      "[[['', '\" \\\\\"Version\\\\\": \\\\\"2012-10-17\\\\\",\\\\n\" +\\n\" \\\\\"Statement\\\\\": [\\\\n\" +\\n\" {\\\\n\" +\\n\" \\\\\"Sid\\\\\": \\\\\"S3ServerAccessLogsPolicy\\\\\",\\\\n\" +\\n\" \\\\\"Effect\\\\\": \\\\\"Allow\\\\\",\\\\n\" +\\n\" \\\\\"Principal\\\\\": {\\\\\"Service\\\\\": \\\\\"logging.s3.amazonaws.com\\n\\\\\"},\\\\n\" +\\n\" \\\\\"Action\\\\\": [\\\\n\" +\\n\" \\\\\"s3:PutObject\\\\\"\\\\n\" +\\n\" ],\\\\n\" +\\n\" \\\\\"Resource\\\\\": \\\\\"arn:aws:s3:::\" + targetBucketName + \"/*\\n\\\\\",\\\\n\" +\\n\" \\\\\"Condition\\\\\": {\\\\n\" +\\n\" \\\\\"ArnLike\\\\\": {\\\\n\" +\\n\" \\\\\"aws:SourceArn\\\\\": \\\\\"arn:aws:s3:::\" +\\nsourceBucketName + \"\\\\\"\\\\n\" +\\n\" },\\\\n\" +\\n\" \\\\\"StringEquals\\\\\": {\\\\n\" +\\n\" \\\\\"aws:SourceAccount\\\\\": \\\\\"\" + sourceAccountId +\\n\"\\\\\"\\\\n\" +\\n\" }\\\\n\" +\\n\" }\\\\n\" +\\n\" }\\\\n\" +\\n\" ]\\\\n\" +\\n\"}\";\\ns3Client.putBucketPolicy(b -> b.bucket(targetBucketName).policy(policy));\\n}\\n// Function to enable server access logging on the source S3 bucket.\\npublic void enableServerAccessLogging(String sourceBucketName, String\\ntargetBucketName,\\nString targetPrefix) {\\nTargetObjectKeyFormat targetObjectKeyFormat =\\nTargetObjectKeyFormat.builder()\\n.partitionedPrefix(PartitionedPrefix.builder().partitionDateSource(\"EventTime\").buil\\n.build();\\nLoggingEnabled loggingEnabled = LoggingEnabled.builder()\\n.targetBucket(targetBucketName)\\n.targetPrefix(targetPrefix)\\n.targetObjectKeyFormat(targetObjectKeyFormat)\\n.build();\\nBucketLoggingStatus bucketLoggingStatus = BucketLoggingStatus.builder()\\n.loggingEnabled(loggingEnabled)', 'd']]]\n",
      "[[['', '.build();\\ns3Client.putBucketLogging(PutBucketLoggingRequest.builder()\\n.bucket(sourceBucketName)\\n.bucketLoggingStatus(bucketLoggingStatus)\\n.build());\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe final step of the following procedure provides example bash scripts that you can use\\nto create your logging buckets and enable server access logging on these buckets. To use\\nthose scripts, you must create the policy.json and logging.json files, as described in\\nthe following procedure.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-policy --bucket example-s3-destination-bucket-logs --policy\\nfile://policy.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",', '']]]\n",
      "[[['', '\"Statement\": [\\n{\\n\"Sid\": \"S3ServerAccessLogsPolicy\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"logging.s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"s3:PutObject\"\\n],\\n\"Resource\": \"arn:aws:s3:::example-s3-destination-bucket-logs/*\",\\n\"Condition\": {\\n\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:::example-s3-source-bucket\"\\n},\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"SOURCE-ACCOUNT-ID\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-acl --bucket example-s3-destination-bucket-logs --grant-\\nwrite URI=http://acs.amazonaws.com/groups/s3/LogDelivery --grant-read-acp\\nURI=http://acs.amazonaws.com/groups/s3/LogDelivery', ''], ['', '', '']], [['', '', ''], ['', '', '']]]\n",
      "[[['', 'aws s3api put-bucket-logging --bucket example-s3-destination-bucket-logs --bucket-\\nlogging-status file://logging.json', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nInstead of using this put-bucket-logging command to apply the logging\\nconfiguration on each destination bucket, you can use one of the bash scripts\\nprovided in the next step. To use those scripts, you must create the policy.json and\\nlogging.json files, as described in this procedure.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"LoggingEnabled\": {\\n\"TargetBucket\": \"example-s3-destination-bucket-logs\",\\n\"TargetPrefix\": \"example-s3-destination-bucket/\"\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"LoggingEnabled\": {\\n\"TargetBucket\": \"example-s3-destination-bucket-logs\",\\n\"TargetPrefix\": \"example-s3-destination-bucket/\",\\n\"TargetGrants\": [\\n{\\n\"Grantee\": {\\n\"Type\": \"AmazonCustomerByEmail\",\\n\"EmailAddress\": \"user@example.com\"\\n},\\n\"Permission\": \"FULL_CONTROL\"\\n}\\n]\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"LoggingEnabled\": {\\n\"TargetBucket\": \"example-s3-destination-bucket-logs\",\\n\"TargetPrefix\": \"example-s3-destination-bucket/\",\\n\"TargetObjectKeyFormat\": {\\n\"PartitionedPrefix\": {\\n\"PartitionDateSource\": \"EventTime\"\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThis script works only if all of your buckets are in the same Region. If you have buckets\\nin multiple Regions, you must adjust the script.', ''], ['', '', '']], [['', '', ''], ['', 'loggingBucket=\\'example-s3-destination-bucket-logs\\'\\nregion=\\'us-west-2\\'\\n# Create the logging bucket.\\naws s3 mb s3://$loggingBucket --region $region\\naws s3api put-bucket-policy --bucket $loggingBucket --policy file://policy.json\\n# List the buckets in this account.\\nbuckets=\"$(aws s3 ls | awk \\'{print $3}\\')\"\\n# Put a bucket logging configuration on each bucket.\\nfor bucket in $buckets\\ndo\\n# This if statement excludes the logging bucket.\\nif [ \"$bucket\" != \"$loggingBucket\" ] ; then\\ncontinue;\\nfi\\nprintf \\'{\\n\"LoggingEnabled\": {\\n\"TargetBucket\": \"%s\",\\n\"TargetPrefix\": \"%s/\"\\n}\\n}\\' \"$loggingBucket\" \"$bucket\" > logging.json\\naws s3api put-bucket-logging --bucket $bucket --bucket-logging-status file://\\nlogging.json\\necho \"$bucket done\"\\ndone', '']]]\n",
      "[[['', 'rm logging.json\\necho \"Complete\"', ''], ['', '', '']], [['', '', ''], ['', 'loggingBucket=\\'example-s3-destination-bucket-logs\\'\\nregion=\\'us-west-2\\'\\n# Create the logging bucket.\\naws s3 mb s3://$loggingBucket --region $region\\naws s3api put-bucket-acl --bucket $loggingBucket --grant-write URI=http://\\nacs.amazonaws.com/groups/s3/LogDelivery --grant-read-acp URI=http://\\nacs.amazonaws.com/groups/s3/LogDelivery\\n# List the buckets in this account.\\nbuckets=\"$(aws s3 ls | awk \\'{print $3}\\')\"\\n# Put a bucket logging configuration on each bucket.\\nfor bucket in $buckets\\ndo\\n# This if statement excludes the logging bucket.\\nif [ \"$bucket\" != \"$loggingBucket\" ] ; then\\ncontinue;\\nfi\\nprintf \\'{\\n\"LoggingEnabled\": {\\n\"TargetBucket\": \"%s\",\\n\"TargetPrefix\": \"%s/\"\\n}\\n}\\' \"$loggingBucket\" \"$bucket\" > logging.json\\naws s3api put-bucket-logging --bucket $bucket --bucket-logging-status file://\\nlogging.json\\necho \"$bucket done\"\\ndone\\nrm logging.json\\necho \"Complete\"', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be\\nDOC-EXAMPLE-BUCKET1 [06/Feb/2019:00:00:38 +0000] 192.0.2.3\\n79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be 3E57427F3EXAMPLE\\nREST.GET.VERSIONING - \"GET /DOC-EXAMPLE-BUCKET1?versioning HTTP/1.1\" 200 - 113 - 7 -\\n\"-\" \"S3Console/0.4\" - s9lzHYrFp76ZVxRcpX9+5cjAnEH2ROuNkd2BHfIa6UkFVdtjf5mKR3/eTPFvsiP/\\nXV/VLi31234= SigV4 ECDHE-RSA-AES128-GCM-SHA256 AuthHeader DOC-EXAMPLE-BUCKET1.s3.us-\\nwest-1.amazonaws.com TLSV1.2 arn:aws:s3:us-west-1:123456789012:accesspoint/example-AP\\nYes\\n79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be\\nDOC-EXAMPLE-BUCKET1 [06/Feb/2019:00:00:38 +0000] 192.0.2.3', '']]]\n",
      "[[['', '79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be 891CE47D2EXAMPLE\\nREST.GET.LOGGING_STATUS - \"GET /DOC-EXAMPLE-BUCKET1?logging HTTP/1.1\" 200 -\\n242 - 11 - \"-\" \"S3Console/0.4\" - 9vKBE6vMhrNiWHZmb2L0mXOcqPGzQOI5XLnCtZNPxev+Hf\\n+7tpT6sxDwDty4LHBUOZJG96N1234= SigV4 ECDHE-RSA-AES128-GCM-SHA256 AuthHeader DOC-\\nEXAMPLE-BUCKET1.s3.us-west-1.amazonaws.com TLSV1.2 - -\\n79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be\\nDOC-EXAMPLE-BUCKET1 [06/Feb/2019:00:00:38 +0000] 192.0.2.3\\n79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be A1206F460EXAMPLE\\nREST.GET.BUCKETPOLICY - \"GET /DOC-EXAMPLE-BUCKET1?policy HTTP/1.1\" 404\\nNoSuchBucketPolicy 297 - 38 - \"-\" \"S3Console/0.4\" - BNaBsXZQQDbssi6xMBdBU2sLt\\n+Yf5kZDmeBUP35sFoKa3sLLeMC78iwEIWxs99CRUrbS4n11234= SigV4 ECDHE-RSA-AES128-GCM-SHA256\\nAuthHeader DOC-EXAMPLE-BUCKET1.s3.us-west-1.amazonaws.com TLSV1.2 - Yes\\n79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be\\nDOC-EXAMPLE-BUCKET1 [06/Feb/2019:00:01:00 +0000] 192.0.2.3\\n79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be 7B4A0FABBEXAMPLE\\nREST.GET.VERSIONING - \"GET /DOC-EXAMPLE-BUCKET1?versioning HTTP/1.1\" 200 -\\n113 - 33 - \"-\" \"S3Console/0.4\" - Ke1bUcazaN1jWuUlPJaxF64cQVpUEhoZKEG/hmy/gijN/\\nI1DeWqDfFvnpybfEseEME/u7ME1234= SigV4 ECDHE-RSA-AES128-GCM-SHA256 AuthHeader DOC-\\nEXAMPLE-BUCKET1.s3.us-west-1.amazonaws.com TLSV1.2 - -\\n79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be\\nDOC-EXAMPLE-BUCKET1 [06/Feb/2019:00:01:57 +0000] 192.0.2.3\\n79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be\\nDD6CC733AEXAMPLE REST.PUT.OBJECT s3-dg.pdf \"PUT /DOC-EXAMPLE-BUCKET1/\\ns3-dg.pdf HTTP/1.1\" 200 - - 4406583 41754 28 \"-\" \"S3Console/0.4\" -\\n10S62Zv81kBW7BB6SX4XJ48o6kpcl6LPwEoizZQQxJd5qDSCTLX0TgS37kYUBKQW3+bPdrg1234= SigV4\\nECDHE-RSA-AES128-SHA AuthHeader DOC-EXAMPLE-BUCKET1.s3.us-west-1.amazonaws.com TLSV1.2\\n- Yes', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nAny field can be set to - to indicate that the data was unknown or unavailable, or that the\\nfield was not applicable to this request.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be', ''], ['', '', '']], [['', '', ''], ['', 'DOC-EXAMPLE-BUCKET1', ''], ['', '', '']], [['', '', ''], ['', '[06/Feb/2019:00:00:38 +0000]', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '192.0.2.3', ''], ['', '', '']], [['', '', ''], ['', '79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:sts::123456789012:assumed-role/roleName/test-role', ''], ['', '', '']], [['', '', ''], ['', '3E57427F33A59F07', ''], ['', '', '']], [['', '', ''], ['', 'REST.PUT.OBJECT', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '/photos/2019/08/puppy.jpg', ''], ['', '', '']], [['', '', ''], ['', '\"GET /DOC-EXAMPLE-BUCKET1/photos/2019/08/puppy.jpg?x-foo=bar HTTP/1.1\"', ''], ['', '', '']], [['', '', ''], ['', '200', ''], ['', '', '']], [['', '', ''], ['', 'NoSuchBucket', ''], ['', '', '']], [['', '', ''], ['', '2662992', ''], ['', '', '']], [['', '', ''], ['', '3462992', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '70', ''], ['', '', '']], [['', '', ''], ['', '10', ''], ['', '', '']], [['', '', ''], ['', '\"http://www.example.com/webservices\"', ''], ['', '', '']], [['', '', ''], ['', '\"curl/7.15.1\"', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '3HL4kqtJvjVBH40Nrjfkd', ''], ['', '', '']], [['', '', ''], ['', 's9lzHYrFp76ZVxRcpX9+5cjAnEH2ROuNkd2BHfIa6UkFVdtjf5mKR3/eTPFvsiP/XV/VLi31234=', ''], ['', '', '']], [['', '', ''], ['', 'SigV2', ''], ['', '', '']], [['', '', ''], ['', 'ECDHE-RSA-AES128-GCM-SHA256', ''], ['', '', '']], [['', '', ''], ['', 'AuthHeader', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 's3.us-west-2.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'TLSv1.2', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3:us-east-1:123456789012:accesspoint/example-AP', ''], ['', '', '']], [['', '', ''], ['', 'Yes', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be', ''], ['', '', '']], [['', '', ''], ['', 'DOC-EXAMPLE-BUCKET1', ''], ['', '', '']], [['', '', ''], ['', '[06/Feb/2019:00:00:38 +0000]', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '192.0.2.3', ''], ['', '', '']], [['', '', ''], ['', '79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:sts::123456789012:assumed-role/roleName/test-role', ''], ['', '', '']], [['', '', ''], ['', '3E57427F33A59F07', ''], ['', '', '']], [['', '', ''], ['', 'REST.COPY.OBJECT_GET', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '/photos/2019/08/puppy.jpg', ''], ['', '', '']], [['', '', ''], ['', '\"GET /DOC-EXAMPLE-BUCKET1/photos/2019/08/puppy.jpg?x-foo=bar\"', ''], ['', '', '']], [['', '', ''], ['', '200', ''], ['', '', '']], [['', '', ''], ['', 'NoSuchBucket', ''], ['', '', '']], [['', '', ''], ['', '2662992', ''], ['', '', '']], [['', '', ''], ['', '3462992', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '70', ''], ['', '', '']], [['', '', ''], ['', '10', ''], ['', '', '']], [['', '', ''], ['', '\"http://www.example.com/webservices\"', ''], ['', '', '']], [['', '', ''], ['', '\"curl/7.15.1\"', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '3HL4kqtJvjVBH40Nrjfkd', ''], ['', '', '']], [['', '', ''], ['', 's9lzHYrFp76ZVxRcpX9+5cjAnEH2ROuNkd2BHfIa6UkFVdtjf5mKR3/eTPFvsiP/XV/VLi31234=', ''], ['', '', '']], [['', '', ''], ['', 'SigV4', ''], ['', '', '']], [['', '', ''], ['', 'ECDHE-RSA-AES128-GCM-SHA256', ''], ['', '', '']], [['', '', ''], ['', 'AuthHeader', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 's3.us-west-2.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'TLSv1.2', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3:us-east-1:123456789012:accesspoint/example-AP', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Yes', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• To identify Amazon S3 requests, we recommend that you use AWS CloudTrail data events\\ninstead of Amazon S3 server access logs. CloudTrail data events are easier to set up and\\ncontain more information. For more information, see Identifying Amazon S3 requests\\nusing CloudTrail.\\n• Depending on how many access requests you get, analyzing your logs might require more\\nresources or time than using CloudTrail data events.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo specify an Amazon S3 location in an Athena query, you must provide an S3 URI for the\\nbucket where your logs are delivered to. This URI must include the bucket name and prefix\\nin the following format: s3://example-s3-bucket1-logs/prefix/', ''], ['', '', '']], [['', '', ''], ['', 'CREATE DATABASE s3_access_logs_db', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIt's a best practice to create the database in the same AWS Region as your S3 bucket.\", ''], ['', '', '']], [['', '', ''], ['', 'CREATE EXTERNAL TABLE `s3_access_logs_db.mybucket_logs`(\\n`bucketowner` STRING,\\n`bucket_name` STRING,\\n`requestdatetime` STRING,\\n`remoteip` STRING,\\n`requester` STRING,\\n`requestid` STRING,\\n`operation` STRING,\\n`key` STRING,\\n`request_uri` STRING,\\n`httpstatus` STRING,\\n`errorcode` STRING,\\n`bytessent` BIGINT,\\n`objectsize` BIGINT,\\n`totaltime` STRING,\\n`turnaroundtime` STRING,', '']]]\n",
      "[[['', '`referrer` STRING,\\n`useragent` STRING,\\n`versionid` STRING,\\n`hostid` STRING,\\n`sigv` STRING,\\n`ciphersuite` STRING,\\n`authtype` STRING,\\n`endpoint` STRING,\\n`tlsversion` STRING,\\n`accesspointarn` STRING,\\n`aclrequired` STRING)\\nROW FORMAT SERDE\\n\\'org.apache.hadoop.hive.serde2.RegexSerDe\\'\\nWITH SERDEPROPERTIES (\\n\\'input.regex\\'=\\'([^ ]*) ([^ ]*) \\\\\\\\[(.*?)\\\\\\\\] ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*)\\n([^ ]*) (\\\\\"[^\\\\\"]*\\\\\"|-) (-|[0-9]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*)\\n(\\\\\"[^\\\\\"]*\\\\\"|-) ([^ ]*)(?: ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*)\\n([^ ]*))?.*$\\')\\nSTORED AS INPUTFORMAT\\n\\'org.apache.hadoop.mapred.TextInputFormat\\'\\nOUTPUTFORMAT\\n\\'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\\'\\nLOCATION\\n\\'s3://DOC-EXAMPLE-BUCKET1-logs/prefix/\\'', ''], ['', '', '']], [['', '', ''], ['', \"SELECT requestdatetime, remoteip, requester, key\\nFROM s3_access_logs_db.mybucket_logs\\nWHERE key = 'images/picture.jpg' AND operation like '%DELETE%';\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"SELECT *\\nFROM s3_access_logs_db.mybucket_logs\\nWHERE requester='arn:aws:iam::123456789123:user/user_name';\", ''], ['', '', '']], [['', '', ''], ['', \"SELECT *\\nFROM s3_access_logs_db.mybucket_logs\\nWHERE Key='prefix/images/picture.jpg'\\nAND parse_datetime(requestdatetime,'dd/MMM/yyyy:HH:mm:ss Z')\\nBETWEEN parse_datetime('2017-02-18:07:00:00','yyyy-MM-dd:HH:mm:ss')\\nAND parse_datetime('2017-02-18:08:00:00','yyyy-MM-dd:HH:mm:ss');\", ''], ['', '', '']], [['', '', ''], ['', \"SELECT coalesce(SUM(bytessent), 0) AS bytessenttotal\\nFROM s3_access_logs_db.mybucket_logs\\nWHERE remoteip='192.0.2.1'\\nAND parse_datetime(requestdatetime,'dd/MMM/yyyy:HH:mm:ss Z')\\nBETWEEN parse_datetime('2022-06-01','yyyy-MM-dd')\\nAND parse_datetime('2022-07-01','yyyy-MM-dd');\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo reduce the time that you retain your logs, you can create an S3 Lifecycle configuration\\nfor your server access logs bucket. Create lifecycle configuration rules to remove log files\\nperiodically. Doing so reduces the amount of data that Athena analyzes for each query. For\\nmore information, see Setting a lifecycle configuration on a bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo identify Signature Version 2 requests, we recommend that you use AWS CloudTrail\\ndata events instead of Amazon S3 server access logs. CloudTrail data events are easier to\\nset up and contain more information than server access logs. For more information, see\\nIdentifying Amazon S3 Signature Version 2 requests by using CloudTrail.', ''], ['', '', '']], [['', '', ''], ['', 'SELECT requester, sigv, Count(sigv) as sigcount\\nFROM s3_access_logs_db.mybucket_logs\\nGROUP BY requester, sigv;', ''], ['', '', '']], [['', '', ''], ['', \"SELECT bucket_name, requester, remoteip, key, httpstatus, errorcode, requestdatetime\\nFROM s3_access_logs_db\\nWHERE operation='REST.PUT.OBJECT' AND\\nparse_datetime(requestdatetime,'dd/MMM/yyyy:HH:mm:ss Z')\\nBETWEEN parse_datetime('2019-07-01:00:42:42','yyyy-MM-dd:HH:mm:ss')\\nAND\", '']]]\n",
      "[[['', \"parse_datetime('2019-07-02:00:42:42','yyyy-MM-dd:HH:mm:ss')\", ''], ['', '', '']], [['', '', ''], ['', \"SELECT bucket_name, requester, remoteip, key, httpstatus, errorcode, requestdatetime\\nFROM s3_access_logs_db\\nWHERE operation='REST.GET.OBJECT' AND\\nparse_datetime(requestdatetime,'dd/MMM/yyyy:HH:mm:ss Z')\\nBETWEEN parse_datetime('2019-07-01:00:42:42','yyyy-MM-dd:HH:mm:ss')\\nAND\\nparse_datetime('2019-07-02:00:42:42','yyyy-MM-dd:HH:mm:ss')\", ''], ['', '', '']], [['', '', ''], ['', \"SELECT bucket_name, requester, remoteip, key, httpstatus, errorcode, requestdatetime\\nFROM s3_access_logs_db.mybucket_logs\\nWHERE requester IS NULL AND\\nparse_datetime(requestdatetime,'dd/MMM/yyyy:HH:mm:ss Z')\\nBETWEEN parse_datetime('2019-07-01:00:42:42','yyyy-MM-dd:HH:mm:ss')\\nAND\\nparse_datetime('2019-07-02:00:42:42','yyyy-MM-dd:HH:mm:ss')\", ''], ['', '', '']], [['', '', ''], ['', 'SELECT bucket_name, requester, key, operation, aclrequired, requestdatetime', '']]]\n",
      "[[['', \"FROM s3_access_logs_db\\nWHERE aclrequired = 'Yes' AND\\nparse_datetime(requestdatetime,'dd/MMM/yyyy:HH:mm:ss Z')\\nBETWEEN parse_datetime('2022-05-10:00:00:00','yyyy-MM-dd:HH:mm:ss')\\nAND parse_datetime('2022-08-10:00:00:00','yyyy-MM-dd:HH:mm:ss')\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\n• You can modify the date range as needed to suit your needs.\\n• These query examples might also be useful for security monitoring. You can review\\nthe results for PutObject or GetObject calls from unexpected or unauthorized IP\\naddresses or requesters and for identifying any anonymous requests to your buckets.\\n• This query only retrieves information from the time at which logging was enabled.\\n• If you are using AWS CloudTrail logs, see Identifying access to S3 objects by using\\nCloudTrail.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['Metric', 'Description'], ['BucketSizeBytes', 'The amount of data in bytes that is stored in a bucket in the following\\nstorage classes:']]]\n",
      "[[['Metric', 'Description'], ['', '• S3 Standard (STANDARD)\\n• S3 Intelligent-Tiering (INTELLIGENT_TIERING )\\n• S3 Standard-Infrequent Access (STANDARD_IA )\\n• S3 One Zone-Infrequent Access (ONEZONE_IA )\\n• Reduced Redundancy Storage (RRS) (REDUCED_REDUNDANCY )\\n• S3 Glacier Instant Retrieval (GLACIER_IR )\\n• S3 Glacier Deep Archive (DEEP_ARCHIVE )\\n• S3 Glacier Flexible Retrieval (GLACIER)\\n• S3 Express One Zone (EXPRESS_ONEZONE )\\nThis value is calculated by summing the size of all objects and\\nmetadata (such as bucket names) in the bucket (both current and\\nnoncurrent objects), including the size of all parts for all incomplete\\nmultipart uploads to the bucket.\\nValid storage-type filters: StandardStorage , Intellige\\nntTieringFAStorage , IntelligentTieringIAStorage\\n, IntelligentTieringAAStorage , IntelligentTiering\\nAIAStorage , IntelligentTieringDAAStorage , StandardI\\nAStorage , StandardIASizeOverhead , StandardIAObjectOv\\nerhead , OneZoneIAStorage , OneZoneIASizeOverhead ,\\nReducedRedundancyStorage , GlacierInstantRetr\\nievalSizeOverhead GlacierInstantRetrievalStorage ,\\nGlacierStorage , GlacierStagingStorage , GlacierOb\\njectOverhead , GlacierS3ObjectOverhead , DeepArchi\\nveStorage , DeepArchiveObjectOverhead , DeepArchi\\nveS3ObjectOverhead , DeepArchiveStagingStorage , and\\nExpressOneZone (see the StorageType dimension)\\nUnits: Bytes\\nValid statistics: Average']]]\n",
      "[[['Metric', 'Description'], ['NumberOfObjects', \"The total number of objects stored in a general purpose bucket for all\\nstorage classes. This value is calculated by counting all objects in the\\nbucket, which includes current and noncurrent objects, delete markers,\\nand the total number of parts for all incomplete multipart uploads to\\nthe bucket. For directory buckets with objects in the S3 Express One\\nZone storage class, this value is calculated by counting all objects in\\nthe bucket, but it doesn't include incomplete multiple uploads to the\\nbucket.\\nValid storage type filters: AllStorageTypes (see the StorageTy\\npe dimension)\\nUnits: Count\\nValid statistics: Average\"]], [['', '', ''], ['', \"Note\\nAmazon S3 request metrics in CloudWatch aren't supported for directory buckets.\", ''], ['', '', '']], [['Metric', 'Description'], ['AllRequests', \"The total number of HTTP requests made to an Amazon S3 bucket,\\nregardless of type. If you're using a metrics configuration with a filter,\\nthen this metric returns only the HTTP requests that meet the filter's\\nrequirements.\\nUnits: Count\\nValid statistics: Sum\"]]]\n",
      "[[['Metric', 'Description'], ['GetRequests', \"The number of HTTP GET requests made for objects in an Amazon S3\\nbucket. This doesn't include list operations. This metric is incremented\\nfor the source of each CopyObject request.\\nUnits: Count\\nValid statistics: Sum\\nNote\\nPaginated list-oriented requests, such as ListMultipartUploads,\\nListParts, ListObjectVersions, and others, are not included in\\nthis metric.\"], ['PutRequests', 'The number of HTTP PUT requests made for objects in an Amazon\\nS3 bucket. This metric is incremented for the destination of each\\nCopyObject request.\\nUnits: Count\\nValid statistics: Sum'], ['DeleteRequests', 'The number of HTTP DELETE requests made for objects in an Amazon\\nS3 bucket. This metric also includes DeleteObjects requests. This\\nmetric shows the number of requests made, not the number of objects\\ndeleted.\\nUnits: Count\\nValid statistics: Sum'], ['HeadRequests', 'The number of HTTP HEAD requests made to an Amazon S3 bucket.\\nUnits: Count\\nValid statistics: Sum']], [['', '', ''], ['', 'Note\\nPaginated list-oriented requests, such as ListMultipartUploads,\\nListParts, ListObjectVersions, and others, are not included in\\nthis metric.', ''], ['', '', '']]]\n",
      "[[['Metric', 'Description'], ['PostRequests', 'The number of HTTP POST requests made to an Amazon S3 bucket.\\nUnits: Count\\nValid statistics: Sum\\nNote\\nDeleteObjects and SelectObjectContent requests are not\\nincluded in this metric.'], ['SelectRequests', 'The number of Amazon S3 SelectObjectContent requests made for\\nobjects in an Amazon S3 bucket.\\nUnits: Count\\nValid statistics: Sum'], ['SelectByt\\nesScanned', 'The number of bytes of data scanned with Amazon S3 SelectObj\\nectContent requests in an Amazon S3 bucket.\\nUnits: Bytes\\nValid statistics: Average (bytes per request), Sum (bytes per period),\\nSample Count, Min, Max (same as p100), any percentile between p0.0\\nand p99.9'], ['SelectByt\\nesReturned', 'The number of bytes of data returned with Amazon S3 SelectObj\\nectContent requests in an Amazon S3 bucket.\\nUnits: Bytes\\nValid statistics: Average (bytes per request), Sum (bytes per period),\\nSample Count, Min, Max (same as p100), any percentile between p0.0\\nand p99.9']], [['', '', ''], ['', 'Note\\nDeleteObjects and SelectObjectContent requests are not\\nincluded in this metric.', ''], ['', '', '']]]\n",
      "[[['Metric', 'Description'], ['ListRequests', 'The number of HTTP requests that list the contents of a bucket.\\nUnits: Count\\nValid statistics: Sum'], ['BytesDownloaded', 'The number of bytes downloaded for requests made to an Amazon S3\\nbucket, where the response includes a body.\\nUnits: Bytes\\nValid statistics: Average (bytes per request), Sum (bytes per period),\\nSample Count, Min, Max (same as p100), any percentile between p0.0\\nand p99.9'], ['BytesUploaded', 'The number of bytes uploaded for requests made to an Amazon S3\\nbucket, where the request includes a body.\\nUnits: Bytes\\nValid statistics: Average (bytes per request), Sum (bytes per period),\\nSample Count, Min, Max (same as p100), any percentile between p0.0\\nand p99.9'], ['4xxErrors', 'The number of HTTP 4xx client error status code requests made to an\\nAmazon S3 bucket with a value of either 0 or 1. The Average statistic\\nshows the error rate, and the Sum statistic shows the count of that type\\nof error, during each period.\\nUnits: Count\\nValid statistics: Average (reports per request), Sum (reports per period),\\nMin, Max, Sample Count']]]\n",
      "[[['Metric', 'Description'], ['5xxErrors', 'The number of HTTP 5xx server error status code requests made to an\\nAmazon S3 bucket with a value of either 0 or 1. The Average statistic\\nshows the error rate, and the Sum statistic shows the count of that type\\nof error, during each period.\\nUnits: Count\\nValid statistics: Average (reports per request), Sum (reports per period),\\nMin, Max, Sample Count'], ['FirstByte\\nLatency', 'The per-request time from the complete request being received by an\\nAmazon S3 bucket to when the response starts to be returned.\\nUnits: Milliseconds\\nValid statistics: Average, Sum, Min, Max (same as p100), Sample Count,\\nany percentile between p0.0 and p100'], ['TotalRequ\\nestLatency', 'The elapsed per-request time from the first byte received to the last\\nbyte sent to an Amazon S3 bucket. This metric includes the time taken\\nto receive the request body and send the response body, which is not\\nincluded in FirstByteLatency .\\nUnits: Milliseconds\\nValid statistics: Average, Sum, Min, Max (same as p100), Sample Count,\\nany percentile between p0.0 and p100']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou can enable alarms for your replication metrics in Amazon CloudWatch. When you\\nset up alarms for your replication metrics, set the Missing data treatment field to Treat\\nmissing data as ignore (maintain the alarm state).', ''], ['', '', '']], [['Metric', 'Description'], ['Replicati\\nonLatency', 'The maximum number of seconds by which the replication destination\\nAWS Region is behind the source AWS Region for a given replication\\nrule.\\nUnits: Seconds\\nValid statistics: Max'], ['BytesPend\\ningReplication', 'The total number of bytes of objects pending replication for a given\\nreplication rule.\\nUnits: Bytes\\nValid statistics: Max'], ['Operation\\nsPendingR\\neplication', 'The number of operations pending replication for a given replication\\nrule.\\nUnits: Count\\nValid statistics: Max'], ['Operation\\nsFailedRe\\nplication', 'The number of operations that failed to replicate for a given replication\\nrule.\\nUnits: Count\\nValid statistics: Sum (total number of failed operations), Average\\n(failure rate), Sample Count (total number of replication operations)']]]\n",
      "[[['Metric', 'Description'], ['AllRequests', 'The total number of HTTP requests made to an Amazon S3 bucket by\\nusing an Object Lambda Access Point.\\nUnits: Count\\nValid statistics: Sum'], ['GetRequests', 'The number of HTTP GET requests made for objects by using an Object\\nLambda Access Point. This metric does not include list operations.\\nUnits: Count\\nValid statistics: Sum'], ['BytesUploaded', 'The number of bytes uploaded to an Amazon S3 bucket by using an\\nObject Lambda Access Point, where the request includes a body.\\nUnits: Bytes\\nValid statistics: Average (bytes per request), Sum (bytes per period),\\nSample Count, Min, Max (same as p100), any percentile between p0.0\\nand p99.9']]]\n",
      "[[['Metric', 'Description'], ['PostRequests', 'The number of HTTP POST requests made to an Amazon S3 bucket by\\nusing an Object Lambda Access Point.\\nUnits: Count\\nValid statistics: Sum'], ['PutRequests', 'The number of HTTP PUT requests made for objects in an Amazon S3\\nbucket by using an Object Lambda Access Point.\\nUnits: Count\\nValid statistics: Sum'], ['DeleteRequests', 'The number of HTTP DELETE requests made for objects in an Amazon\\nS3 bucket by using an Object Lambda Access Point. This metric includes\\nDeleteObjects requests. This metric shows the number of requests\\nmade, not the number of objects deleted.\\nUnits: Count\\nValid statistics: Sum'], ['BytesDownloaded', 'The number of bytes downloaded for requests made to an Amazon S3\\nbucket by using an Object Lambda Access Point, where the response\\nincludes a body.\\nUnits: Bytes\\nValid statistics: Average (bytes per request), Sum (bytes per period),\\nSample Count, Min, Max (same as p100), any percentile between p0.0\\nand p99.9']]]\n",
      "[[['Metric', 'Description'], ['FirstByte\\nLatency', \"The per-request time from the complete request being received by an\\nAmazon S3 bucket through an Object Lambda Access Point to when\\nthe response starts to be returned. This metric is dependent on the\\nAWS Lambda function's running time to transform the object before\\nthe function returns the bytes to the Object Lambda Access Point.\\nUnits: Milliseconds\\nValid statistics: Average, Sum, Min, Max (same as p100), Sample Count,\\nany percentile between p0.0 and p100\"], ['TotalRequ\\nestLatency', 'The elapsed per-request time from the first byte received to the last\\nbyte sent to an Object Lambda Access Point. This metric includes the\\ntime taken to receive the request body and send the response body,\\nwhich is not included in FirstByteLatency .\\nUnits: Milliseconds\\nValid statistics: Average, Sum, Min, Max (same as p100), Sample Count,\\nany percentile between p0.0 and p100'], ['HeadRequests', 'The number of HTTP HEAD requests made to an Amazon S3 bucket by\\nusing an Object Lambda Access Point.\\nUnits: Count\\nValid statistics: Sum'], ['ListRequests', 'The number of HTTP GET requests that list the contents of an Amazon\\nS3 bucket. This metric includes both ListObjects and ListObjec\\ntsV2 operations.\\nUnits: Count\\nValid statistics: Sum']]]\n",
      "[[['Metric', 'Description'], ['4xxErrors', 'The number of HTTP 4xx server error status code requests made to\\nan Amazon S3 bucket by using an Object Lambda Access Point with a\\nvalue of either 0 or 1. The Average statistic shows the error rate, and\\nthe Sum statistic shows the count of that type of error, during each\\nperiod.\\nUnits: Count\\nValid statistics: Average (reports per request), Sum (reports per period),\\nMin, Max, Sample Count'], ['5xxErrors', 'The number of HTTP 5xx server error status code requests made to\\nan Amazon S3 bucket by using an Object Lambda Access Point with a\\nvalue of either 0 or 1. The Average statistic shows the error rate, and\\nthe Sum statistic shows the count of that type of error, during each\\nperiod.\\nUnits: Count\\nValid statistics: Average (reports per request), Sum (reports per period),\\nMin, Max, Sample Count'], ['ProxiedRequests', 'The number of HTTP requests to an Object Lambda Access Point that\\nreturn the standard Amazon S3 API response. (Such requests do not\\nhave a Lambda function configured.)\\nUnits: Count\\nValid statistics: Sum'], ['InvokedLambda', 'The number of HTTP requests to an S3 object where a Lambda\\nfunction was invoked.\\nUnits: Count\\nValid statistics: Sum']]]\n",
      "[[['Metric', 'Description'], ['LambdaRes\\nponseRequests', 'The number of WriteGetObjectResponse requests made by the\\nLambda function. This metric applies only to GetObject requests.'], ['LambdaRes\\nponse4xx', 'The number of HTTP 4xx client errors that occur when calling\\nWriteGetObjectResponse from a Lambda function. This\\nmetric provides the same information as 4xxErrors , but only for\\nWriteGetObjectResponse calls.'], ['LambdaRes\\nponse5xx', 'The number of HTTP 5xx server errors that occur when calling\\nWriteGetObjectResponse from a Lambda function. This\\nmetric provides the same information as 5xxErrors , but only for\\nWriteGetObjectResponse calls.']], [['Dimension', 'Description'], ['BucketName', 'This dimension filters the data that you request for the\\nidentified bucket only.'], ['StorageType', 'This dimension filters the data that you have stored in a bucket\\nby the following types of storage:\\n• StandardStorage – The number of bytes used for objects\\nin the STANDARD storage class.\\n• IntelligentTieringAAStorage – The number\\nof bytes used for objects in the Archive Access tier of the\\nINTELLIGENT_TIERING storage class.']]]\n",
      "[[['Dimension', 'Description'], ['', '• IntelligentTieringAIAStorage – The number of\\nbytes used for objects in the Archive Instant Access tier of\\nthe INTELLIGENT_TIERING storage class.\\n• IntelligentTieringDAAStorage – The number of\\nbytes used for objects in the Deep Archive Access tier of the\\nINTELLIGENT_TIERING storage class.\\n• IntelligentTieringFAStorage – The number of\\nbytes used for objects in the Frequent Access tier of the\\nINTELLIGENT_TIERING storage class.\\n• IntelligentTieringIAStorage – The number of\\nbytes used for objects in the Infrequent Access tier of the\\nINTELLIGENT_TIERING storage class.\\n• StandardIAStorage – The number of bytes used for\\nobjects in the S3 Standard-Infrequent Access (STANDARD_\\nIA ) storage class.\\n• StandardIASizeOverhead – The number of bytes\\nused for objects smaller than 128 KB in the STANDARD_IA\\nstorage class.\\n• IntAAObjectOverhead – For each object in the\\nINTELLIGENT_TIERING storage class in the Archive\\nAccess tier, S3 Glacier adds 32 KB of storage for index and\\nrelated metadata. This extra data is necessary to identify\\nand restore your object. You are charged S3 Glacier Flexible\\nRetrieval rates for this additional storage.\\n• IntAAS3ObjectOverhead – For each object in the\\nINTELLIGENT_TIERING storage class in the Archive\\nAccess tier, Amazon S3 uses 8 KB of storage for the name of\\nthe object and other metadata. You are charged S3 Standard\\nrates for this additional storage.\\n• IntDAAObjectOverhead – For each object in the\\nINTELLIGENT_TIERING storage class in the Deep\\nArchive Access tier, S3 Glacier adds 32 KB of storage for\\nindex and related metadata. This extra data is necessary to']]]\n",
      "[[['Dimension', 'Description'], ['', 'identify and restore your object. You are charged S3 Glacier\\nDeep Archive storage rates for this additional storage.\\n• IntDAAS3ObjectOverhead – For each object in the\\nINTELLIGENT_TIERING storage class in the Deep\\nArchive Access tier, Amazon S3 adds 8 KB of storage for\\nindex and related metadata. This extra data is necessary\\nto identify and restore your object. You are charged S3\\nStandard rates for this additional storage.\\n• OneZoneIAStorage – The number of bytes used for\\nobjects in the S3 One Zone-Infrequent Access (ONEZONE_I\\nA ) storage class.\\n• OneZoneIASizeOverhead – The number of bytes used\\nfor objects smaller than 128 KB in the ONEZONE_IA storage\\nclass.\\n• ReducedRedundancyStorage – The number of bytes\\nused for objects in the Reduced Redundancy Storage (RRS)\\nclass.\\n• GlacierInstantRetrievalSizeOverhead – The\\nnumber of bytes used for objects smaller than 128 KB in the\\nS3 Glacier Instant Retrieval storage class.\\n• GlacierInstantRetrievalStorage – The number\\nof bytes used for objects in the S3 Glacier Instant Retrieval\\nstorage class.\\n• GlacierStorage – The number of bytes used for objects\\nin the S3 Glacier Flexible Retrieval storage class.\\n• GlacierStagingStorage – The number of bytes\\nused for parts of multipart objects before the CompleteM\\nultipartUpload request is completed on objects in the\\nS3 Glacier Flexible Retrieval storage class.\\n• GlacierObjectOverhead – For each archived object,\\nS3 Glacier adds 32 KB of storage for index and related\\nmetadata. This extra data is necessary to identify and restore']]]\n",
      "[[['Dimension', 'Description'], ['', 'your object. You are charged S3 Glacier Flexible Retrieval\\nrates for this additional storage.\\n• GlacierS3ObjectOverhead – For each object archived\\nto S3 Glacier Flexible Retrieval, Amazon S3 uses 8 KB of\\nstorage for the name of the object and other metadata. You\\nare charged S3 Standard rates for this additional storage.\\n• DeepArchiveStorage – The number of bytes used for\\nobjects in the S3 Glacier Deep Archive storage class.\\n• DeepArchiveObjectOverhead – For each archived\\nobject, S3 Glacier adds 32 KB of storage for index and\\nrelated metadata. This extra data is necessary to identify and\\nrestore your object. You are charged S3 Glacier Deep Archive\\nrates for this additional storage.\\n• DeepArchiveS3ObjectOverhead – For each object\\narchived to S3 Glacier Deep Archive, Amazon S3 uses 8 KB of\\nstorage for the name of the object and other metadata. You\\nare charged S3 Standard rates for this additional storage.\\n• DeepArchiveStagingStorage – The number of bytes\\nused for parts of multipart objects before the CompleteM\\nultipartUpload request is completed on objects in the\\nS3 Glacier Deep Archive storage class.\\n• ExpressOneZone – The number of bytes used for objects\\nin the S3 Express One Zone storage class.'], ['FilterId', 'This dimension filters metrics configurations that you specify\\nfor the request metrics on a bucket. When you create a metrics\\nconfiguration, you specify a filter ID (for example, a prefix, a\\ntag, or an access point). For more information, see Creating a\\nmetrics configuration.']]]\n",
      "[[['Dimension', 'Description'], ['SourceBucket', 'The name of the bucket objects are replicated from.'], ['DestinationBucket', 'The name of the bucket objects are replicated to.'], ['RuleId', 'A unique identifier for the rule that triggered this replication\\nmetric to update.']], [['Dimension', 'Description'], ['AccessPointName', 'The name of the access point of which requests are being made.'], ['DataSourceARN', 'The source the Object Lambda Access Point is retrieving the data from.\\nIf the request invokes a Lambda function this refers to the Lambda\\nAmazon Resource Name (ARN). Otherwise this refers to the access\\npoint ARN.']]]\n",
      "[[['', '', ''], ['', 'aws cloudwatch get-metric-statistics --metric-name BucketSizeBytes --namespace AWS/S3\\n--start-time 2016-10-19T00:00:00Z --end-time 2016-10-20T00:00:00Z --statistics Average\\n--unit Bytes --region us-west-2 --dimensions Name=BucketName,Value=DOC-EXAMPLE-BUCKET\\nName=StorageType,Value=StandardStorage --period 86400 --output json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Datapoints\": [\\n{\\n\"Timestamp\": \"2016-10-19T00:00:00Z\",\\n\"Average\": 1025328.0,\\n\"Unit\": \"Bytes\"\\n}\\n],\\n\"Label\": \"BucketSizeBytes\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws cloudwatch list-metrics --namespace \"AWS/S3\"', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-metrics-configuration --endpoint https://s3.us-\\nwest-2.amazonaws.com --bucket bucket-name --id metrics-config-id --metrics-\\nconfiguration \\'{\"Id\":\"metrics-config-id\"}\\'', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen you create a metrics configuration that filters by access point, you must use the\\naccess point Amazon Resource Name (ARN), not the access point alias. Make sure that\\nyou use the ARN for the access point itself, not the ARN for a specific object. For more\\ninformation about access point ARNs, see Using access points.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nYou cannot enter an access point alias. You must enter the ARN for the access point\\nitself, not the ARN for a specific object.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-metrics-configuration --bucket DOC-EXAMPLE-BUCKET1 --\\nid metrics-config-id --metrics-configuration \\'{\"Id\":\"metrics-config-id\", \"Filter\":\\n{\"Prefix\":\"prefix1\"}} \\'', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-metrics-configuration --bucket DOC-EXAMPLE-BUCKET1 --\\nid metrics-config-id --metrics-configuration \\'{\"Id\":\"metrics-config-id\", \"Filter\":\\n{\"Tag\": {\"Key\": \"string\", \"Value\": \"string\"}} \\'', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-metrics-configuration --bucket DOC-EXAMPLE-BUCKET1 --\\nid metrics-config-id --metrics-configuration \\'{\"Id\":\"metrics-config-id\", \"Filter\":\\n{\"AccessPointArn\":\"arn:aws:s3:Region:account-id:accesspoint/access-point-name\"}} \\'', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-metrics-configuration --endpoint https://\\ns3.Region.amazonaws.com --bucket DOC-EXAMPLE-BUCKET1 --id metrics-config-id --\\nmetrics-configuration \\'\\n{\\n\"Id\": \"metrics-config-id\",\\n\"Filter\": {\\n\"And\": {\\n\"Prefix\": \"string\",', '']]]\n",
      "[[['', '\"Tags\": [\\n{\\n\"Key\": \"string\",\\n\"Value\": \"string\"\\n}\\n],\\n\"AccessPointArn\": \"arn:aws:s3:Region:account-id:accesspoint/access-\\npoint-name\"\\n}\\n}\\n}\\'', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nDeleting a filter cannot be undone.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAmazon S3 event notifications are designed to be delivered at least once. Typically, event\\nnotifications are delivered in seconds but can sometimes take a minute or longer.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nAmazon Simple Queue Service FIFO (First-In-First-Out) queues aren't supported as an\\nAmazon S3 event notification destination. To send a notification for an Amazon S3 event to\\nan Amazon SQS FIFO queue, you can use Amazon EventBridge. For more information, see\\nEnabling Amazon EventBridge.\", ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nIf your notification writes to the same bucket that triggers the notification, it could cause\\nan execution loop. For example, if the bucket triggers a Lambda function each time an\\nobject is uploaded, and the function uploads an object to the bucket, then the function\\nindirectly triggers itself. To avoid this, use two buckets, or configure the trigger to only\\napply to a prefix used for incoming objects.\\nFor more information and an example of using Amazon S3 notifications with AWS Lambda,\\nsee Using AWS Lambda with Amazon S3 in the AWS Lambda Developer Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou must grant Amazon S3 permissions to post messages to an Amazon SNS topic or an\\nAmazon SQS queue. You must also grant Amazon S3 permission to invoke an AWS Lambda\\nfunction on your behalf. For instructions on how to grant these permissions, see Granting\\npermissions to publish event notification messages to a destination.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nAmazon Simple Queue Service FIFO (First-In-First-Out) queues aren't supported as an\\nAmazon S3 event notification destination. To send a notification for an Amazon S3 event to\\nan Amazon SQS FIFO queue, you can use Amazon EventBridge. For more information, see\\nEnabling Amazon EventBridge.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nIf your notification writes to the same bucket that triggers the notification, it could cause\\nan execution loop. For example, if the bucket triggers a Lambda function each time an\\nobject is uploaded, and the function uploads an object to the bucket, then the function\\nindirectly triggers itself. To avoid this, use two buckets, or configure the trigger to only\\napply to a prefix used for incoming objects.\\nFor more information and an example of using Amazon S3 notifications with AWS Lambda,\\nsee Using AWS Lambda with Amazon S3 in the AWS Lambda Developer Guide.', ''], ['', '', '']]]\n",
      "[[['Event types', 'Description'], ['s3:TestEvent', \"When a notification is enabled, Amazon S3 publishes a\\ntest notification. This is to ensure that the topic exists\\nand that the bucket owner has permission to publish the\\nspecified topic.\\nIf enabling the notification fails, you don't receive a test\\nnotification.\"], ['s3:ObjectCreated:*\\ns3:ObjectCreated:Put\\ns3:ObjectCreated:Post\\ns3:ObjectCreated:Copy\\ns3:ObjectCreated:CompleteMu\\nltipartUpload', 'Amazon S3 API operations such as PUT, POST, and COPY\\ncan create an object. With these event types, you can\\nenable notifications when an object is created using a\\nspecific API operation. Alternatively, you can use the\\ns3:ObjectCreated:* event type to request notificat\\nion regardless of the API that was used to create an\\nobject.\\ns3:ObjectCreated:CompleteMultipartUpload\\nincludes objects that are created using UploadPartCopy\\nfor Copy operations.'], ['s3:ObjectRemoved:*\\ns3:ObjectRemoved:Delete\\ns3:ObjectRemoved:DeleteMark\\nerCreated', 'By using the ObjectRemoved event types, you can enable\\nnotification when an object or a batch of objects is\\nremoved from a bucket.\\nYou can request notification when an object is deleted or\\na versioned object is permanently deleted by using the\\ns3:ObjectRemoved:Delete event type. Alternati\\nvely, you can request notification when a delete marker\\nis created for a versioned object using s3:Object\\nRemoved:DeleteMarkerCreated . For instructi\\nons on how to delete versioned objects, see Deleting\\nobject versions from a versioning-enabled bucket. You can\\nalso use a wildcard s3:ObjectRemoved:* to request\\nnotification anytime an object is deleted.']]]\n",
      "[[['Event types', 'Description'], ['', \"These event notifications don't alert you for automatic\\ndeletes from lifecycle configurations or from failed\\noperations.\"], ['s3:ObjectRestore:*\\ns3:ObjectRestore:Post\\ns3:ObjectRestore:Completed\\ns3:ObjectRestore:Delete', 'By using the ObjectRestore event types, you can receive\\nnotifications for event initiation and completion when\\nrestoring objects from the S3 Glacier Flexible Retrieval\\nstorage class, S3 Glacier Deep Archive storage class, S3\\nIntelligent-Tiering Archive Access tier, and S3 Intellige\\nnt-Tiering Deep Archive Access tier. You can also receive\\nnotifications for when the restored copy of an object\\nexpires.\\nThe s3:ObjectRestore:Post event type notifies\\nyou of object restoration initiation. The s3:Object\\nRestore:Completed event type notifies you of\\nrestoration completion. The s3:ObjectRestore:D\\nelete event type notifies you when the temporary copy\\nof a restored object expires.'], ['s3:ReducedRedundancyLostObject', 'You receive this notification event when Amazon S3\\ndetects that an object of the RRS storage class is lost.']]]\n",
      "[[['Event types', 'Description'], ['s3:Replication:*\\ns3:Replication:OperationFai\\nledReplication\\ns3:Replication:OperationMis\\nsedThreshold\\ns3:Replication:OperationRep\\nlicatedAfterThreshold\\ns3:Replication:OperationNot\\nTracked', 'By using the Replication event types, you can receive\\nnotifications for replication configurations that have S3\\nReplication metrics or S3 Replication Time Control (S3\\nRTC) enabled. You can monitor the minute-by-minute\\nprogress of replication events by tracking bytes pending,\\noperations pending, and replication latency. For informati\\non about replication metrics, see Monitoring progress with\\nreplication metrics and S3 Event Notifications.\\nThe s3:Replication:OperationFailedReplic\\nation event type notifies you when an object that was\\neligible for replication failed to replicate. The s3:Replic\\nation:OperationMissedThreshold event type\\nnotifies you when an object that was eligible for replicati\\non exceeds the 15-minute threshold for replication.\\nThe s3:Replication:OperationReplicatedAf\\nterThreshold event type notifies you when an object\\nthat was eligible for replication that uses S3 Replication\\nTime Control replicates after the 15-minute threshold\\n. The s3:Replication:OperationNotTracked\\nevent type notifies you when an object that was eligible\\nfor replication that uses S3 Replication Time Control but is\\nno longer tracked by replication metrics.']]]\n",
      "[[['Event types', 'Description'], ['s3:LifecycleExpiration:*\\ns3:LifecycleExpiration:Delete\\ns3:LifecycleExpiration:Dele\\nteMarkerCreated', 'By using the LifecycleExpiration event types, you can\\nreceive a notification when Amazon S3 deletes an object\\nbased on your S3 Lifecycle configuration.\\nThe s3:LifecycleExpiration:Delete event type\\nnotifies you when an object in an unversioned bucket\\nis deleted. It also notifies you when an object version is\\npermanently deleted by an S3 Lifecycle configuration.\\nThe s3:LifecycleExpiration:DeleteMarkerC\\nreated event type notifies you when S3 Lifecycle\\ncreates a delete marker when a current version of an\\nobject in versioned bucket is deleted.'], ['s3:LifecycleTransition', 'You receive this notification event when an object is\\ntransitioned to another Amazon S3 storage class by an S3\\nLifecycle configuration.'], ['s3:IntelligentTiering', 'You receive this notification event when an object within\\nthe S3 Intelligent-Tiering storage class moved to the\\nArchive Access tier or Deep Archive Access tier.'], ['s3:ObjectTagging:*\\ns3:ObjectTagging:Put\\ns3:ObjectTagging:Delete', 'By using the ObjectTagging event types, you can enable\\nnotification when an object tag is added or deleted from\\nan object.\\nThe s3:ObjectTagging:Put event type notifies\\nyou when a tag is PUT on an object or an existing tag is\\nupdated. The s3:ObjectTagging:Delete event type\\nnotifies you when a tag is removed from an object.'], ['s3:ObjectAcl:Put', 'You receive this notification event when an ACL is PUT on\\nan object or when an existing ACL is changed. An event is\\nnot generated when a request results in no change to an\\nobject’s ACL.']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou can also make the Amazon S3 REST API calls directly from your code. However, this\\ncan be cumbersome because to do so you must write code to authenticate your requests.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"example-ID\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Example SNS topic policy\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"SNS:Publish\"\\n],\\n\"Resource\": \"SNS-topic-ARN\",\\n\"Condition\": {\\n\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:*:*:bucket-name\"\\n},\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"bucket-owner-account-id\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"example-ID\",\\n\"Statement\": [\\n{\\n\"Sid\": \"example-statement-ID\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"SQS:SendMessage\"\\n],\\n\"Resource\": \"arn:aws:sqs:Region:account-id:queue-name\",\\n\"Condition\": {\\n\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:*:*:awsexamplebucket1\"\\n},\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"bucket-owner-account-id\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '\"Condition\": {\\n\"StringLike\": { \"aws:SourceArn\": \"arn:aws:s3:*:*:bucket-name\" }\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"example-ID\",\\n\"Statement\": [\\n{\\n\"Sid\": \"example-statement-ID\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"kms:GenerateDataKey\",\\n\"kms:Decrypt\"\\n],\\n\"Resource\": \"*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nAmazon Simple Queue Service FIFO (First-In-First-Out) queues aren't supported as an\\nAmazon S3 event notification destination. To send a notification for an Amazon S3 event to\\nan Amazon SQS FIFO queue, you can use Amazon EventBridge. For more information, see\\nEnabling Amazon EventBridge.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nBefore you can publish event notifications, you must grant the Amazon S3 principal\\nthe necessary permissions to call the relevant API. This is so that it can publish\\nnotifications to a Lambda function, SNS topic, or SQS queue.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<NotificationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n</NotificationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<NotificationConfiguration>\\n<QueueConfiguration>\\n<Id>optional-id-string</Id>\\n<Queue>sqs-queue-arn</Queue>\\n<Event>event-type</Event>\\n<Event>event-type</Event>\\n...\\n</QueueConfiguration>\\n...\\n</NotificationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<NotificationConfiguration>\\n<TopicConfiguration>\\n<Id>optional-id-string</Id>\\n<Topic>sns-topic-arn</Topic>\\n<Event>event-type</Event>\\n<Event>event-type</Event>', '']]]\n",
      "[[['', '...\\n</TopicConfiguration>\\n...\\n</NotificationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<NotificationConfiguration>\\n<CloudFunctionConfiguration>\\n<Id>optional-id-string</Id>\\n<CloudFunction>cloud-function-arn</CloudFunction>\\n<Event>event-type</Event>\\n<Event>event-type</Event>\\n...\\n</CloudFunctionConfiguration>\\n...\\n</NotificationConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nAmazon Simple Queue Service FIFO (First-In-First-Out) queues aren't supported as an\\nAmazon S3 event notification destination. To send a notification for an Amazon S3 event to\\nan Amazon SQS FIFO queue, you can use Amazon EventBridge. For more information, see\\nEnabling Amazon EventBridge.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"example-ID\",\\n\"Statement\": [\\n{\\n\"Sid\": \"example-statement-ID\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"SQS:SendMessage\"\\n],\\n\"Resource\": \"SQS-queue-ARN\",\\n\"Condition\": {', '']]]\n",
      "[[['', '\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:*:*:awsexamplebucket1\"\\n},\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"bucket-owner-account-id\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"example-ID\",\\n\"Statement\": [\\n{\\n\"Sid\": \"example-statement-ID\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"kms:GenerateDataKey\",\\n\"kms:Decrypt\"\\n],\\n\"Resource\": \"*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'arn:aws:sqs:aws-region:account-id:queue-name', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"example-ID\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Example SNS topic policy\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"SNS:Publish\"\\n],\\n\"Resource\": \"SNS-topic-ARN\",\\n\"Condition\": {', '']]]\n",
      "[[['', '\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:*:*:bucket-name\"\\n},\\n\"StringEquals\": {\\n\"aws:SourceAccount\": \"bucket-owner-account-id\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:sns:aws-region:account-id:topic-name', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass EnableNotificationsTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\nprivate const string snsTopic = \"*** SNS topic ARN ***\";\\nprivate const string sqsQueue = \"*** SQS topic ARN ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nclient = new AmazonS3Client(bucketRegion);\\nEnableNotificationAsync().Wait();\\n}\\nstatic async Task EnableNotificationAsync()\\n{\\ntry\\n{\\nPutBucketNotificationRequest request = new\\nPutBucketNotificationRequest\\n{\\nBucketName = bucketName\\n};', '']]]\n",
      "[[['', 'TopicConfiguration c = new TopicConfiguration\\n{\\nEvents = new List<EventType> { EventType.ObjectCreatedCopy },\\nTopic = snsTopic\\n};\\nrequest.TopicConfigurations = new List<TopicConfiguration>();\\nrequest.TopicConfigurations.Add(c);\\nrequest.QueueConfigurations = new List<QueueConfiguration>();\\nrequest.QueueConfigurations.Add(new QueueConfiguration()\\n{\\nEvents = new List<EventType> { EventType.ObjectCreatedPut },\\nQueue = sqsQueue\\n});\\nPutBucketNotificationResponse response = await\\nclient.PutBucketNotificationAsync(request);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' \",\\ne.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown error encountered on server.\\nMessage:\\'{0}\\' \", e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;', '']]]\n",
      "[[['', 'import com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.*;\\nimport java.io.IOException;\\nimport java.util.EnumSet;\\npublic class EnableNotificationOnABucket {\\npublic static void main(String[] args) throws IOException {\\nString bucketName = \"*** Bucket name ***\";\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString snsTopicARN = \"*** SNS Topic ARN ***\";\\nString sqsQueueARN = \"*** SQS Queue ARN ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\nBucketNotificationConfiguration notificationConfiguration = new\\nBucketNotificationConfiguration();\\n// Add an SNS topic notification.\\nnotificationConfiguration.addConfiguration(\"snsTopicConfig\",\\nnew TopicConfiguration(snsTopicARN,\\nEnumSet.of(S3Event.ObjectCreated)));\\n// Add an SQS queue notification.\\nnotificationConfiguration.addConfiguration(\"sqsQueueConfig\",\\nnew QueueConfiguration(sqsQueueARN,\\nEnumSet.of(S3Event.ObjectCreated)));\\n// Create the notification configuration request and set the bucket\\nnotification\\n// configuration.\\nSetBucketNotificationConfigurationRequest request = new\\nSetBucketNotificationConfigurationRequest(\\nbucketName, notificationConfiguration);\\ns3Client.setBucketNotificationConfiguration(request);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();', '']]]\n",
      "[[['', \"} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nA wildcard character (\"*\") can\\'t be used in filters as a prefix or suffix. If your prefix or\\nsuffix contains a space, you must replace it with the \"+\" character. If you use any other\\nspecial characters in the value of the prefix or suffix, you must enter them in URL-encoded\\n(percent-encoded) format. For a complete list of special characters that must be converted\\nto URL-encoded format when used in a prefix or suffix for event notifications, see Safe\\ncharacters.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<NotificationConfiguration>\\n<QueueConfiguration>\\n<Id>1</Id>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>prefix</Name>\\n<Value>images/</Value>\\n</FilterRule>\\n<FilterRule>\\n<Name>suffix</Name>\\n<Value>jpg</Value>\\n</FilterRule>\\n</S3Key>', '']]]\n",
      "[[['', '</Filter>\\n<Queue>arn:aws:sqs:us-west-2:444455556666:s3notificationqueue</Queue>\\n<Event>s3:ObjectCreated:Put</Event>\\n</QueueConfiguration>\\n</NotificationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<NotificationConfiguration>\\n<QueueConfiguration>\\n<Id>1</Id>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>prefix</Name>\\n<Value>images/</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n<Queue>arn:aws:sqs:us-west-2:444455556666:sqs-queue-A</Queue>\\n<Event>s3:ObjectCreated:Put</Event>\\n</QueueConfiguration>\\n<QueueConfiguration>\\n<Id>2</Id>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>prefix</Name>\\n<Value>logs/</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n<Queue>arn:aws:sqs:us-west-2:444455556666:sqs-queue-B</Queue>\\n<Event>s3:ObjectCreated:Put</Event>\\n</QueueConfiguration>\\n</NotificationConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<NotificationConfiguration>\\n<CloudFunctionConfiguration>\\n<Id>1</Id>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>suffix</Name>\\n<Value>.jpg</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n<CloudFunction>arn:aws:lambda:us-west-2:444455556666:cloud-function-A</\\nCloudFunction>\\n<Event>s3:ObjectCreated:Put</Event>\\n</CloudFunctionConfiguration>\\n<CloudFunctionConfiguration>\\n<Id>2</Id>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>suffix</Name>\\n<Value>.png</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n<CloudFunction>arn:aws:lambda:us-west-2:444455556666:cloud-function-B</\\nCloudFunction>\\n<Event>s3:ObjectCreated:Put</Event>\\n</CloudFunctionConfiguration>\\n</NotificationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<NotificationConfiguration>\\n<CloudFunctionConfiguration>', '']]]\n",
      "[[['', '<Id>1</Id>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>prefix</Name>\\n<Value>images</Value>\\n</FilterRule>\\n<FilterRule>\\n<Name>suffix</Name>\\n<Value>.jpg</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n<CloudFunction>arn:aws:lambda:us-west-2:444455556666:cloud-function-A</\\nCloudFunction>\\n<Event>s3:ObjectCreated:Put</Event>\\n</CloudFunctionConfiguration>\\n<CloudFunctionConfiguration>\\n<Id>2</Id>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>prefix</Name>\\n<Value>images</Value>\\n</FilterRule>\\n<FilterRule>\\n<Name>suffix</Name>\\n<Value>.png</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n<CloudFunction>arn:aws:lambda:us-west-2:444455556666:cloud-function-B</\\nCloudFunction>\\n<Event>s3:ObjectCreated:Put</Event>\\n</CloudFunctionConfiguration>\\n</NotificationConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<NotificationConfiguration>\\n<TopicConfiguration>\\n<Topic>arn:aws:sns:us-west-2:444455556666:sns-notification-one</Topic>\\n<Event>s3:ObjectCreated:*</Event>\\n</TopicConfiguration>\\n<TopicConfiguration>\\n<Topic>arn:aws:sns:us-west-2:444455556666:sns-notification-two</Topic>\\n<Event>s3:ObjectCreated:*</Event>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>prefix</Name>\\n<Value>images</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n</TopicConfiguration>\\n</NotificationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '<NotificationConfiguration>\\n<TopicConfiguration>\\n<Topic>arn:aws:sns:us-west-2:444455556666:sns-topic-one</Topic>', '']]]\n",
      "[[['', '<Event>s3:ObjectCreated:*</Event>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>suffix</Name>\\n<Value>jpg</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n</TopicConfiguration>\\n<TopicConfiguration>\\n<Topic>arn:aws:sns:us-west-2:444455556666:sns-topic-two</Topic>\\n<Event>s3:ObjectCreated:Put</Event>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>suffix</Name>\\n<Value>pg</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n</TopicConfiguration>\\n</NotificationConfiguration', ''], ['', '', '']], [['', '', ''], ['', '<NotificationConfiguration>\\n<TopicConfiguration>\\n<Topic>arn:aws:sns:us-west-2:444455556666:sns-topic-one</Topic>\\n<Event>s3:ObjectCreated:*</Event>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>prefix</Name>\\n<Value>images</Value>\\n</FilterRule>\\n<FilterRule>\\n<Name>suffix</Name>\\n<Value>jpg</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n</TopicConfiguration>', '']]]\n",
      "[[['', '<TopicConfiguration>\\n<Topic>arn:aws:sns:us-west-2:444455556666:sns-topic-two</Topic>\\n<Event>s3:ObjectCreated:Put</Event>\\n<Filter>\\n<S3Key>\\n<FilterRule>\\n<Name>suffix</Name>\\n<Value>jpg</Value>\\n</FilterRule>\\n</S3Key>\\n</Filter>\\n</TopicConfiguration>\\n</NotificationConfiguration>', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Records\":[\\n{\\n\"eventVersion\":\"2.2\",\\n\"eventSource\":\"aws:s3\",\\n\"awsRegion\":\"us-west-2\",\\n\"eventTime\":\"The time, in ISO-8601 format, for example,\\n1970-01-01T00:00:00.000Z, when Amazon S3 finished processing the request\",\\n\"eventName\":\"event-type\",\\n\"userIdentity\":{\\n\"principalId\":\"Amazon-customer-ID-of-the-user-who-caused-the-event\"\\n},\\n\"requestParameters\":{\\n\"sourceIPAddress\":\"ip-address-where-request-came-from\"', '']]]\n",
      "[[['', '},\\n\"responseElements\":{\\n\"x-amz-request-id\":\"Amazon S3 generated request ID\",\\n\"x-amz-id-2\":\"Amazon S3 host that processed the request\"\\n},\\n\"s3\":{\\n\"s3SchemaVersion\":\"1.0\",\\n\"configurationId\":\"ID found in the bucket notification configuration\",\\n\"bucket\":{\\n\"name\":\"bucket-name\",\\n\"ownerIdentity\":{\\n\"principalId\":\"Amazon-customer-ID-of-the-bucket-owner\"\\n},\\n\"arn\":\"bucket-ARN\"\\n},\\n\"object\":{\\n\"key\":\"object-key\",\\n\"size\":\"object-size in bytes\",\\n\"eTag\":\"object eTag\",\\n\"versionId\":\"object version if bucket is versioning-enabled, otherwise\\nnull\",\\n\"sequencer\": \"a string representation of a hexadecimal value used to\\ndetermine event sequence, only used with PUTs and DELETEs\"\\n}\\n},\\n\"glacierEventData\": {\\n\"restoreEventData\": {\\n\"lifecycleRestorationExpiryTime\": \"The time, in ISO-8601 format, for\\nexample, 1970-01-01T00:00:00.000Z, of Restore Expiry\",\\n\"lifecycleRestoreStorageClass\": \"Source storage class for restore\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Service\":\"Amazon S3\",\\n\"Event\":\"s3:TestEvent\",\\n\"Time\":\"2014-10-13T15:57:02.089Z\",\\n\"Bucket\":\"bucketname\",\\n\"RequestId\":\"5582815E1AEA5ADF\",\\n\"HostId\":\"8cLeGAmw098X5cv4Zkwcmo8vvZa3eH3eKxsPzbB9wrR+YstdA6Knx4Ip8EXAMPLE\"\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Records\":[\\n{\\n\"eventVersion\":\"2.1\",\\n\"eventSource\":\"aws:s3\",\\n\"awsRegion\":\"us-west-2\",\\n\"eventTime\":\"1970-01-01T00:00:00.000Z\",', '']]]\n",
      "[[['', '\"eventName\":\"ObjectCreated:Put\",\\n\"userIdentity\":{\\n\"principalId\":\"AIDAJDPLRKLG7UEXAMPLE\"\\n},\\n\"requestParameters\":{\\n\"sourceIPAddress\":\"127.0.0.1\"\\n},\\n\"responseElements\":{\\n\"x-amz-request-id\":\"C3D13FE58DE4C810\",\\n\"x-amz-id-2\":\"FMyUVURIY8/IgAtTv8xRjskZQpcIZ9KG4V5Wp6S7S/\\nJRWeUWerMUE5JgHvANOjpD\"\\n},\\n\"s3\":{\\n\"s3SchemaVersion\":\"1.0\",\\n\"configurationId\":\"testConfigRule\",\\n\"bucket\":{\\n\"name\":\"mybucket\",\\n\"ownerIdentity\":{\\n\"principalId\":\"A3NL1KOZZKExample\"\\n},\\n\"arn\":\"arn:aws:s3:::mybucket\"\\n},\\n\"object\":{\\n\"key\":\"HappyFace.jpg\",\\n\"size\":1024,\\n\"eTag\":\"d41d8cd98f00b204e9800998ecf8427e\",\\n\"versionId\":\"096fKKXTRTtl3on89fVO.nfljtsv6qko\",\\n\"sequencer\":\"0055AED6DCD90281E5\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['Event type', 'Description'], ['Object Created', 'An object was created.\\nThe reason field in the event message structure indicates\\nwhich S3 API was used to create the object: PutObject,\\nPOST Object, CopyObject, or CompleteMultipartUpload.'], ['Object Deleted (DeleteObject)\\nObject Deleted (Lifecycle expiration)', 'An object was deleted.\\nWhen an object is deleted using an S3 API call, the reason\\nfield is set to DeleteObject. When an object is deleted by\\nan S3 Lifecycle expiration rule, the reason field is set to\\nLifecycle Expiration. For more information, see Expiring\\nobjects.\\nWhen an unversioned object is deleted, or a versioned\\nobject is permanently deleted, the deletion-type field\\nis set to Permanently Deleted. When a delete marker is\\ncreated for a versioned object, the deletion-type field\\nis set to Delete Marker Created. For more information,\\nsee Deleting object versions from a versioning-enabled\\nbucket.'], ['Object Restore Initiated', 'An object restore was initiated from S3 Glacier or S3\\nGlacier Deep Archive storage class or from S3 Intellige\\nnt-Tiering Archive Access or Deep Archive Access tier. For\\nmore information, see Working with archived objects.'], ['Object Restore Completed', 'An object restore was completed.'], ['Object Restore Expired', 'The temporary copy of an object restored from S3 Glacier\\nor S3 Glacier Deep Archive expired and was deleted.']]]\n",
      "[[['Event type', 'Description'], ['Object Storage Class Changed', 'An object was transitioned to a different storage class. For\\nmore information, see Transitioning objects using Amazon\\nS3 Lifecycle.'], ['Object Access Tier Changed', 'An object was transitioned to the S3 Intelligent-Tiering\\nArchive Access tier or Deep Archive Access tier. For more\\ninformation, see Amazon S3 Intelligent-Tiering.'], ['Object ACL Updated', \"An object's access control list (ACL) was set using\\nPutObjectACL. An event is not generated when a request\\nresults in no change to an object’s ACL. For more informati\\non, see Access control list (ACL) overview.\"], ['Object Tags Added', 'A set of tags was added to an object using PutObject\\nTagging. For more information, see Categorizing your\\nstorage using tags.'], ['Object Tags Deleted', 'All tags were removed from an object using DeleteObj\\nectTagging. For more information, see Categorizing your\\nstorage using tags.']], [['', '', ''], ['', 'Note\\nFor more information about how Amazon S3 event types map to EventBridge event types,\\nsee Amazon EventBridge mapping and troubleshooting.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAfter you enable EventBridge, it takes around five minutes for the changes to take\\neffect.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-notification-configuration --bucket example-s3-bucket1 --\\nnotification-configuration=\\'{ \"EventBridgeConfiguration\": {} }\\'', ''], ['', '', '']], [['', '', ''], ['', '<NotificationConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\\n<EventBridgeConfiguration>\\n</EventBridgeConfiguration>\\n</NotificationConfiguration>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"version\": \"0\",\\n\"id\": \"17793124-05d4-b198-2fde-7ededc63b103\",\\n\"detail-type\": \"Object Created\",\\n\"source\": \"aws.s3\",\\n\"account\": \"111122223333\",\\n\"time\": \"2021-11-12T00:00:00Z\",\\n\"region\": \"ca-central-1\",\\n\"resources\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1\"\\n],\\n\"detail\": {\\n\"version\": \"0\",\\n\"bucket\": {\\n\"name\": \"DOC-EXAMPLE-BUCKET1\"\\n},\\n\"object\": {\\n\"key\": \"example-key\",\\n\"size\": 5,\\n\"etag\": \"b1946ac92492d2347c6235b4d2611184\",\\n\"version-id\": \"IYV3p45BT0ac8hjHg1houSdS1a.Mro8e\",\\n\"sequencer\": \"617f08299329d189\"\\n},\\n\"request-id\": \"N4N7GDK58NMKJ12R\",\\n\"requester\": \"123456789012\",\\n\"source-ip-address\": \"1.2.3.4\",\\n\"reason\": \"PutObject\"\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"version\": \"0\",\\n\"id\": \"2ee9cc15-d022-99ea-1fb8-1b1bac4850f9\",\\n\"detail-type\": \"Object Deleted\",\\n\"source\": \"aws.s3\",\\n\"account\": \"111122223333\",\\n\"time\": \"2021-11-12T00:00:00Z\",\\n\"region\": \"ca-central-1\",\\n\"resources\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1\"\\n],\\n\"detail\": {\\n\"version\": \"0\",\\n\"bucket\": {\\n\"name\": \"DOC-EXAMPLE-BUCKET1\"\\n},\\n\"object\": {\\n\"key\": \"example-key\",\\n\"etag\": \"d41d8cd98f00b204e9800998ecf8427e\",\\n\"version-id\": \"1QW9g1Z99LUNbvaaYVpW9xDlOLU.qxgF\",\\n\"sequencer\": \"617f0837b476e463\"\\n},\\n\"request-id\": \"0BH729840619AG5K\",\\n\"requester\": \"123456789012\",\\n\"source-ip-address\": \"1.2.3.4\",\\n\"reason\": \"DeleteObject\",\\n\"deletion-type\": \"Delete Marker Created\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"version\": \"0\",\\n\"id\": \"ad1de317-e409-eba2-9552-30113f8d88e3\",\\n\"detail-type\": \"Object Deleted\",\\n\"source\": \"aws.s3\",\\n\"account\": \"111122223333\",\\n\"time\": \"2021-11-12T00:00:00Z\",\\n\"region\": \"ca-central-1\",', '']]]\n",
      "[[['', '\"resources\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1\"\\n],\\n\"detail\": {\\n\"version\": \"0\",\\n\"bucket\": {\\n\"name\": \"DOC-EXAMPLE-BUCKET1\"\\n},\\n\"object\": {\\n\"key\": \"example-key\",\\n\"etag\": \"d41d8cd98f00b204e9800998ecf8427e\",\\n\"version-id\": \"mtB0cV.jejK63XkRNceanNMC.qXPWLeK\",\\n\"sequencer\": \"617b398000000000\"\\n},\\n\"request-id\": \"20EB74C14654DC47\",\\n\"requester\": \"s3.amazonaws.com\",\\n\"reason\": \"Lifecycle Expiration\",\\n\"deletion-type\": \"Delete Marker Created\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"version\": \"0\",\\n\"id\": \"6924de0d-13e2-6bbf-c0c1-b903b753565e\",\\n\"detail-type\": \"Object Restore Completed\",\\n\"source\": \"aws.s3\",\\n\"account\": \"111122223333\",\\n\"time\": \"2021-11-12T00:00:00Z\",\\n\"region\": \"ca-central-1\",\\n\"resources\": [\\n\"arn:aws:s3:::DOC-EXAMPLE-BUCKET1\"\\n],\\n\"detail\": {\\n\"version\": \"0\",\\n\"bucket\": {\\n\"name\": \"DOC-EXAMPLE-BUCKET1\"\\n},\\n\"object\": {\\n\"key\": \"example-key\",\\n\"size\": 5,', '']]]\n",
      "[[['', '\"etag\": \"b1946ac92492d2347c6235b4d2611184\",\\n\"version-id\": \"KKsjUC1.6gIjqtvhfg5AdMI0eCePIiT3\"\\n},\\n\"request-id\": \"189F19CB7FB1B6A4\",\\n\"requester\": \"s3.amazonaws.com\",\\n\"restore-expiry-time\": \"2021-11-13T00:00:00Z\",\\n\"source-storage-class\": \"GLACIER\"\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['S3 event type', 'Amazon EventBridge detail type'], ['ObjectCreated:Put\\nObjectCreated:Post\\nObjectCreated:Copy\\nObjectCreated:CompleteMulti\\npartUpload', 'Object Created'], ['ObjectRemoved:Delete\\nObjectRemoved:DeleteMarkerC\\nreated\\nLifecycleExpiration:Delete\\nLifecycleExpiration:DeleteM\\narkerCreated', 'Object Deleted'], ['ObjectRestore:Post', 'Object Restore Initiated'], ['ObjectRestore:Completed', 'Object Restore Completed'], ['ObjectRestore:Delete', 'Object Restore Expired'], ['LifecycleTransition', 'Object Storage Class Changed'], ['IntelligentTiering', 'Object Access Tier Changed']]]\n",
      "[[['S3 event type', 'Amazon EventBridge detail type'], ['ObjectTagging:Put', 'Object Tags Added'], ['ObjectTagging:Delete', 'Object Tags Deleted'], ['ObjectAcl:Put', 'Object ACL Updated']]]\n",
      "[[['', '', ''], ['', 'Important\\nStorage class analysis only provides recommendations for Standard to Standard IA classes.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nStorage class analysis does not give recommendations for transitions to the ONEZONE_IA\\nor S3 Glacier Flexible Retrieval storage classes.\\nIf you want to configure storage class analysis to export your findings as a .csv file and\\nthe destination bucket uses default bucket encryption with a AWS KMS key, you must\\nupdate the AWS KMS key policy to grant Amazon S3 permission to encrypt the .csv file. For\\ninstructions, see Granting Amazon S3 permission to use your customer managed key for\\nencryption.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nAmazon S3 now applies server-side encryption with Amazon S3 managed keys (SSE-S3) as\\nthe base level of encryption for every bucket in Amazon S3. Starting January 5, 2023, all\\nnew object uploads to Amazon S3 are automatically encrypted at no additional cost and\\nwith no impact on performance. The automatic encryption status for S3 bucket default\\nencryption configuration and for new object uploads is available in AWS CloudTrail logs, S3\\nInventory, S3 Storage Lens, the Amazon S3 console, and as an additional Amazon S3 API\\nresponse header in the AWS Command Line Interface and AWS SDKs. For more information,\\nsee Default encryption FAQ.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nIf you disable your default dashboard, it is no longer updated. You'll no longer receive\\nany new daily metrics in your S3 Storage Lens dashboard, your metrics export, or the\\naccount snapshot on the S3 Buckets page. If your dashboard uses advanced metrics\\nand recommendations, you'll no longer be charged. You can still see historic data in the\\ndashboard until the 14-day period for data queries expires. This period is 15 months if\\nyou've enabled advanced metrics and recommendations. To access historic data, you can re-\\nenable the dashboard within the expiration period.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• You can use S3 Storage Lens to create up to 50 dashboards per home Region.\\n• Organization-level dashboards can be limited only to a Regional scope.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf you disable your default dashboard, your Account snapshot is no longer updated. To\\ncontinue displaying metrics in the Account snapshot, you can re-enable the default-\\naccount-dashboard.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can choose one of the following Regions as your home Region:\\n• US East (N. Virginia) – us-east-1\\n• US East (Ohio) – us-east-2\\n• US West (N. California) – us-west-1\\n• US West (Oregon) – us-west-2\\n• Asia Pacific (Mumbai) – ap-south-1\\n• Asia Pacific (Seoul) – ap-northeast-2\\n• Asia Pacific (Singapore) – ap-southeast-1\\n• Asia Pacific (Sydney) – ap-southeast-2', '']]]\n",
      "[[['', '• Asia Pacific (Tokyo) – ap-northeast-1\\n• Canada (Central) – ca-central-1\\n• China (Beijing) – cn-north-1\\n• China (Ningxia) – cn-northwest-1\\n• Europe (Frankfurt) – eu-central-1\\n• Europe (Ireland) – eu-west-1\\n• Europe (London) – eu-west-2\\n• Europe (Paris) – eu-west-3\\n• Europe (Stockholm) – eu-north-1\\n• South America (São Paulo) – sa-east-1', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nRecommendations are available only when you use the S3 Storage Lens dashboard on\\nthe Amazon S3 console.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nOnly the management account can enable trusted access for Amazon S3 Storage Lens.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• Disabling trusted access for S3 Storage Lens also automatically stops all organization-\\nlevel dashboards from collecting and aggregating storage metrics.\\n• Your management and delegated administrator accounts will still be able to see the\\nhistoric data for your existing organization-level dashboards during the period that data\\nis available for queries.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nBefore you can designate a delegated administrator by using the AWS Organizations REST\\nAPI, AWS CLI, or SDKs, you must call the EnableAWSOrganizationsAccess operation.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• Deregistering a delegated administrator also automatically stops all organization-level\\ndashboards created by that delegated administrator from aggregating new storage\\nmetrics.\\n• The deregistered delegated administrator will still be able to see the historic data for the\\ndashboards that they created while data is available for queries.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\n• You can't use your account's root user credentials to view Amazon S3 Storage Lens\\ndashboards. To access S3 Storage Lens dashboards, you must grant the required IAM\\npermissions to a new or existing IAM user. Then, sign in with those user credentials to\\naccess S3 Storage Lens dashboards. For more information, see Security best practices in\\nIAM in the IAM User Guide.\\n• Using S3 Storage Lens on the Amazon S3 console can require multiple permissions. For\\nexample, to edit a dashboard on the console, you need the following permissions:\\n• s3:ListStorageLensConfigurations\\n• s3:GetStorageLensConfiguration\\n• s3:PutStorageLensConfiguration\", ''], ['', '', '']], [['Action', 'IAM permissions'], ['Create or update an S3 Storage Lens\\ndashboard in the Amazon S3 console.', 's3:ListStorageLensConfigurations\\ns3:GetStorageLensConfiguration\\ns3:GetStorageLensConfigurat\\nionTagging\\ns3:PutStorageLensConfiguration']]]\n",
      "[[['Action', 'IAM permissions'], ['', 's3:PutStorageLensConfigurat\\nionTagging'], ['Get the tags of an S3 Storage Lens dashboard\\non the Amazon S3 console.', 's3:ListStorageLensConfigurations\\ns3:GetStorageLensConfigurat\\nionTagging'], ['View an S3 Storage Lens dashboard on the\\nAmazon S3 console.', 's3:ListStorageLensConfigurations\\ns3:GetStorageLensConfiguration\\ns3:GetStorageLensDashboard'], ['Delete an S3 Storage Lens dashboard on\\nAmazon S3 console.', 's3:ListStorageLensConfigurations\\ns3:GetStorageLensConfiguration\\ns3:DeleteStorageLensConfigu\\nration'], ['Create or update an S3 Storage Lens configura\\ntion by using the AWS CLI or an AWS SDK.', 's3:PutStorageLensConfiguration\\ns3:PutStorageLensConfigurat\\nionTagging'], ['Get the tags of an S3 Storage Lens configura\\ntion by using the AWS CLI or an AWS SDK.', 's3:GetStorageLensConfigurat\\nionTagging'], ['View an S3 Storage Lens configuration by\\nusing the AWS CLI or an AWS SDK.', 's3:GetStorageLensConfiguration'], ['Delete an S3 Storage Lens configuration by\\nusing the AWS CLI or AWS SDK.', 's3:DeleteStorageLensConfigu\\nration']], [['', '', ''], ['', 'Note\\n• You can use resource tags in an IAM policy to manage permissions.', '']]]\n",
      "[[['', \"• An IAM user or role with these permissions can see metrics from buckets and prefixes\\nthat they might not have direct permission to read or list objects from.\\n• For S3 Storage Lens dashboards with prefix-level metrics enabled, if a selected prefix\\npath matches with an object key, the dashboard might display the object key as another\\nprefix.\\n• For metrics exports, which are stored in a bucket in your account, permissions are granted\\nby using the existing s3:GetObject permission in the IAM policy. Similarly, for an AWS\\nOrganizations entity, the organization's management account or delegated administrator\\naccounts can use IAM policies to manage access permissions for organization-level\\ndashboard and configurations.\", ''], ['', '', '']], [['Action', 'IAM Permissions'], ['Enable trusted access for S3 Storage Lens for\\nyour organization.', 'organizations:EnableAWSServ\\niceAccess'], ['Disable trusted access for S3 Storage Lens for\\nyour organization.', 'organizations:DisableAWSSer\\nviceAccess']]]\n",
      "[[['Action', 'IAM Permissions'], ['Register a delegated administrator to create\\nS3 Storage Lens dashboards or configurations\\nfor your organization.', 'organizations:RegisterDeleg\\natedAdministrator'], ['Deregister a delegated administrator so\\nthat they can no longer create S3 Storage\\nLens dashboards or configurations for your\\norganization.', 'organizations:DeregisterDel\\negatedAdministrator'], ['Additional permissions to create S3 Storage\\nLens organization-wide configurations.', 'organizations:DescribeOrgan\\nization\\norganizations:ListAccounts\\norganizations:ListAWSServic\\neAccessForOrganization\\norganizations:ListDelegated\\nAdministrators\\niam:CreateServiceLinkedRole']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't use your account's root user credentials to view Amazon S3 Storage Lens\\ndashboards. To access S3 Storage Lens dashboards, you must grant the required AWS\\nIdentity and Access Management (IAM) permissions to a new or existing IAM user. Then,\\nsign in with those user credentials to access S3 Storage Lens dashboards. For more\\ninformation, see Amazon S3 Storage Lens permissions and Security best practices in IAM in\\nthe IAM User Guide.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe Prefixes filter and the Storage Lens groups filter can’t be applied at the same\\ntime.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Sid\": \"Allow Amazon S3 Storage Lens use of the KMS key\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"storage-lens.s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"kms:GenerateDataKey\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:SourceArn\": \"arn:aws:s3:us-east-1:source-account-id:storage-\\nlens/your-dashboard-name\",\\n\"aws:SourceAccount\": \"source-account-id\"\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"sourceAccountId\":\"123456789012\",\\n\"configId\":\"my-dashboard-configuration-id\",\\n\"destinationBucket\":\"arn:aws:s3:::destination-bucket\",\\n\"reportVersion\":\"V_1\",\\n\"reportDate\":\"2020-11-03\",', '']]]\n",
      "[[['', '\"reportFormat\":\"CSV\",\\n\"reportSchema\":\"version_number,configuration_id,report_date,aws_account_number,aws_regi\\n\"reportFiles\":[\\n{\\n\"key\":\"DestinationPrefix/StorageLens/123456789012/my-dashboard-\\nconfiguration-id/V_1/reports/dt=2020-11-03/a38f6bc4-2e3d-4355-ac8a-e2fdcf3de158.csv\",\\n\"size\":1603959,\\n\"md5Checksum\":\"2177e775870def72b8d84febe1ad3574\"\\n}\\n]\\n}', 'o'], ['', '', '']], [['', '', ''], ['', '{\\n\"sourceAccountId\":\"123456789012\",\\n\"configId\":\"my-dashboard-configuration-id\",\\n\"destinationBucket\":\"arn:aws:s3:::destination-bucket\",\\n\"reportVersion\":\"V_1\",\\n\"reportDate\":\"2020-11-03\",\\n\"reportFormat\":\"Parquet\",\\n\"reportSchema\":\"message s3.storage.lens { required string version_number;\\nrequired string configuration_id; required string report_date; required string\\naws_account_number; required string aws_region; required string storage_class;\\nrequired string record_type; required string record_value; required string\\nbucket_name; required string metric_name; required long metric_value; }\",\\n\"reportFiles\":[\\n{\\n\"key\":\"DestinationPrefix/StorageLens/123456789012/my-dashboard-configuration-\\nid/V_1/reports/dt=2020-11-03/bd23de7c-b46a-4cf4-bcc5-b21aac5be0f5.par\",\\n\"size\":14714,\\n\"md5Checksum\":\"b5c741ee0251cd99b90b3e8eff50b944\"\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['Attribute name', 'Data type', 'Column name', 'Description'], ['VersionNumber', 'String', 'version_number', 'The version of the S3\\nStorage Lens metrics\\nbeing used.'], ['ConfigurationId', 'String', 'configura\\ntion_id', 'The configura\\ntion_id of your\\nS3 Storage Lens\\nconfiguration.'], ['ReportDate', 'String', 'report_date', 'The date that the\\nmetrics were tracked.'], ['AwsAccoun\\ntNumber', 'String', 'aws_accou\\nnt_number', 'Your AWS account\\nnumber.'], ['AwsRegion', 'String', 'aws_region', 'The AWS Region for\\nwhich the metrics are\\nbeing tracked.'], ['StorageClass', 'String', 'storage_class', 'The storage class\\nof the bucket in\\nquestion.'], ['RecordType', 'ENUM', 'record_type', 'The type of artifact\\nthat is being reported\\n(ACCOUNT, BUCKET,\\nor PREFIX).'], ['RecordValue', 'String', 'record_value', 'The value of the\\nRecordType\\nartifact.\\nNote\\nThe\\nrecord_va']], [['', '', ''], ['', 'Note\\nThe\\nrecord_va', '']]]\n",
      "[[['Attribute name', 'Data type', 'Column name', 'Description'], ['', '', '', 'lue is URL-\\nencoded.'], ['BucketName', 'String', 'bucket_name', 'The name of the\\nbucket that is being\\nreported.'], ['MetricName', 'String', 'metric_name', 'The name of the\\nmetric that is being\\nreported.'], ['MetricValue', 'Long', 'metric_value', 'The value of the\\nmetric that is being\\nreported.']], [['', 'lue is URL-\\nencoded.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can identify metrics for Storage Lens groups by looking for the\\nSTORAGE_LENS_GROUP_BUCKET or STORAGE_LENS_GROUP_ACCOUNT values in\\nthe record_type column. The record_value column will display the Amazon\\nResource Name (ARN) for the Storage Lens group, for example, arn:aws:s3:us-\\neast-1:123456789012:storage-lens-group/slg-1.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nS3 Storage Lens metrics are daily metrics and are published to CloudWatch once per day.\\nWhen you query S3 Storage Lens metrics in CloudWatch, the period for the query must\\nbe 1 day (86400 seconds). After your daily S3 Storage Lens metrics appear in your S3\\nStorage Lens dashboard in the Amazon S3 console, it can take a few hours for these same\\nmetrics to appear in CloudWatch. When you enable the CloudWatch publishing option for\\nS3 Storage Lens metrics for the first time, it can take up to 24 hours for your metrics to\\npublish to CloudWatch.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nS3 Storage Lens metrics are daily metrics and are published to CloudWatch once per day.\\nWhen you query S3 Storage Lens metrics in CloudWatch, the period for the query must\\nbe 1 day (86400 seconds). After your daily S3 Storage Lens metrics appear in your S3\\nStorage Lens dashboard in the Amazon S3 console, it can take a few hours for these same\\nmetrics to appear in CloudWatch. When you enable the CloudWatch publishing option for', '']]]\n",
      "[[['', 'S3 Storage Lens metrics for the first time, it can take up to 24 hours for your metrics to\\npublish to CloudWatch.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe valid statistic for S3 Storage Lens metrics in CloudWatch is Average. For more\\ninformation about statistics in CloudWatch, see CloudWatch statistics definitions in the\\nAmazon CloudWatch User Guide.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['Dimension', 'Description B', 'uAckc', 'Oceotrgu', 'Oanrntg\\nion\\nb', 'Oiazrnag\\nion\\nand\\nuacck', 'tiaz na\\ni on\\na nd\\nceotu'], ['configura\\ntion_id', 'The dashboard name for the S3 Storage\\nLens configuration reported in the metrics', '', '', '', '', ''], ['metrics_v\\nersion', 'The version of the S3 Storage Lens metrics.\\nThe metrics version has a fixed value of 1.0.', '', '', '', '', ''], ['organizat\\nion_id', 'The AWS Organizations ID for the metrics', '', '', '', '', ''], ['aws_accou\\nnt_number', \"The AWS account that's associated with the metrics\", '', '', '', '', ''], ['aws_regio\\nn', 'The AWS Region for the metrics', '', '', '', '', ''], ['bucket_na\\nme', \"The name of the S3 bucket that's reported in the metrics\", '', '', '', '', ''], ['storage_c\\nlass', \"The storage class for the bucket that's reported in the metrics\", '', '', '', '', ''], ['record_ty\\npe', 'The granularity of the metrics:\\nORGANIZATION, ACCOUNT, BUCKET\\nBU', 'ACCKC', 'EOBTUU', 'O\\nACNCKCT', 'RG\\nEOTU\\nI', 'AN\\nNT\\nON']]]\n",
      "[[['', '', ''], ['', 'Note\\nS3 Storage Lens metrics are daily metrics and are published to CloudWatch once per day.\\nWhen you query S3 Storage Lens metrics in CloudWatch, the period for the query must\\nbe 1 day (86400 seconds). After your daily S3 Storage Lens metrics appear in your S3\\nStorage Lens dashboard in the Amazon S3 console, it can take a few hours for these same\\nmetrics to appear in CloudWatch. When you enable the CloudWatch publishing option for\\nS3 Storage Lens metrics for the first time, it can take up to 24 hours for your metrics to\\npublish to CloudWatch.\\nCurrently, S3 Storage Lens metrics cannot be consumed through CloudWatch streams.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf your configuration enables prefix aggregation for usage metrics, prefix-level metrics\\nwill not be published to CloudWatch. Only bucket, account, and organization-level S3\\nStorage Lens metrics are published to CloudWatch.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nYou can add up to 50 tags to your dashboard configuration.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen you create an organization-level configuration that includes all accounts,\\nyou can include or exclude only Regions, not buckets.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can choose up to 50 buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf you enable prefix aggregation for your S3 Storage Lens configuration, prefix-\\nlevel metrics will not be published to CloudWatch. Only bucket, account, and\\norganization-level S3 Storage Lens metrics are published to CloudWatch.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor more information about advanced metrics and recommendations features, see\\nMetrics selection.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control put-storage-lens-configuration --account-id=555555555555 --config-\\nid=your-configuration-id --region=us-east-1 --storage-lens-configuration=file://./\\nconfig.json\\nconfig.json\\n{\\n\"Id\": \"SampleS3StorageLensConfiguration\", //Use this property to identify your S3\\nStorage Lens configuration.\\n\"AwsOrg\": { //Use this property when enabling S3 Storage Lens for AWS Organizations.\\n\"Arn\": \"arn:aws:organizations::123456789012:organization/o-abcdefgh\"\\n},\\n\"AccountLevel\": {\\n\"ActivityMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"AdvancedCostOptimizationMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"AdvancedDataProtectionMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"DetailedStatusCodesMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"BucketLevel\": {\\n\"ActivityMetrics\": {\\n\"IsEnabled\":true //Mark this as false if you want only free metrics.\\n},\\n\"ActivityMetrics\": {\\n\"IsEnabled\":true //Mark this as false if you want only free metrics.\\n},\\n\"AdvancedCostOptimizationMetrics\": {\\n\"IsEnabled\":true //Mark this as false if you want only free metrics.\\n},\\n\"DetailedStatusCodesMetrics\": {\\n\"IsEnabled\":true //Mark this as false if you want only free metrics.\\n},', '']]]\n",
      "[[['', '\"PrefixLevel\":{\\n\"StorageMetrics\":{\\n\"IsEnabled\":true, //Mark this as false if you want only free metrics.\\n\"SelectionCriteria\":{\\n\"MaxDepth\":5,\\n\"MinStorageBytesPercentage\":1.25,\\n\"Delimiter\":\"/\"\\n}\\n}\\n}\\n}\\n},\\n\"Exclude\": { //Replace with \"Include\" if you prefer to include Regions.\\n\"Regions\": [\\n\"eu-west-1\"\\n],\\n\"Buckets\": [ //This attribute is not supported for AWS Organizations-level\\nconfigurations.\\n\"arn:aws:s3:::source_bucket1\"\\n]\\n},\\n\"IsEnabled\": true, //Whether the configuration is enabled\\n\"DataExport\": { //Details about the metrics export\\n\"S3BucketDestination\": {\\n\"OutputSchemaVersion\": \"V_1\",\\n\"Format\": \"CSV\", //You can add \"Parquet\" if you prefer.\\n\"AccountId\": \"111122223333\",\\n\"Arn\": \"arn:aws:s3:::destination-bucket-name\", // The destination bucket for your\\nmetrics export must be in the same Region as your S3 Storage Lens configuration.\\n\"Prefix\": \"prefix-for-your-export-destination\",\\n\"Encryption\": {\\n\"SSES3\": {}\\n}\\n},\\n\"CloudWatchMetrics\": {\\n\"IsEnabled\": true //Mark this as false if you want to export only free metrics.\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;', '']]]\n",
      "[[['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.AccountLevel;\\nimport com.amazonaws.services.s3control.model.ActivityMetrics;\\nimport com.amazonaws.services.s3control.model.BucketLevel;\\nimport com.amazonaws.services.s3control.model.CloudWatchMetrics;\\nimport com.amazonaws.services.s3control.model.Format;\\nimport com.amazonaws.services.s3control.model.Include;\\nimport com.amazonaws.services.s3control.model.OutputSchemaVersion;\\nimport com.amazonaws.services.s3control.model.PrefixLevel;\\nimport com.amazonaws.services.s3control.model.PrefixLevelStorageMetrics;\\nimport com.amazonaws.services.s3control.model.PutStorageLensConfigurationRequest;\\nimport com.amazonaws.services.s3control.model.S3BucketDestination;\\nimport com.amazonaws.services.s3control.model.SSES3;\\nimport com.amazonaws.services.s3control.model.SelectionCriteria;\\nimport com.amazonaws.services.s3control.model.StorageLensAwsOrg;\\nimport com.amazonaws.services.s3control.model.StorageLensConfiguration;\\nimport com.amazonaws.services.s3control.model.StorageLensDataExport;\\nimport com.amazonaws.services.s3control.model.StorageLensDataExportEncryption;\\nimport com.amazonaws.services.s3control.model.StorageLensTag;\\nimport java.util.Arrays;\\nimport java.util.List;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class CreateAndUpdateDashboard {\\npublic static void main(String[] args) {\\nString configurationId = \"ConfigurationId\";\\nString sourceAccountId = \"Source Account ID\";\\nString exportAccountId = \"Destination Account ID\";\\nString exportBucketArn = \"arn:aws:s3:::destBucketName\"; // The destination\\nbucket for your metrics export must be in the same Region as your S3 Storage Lens\\nconfiguration.\\nString awsOrgARN = \"arn:aws:organizations::123456789012:organization/o-\\nabcdefgh\";\\nFormat exportFormat = Format.CSV;\\ntry {', '']]]\n",
      "[[['', 'SelectionCriteria selectionCriteria = new SelectionCriteria()\\n.withDelimiter(\"/\")\\n.withMaxDepth(5)\\n.withMinStorageBytesPercentage(10.0);\\nPrefixLevelStorageMetrics prefixStorageMetrics = new\\nPrefixLevelStorageMetrics()\\n.withIsEnabled(true)\\n.withSelectionCriteria(selectionCriteria);\\nBucketLevel bucketLevel = new BucketLevel()\\n.withActivityMetrics(new ActivityMetrics().withIsEnabled(true))\\n.withAdvancedCostOptimizationMetrics(new\\nAdvancedCostOptimizationMetrics().withIsEnabled(true))\\n.withAdvancedDataProtectionMetrics(new\\nAdvancedDataProtectionMetrics().withIsEnabled(true))\\n.withDetailedStatusCodesMetrics(new\\nDetailedStatusCodesMetrics().withIsEnabled(true))\\n.withPrefixLevel(new\\nPrefixLevel().withStorageMetrics(prefixStorageMetrics));\\nAccountLevel accountLevel = new AccountLevel()\\n.withActivityMetrics(new ActivityMetrics().withIsEnabled(true))\\n.withAdvancedCostOptimizationMetrics(new\\nAdvancedCostOptimizationMetrics().withIsEnabled(true))\\n.withAdvancedDataProtectionMetrics(new\\nAdvancedDataProtectionMetrics().withIsEnabled(true))\\n.withDetailedStatusCodesMetrics(new\\nDetailedStatusCodesMetrics().withIsEnabled(true))\\n.withBucketLevel(bucketLevel);\\nInclude include = new Include()\\n.withBuckets(Arrays.asList(\"arn:aws:s3:::bucketName\"))\\n.withRegions(Arrays.asList(\"us-west-2\"));\\nStorageLensDataExportEncryption exportEncryption = new\\nStorageLensDataExportEncryption()\\n.withSSES3(new SSES3());\\nS3BucketDestination s3BucketDestination = new S3BucketDestination()\\n.withAccountId(exportAccountId)\\n.withArn(exportBucketArn)\\n.withEncryption(exportEncryption)\\n.withFormat(exportFormat)\\n.withOutputSchemaVersion(OutputSchemaVersion.V_1)\\n.withPrefix(\"Prefix\");\\nCloudWatchMetrics cloudWatchMetrics = new CloudWatchMetrics()\\n.withIsEnabled(true);', '']]]\n",
      "[[['', 'StorageLensDataExport dataExport = new StorageLensDataExport()\\n.withCloudWatchMetrics(cloudWatchMetrics)\\n.withS3BucketDestination(s3BucketDestination);\\nStorageLensAwsOrg awsOrg = new StorageLensAwsOrg()\\n.withArn(awsOrgARN);\\nStorageLensConfiguration configuration = new StorageLensConfiguration()\\n.withId(configurationId)\\n.withAccountLevel(accountLevel)\\n.withInclude(include)\\n.withDataExport(dataExport)\\n.withAwsOrg(awsOrg)\\n.withIsEnabled(true);\\nList<StorageLensTag> tags = Arrays.asList(\\nnew StorageLensTag().withKey(\"key-1\").withValue(\"value-1\"),\\nnew StorageLensTag().withKey(\"key-2\").withValue(\"value-2\")\\n);\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\ns3ControlClient.putStorageLensConfiguration(new\\nPutStorageLensConfigurationRequest()\\n.withAccountId(sourceAccountId)\\n.withConfigId(configurationId)\\n.withStorageLensConfiguration(configuration)\\n.withTags(tags)\\n);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nS3 Storage Lens metrics are daily metrics and are published to CloudWatch once per day.\\nWhen you query S3 Storage Lens metrics in CloudWatch, the period for the query must', '']]]\n",
      "[[['', 'be 1 day (86400 seconds). After your daily S3 Storage Lens metrics appear in your S3\\nStorage Lens dashboard in the Amazon S3 console, it can take a few hours for these same\\nmetrics to appear in CloudWatch. When you enable the CloudWatch publishing option for\\nS3 Storage Lens metrics for the first time, it can take up to 24 hours for your metrics to\\npublish to CloudWatch.\\nCurrently, S3 Storage Lens metrics cannot be consumed through CloudWatch streams.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'SUM(SEARCH(\\'{AWS/S3/Storage-\\nLens,aws_account_number,aws_region,configuration_id,metrics_version,record_type,storage_\\nMetricName=\"IncompleteMultipartUploadStorageBytes\"\\', \\'Average\\',86400))', 'c'], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nWith S3 Storage Lens advanced metrics and recommendations, metrics are available\\nfor queries for 15 months. For more information, see Metrics selection.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nDownload bytes are duplicated in cases where the same object is downloaded multiple\\ntimes during the day.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nWith S3 Storage Lens advanced metrics and recommendations, metrics are available\\nfor queries for 15 months. For more information, see Metrics selection.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf your % noncurrent version bytes is greater than 10 percent of your storage at the\\naccount level, you might be storing too many object versions.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWith S3 Storage Lens advanced metrics and recommendations, metrics are\\navailable for queries for 15 months. For more information, see Metrics selection.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nWith S3 Storage Lens advanced metrics and recommendations, metrics are available\\nfor queries for 15 months. For more information, see Metrics selection.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nWith S3 Storage Lens advanced metrics and recommendations, metrics are\\navailable for queries for 15 months. For more information, see Metrics selection.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nWith S3 Storage Lens advanced metrics and recommendations, metrics are available\\nfor queries for 15 months. For more information, see Metrics selection.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor Storage Lens groups, only free tier storage metrics are available. Advanced tier\\nmetrics are not available at the Storage Lens group level.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\n• The unit of measurement for S3 storage bytes is in binary gigabytes (GB), where 1 GB\\n30 40 50\\nis 2 bytes, 1 TB is 2 bytes, and 1 PB is 2 bytes. This unit of measurement is also\\nknown as a gibibyte (GiB), as defined by the International Electrotechnical Commission\\n(IEC).\\n• When an object reaches the end of its lifetime based on its lifecycle configuration,\\nAmazon S3 queues the object for removal and removes it asynchronously. Therefore,\\nthere might be a delay between the expiration date and the date when Amazon S3\\nremoves an object. S3 Storage Lens doesn't include metrics for objects that have expired\\nbut haven't been removed. For more information about expiration actions in S3 Lifecycle,\\nsee Expiring objects.\", ''], ['', '', '']], [['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Total storage', 'StorageBytes', 'The total storage,\\ninclusive of\\nincomplet\\ne multipart\\nuploads, object\\nmetadata, and\\ndelete markers', 'Free', 'Sum', 'mNa', 'ry-', ''], ['Object count', 'ObjectCount', 'The total object\\ncount', 'Free', 'Sum', 'mNa', 'ry-', ''], ['Average object\\nsize', '-', 'The average\\nobject size', 'Free', 'Sum', 'mYa', 'rysum\\ngeBy\\nsum\\ntCou', '(Stora\\ntes)/\\n(Objec\\nnt)'], ['Active buckets', '-', 'The total number\\nof buckets in', 'Free', 'Sum', 'mYa', 'ry-', None]]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['', '', 'active usage with\\nstorage > 0 bytes', '', '', '', '', ''], ['Buckets', '-', 'The total number\\nof buckets', 'Free', 'Sum', 'mYa', 'ry-', ''], ['Accounts', '-', 'The number of\\naccounts whose\\nstorage is in scope', 'Free', 'Sum', 'mYa', 'ry-', ''], ['Current version\\nbytes', 'CurrentVe\\nrsionStor\\nageBytes', 'The number of\\nbytes that are a\\ncurrent version of\\nan object', 'Free', 'Cost\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['% current\\nversion bytes', '-', 'The percentag\\ne of bytes in\\nscope that are\\ncurrent versions\\nof objects', 'Free', 'Cost\\nopti\\nion', 'Y\\nmiz', 'sum\\natn tVe\\nStor\\ntes)/\\nsum\\nStor\\ntes)', '(Curre\\nrsion\\nageBy\\n(\\nageBy'], ['Current version\\nobject count', 'CurrentVe\\nrsionObje\\nctCount', 'The count of\\ncurrent version\\nobjects', 'Free', 'Cost\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['% current\\nversion objects', '-', 'The percentage of\\nobjects in scope\\nthat are a current\\nversion', 'Free', 'Cost\\nopti\\nion', 'Y\\nmiz', 'sum\\natn tVe\\nObje\\nnt)/\\nsum\\nbject\\nt)', '(Curre\\nrsion\\nctCou\\n(O\\nCoun']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Noncurrent\\nversion bytes', 'NonCurren\\ntVersionS\\ntorageBytes', 'The number\\nof noncurrent\\nversion bytes', 'Free', 'Cost\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['% noncurrent\\nversion bytes', '-', 'The percentag\\ne of bytes in\\nscope that are\\nnoncurrent\\nversions', 'Free', 'Cost\\nopti\\nion', 'Y\\nmiz', 'sum\\natr rent\\nionS\\neByt\\ns\\num(\\neByt', '(NonCu\\nVers\\ntorag\\nes)/\\nStorag\\nes)'], ['Noncurrent\\nversion object\\ncount', 'NonCurren\\ntVersionO\\nbjectCount', 'The count of the\\nnoncurrent object\\nversions', 'Free', 'Cost\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['% noncurrent\\nversion objects', '-', 'The percentag\\ne of objects in\\nscope that are\\na noncurrent\\nversion', 'Free', 'Cost\\nopti\\nion', 'Y\\nmiz', 'sum\\natr rent\\nionO\\nCou\\nsu\\nm(O\\nount', '(NonCu\\nVers\\nbject\\nnt)/\\nbjectC\\n)'], ['Delete marker\\nbytes', 'DeleteMar\\nkerStorag\\neBytes', 'The number of\\nbytes in scope\\nthat are delete\\nmarkers', 'Free', 'Cost\\nopti\\nion', 'N\\nmiz', '-\\nat', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['% delete\\nmarker bytes', '-', 'The percentage\\nof bytes in scope\\nthat are delete\\nmarkers', 'Free', 'Cost\\nopti\\nion', 'Y\\nmiz', 'sum\\nate Ma\\norag\\ns)/\\nsum\\norag\\ns)', '(Delet\\nrkerSt\\neByte\\n(St\\neByte'], ['Delete marker\\nobject count', 'DeleteMar\\nkerObject\\nCount', 'The total number\\nof objects with a\\ndelete marker', 'Free', 'Cost\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['% delete\\nmarker objects', '-', 'The percentage of\\nobjects in scope\\nwith a delete\\nmarker', 'Free', 'Cost\\nopti\\nion', 'Y\\nmiz', 'sum\\nate Ma\\njectC\\n)/\\nsum\\nectC', '(Delet\\nrkerOb\\nount\\n(Obj\\nount)'], ['Incomplet\\ne multipart\\nupload bytes', 'Incomplet\\neMultipar\\ntUploadSt\\norageBytes', 'The total bytes\\nin scope for\\nincomplete\\nmultipart uploads', 'Free', 'Cost\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['% incomplet\\ne multipart\\nupload bytes', '-', 'The percentage\\nof bytes in scope\\nthat are the result\\nof incomplete\\nmultipart uploads', 'Free', 'Cost\\nopti\\nion', 'Y\\nmiz', 'sum\\natp let\\nipart\\nadSt\\nByte\\nsu\\nm(St\\nByte', '(Incom\\neMult\\nUplo\\norage\\ns)/\\norage\\ns)']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Incomplet\\ne multipart\\nupload object\\ncount', 'Incomplet\\neMultipar\\ntUploadOb\\njectCount', 'The number\\nof objects in\\nscope that are\\nincomplete\\nmultipart uploads', 'Free', 'Cost\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['% incomplet\\ne multipart\\nupload objects', '-', 'The percentag\\ne of objects\\nin scope that\\nare incomplete\\nmultipart uploads', 'Free', 'Cost\\nopti\\nion', 'Y\\nmiz', 'sum\\natp let\\nipart\\nadO\\nount\\nsum\\n(Obj\\nunt)', '(Incom\\neMult\\nUplo\\nbjectC\\n)/\\nectCo'], ['Incomplet\\ne multipart\\nupload storage\\nbytes greater\\nthan 7 days old', 'Incomplet\\neMPUStora\\ngeBytesOl\\nderThan7Days', 'The total bytes\\nin scope for\\nincomplete\\nmultipart uploads\\nthat are more\\nthan 7 days old', 'Free', 'Cost\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['% incomplet\\ne multipart\\nupload storage\\nbytes greater\\nthan 7 days old', '-', 'The percentag\\ne of bytes for\\nincomplete\\nmultipart uploads\\nthat are more\\nthan 7 days old', 'Free', 'Cost\\nopti\\nion', 'Y\\nmiz', 'sum\\natp let\\ntora\\nesOl\\nan7D\\nsum\\ngeBy', '(Incom\\neMPUS\\ngeByt\\nderTh\\nays)/\\n(Stora\\ntes)']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Incomplet\\ne multipart\\nupload object\\ncount greater\\nthan 7 days old', 'Incomplet\\neMPUObjec\\ntCountOld\\nerThan7Days', 'The number\\nof objects that\\nare incomplete\\nmultipart uploads\\nmore than 7 days\\nold', 'Free', 'Cost\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['% incomplet\\ne multipart\\nupload object\\ncount greater\\nthan 7 days old', '-', 'The percentage\\nof objects that\\nare incomplete\\nmultipart uploads\\nmore than 7 days\\nold', 'Free', 'Cost\\nopti\\nion', 'Y\\nmiz', 'sum\\natp let\\nbject\\ntOld\\nn7D\\ns\\num(\\nCou', '(Incom\\neMPUO\\nCoun\\nerTha\\nays)/\\nObject\\nnt)'], ['Transition\\nlifecycle rule\\ncount', 'Transitio\\nnLifecycl\\neRuleCount', 'The count of\\nlifecycle rules\\nto transition\\nobjects to another\\nstorage class', 'Adva', 'nCceodst\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['Average\\ntransition\\nlifecycle rules\\nper bucket', '-', 'The average\\nnumber of\\nlifecycle rules\\nto transition\\nobjects to another\\nstorage class', 'Adva', 'nCceodst\\nopti\\nion', 'Y\\nmiz', 'sum\\nati tion\\ncycle\\nCou\\nsu\\nm(Di\\ntNu\\nBuck', '(Trans\\nLife\\nRule\\nnt)/\\nstinc\\nmberOf\\nets)']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Expiration\\nlifecycle rule\\ncount', 'Expiratio\\nnLifecycl\\neRuleCount', 'The count of\\nlifecycle rules to\\nexpire objects', 'Adva', 'nCceodst\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['Average\\nexpiration\\nlifecycle rules\\nper bucket', '-', 'The average\\nnumber of\\nlifecycle rules to\\nexpire objects', 'Adva', 'nCceodst\\nopti\\nion', 'Y\\nmiz', 'sum\\nata tion\\ncycle\\nCou\\nsu\\nm(Di\\ntNu\\nBuck', '(Expir\\nLife\\nRule\\nnt)/\\nstinc\\nmberOf\\nets)'], ['Noncurren\\nt version\\ntransition\\nlifecycle rule\\ncount', 'Noncurren\\ntVersionT\\nransition\\nLifecycle\\nRuleCount', 'The count of\\nlifecycle rules\\nto transitio\\nn noncurrent\\nobject versions to\\nanother storage\\nclass', 'Adva', 'nCceodst\\nopti\\nion', 'N\\nmiz', 'at', ''], ['Average\\nnoncurren\\nt version\\ntransition\\nlifecycle rules\\nper bucket', '-', 'The average\\nnumber of\\nlifecycle rules\\nto transitio\\nn noncurrent\\nobject versions to\\nanother storage\\nclass', 'Adva', 'nCceodst\\nopti\\nion', 'Y\\nmiz', 'sum\\natr rent\\nionT\\ntion\\nycle\\nount\\nsum\\n(Dist\\nNum\\nucke', '(Noncu\\nVers\\nransi\\nLifec\\nRuleC\\n)/\\ninct\\nberOfB\\nts)']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Noncurren\\nt version\\nexpiration\\nlifecycle rule\\ncount', 'Noncurren\\ntVersionE\\nxpiration\\nLifecycle\\nRuleCount', 'The count of\\nlifecycle rules to\\nexpire noncurrent\\nobject versions', 'Adva', 'nCceodst\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['Average\\nnoncurren\\nt version\\nexpiration\\nlifecycle rules\\nper bucket', '-', 'The average\\nnumber of\\nlifecycle rules to\\nexpire noncurrent\\nobject versions', 'Adva', 'nCceodst\\nopti\\nion', 'Y\\nmiz', 'sum\\natr rent\\nionE\\ntion\\nycle\\nount\\nsum\\n(Dist\\nNum\\nucke', '(Noncu\\nVers\\nxpira\\nLifec\\nRuleC\\n)/\\ninct\\nberOfB\\nts)'], ['Abort\\nincomplet\\ne multipart\\nupload\\nlifecycle rule\\ncount', 'AbortInco\\nmpleteMPU\\nLifecycle\\nRuleCount', 'The count of\\nlifecycle rules to\\ndelete incomplete\\nmultipart uploads', 'Adva', 'nCceodst\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['Average abort\\nincomplet\\ne multipart\\nupload\\nlifecycle rules\\nper bucket', '-', 'The average\\nnumber of\\nlifecycle rules to\\ndelete incomplete\\nmultipart uploads', 'Adva', 'nCceodst\\nopti\\nion', 'Y\\nmiz', 'sum\\natI nco\\neMP\\nycle\\nount\\nsum\\n(Dist\\nNum\\nucke', '(Abort\\nmplet\\nULifec\\nRuleC\\n)/\\ninct\\nberOfB\\nts)']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Expired object\\ndelete marker\\nlifecycle rule\\ncount', 'ExpiredOb\\njectDelet\\neMarkerLi\\nfecycleRu\\nleCount', 'The count of\\nlifecycle rules to\\nremove expired\\nobject delete\\nmarkers', 'Adva', 'nCceodst\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['Average\\nexpired object\\ndelete marker\\nlifecycle rules\\nper bucket', '-', 'The average\\nnumber of\\nlifecycle rules to\\nremove expired\\nobject delete\\nmarkers', 'Adva', 'nCceodst\\nopti\\nion', 'Y\\nmiz', 'sum\\nate dO\\nelete\\nerLif\\nleRu\\nnt)/\\nsum\\nistin\\nmbe\\nkets)', '(Expir\\nbjectD\\nMark\\necyc\\nleCou\\n(D\\nctNu\\nrOfBuc'], ['Total lifecycle\\nrule count', 'TotalLife\\ncycleRuleCount', 'The total count of\\nlifecycle rules', 'Adva', 'nCceodst\\nopti\\nion', 'N\\nmiz', '-\\nat', ''], ['Average\\nlifecycle rule\\ncount per\\nbucket', '-', 'The average\\nnumber of\\nlifecycle rules', 'Adva', 'nCceodst\\nopti\\nion', 'Y\\nmiz', 'sum\\natL ifec\\nRule\\n)/\\nsum\\ntinct\\nerOf\\nts)', '(Total\\nycle\\nCount\\n(Dis\\nNumb\\nBucke'], ['Encrypted\\nbytes', 'Encrypted\\nStorageBytes', 'The total number\\nof encrypted\\nbytes', 'Free', 'Data\\nprot\\nn', 'N\\necti', '-\\no', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['% encrypted\\nbytes', '-', 'The percentage of\\ntotal bytes that\\nare encrypted', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\nop ted\\ntCou\\ns\\num(\\neByt', '(Encry\\nObjec\\nnt)/\\nStorag\\nes)'], ['Encrypted\\nobject count', 'Encrypted\\nObjectCount', 'The total count of\\nobjects that are\\nencrypted', 'Free', 'Data\\nprot\\nn', 'N\\necti', '-\\no', ''], ['% encrypted\\nobjects', '-', 'The percentage of\\nobjects that are\\nencrypted', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\nop ted\\ngeBy\\nsum\\ntCou', '(Encry\\nStora\\ntes)/\\n(Objec\\nnt)'], ['Unencrypted\\nbytes', 'Unencrypt\\nedStorage\\nBytes', 'The number of\\nbytes that are\\nunencrypted', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\nog eBy\\n-\\nsum\\npted\\ngeBy', '(Stora\\ntes)\\n(Encry\\nStora\\ntes)'], ['% unencrypted\\nbytes', '-', 'The percentage\\nof bytes that are\\nunencrypted', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\nor ypt\\nrage\\n)/\\nsum\\nrage\\n)', '(Unenc\\nedSto\\nBytes\\n(Sto\\nBytes']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Unencrypted\\nobject count', 'Unencrypt\\nedObjectCount', 'The total count of\\nobjects that are\\nunencrypted', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\not Cou\\n-\\nsum\\npted\\ntCou', '(Objec\\nnt)\\n(Encry\\nObjec\\nnt)'], ['% unencrypted\\nobjects', '-', 'The percentage\\nof unencrypted\\nobjects', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\nor ypt\\nrage\\n)/\\nsum\\nectC', '(Unenc\\nedSto\\nBytes\\n(Obj\\nount)'], ['Replicated\\nstorage bytes\\nsource', 'Replicate\\ndStorageB\\nytesSource', 'The total number\\nof bytes that are\\nreplicated from\\nthe source bucket', 'Free', 'Data\\nprot\\nn', 'N\\necti', '-\\no', ''], ['% replicated\\nbytes source', '-', 'The percentag\\ne of total bytes\\nthat are replicate\\nd from the source\\nbucket', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\noc ate\\nageB\\nourc\\nsu\\nm(St\\nByte', '(Repli\\ndStor\\nytesS\\ne)/\\norage\\ns)'], ['Replicated\\nobject count\\nsource', 'Replicate\\ndObjectCo\\nuntSource', 'The count of\\nreplicated objects\\nfrom the source\\nbucket', 'Free', 'Data\\nprot\\nn', 'N\\necti', '-\\no', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['% replicated\\nobjects source', '-', 'The percentage\\nof total objects\\nthat are replicate\\nd from the source\\nbucket', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\noc ate\\nageO\\nCou\\nsu\\nm(O\\nount', '(Repli\\ndStor\\nbject\\nnt)/\\nbjectC\\n)'], ['Replication\\nstorage bytes\\ndestination', 'Replicate\\ndStorageBytes', 'The total number\\nof bytes that\\nare replicated to\\nthe destination\\nbucket', 'Free', 'Data\\nprot\\nn', 'Y\\necti', '-\\no', ''], ['% replicated\\nbytes destinati\\non', '-', 'The percentage of\\ntotal bytes that\\nare replicated to\\nthe destination\\nbucket', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\noc ate\\nageB\\n/\\nsum\\nageB', '(Repli\\ndStor\\nytes)\\n(Stor\\nytes)'], ['Replicated\\nobject count\\ndestination', 'Replicate\\ndObjectCount', 'The count of\\nobjects that are\\nreplicated to\\nthe destination\\nbucket', 'Free', 'Data\\nprot\\nn', 'Y\\necti', '-\\no', ''], ['% replicate\\nd objects\\ndestination', '-', 'The percentage of\\ntotal objects that\\nare replicated to\\nthe destination\\nbucket', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\noc ate\\nctCo\\nsum\\ntCou', '(Repli\\ndObje\\nunt)/\\n(Objec\\nnt)']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Object Lock\\nbytes', 'ObjectLoc\\nkEnabledS\\ntorageBytes', 'The total count\\nof Object Lock\\nenabled storage\\nbytes', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\nor ypt\\nrage\\n)/\\nsum\\nectL\\nable\\nageC\\n-\\nsum\\nctLo\\nbled\\ngeBy', '(Unenc\\nedSto\\nBytes\\n(Obj\\nockEn\\ndStor\\nount)\\n(Obje\\nckEna\\nStora\\ntes)'], ['% Object Lock\\nbytes', '-', 'The percentage\\nof Object Lock\\nenabled storage\\nbytes', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\not Loc\\nledS\\neByt\\ns\\num(\\neByt', '(Objec\\nkEnab\\ntorag\\nes)/\\nStorag\\nes)'], ['Object Lock\\nobject count', 'ObjectLoc\\nkEnabledO\\nbjectCount', 'The total count\\nof Object Lock\\nobjects', 'Free', 'Data\\nprot\\nn', 'Y\\necti', '-\\no', ''], ['% Object Lock\\nobjects', '-', 'The percentage of\\ntotal objects that\\nhave Object Lock\\nenabled', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\not Loc\\nledO\\nCou\\nsu\\nm(O\\nount', '(Objec\\nkEnab\\nbject\\nnt)/\\nbjectC\\n)']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Versioning-\\nenabled bucket\\ncount', 'Versionin\\ngEnabledB\\nucketCount', 'The count of\\nbuckets that have\\nS3 Versioning\\nenabled', 'Free', 'Data\\nprot\\nn', 'N\\necti', '-\\no', ''], ['% versionin\\ng-enabled\\nbuckets', '-', 'The percentage\\nof buckets that\\nhave S3 Versionin\\ng enabled', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\noo nin\\nledB\\nCou\\nsu\\nm(Di\\ntNu\\nBuck', '(Versi\\ngEnab\\nucket\\nnt)/\\nstinc\\nmberOf\\nets)'], ['MFA delete-en\\nabled bucket\\ncount', 'MFADelete\\nEnabledBu\\ncketCount', 'The count of\\nbuckets that have\\nMFA (multi-factor\\nauthentication)\\ndelete enabled', 'Free', 'Data\\nprot\\nn', 'N\\necti', '-\\no', ''], ['% MFA delete-\\nenabled\\nbuckets', '-', 'The percentage of\\nbuckets that have\\nMFA (multi-factor\\nauthentication)\\ndelete enabled', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\nol eteE\\nedBu\\nount\\nsum\\n(Dist\\nNum\\nucke', '(MFADe\\nnabl\\ncketC\\n)/\\ninct\\nberOfB\\nts)']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['SSE-KMS\\nenabled bucket\\ncount', 'SSEKMSEna\\nbledBucke\\ntCount', 'The count of\\nbuckets that\\nuse server-si\\nde encryption\\nwith AWS Key\\nManagement\\nService keys (SSE-\\nKMS) for default\\nbucket encryption', 'Free', 'Data\\nprot\\nn', 'N\\necti', '-\\no', ''], ['% SSE-KMS\\nenabled\\nbuckets', '-', 'The percentage of\\nbuckets that SSE-\\nKMS for default\\nbucket encryption', 'Free', 'Data\\nprot\\nn', 'Y\\necti', 'sum\\noS Ena\\nucke\\nt)/\\nsum\\nstinc\\nberO\\nets)', '(SSEKM\\nbledB\\ntCoun\\n(Di\\ntNum\\nfBuck'], ['All unsupport\\ned signature\\nrequests', 'AllUnsupp\\nortedSign\\natureRequests', 'The total number\\nof requests that\\nuse unsupported\\nAWS signature\\nversions', 'Adva', 'nDceadta\\nprot\\nn', 'N\\necti', '-\\no', ''], ['% all\\nunsupport\\ned signature\\nrequests', '-', 'The percentage of\\nrequests that use\\nunsupported AWS\\nsignature versions', 'Adva', 'nDceadta\\nprot\\nn', 'Y\\necti', 'sum\\nos upp\\nSign\\nRequ\\n/\\nsum\\neque', '(AllUn\\norted\\nature\\nests)\\n(AllR\\nsts)']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['All unsupport\\ned TLS\\nrequests', 'AllUnsupp\\nortedTLSR\\nequests', 'The number of\\nrequests that\\nuse unsupported\\nTransport Layer\\nSecurity (TLS)\\nversions', 'Adva', 'nDceadta\\nprot\\nn', 'N\\necti', '-\\no', ''], ['% all\\nunsupported\\nTLS requests', '-', 'The percentage of\\nrequests that use\\nunsupported TLS\\nversions', 'Adva', 'nDceadta\\nprot\\nn', 'Y\\necti', 'sum\\nos upp\\nTLSR\\nts)/\\nsum\\nllReq\\ns)', '(AllUn\\norted\\neques\\n(A\\nuest'], ['All SSE-KMS\\nrequests', 'AllSSEKMS\\nRequests', 'The total number\\nof requests that\\nspecify SSE-KMS', 'Adva', 'nDceadta\\nprot\\nn', 'N\\necti', '-\\no', ''], ['% all SSE-KMS\\nrequests', '-', 'The percentage\\nof requests that\\nspecify SSE-KMS', 'Adva', 'nDceadta\\nprot\\nn', 'Y\\necti', 'sum\\noE KM\\nsts)/\\nsum\\nAllR\\nts)', '(AllSS\\nSReque\\n(\\neques'], ['Same-Region\\nReplication rule\\ncount', 'SameRegio\\nnReplicat\\nionRuleCount', 'The count of\\nreplication rules\\nfor Same-Region\\nReplication (SRR)', 'Adva', 'nDceadta\\nprot\\nn', 'N\\necti', '-\\no', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Average\\nSame-Regi\\non Replicati\\non rules per\\nbucket', '-', 'The average\\nnumber of\\nreplication rules\\nfor SRR', 'Adva', 'nDceadta\\nprot\\nn', 'Y\\necti', 'sum\\noe gio\\nicati\\nleCo\\nsum\\nnctN\\nOfBu\\n)', '(SameR\\nnRepl\\nonRu\\nunt)/\\n(Disti\\number\\nckets'], ['Cross-Region\\nReplication rule\\ncount', 'CrossRegi\\nonReplica\\ntionRuleCount', 'The count of\\nreplication rules\\nfor Cross-Region\\nReplication (CRR)', 'Adva', 'nDceadta\\nprot\\nn', 'N\\necti', '-\\no', ''], ['Average\\nCross-Reg\\nion Replicati\\non rules per\\nbucket', '-', 'The average\\nnumber of\\nreplication rules\\nfor CRR', 'Adva', 'nDceadta\\nprot\\nn', 'Y\\necti', 'sum\\noR egi\\nlicati\\nuleC\\n/\\nsum\\ninct\\nrOfB\\ns)', '(Cross\\nonRep\\nonR\\nount)\\n(Dist\\nNumbe\\nucket'], ['Same-account\\nreplication rule\\ncount', 'SameAccou\\nntReplica\\ntionRuleCount', 'The count of\\nreplication rules\\nfor replication\\nwithin the same\\naccount', 'Adva', 'nDceadta\\nprot\\nn', 'N\\necti', '-\\no', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Average\\nsame-acco\\nunt replicati\\non rules per\\nbucket', '-', 'The average\\nnumber of\\nreplication rules\\nfor replication\\nwithin the same\\naccount', 'Adva', 'nDceadta\\nprot\\nn', 'Y\\necti', 'sum\\noc cou\\nlicati\\nuleC\\n/\\nsum\\ninct\\nrOfB\\ns)', '(SameA\\nntRep\\nonR\\nount)\\n(Dist\\nNumbe\\nucket'], ['Cross-account\\nreplication rule\\ncount', 'CrossAcco\\nuntReplic\\nationRuleCount', 'The count of\\nreplication rules\\nfor cross-account\\nreplication', 'Adva', 'nDceadta\\nprot\\nn', 'N\\necti', '-\\no', ''], ['Average\\ncross-acc\\nount replicati\\non rules per\\nbucket', '-', 'The average\\nnumber of\\nreplication rules\\nfor cross-account\\nreplication', 'Adva', 'nDceadta\\nprot\\nn', 'Y\\necti', 'sum\\noA cco\\nplica\\nRule\\n)/\\nsum\\ntinct\\nerOf\\nts)', '(Cross\\nuntRe\\ntion\\nCount\\n(Dis\\nNumb\\nBucke'], ['Invalid\\ndestination\\nreplication rule\\ncount', 'InvalidDe\\nstination\\nReplicati\\nonRuleCount', \"The count of\\nreplication rules\\nwith a replication\\ndestination that's\\nnot valid\", 'Adva', 'nDceadta\\nprot\\nn', 'N\\necti', '-\\no', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Average invalid\\ndestinati\\non replicati\\non rules per\\nbucket', '-', \"The average\\nnumber of\\nreplication rules\\nwith a replication\\ndestination that's\\nnot valid\", 'Adva', 'nDceadta\\nprot\\nn', 'Y\\necti', 'sum\\noi dRe\\ntion\\nount\\nsum\\n(Dist\\nNum\\nucke', '(Inval\\nplica\\nRuleC\\n)/\\ninct\\nberOfB\\nts)'], ['Total replicati\\non rule count', '-', 'The total replicati\\non rule count', 'Adva', 'nDceadta\\nprot\\nn', 'Y\\necti', '-\\no', ''], ['Average\\nreplication\\nrule count per\\nbucket', '-', 'The average total\\nreplication rule\\ncount', 'Adva', 'nDceadta\\nprot\\nn', 'Y\\necti', 'sum\\nor epli\\non\\nrule\\ncoun\\nmetr\\nsum\\nnctN\\nOfBu\\n)', '(all\\ncati\\nt\\nics)/\\n(Disti\\number\\nckets'], ['Object\\nOwnership\\nbucket owner\\nenforced\\nbucket count', 'ObjectOwn\\nershipBuc\\nketOwnerE\\nnforcedBu\\ncketCount', 'The total count of\\nbuckets that have\\naccess control lists\\n(ACLs) disabled by\\nusing the bucket\\nowner enforced\\nsetting for Object\\nOwnership', 'Free', 'Acce\\nman\\nt', 'ssN\\nage', '-\\nmen', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['% Object\\nOwnership\\nbucket owner\\nenforced\\nbuckets', '-', 'The percentage of\\nbuckets that have\\nACLs disabled by\\nusing the bucket\\nowner enforced\\nsetting for Object\\nOwnership', 'Free', 'Acce\\nman\\nt', 'ssY\\nage', 'sum\\nmtOenw\\npBuc\\nnerE\\nedBu\\nount\\nsum\\n(Dist\\nNum\\nucke', '(Objec\\nnershi\\nketOw\\nnforc\\ncketC\\n)/\\ninct\\nberOfB\\nts)'], ['Object\\nOwnership\\nbucket owner\\npreferred\\nbucket count', 'ObjectOwn\\nershipBuc\\nketOwnerP\\nreferredB\\nucketCount', 'The total count\\nof buckets that\\nuse the bucket\\nowner preferred\\nsetting for Object\\nOwnership', 'Free', 'Acce\\nman\\nt', 'ssN\\nage', '-\\nmen', ''], ['% Object\\nOwnership\\nbucket owner\\npreferred\\nbuckets', '-', 'The percentage\\nof buckets that\\nuse the bucket\\nowner preferred\\nsetting for Object\\nOwnership', 'Free', 'Acce\\nman\\nt', 'ssY\\nage', 'sum\\nmtOenw\\npBuc\\nnerP\\nredB\\nCou\\nsu\\nm(Di\\ntNu\\nBuck', '(Objec\\nnershi\\nketOw\\nrefer\\nucket\\nnt)/\\nstinc\\nmberOf\\nets)']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Object\\nOwnership\\nobject writer\\nbucket count', 'ObjectOwn\\nershipObj\\nectWriter\\nBucketCount', 'The total count of\\nbuckets that use\\nthe object writer\\nsetting for Object\\nOwnership', 'Free', 'Acce\\nman\\nt', 'ssN\\nage', '-\\nmen', ''], ['% Object\\nOwnership\\nobject writer\\nbuckets', '-', 'The percentage of\\nbuckets that use\\nthe object writer\\nsetting for Object\\nOwnership', 'Free', 'Acce\\nman\\nt', 'ssY\\nage', 'sum\\nmtOenw\\npOb\\niterB\\ntCou\\ns\\num(\\nctNu\\nfBuc', '(Objec\\nnershi\\njectWr\\nucke\\nnt)/\\nDistin\\nmberO\\nkets)'], ['Transfer\\nAcceleration\\nenabled bucket\\ncount', 'TransferA\\nccelerati\\nonEnabled\\nBucketCount', 'The total count of\\nbuckets that have\\nTransfer Accelerat\\nion enabled', 'Free', 'Perf\\nce', 'orNm', 'a-n', ''], ['% Transfer\\nAccelerat\\nion enabled\\nbuckets', '-', 'The percentage of\\nbuckets that have\\nTransfer Accelerat\\nion enabled', 'Free', 'Perf\\nce', 'orYm', 'asnu m\\nferA\\nratio\\nbled\\ntCou\\ns\\num(\\nctNu\\nfBuc', '(Trans\\nccele\\nnEna\\nBucke\\nnt)/\\nDistin\\nmberO\\nkets)']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['Event Notificat\\nion enabled\\nbucket count', 'EventNoti\\nficationE\\nnabledBuc\\nketCount', 'The total count of\\nbuckets that have\\nEvent Notificat\\nions enabled', 'Free', 'Even', 'tsN', '', ''], ['% Event\\nNotificat\\nion enabled\\nbuckets', '-', 'The percentage of\\nbuckets that have\\nEvent Notificat\\nions enabled', 'Free', 'Even', 'tsY', 'sum\\nNoti\\nionE\\ndBuc\\nunt)\\nsum\\nDisti\\numb\\ncket', '(Event\\nficat\\nnable\\nketCo\\n/\\n(\\nnctN\\nerOfBu\\ns)'], ['All requests', 'AllRequests', 'The total number\\nof requests made', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['Get requests', 'GetRequests', 'The total number\\nof GET requests\\nmade', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['Put requests', 'PutRequests', 'The total number\\nof PUT requests\\nmade', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['Head requests', 'HeadRequests', 'The total number\\nof HEAD requests\\nmade', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['Delete requests', 'DeleteRequests', 'The total number\\nof DELETE\\nrequests made', 'Adva', 'nAcecdti', 'vitNy', '-', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['List requests', 'ListRequests', 'The total number\\nof LIST requests\\nmade', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['Post requests', 'PostRequests', 'The total number\\nof POST requests\\nmade', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['Select requests', 'SelectRequests', 'The total number\\nof S3 Select\\nrequests', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['Select scanned\\nbytes', 'SelectSca\\nnnedBytes', 'The number of\\nS3 Select bytes\\nscanned', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['Select returned\\nbytes', 'SelectRet\\nurnedBytes', 'The number of\\nS3 Select bytes\\nreturned', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['Bytes\\ndownloaded', 'BytesDown\\nloaded', 'The number of\\nbytes downloaded', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['% retrieval rate', '-', 'The percentage of\\nbytes downloaded', 'Adva', 'nAcecdti', 'vitYy', 'sum\\nDow\\nd)/\\nsum\\norag\\ns)', '(Bytes\\nnloade\\n(St\\neByte'], ['Bytes uploaded', 'BytesUploaded', 'The number of\\nbytes uploaded', 'Adva', 'nAcecdti', 'vitNy', '-', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['% ingest ratio', '-', 'The percentage of\\nbytes uploaded', 'Adva', 'nAcecdti', 'vitYy', 'sum\\nUplo\\n/\\nsum\\nageB', '(Bytes\\naded)\\n(Stor\\nytes)'], ['4xx errors', '4xxErrors', 'The total number\\nof HTTP 4xx\\nstatus codes', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['5xx errors', '5xxErrors', 'The total number\\nof HTTP 5xx\\nstatus codes', 'Adva', 'nAcecdti', 'vitNy', '-', ''], ['Total errors', '-', 'The sum of all 4xx\\nand 5xx errors', 'Adva', 'nAcecdti', 'vitYy', 'sum\\nrors)\\n+\\nsum\\nrors)', '(4xxEr\\n(5xxEr'], ['% error rate', '-', 'The total number\\nof 4xx and\\n5xx errors as a\\npercentage of\\ntotal requests', 'Adva', 'nAcecdti', 'vitYy', 'sum\\nErro\\ns\\num(\\neque', '(Total\\nrs)/\\nTotalR\\nsts)'], ['200 OK status\\ncount', '200OKStat\\nusCount', 'The total count\\nof 200 OK status\\ncodes', 'Adva', 'nDceedta\\nstat\\ncode', 'ilNe\\nus', 'd -', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['% 200 OK\\nstatus', '-', 'The total number\\nof 200 OK\\nstatus codes as\\na percentage of\\ntotal requests', 'Adva', 'nDceedta\\nstat\\ncode', 'ilYe\\nus', 'd sum\\nStat\\nnt)/\\nsum\\nllReq\\ns)', '(200OK\\nusCou\\n(A\\nuest'], ['206 Partial\\nContent status\\ncount', '206Partia\\nlContentS\\ntatusCount', 'The total count\\nof 206 Partial\\nContent status\\ncodes', 'Adva', 'nDceedta\\nstat\\ncode', 'ilNe\\nus', 'd -', ''], ['% 206 Partial\\nContent status', '-', 'The total number\\nof 206 Partial\\nContent status\\ncodes as a\\npercentage of\\ntotal requests', 'Adva', 'nDceedta\\nstat\\ncode', 'ilYe\\nus', 'd sum\\nrtial\\nentS\\nCou\\nsu\\nm(Al\\nests)', '(206Pa\\nCont\\ntatus\\nnt)/\\nlRequ'], ['400 Bad\\nRequest error\\ncount', '400BadReq\\nuestErrorCount', 'The total count of\\n400 Bad Request\\nstatus codes', 'Adva', 'nDceedta\\nstat\\ncode', 'ilNe\\nus', 'd -', ''], ['% 400 Bad\\nRequest errors', '-', 'The total number\\nof 400 Bad\\nRequest status\\ncodes as a\\npercentage of\\ntotal requests', 'Adva', 'nDceedta\\nstat\\ncode', 'ilYe\\nus', 'd sum\\ndReq\\nrrorC\\n)/\\nsum\\nRequ', '(400Ba\\nuestE\\nount\\n(All\\nests)'], ['403 Forbidden\\nerror count', '403Forbid\\ndenErrorCount', 'The total count\\nof 403 Forbidden\\nstatus codes', 'Adva', 'nDceedta\\nstat\\ncode', 'ilNe\\nus', 'd -', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['% 403\\nForbidden\\nerrors', '-', 'The total number\\nof 403 Forbidden\\nstatus codes as\\na percentage of\\ntotal requests', 'Adva', 'nDceedta\\nstat\\ncode', 'ilYe\\nus', 'd sum\\nrbid\\nrorC\\n/\\nsum\\neque', '(403Fo\\ndenEr\\nount)\\n(AllR\\nsts)'], ['404 Not Found\\nerror count', '404NotFou\\nndErrorCount', 'The total count\\nof 404 Not Found\\nstatus codes', 'Adva', 'nDceedta\\nstat\\ncode', 'ilNe\\nus', 'd -', ''], ['% 404 Not\\nFound errors', '-', 'The total number\\nof 404 Not Found\\nstatus codes as\\na percentage of\\ntotal requests', 'Adva', 'nDceedta\\nstat\\ncode', 'ilYe\\nus', 'd sum\\ntFou\\norCo\\nsum\\nques', '(404No\\nndErr\\nunt)/\\n(AllRe\\nts)'], ['500 Internal\\nServer Error\\ncount', '500Intern\\nalServerE\\nrrorCount', 'The total count\\nof 500 Internal\\nServer Error\\nstatus codes', 'Adva', 'nDceedta\\nstat\\ncode', 'ilNe\\nus', 'd -', ''], ['% 500 Internal\\nServer Errors', '-', 'The total number\\nof 500 Internal\\nServer Error\\nstatus codes as\\na percentage of\\ntotal requests', 'Adva', 'nDceedta\\nstat\\ncode', 'ilYe\\nus', 'd sum\\ntern\\nverE\\nount\\nsum\\n(AllR\\nsts)', '(500In\\nalSer\\nrrorC\\n)/\\neque'], ['503 Service\\nUnavailable\\nerror count', '503Servic\\neUnavaila\\nbleErrorCount', 'The total count\\nof 503 Service\\nUnavailable status\\ncodes', 'Adva', 'nDceedta\\nstat\\ncode', 'ilNe\\nus', 'd -', '']]]\n",
      "[[['Metric name', 'CloudWatch\\nand export', 'Description', '1\\nTier', 'Cate', 'gDo', '2\\nreyrDiveerdi\\nmet\\nform', 'ved\\nric\\nula'], ['% 503 Service\\nUnavailable\\nerrors', '-', 'The total number\\nof 503 Service\\nUnavailable\\nstatus codes as\\na percentage of\\ntotal requests', 'Adva', 'nDceedta\\nstat\\ncode', 'ilYe\\nus', 'd sum\\nrvice\\nailab\\nrorC\\n/\\nsum\\neque', '(503Se\\nUnav\\nleEr\\nount)\\n(AllR\\nsts)']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nAny updates to your dashboard configuration can take up to 48 hours to accurately display\\nor visualize.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't change this dashboard name after the dashboard is created.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can add up to 50 tags to your dashboard configuration.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• You can either include or exclude Regions and buckets. This option is limited to\\nRegions only when creating organization-level dashboards across member accounts\\nin your organization.\\n• You can choose up to 50 buckets to include or exclude.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nIf you enable prefix aggregation for your S3 Storage Lens configuration, prefix-level\\nmetrics will not be published to CloudWatch. Only bucket, account, and organization-\\nlevel S3 Storage Lens metrics are published to CloudWatch.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't change the following:\\n• The dashboard name\\n• The home Region\\n• The dashboard scope of the default dashboard, which is scoped to your entire\\naccount's storage\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can add up to 50 tags to your dashboard configuration.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• You can either include or exclude Regions and buckets. This option is limited\\nto Regions only when creating organization-level dashboards across member\\naccounts in your organization.\\n• You can choose up to 50 buckets to include or exclude.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf you enable prefix aggregation for your S3 Storage Lens configuration, prefix-level\\nmetrics will not be published to CloudWatch. Only bucket, account, and organization-\\nlevel S3 Storage Lens metrics are published to CloudWatch.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nThe AWS managed key (aws/S3) is not supported for SSE-KMS encryption with S3\\nStorage Lens.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't delete the default dashboard. However, you can disable it. Before deleting a\\ndashboard that you've created, consider the following:\\n• As an alternative to deleting a dashboard, you can disable the dashboard so that it is\\navailable to be re-enabled in the future. For more information, see Disabling an Amazon\\nS3 Storage Lens dashboard.\\n• Deleting the dashboard deletes all the configuration settings that are associated with it.\\n• Deleting a dashboard makes all the historic metrics data unavailable. This historical data\\nis still retained for 15 months. If you want to access this data again, create a dashboard\\nwith the same name in the same home Region as the one that was deleted.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\n• Trusted access can be enabled only by the management account.\\n• Only the management account and delegated administrators can create S3 Storage Lens\\ndashboards or configurations for your organization.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\n• Disabling trusted access also automatically disables all organization-level dashboards\\nbecause S3 Storage Lens will no longer have trusted access to the organization accounts\\nto collect and aggregate storage metrics.', '']]]\n",
      "[[['', '• The management and delegate administrator accounts can still see the historic data for\\nthese disabled dashboards and can query this data while it is available.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• Deregistering a delegated administrator also automatically disables all organization-level\\ndashboards created by the delegated administrator.\\n• The delegate administrator accounts can still see the historic data for these disabled\\ndashboards according to the respective period that data is available for queries.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAdditional charges apply for advanced metrics and recommendations. For more\\ninformation, see advanced metrics and recommendations.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Id\": \"SampleS3StorageLensConfiguration\", //Use this property to identify your S3\\nStorage Lens configuration.\\n\"AwsOrg\": { //Use this property when enabling S3 Storage Lens for AWS Organizations.\\n\"Arn\": \"arn:aws:organizations::123456789012:organization/o-abcdefgh\"\\n},\\n\"AccountLevel\": {\\n\"ActivityMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"AdvancedCostOptimizationMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"AdvancedDataProtectionMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"DetailedStatusCodesMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"BucketLevel\": {\\n\"ActivityMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"AdvancedDataProtectionMetrics\": {\\n\"IsEnabled\":true', '']]]\n",
      "[[['', '},\\n\"AdvancedCostOptimizationMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"DetailedStatusCodesMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"PrefixLevel\":{\\n\"StorageMetrics\":{\\n\"IsEnabled\":true,\\n\"SelectionCriteria\":{\\n\"MaxDepth\":5,\\n\"MinStorageBytesPercentage\":1.25,\\n\"Delimiter\":\"/\"\\n}\\n}\\n}\\n}\\n},\\n\"Exclude\": { //Replace with \"Include\" if you prefer to include Regions.\\n\"Regions\": [\\n\"eu-west-1\"\\n],\\n\"Buckets\": [ //This attribute is not supported for AWS Organizations-level\\nconfigurations.\\n\"arn:aws:s3:::source_bucket1\"\\n]\\n},\\n\"IsEnabled\": true, //Whether the configuration is enabled\\n\"DataExport\": { //Details about the metrics export\\n\"S3BucketDestination\": {\\n\"OutputSchemaVersion\": \"V_1\",\\n\"Format\": \"CSV\", //You can add \"Parquet\" if you prefer.\\n\"AccountId\": \"111122223333\",\\n\"Arn\": \"arn:aws:s3:::destination-bucket-name\", // The destination bucket for your\\nmetrics export must be in the same Region as your S3 Storage Lens configuration.\\n\"Prefix\": \"prefix-for-your-export-destination\",\\n\"Encryption\": {\\n\"SSES3\": {}\\n}\\n},\\n\"CloudWatchMetrics\": {\\n\"IsEnabled\": true\\n}', '']]]\n",
      "[[['', '}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Id\": \"ExampleS3StorageLensConfiguration\",\\n\"AccountLevel\": {\\n\"ActivityMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"AdvancedCostOptimizationMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"AdvancedDataProtectionMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"BucketLevel\": {\\n\"ActivityMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"StorageLensGroupLevel\": {},\\n\"IsEnabled\": true\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Id\": \"ExampleS3StorageLensConfiguration\",\\n\"AccountLevel\": {\\n\"ActivityMetrics\": {\\n\"IsEnabled\":true', '']]]\n",
      "[[['', '},\\n\"AdvancedCostOptimizationMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"AdvancedDataProtectionMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"BucketLevel\": {\\n\"ActivityMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"StorageLensGroupLevel\": {\\n\"SelectionCriteria\": {\\n\"Include\": [\\n\"arn:aws:s3:us-east-1:111122223333:storage-lens-group/slg-1\",\\n\"arn:aws:s3:us-east-1:444455556666:storage-lens-group/slg-2\"\\n]\\n},\\n\"IsEnabled\": true\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Id\": \"ExampleS3StorageLensConfiguration\",\\n\"AccountLevel\": {\\n\"ActivityMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"AdvancedCostOptimizationMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"AdvancedDataProtectionMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"BucketLevel\": {\\n\"ActivityMetrics\": {\\n\"IsEnabled\":true\\n},\\n\"StorageLensGroupLevel\": {\\n\"SelectionCriteria\": {\\n\"Exclude\": [', '']]]\n",
      "[[['', '\"arn:aws:s3:us-east-1:111122223333:storage-lens-group/slg-1\",\\n\"arn:aws:s3:us-east-1:444455556666:storage-lens-group/slg-2\"\\n]\\n},\\n\"IsEnabled\": true\\n}', ''], ['', '', '']], [['', '', ''], ['', '[\\n{\\n\"Key\": \"key1\",\\n\"Value\": \"value1\"\\n},\\n{\\n\"Key\": \"key2\",\\n\"Value\": \"value2\"\\n}\\n]', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetStorageLensConfiguration\",', '']]]\n",
      "[[['', '\"s3:DeleteStorageLensConfiguration\",\\n\"s3:PutStorageLensConfiguration\"\\n],\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:ResourceTag/key1\": \"value1\"\\n}\\n},\\n\"Resource\": \"arn:aws:s3:us-east-1:example-account-id:storage-lens/your-\\ndashboard-name\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetStorageLensConfiguration\",\\n\"s3:DeleteStorageLensConfiguration\",\\n\"s3:PutStorageLensConfiguration\"\\n],\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:ResourceTag/key1\": \"value1\"\\n}\\n},\\n\"Resource\": \"arn:aws:s3:us-east-1:example-account-id:storage-lens/*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control put-storage-lens-configuration --account-id=111122223333 --\\nconfig-id=example-dashboard-configuration-id --region=us-east-1 --storage-lens-\\nconfiguration=file://./config.json --tags=file://./tags.json', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-storage-lens-configuration --account-id=222222222222 --config-\\nid=your-configuration-id --region=us-east-1 --storage-lens-configuration=file://./\\nconfig.json', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control get-storage-lens-configuration --account-id=222222222222 --config-\\nid=your-configuration-id --region=us-east-1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control list-storage-lens-configurations --account-id=222222222222 --region=us-\\neast-1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control list-storage-lens-configurations --account-id=222222222222 --region=us-\\neast-1 --next-token=abcdefghij1234', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control delete-storage-lens-configuration --account-id=222222222222 --region=us-\\neast-1 --config-id=your-configuration-id', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-storage-lens-configuration-tagging --account-id=222222222222 --\\nregion=us-east-1 --config-id=your-configuration-id --tags=file://./tags.json', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control get-storage-lens-configuration-tagging --account-id=222222222222 --\\nregion=us-east-1 --config-id=your-configuration-id', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control delete-storage-lens-configuration-tagging --account-id=222222222222 --\\nregion=us-east-1 --config-id=your-configuration-id', ''], ['', '', '']], [['', '', ''], ['', 'aws organizations enable-aws-service-access --service-principal storage-\\nlens.s3.amazonaws.com', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws organizations disable-aws-service-access --service-principal storage-\\nlens.s3.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'aws organizations register-delegated-administrator --service-principal storage-\\nlens.s3.amazonaws.com --account-id 111122223333', ''], ['', '', '']], [['', '', ''], ['', 'aws organizations deregister-delegated-administrator --service-principal storage-\\nlens.s3.amazonaws.com --account-id 111122223333', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.AccountLevel;\\nimport com.amazonaws.services.s3control.model.ActivityMetrics;\\nimport com.amazonaws.services.s3control.model.BucketLevel;\\nimport com.amazonaws.services.s3control.model.CloudWatchMetrics;\\nimport com.amazonaws.services.s3control.model.Format;\\nimport com.amazonaws.services.s3control.model.Include;\\nimport com.amazonaws.services.s3control.model.OutputSchemaVersion;\\nimport com.amazonaws.services.s3control.model.PrefixLevel;\\nimport com.amazonaws.services.s3control.model.PrefixLevelStorageMetrics;\\nimport com.amazonaws.services.s3control.model.PutStorageLensConfigurationRequest;', '']]]\n",
      "[[['', 'import com.amazonaws.services.s3control.model.S3BucketDestination;\\nimport com.amazonaws.services.s3control.model.SSES3;\\nimport com.amazonaws.services.s3control.model.SelectionCriteria;\\nimport com.amazonaws.services.s3control.model.StorageLensAwsOrg;\\nimport com.amazonaws.services.s3control.model.StorageLensConfiguration;\\nimport com.amazonaws.services.s3control.model.StorageLensDataExport;\\nimport com.amazonaws.services.s3control.model.StorageLensDataExportEncryption;\\nimport com.amazonaws.services.s3control.model.StorageLensTag;\\nimport java.util.Arrays;\\nimport java.util.List;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class CreateAndUpdateDashboard {\\npublic static void main(String[] args) {\\nString configurationId = \"ConfigurationId\";\\nString sourceAccountId = \"Source Account ID\";\\nString exportAccountId = \"Destination Account ID\";\\nString exportBucketArn = \"arn:aws:s3:::destBucketName\"; // The destination\\nbucket for your metrics export must be in the same Region as your S3 Storage Lens\\nconfiguration.\\nString awsOrgARN = \"arn:aws:organizations::123456789012:organization/o-\\nabcdefgh\";\\nFormat exportFormat = Format.CSV;\\ntry {\\nSelectionCriteria selectionCriteria = new SelectionCriteria()\\n.withDelimiter(\"/\")\\n.withMaxDepth(5)\\n.withMinStorageBytesPercentage(10.0);\\nPrefixLevelStorageMetrics prefixStorageMetrics = new\\nPrefixLevelStorageMetrics()\\n.withIsEnabled(true)\\n.withSelectionCriteria(selectionCriteria);\\nBucketLevel bucketLevel = new BucketLevel()\\n.withActivityMetrics(new ActivityMetrics().withIsEnabled(true))\\n.withAdvancedCostOptimizationMetrics(new\\nAdvancedCostOptimizationMetrics().withIsEnabled(true))\\n.withAdvancedDataProtectionMetrics(new\\nAdvancedDataProtectionMetrics().withIsEnabled(true))\\n.withDetailedStatusCodesMetrics(new\\nDetailedStatusCodesMetrics().withIsEnabled(true))', '']]]\n",
      "[[['', '.withPrefixLevel(new\\nPrefixLevel().withStorageMetrics(prefixStorageMetrics));\\nAccountLevel accountLevel = new AccountLevel()\\n.withActivityMetrics(new ActivityMetrics().withIsEnabled(true))\\n.withAdvancedCostOptimizationMetrics(new\\nAdvancedCostOptimizationMetrics().withIsEnabled(true))\\n.withAdvancedDataProtectionMetrics(new\\nAdvancedDataProtectionMetrics().withIsEnabled(true))\\n.withDetailedStatusCodesMetrics(new\\nDetailedStatusCodesMetrics().withIsEnabled(true))\\n.withBucketLevel(bucketLevel);\\nInclude include = new Include()\\n.withBuckets(Arrays.asList(\"arn:aws:s3:::bucketName\"))\\n.withRegions(Arrays.asList(\"us-west-2\"));\\nStorageLensDataExportEncryption exportEncryption = new\\nStorageLensDataExportEncryption()\\n.withSSES3(new SSES3());\\nS3BucketDestination s3BucketDestination = new S3BucketDestination()\\n.withAccountId(exportAccountId)\\n.withArn(exportBucketArn)\\n.withEncryption(exportEncryption)\\n.withFormat(exportFormat)\\n.withOutputSchemaVersion(OutputSchemaVersion.V_1)\\n.withPrefix(\"Prefix\");\\nCloudWatchMetrics cloudWatchMetrics = new CloudWatchMetrics()\\n.withIsEnabled(true);\\nStorageLensDataExport dataExport = new StorageLensDataExport()\\n.withCloudWatchMetrics(cloudWatchMetrics)\\n.withS3BucketDestination(s3BucketDestination);\\nStorageLensAwsOrg awsOrg = new StorageLensAwsOrg()\\n.withArn(awsOrgARN);\\nStorageLensConfiguration configuration = new StorageLensConfiguration()\\n.withId(configurationId)\\n.withAccountLevel(accountLevel)\\n.withInclude(include)\\n.withDataExport(dataExport)\\n.withAwsOrg(awsOrg)\\n.withIsEnabled(true);\\nList<StorageLensTag> tags = Arrays.asList(', '']]]\n",
      "[[['', 'new StorageLensTag().withKey(\"key-1\").withValue(\"value-1\"),\\nnew StorageLensTag().withKey(\"key-2\").withValue(\"value-2\")\\n);\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\ns3ControlClient.putStorageLensConfiguration(new\\nPutStorageLensConfigurationRequest()\\n.withAccountId(sourceAccountId)\\n.withConfigId(configurationId)\\n.withStorageLensConfiguration(configuration)\\n.withTags(tags)\\n);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.DeleteStorageLensConfigurationRequest;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;', '']]]\n",
      "[[['', 'public class DeleteDashboard {\\npublic static void main(String[] args) {\\nString configurationId = \"ConfigurationId\";\\nString sourceAccountId = \"Source Account ID\";\\ntry {\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\ns3ControlClient.deleteStorageLensConfiguration(new\\nDeleteStorageLensConfigurationRequest()\\n.withAccountId(sourceAccountId)\\n.withConfigId(configurationId)\\n);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.GetStorageLensConfigurationRequest;\\nimport com.amazonaws.services.s3control.model.GetStorageLensConfigurationResult;\\nimport com.amazonaws.services.s3control.model.StorageLensConfiguration;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;', '']]]\n",
      "[[['', 'public class GetDashboard {\\npublic static void main(String[] args) {\\nString configurationId = \"ConfigurationId\";\\nString sourceAccountId = \"Source Account ID\";\\ntry {\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\nfinal StorageLensConfiguration configuration =\\ns3ControlClient.getStorageLensConfiguration(new\\nGetStorageLensConfigurationRequest()\\n.withAccountId(sourceAccountId)\\n.withConfigId(configurationId)\\n).getStorageLensConfiguration();\\nSystem.out.println(configuration.toString());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;', '']]]\n",
      "[[['', 'import com.amazonaws.services.s3control.model.ListStorageLensConfigurationEntry;\\nimport com.amazonaws.services.s3control.model.ListStorageLensConfigurationsRequest;\\nimport java.util.List;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class ListDashboard {\\npublic static void main(String[] args) {\\nString sourceAccountId = \"Source Account ID\";\\nString nextToken = \"nextToken\";\\ntry {\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\nfinal List<ListStorageLensConfigurationEntry> configurations =\\ns3ControlClient.listStorageLensConfigurations(new\\nListStorageLensConfigurationsRequest()\\n.withAccountId(sourceAccountId)\\n.withNextToken(nextToken)\\n).getStorageLensConfigurationList();\\nSystem.out.println(configurations.toString());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;', '']]]\n",
      "[[['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport\\ncom.amazonaws.services.s3control.model.PutStorageLensConfigurationTaggingRequest;\\nimport com.amazonaws.services.s3control.model.StorageLensTag;\\nimport java.util.Arrays;\\nimport java.util.List;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class PutDashboardTagging {\\npublic static void main(String[] args) {\\nString configurationId = \"ConfigurationId\";\\nString sourceAccountId = \"Source Account ID\";\\ntry {\\nList<StorageLensTag> tags = Arrays.asList(\\nnew StorageLensTag().withKey(\"key-1\").withValue(\"value-1\"),\\nnew StorageLensTag().withKey(\"key-2\").withValue(\"value-2\")\\n);\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\ns3ControlClient.putStorageLensConfigurationTagging(new\\nPutStorageLensConfigurationTaggingRequest()\\n.withAccountId(sourceAccountId)\\n.withConfigId(configurationId)\\n.withTags(tags)\\n);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client', '']]]\n",
      "[[['', \"// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.DeleteStorageLensConfigurationRequest;\\nimport\\ncom.amazonaws.services.s3control.model.GetStorageLensConfigurationTaggingRequest;\\nimport com.amazonaws.services.s3control.model.StorageLensTag;\\nimport java.util.List;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class GetDashboardTagging {\\npublic static void main(String[] args) {\\nString configurationId = \"ConfigurationId\";\\nString sourceAccountId = \"Source Account ID\";\\ntry {\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\nfinal List<StorageLensTag> s3Tags = s3ControlClient\\n.getStorageLensConfigurationTagging(new\\nGetStorageLensConfigurationTaggingRequest()\\n.withAccountId(sourceAccountId)\\n.withConfigId(configurationId)\\n).getTags();', '']]]\n",
      "[[['', \"System.out.println(s3Tags.toString());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport\\ncom.amazonaws.services.s3control.model.DeleteStorageLensConfigurationTaggingRequest;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class DeleteDashboardTagging {\\npublic static void main(String[] args) {\\nString configurationId = \"ConfigurationId\";\\nString sourceAccountId = \"Source Account ID\";\\ntry {\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\ns3ControlClient.deleteStorageLensConfigurationTagging(new\\nDeleteStorageLensConfigurationTaggingRequest()\\n.withAccountId(sourceAccountId)\\n.withConfigId(configurationId)', '']]]\n",
      "[[['', \");\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.AccountLevel;\\nimport com.amazonaws.services.s3control.model.ActivityMetrics;\\nimport com.amazonaws.services.s3control.model.BucketLevel;\\nimport com.amazonaws.services.s3control.model.Format;\\nimport com.amazonaws.services.s3control.model.Include;\\nimport com.amazonaws.services.s3control.model.OutputSchemaVersion;\\nimport com.amazonaws.services.s3control.model.PrefixLevel;\\nimport com.amazonaws.services.s3control.model.PrefixLevelStorageMetrics;\\nimport com.amazonaws.services.s3control.model.PutStorageLensConfigurationRequest;\\nimport com.amazonaws.services.s3control.model.S3BucketDestination;\\nimport com.amazonaws.services.s3control.model.SSES3;\\nimport com.amazonaws.services.s3control.model.SelectionCriteria;\\nimport com.amazonaws.services.s3control.model.StorageLensAwsOrg;\\nimport com.amazonaws.services.s3control.model.StorageLensConfiguration;\\nimport com.amazonaws.services.s3control.model.StorageLensDataExport;\\nimport com.amazonaws.services.s3control.model.StorageLensDataExportEncryption;\\nimport com.amazonaws.services.s3control.model.StorageLensTag;', '']]]\n",
      "[[['', 'import java.util.Arrays;\\nimport java.util.List;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class UpdateDefaultConfigWithPaidFeatures {\\npublic static void main(String[] args) {\\nString configurationId = \"default-account-dashboard\"; // This configuration ID\\ncannot be modified.\\nString sourceAccountId = \"Source Account ID\";\\ntry {\\nSelectionCriteria selectionCriteria = new SelectionCriteria()\\n.withDelimiter(\"/\")\\n.withMaxDepth(5)\\n.withMinStorageBytesPercentage(10.0);\\nPrefixLevelStorageMetrics prefixStorageMetrics = new\\nPrefixLevelStorageMetrics()\\n.withIsEnabled(true)\\n.withSelectionCriteria(selectionCriteria);\\nBucketLevel bucketLevel = new BucketLevel()\\n.withActivityMetrics(new ActivityMetrics().withIsEnabled(true))\\n.withPrefixLevel(new\\nPrefixLevel().withStorageMetrics(prefixStorageMetrics));\\nAccountLevel accountLevel = new AccountLevel()\\n.withActivityMetrics(new ActivityMetrics().withIsEnabled(true))\\n.withBucketLevel(bucketLevel);\\nStorageLensConfiguration configuration = new StorageLensConfiguration()\\n.withId(configurationId)\\n.withAccountLevel(accountLevel)\\n.withIsEnabled(true);\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\ns3ControlClient.putStorageLensConfiguration(new\\nPutStorageLensConfigurationRequest()\\n.withAccountId(sourceAccountId)\\n.withConfigId(configurationId)\\n.withStorageLensConfiguration(configuration)', '']]]\n",
      "[[['', \");\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nAdditional charges apply for advanced metrics and recommendations. For more\\ninformation, see advanced metrics and recommendations.', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.BucketLevel;\\nimport com.amazonaws.services.s3control.model.PutStorageLensConfigurationRequest;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.model.AccountLevel;\\nimport com.amazonaws.services.s3control.model.StorageLensConfiguration;\\nimport com.amazonaws.services.s3control.model.StorageLensGroupLevel;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;', '']]]\n",
      "[[['', 'public class CreateDashboardWithStorageLensGroups {\\npublic static void main(String[] args) {\\nString configurationId = \"ExampleDashboardConfigurationId\";\\nString sourceAccountId = \"111122223333\";\\ntry {\\nStorageLensGroupLevel storageLensGroupLevel = new StorageLensGroupLevel();\\nAccountLevel accountLevel = new AccountLevel()\\n.withBucketLevel(new BucketLevel())\\n.withStorageLensGroupLevel(storageLensGroupLevel);\\nStorageLensConfiguration configuration = new StorageLensConfiguration()\\n.withId(configurationId)\\n.withAccountLevel(accountLevel)\\n.withIsEnabled(true);\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\ns3ControlClient.putStorageLensConfiguration(new\\nPutStorageLensConfigurationRequest()\\n.withAccountId(sourceAccountId)\\n.withConfigId(configurationId)\\n.withStorageLensConfiguration(configuration)\\n);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.AccountLevel;\\nimport com.amazonaws.services.s3control.model.BucketLevel;\\nimport com.amazonaws.services.s3control.model.PutStorageLensConfigurationRequest;\\nimport com.amazonaws.services.s3control.model.StorageLensConfiguration;\\nimport com.amazonaws.services.s3control.model.StorageLensGroupLevel;\\nimport com.amazonaws.services.s3control.model.StorageLensGroupLevelSelectionCriteria;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class CreateDashboardWith2StorageLensGroups {\\npublic static void main(String[] args) {\\nString configurationId = \"ExampleDashboardConfigurationId\";\\nString storageLensGroupName1 = \"StorageLensGroupName1\";\\nString storageLensGroupName2 = \"StorageLensGroupName2\";\\nString sourceAccountId = \"111122223333\";\\ntry {\\nStorageLensGroupLevelSelectionCriteria selectionCriteria = new\\nStorageLensGroupLevelSelectionCriteria()\\n.withInclude(\\n\"arn:aws:s3:\" + US_WEST_2.getName() + \":\" + sourceAccountId\\n+ \":storage-lens-group/\" + storageLensGroupName1,\\n\"arn:aws:s3:\" + US_WEST_2.getName() + \":\" + sourceAccountId\\n+ \":storage-lens-group/\" + storageLensGroupName2);\\nSystem.out.println(selectionCriteria);\\nStorageLensGroupLevel storageLensGroupLevel = new StorageLensGroupLevel()\\n.withSelectionCriteria(selectionCriteria);\\nAccountLevel accountLevel = new AccountLevel()', '']]]\n",
      "[[['', \".withBucketLevel(new BucketLevel())\\n.withStorageLensGroupLevel(storageLensGroupLevel);\\nStorageLensConfiguration configuration = new StorageLensConfiguration()\\n.withId(configurationId)\\n.withAccountLevel(accountLevel)\\n.withIsEnabled(true);\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();\\ns3ControlClient.putStorageLensConfiguration(new\\nPutStorageLensConfigurationRequest()\\n.withAccountId(sourceAccountId)\\n.withConfigId(configurationId)\\n.withStorageLensConfiguration(configuration)\\n);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;', '']]]\n",
      "[[['', 'import com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\nimport com.amazonaws.services.s3control.model.AccountLevel;\\nimport com.amazonaws.services.s3control.model.BucketLevel;\\nimport com.amazonaws.services.s3control.model.PutStorageLensConfigurationRequest;\\nimport com.amazonaws.services.s3control.model.StorageLensConfiguration;\\nimport com.amazonaws.services.s3control.model.StorageLensGroupLevel;\\nimport com.amazonaws.services.s3control.model.StorageLensGroupLevelSelectionCriteria;\\nimport static com.amazonaws.regions.Regions.US_WEST_2;\\npublic class CreateDashboardWith2StorageLensGroupsExcluded {\\npublic static void main(String[] args) {\\nString configurationId = \"ExampleDashboardConfigurationId\";\\nString storageLensGroupName1 = \"StorageLensGroupName1\";\\nString storageLensGroupName2 = \"StorageLensGroupName2\";\\nString sourceAccountId = \"111122223333\";\\ntry {\\nStorageLensGroupLevelSelectionCriteria selectionCriteria = new\\nStorageLensGroupLevelSelectionCriteria()\\n.withInclude(\\n\"arn:aws:s3:\" + US_WEST_2.getName() + \":\" + sourceAccountId\\n+ \":storage-lens-group/\" + storageLensGroupName1,\\n\"arn:aws:s3:\" + US_WEST_2.getName() + \":\" + sourceAccountId\\n+ \":storage-lens-group/\" + storageLensGroupName2);\\nSystem.out.println(selectionCriteria);\\nStorageLensGroupLevel storageLensGroupLevel = new StorageLensGroupLevel()\\n.withSelectionCriteria(selectionCriteria);\\nAccountLevel accountLevel = new AccountLevel()\\n.withBucketLevel(new BucketLevel())\\n.withStorageLensGroupLevel(storageLensGroupLevel);\\nStorageLensConfiguration configuration = new StorageLensConfiguration()\\n.withId(configurationId)\\n.withAccountLevel(accountLevel)\\n.withIsEnabled(true);\\nAWSS3Control s3ControlClient = AWSS3ControlClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(US_WEST_2)\\n.build();', '']]]\n",
      "[[['', \"s3ControlClient.putStorageLensConfiguration(new\\nPutStorageLensConfigurationRequest()\\n.withAccountId(sourceAccountId)\\n.withConfigId(configurationId)\\n.withStorageLensConfiguration(configuration)\\n);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.organizations.AWSOrganizations;', '']]]\n",
      "[[['', 'import com.amazonaws.services.organizations.AWSOrganizationsClient;\\nimport com.amazonaws.services.organizations.model.EnableAWSServiceAccessRequest;\\npublic class EnableOrganizationsTrustedAccess {\\nprivate static final String S3_STORAGE_LENS_SERVICE_PRINCIPAL = \"storage-\\nlens.s3.amazonaws.com\";\\npublic static void main(String[] args) {\\ntry {\\nAWSOrganizations organizationsClient = AWSOrganizationsClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(Regions.US_EAST_1)\\n.build();\\norganizationsClient.enableAWSServiceAccess(new\\nEnableAWSServiceAccessRequest()\\n.withServicePrincipal(S3_STORAGE_LENS_SERVICE_PRINCIPAL));\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but AWS Organizations couldn\\'t\\nprocess\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// AWS Organizations couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from AWS Organizations.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.organizations.AWSOrganizations;\\nimport com.amazonaws.services.organizations.AWSOrganizationsClient;\\nimport com.amazonaws.services.organizations.model.DisableAWSServiceAccessRequest;\\npublic class DisableOrganizationsTrustedAccess {', '']]]\n",
      "[[['', 'private static final String S3_STORAGE_LENS_SERVICE_PRINCIPAL = \"storage-\\nlens.s3.amazonaws.com\";\\npublic static void main(String[] args) {\\ntry {\\nAWSOrganizations organizationsClient = AWSOrganizationsClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(Regions.US_EAST_1)\\n.build();\\n// Make sure to remove any existing delegated administrator for S3 Storage\\nLens\\n// before disabling access; otherwise, the request will fail.\\norganizationsClient.disableAWSServiceAccess(new\\nDisableAWSServiceAccessRequest()\\n.withServicePrincipal(S3_STORAGE_LENS_SERVICE_PRINCIPAL));\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but AWS Organizations couldn\\'t\\nprocess\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// AWS Organizations couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from AWS Organizations.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.organizations.AWSOrganizations;\\nimport com.amazonaws.services.organizations.AWSOrganizationsClient;\\nimport\\ncom.amazonaws.services.organizations.model.RegisterDelegatedAdministratorRequest;\\npublic class RegisterOrganizationsDelegatedAdministrator {', '']]]\n",
      "[[['', 'private static final String S3_STORAGE_LENS_SERVICE_PRINCIPAL = \"storage-\\nlens.s3.amazonaws.com\";\\npublic static void main(String[] args) {\\ntry {\\nString delegatedAdminAccountId = \"111122223333\"; // Account Id for the\\ndelegated administrator.\\nAWSOrganizations organizationsClient = AWSOrganizationsClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(Regions.US_EAST_1)\\n.build();\\norganizationsClient.registerDelegatedAdministrator(new\\nRegisterDelegatedAdministratorRequest()\\n.withAccountId(delegatedAdminAccountId)\\n.withServicePrincipal(S3_STORAGE_LENS_SERVICE_PRINCIPAL));\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but AWS Organizations couldn\\'t\\nprocess\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// AWS Organizations couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from AWS Organizations.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.organizations.AWSOrganizations;\\nimport com.amazonaws.services.organizations.AWSOrganizationsClient;\\nimport\\ncom.amazonaws.services.organizations.model.DeregisterDelegatedAdministratorRequest;\\npublic class DeregisterOrganizationsDelegatedAdministrator {', '']]]\n",
      "[[['', 'private static final String S3_STORAGE_LENS_SERVICE_PRINCIPAL = \"storage-\\nlens.s3.amazonaws.com\";\\npublic static void main(String[] args) {\\ntry {\\nString delegatedAdminAccountId = \"111122223333\"; // Account Id for the\\ndelegated administrator.\\nAWSOrganizations organizationsClient = AWSOrganizationsClient.builder()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(Regions.US_EAST_1)\\n.build();\\norganizationsClient.deregisterDelegatedAdministrator(new\\nDeregisterDelegatedAdministratorRequest()\\n.withAccountId(delegatedAdminAccountId)\\n.withServicePrincipal(S3_STORAGE_LENS_SERVICE_PRINCIPAL));\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but AWS Organizations couldn\\'t\\nprocess\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// AWS Organizations couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from AWS Organizations.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\n• When you create a S3 Storage Lens group, you're creating an AWS resource. Therefore,\\neach Storage Lens group has its own Amazon Resource Name (ARN), which you can\\nspecify when attaching it to or excluding it from a S3 Storage Lens dashboard.\", '']]]\n",
      "[[['', \"• If your Storage Lens group isn't attached to a dashboard, you won't incur any additional\\ncharges for creating a Storage Lens group.\\n• S3 Storage Lens aggregates usage metrics for an object under all matching Storage Lens\\ngroups. Therefore, if an object matches the filter conditions for two or more Storage Lens\\ngroups, you will see repeated counts for the same object across your storage usage.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo view aggregated metrics for your Storage Lens group, you must attach the group to an\\nS3 Storage Lens dashboard.', ''], ['', '', '']], [['Action', 'IAM permissions'], ['Create a new Storage Lens group', 's3:CreateStorageLensGroup'], ['Create a new Storage Lens group with tags', 's3:CreateStorageLensGroup ,\\ns3:TagResource'], ['Update an existing Storage Lens group', 's3:UpdateStorageLensGroup'], ['Return the details of a Storage Lens group\\nconfiguration', 's3:GetStorageLensGroup'], ['List all Storage Lens groups in your home\\nRegion', 's3:ListStorageLensGroups'], ['Delete a Storage Lens group', 's3:DeleteStorageLensGroup']]]\n",
      "[[['Action', 'IAM permissions'], ['List the tags that were added to your Storage\\nLens group', 's3:ListTagsForResource'], ['Add or update a Storage Lens group tag for an\\nexisting Storage Lens group', 's3:TagResource'], ['Delete a tag from a Storage Lens group', 's3:UntagResource']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"EXAMPLE-Statement-ID\",\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:CreateStorageLensGroup\",\\n\"s3:UpdateStorageLensGroup\",\\n\"s3:GetStorageLensGroup\",\\n\"s3:ListStorageLensGroups\",\\n\"s3:DeleteStorageLensGroup,\\n\"s3:TagResource\",\\n\"s3:UntagResource\",\\n\"s3:ListTagsForResource\"\\n],\\n\"Resource\": \"arn:aws:s3:us-east-1:111122223333:storage-lens-group/example-\\nstorage-lens-group\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Name\": \"Marketing-Department\",\\n\"Filter\": {\\n\"MatchAnyTag\":[\\n{\\n\"Key\": \"object-tag-key-1\",\\n\"Value\": \"object-tag-value-1\"\\n},\\n{\\n\"Key\": \"object-tag-key-2\",\\n\"Value\": \"object-tag-value-2\"\\n}\\n]\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Name\": \"Marketing-Department\",\\n\"Filter\": {\\n\"And\": {\\n\"MatchAnyPrefix\": [\\n\"prefix-1\",\\n\"prefix-2\",\\n\"prefix-3/sub-prefix-1\"\\n],\\n\"MatchObjectAge\": {\\n\"DaysGreaterThan\": 10,\\n\"DaysLessThan\": 60\\n},\\n\"MatchObjectSize\": {\\n\"BytesGreaterThan\": 10,\\n\"BytesLessThan\": 60\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you want to include objects that match any of the conditions in the filters, replace the\\nAnd logical operator with the Or logical operator in this example.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3control create-storage-lens-group --account-id 111122223333 \\\\\\n--region us-east-1 --storage-lens-group=file://./marketing-department.json', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-storage-lens-group --account-id 111122223333 \\\\\\n--region us-east-1 --storage-lens-group=file://./marketing-department.json \\\\', '']]]\n",
      "[[['', '--tags Key=k1,Value=v1 Key=k2,Value=v2', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.CreateStorageLensGroupRequest;\\nimport software.amazon.awssdk.services.s3control.model.MatchObjectAge;\\nimport software.amazon.awssdk.services.s3control.model.StorageLensGroup;\\nimport software.amazon.awssdk.services.s3control.model.StorageLensGroupFilter;\\npublic class CreateStorageLensGroupWithObjectAge {\\npublic static void main(String[] args) {\\nString storageLensGroupName = \"Marketing-Department\";\\nString accountId = \"111122223333\";\\ntry {\\nStorageLensGroupFilter objectAgeFilter = StorageLensGroupFilter.builder()\\n.matchObjectAge(MatchObjectAge.builder()\\n.daysGreaterThan(30)\\n.daysLessThan(90)\\n.build())\\n.build();\\nStorageLensGroup storageLensGroup = StorageLensGroup.builder()\\n.name(storageLensGroupName)\\n.filter(objectAgeFilter)', '']]]\n",
      "[[['', \".build();\\nCreateStorageLensGroupRequest createStorageLensGroupRequest =\\nCreateStorageLensGroupRequest.builder()\\n.storageLensGroup(storageLensGroup)\\n.accountId(accountId).build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();\\ns3ControlClient.createStorageLensGroup(createStorageLensGroupRequest);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.CreateStorageLensGroupRequest;\\nimport software.amazon.awssdk.services.s3control.model.MatchObjectAge;\\nimport software.amazon.awssdk.services.s3control.model.MatchObjectSize;\\nimport software.amazon.awssdk.services.s3control.model.S3Tag;\\nimport software.amazon.awssdk.services.s3control.model.StorageLensGroup;\\nimport software.amazon.awssdk.services.s3control.model.StorageLensGroupAndOperator;', '']]]\n",
      "[[['', 'import software.amazon.awssdk.services.s3control.model.StorageLensGroupFilter;\\npublic class CreateStorageLensGroupWithAndFilter {\\npublic static void main(String[] args) {\\nString storageLensGroupName = \"Marketing-Department\";\\nString accountId = \"111122223333\";\\ntry {\\n// Create object tags.\\nS3Tag tag1 = S3Tag.builder()\\n.key(\"object-tag-key-1\")\\n.value(\"object-tag-value-1\")\\n.build();\\nS3Tag tag2 = S3Tag.builder()\\n.key(\"object-tag-key-2\")\\n.value(\"object-tag-value-2\")\\n.build();\\nStorageLensGroupAndOperator andOperator =\\nStorageLensGroupAndOperator.builder()\\n.matchAnyPrefix(\"prefix-1\", \"prefix-2\", \"prefix-3/sub-prefix-1\")\\n.matchAnySuffix(\".png\", \".gif\", \".jpg\")\\n.matchAnyTag(tag1, tag2)\\n.matchObjectAge(MatchObjectAge.builder()\\n.daysGreaterThan(30)\\n.daysLessThan(90).build())\\n.matchObjectSize(MatchObjectSize.builder()\\n.bytesGreaterThan(1000L)\\n.bytesLessThan(6000L).build())\\n.build();\\nStorageLensGroupFilter andFilter = StorageLensGroupFilter.builder()\\n.and(andOperator)\\n.build();\\nStorageLensGroup storageLensGroup = StorageLensGroup.builder()\\n.name(storageLensGroupName)\\n.filter(andFilter)\\n.build();\\nCreateStorageLensGroupRequest createStorageLensGroupRequest =\\nCreateStorageLensGroupRequest.builder()\\n.storageLensGroup(storageLensGroup)', '']]]\n",
      "[[['', \".accountId(accountId).build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();\\ns3ControlClient.createStorageLensGroup(createStorageLensGroupRequest);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.CreateStorageLensGroupRequest;\\nimport software.amazon.awssdk.services.s3control.model.MatchObjectSize;\\nimport software.amazon.awssdk.services.s3control.model.StorageLensGroup;\\nimport software.amazon.awssdk.services.s3control.model.StorageLensGroupFilter;\\nimport software.amazon.awssdk.services.s3control.model.StorageLensGroupOrOperator;\\npublic class CreateStorageLensGroupWithOrFilter {\\npublic static void main(String[] args) {\\nString storageLensGroupName = \"Marketing-Department\";\\nString accountId = \"111122223333\";', '']]]\n",
      "[[['', 'try {\\nStorageLensGroupOrOperator orOperator =\\nStorageLensGroupOrOperator.builder()\\n.matchAnyPrefix(\"prefix-1\", \"prefix-2\", \"prefix-3/sub-prefix-1\")\\n.matchObjectSize(MatchObjectSize.builder()\\n.bytesGreaterThan(1000L)\\n.bytesLessThan(6000L)\\n.build())\\n.build();\\nStorageLensGroupFilter orFilter = StorageLensGroupFilter.builder()\\n.or(orOperator)\\n.build();\\nStorageLensGroup storageLensGroup = StorageLensGroup.builder()\\n.name(storageLensGroupName)\\n.filter(orFilter)\\n.build();\\nCreateStorageLensGroupRequest createStorageLensGroupRequest =\\nCreateStorageLensGroupRequest.builder()\\n.storageLensGroup(storageLensGroup)\\n.accountId(accountId).build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();\\ns3ControlClient.createStorageLensGroup(createStorageLensGroupRequest);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.CreateStorageLensGroupRequest;\\nimport software.amazon.awssdk.services.s3control.model.StorageLensGroup;\\nimport software.amazon.awssdk.services.s3control.model.StorageLensGroupFilter;\\nimport software.amazon.awssdk.services.s3control.model.Tag;\\npublic class CreateStorageLensGroupWithResourceTags {\\npublic static void main(String[] args) {\\nString storageLensGroupName = \"Marketing-Department\";\\nString accountId = \"111122223333\";\\ntry {\\n// Create AWS resource tags.\\nTag resourceTag1 = Tag.builder()\\n.key(\"resource-tag-key-1\")\\n.value(\"resource-tag-value-1\")\\n.build();\\nTag resourceTag2 = Tag.builder()\\n.key(\"resource-tag-key-2\")\\n.value(\"resource-tag-value-2\")\\n.build();\\nStorageLensGroupFilter suffixFilter = StorageLensGroupFilter.builder()\\n.matchAnySuffix(\".png\", \".gif\", \".jpg\")\\n.build();\\nStorageLensGroup storageLensGroup = StorageLensGroup.builder()\\n.name(storageLensGroupName)\\n.filter(suffixFilter)\\n.build();', '']]]\n",
      "[[['', \"CreateStorageLensGroupRequest createStorageLensGroupRequest =\\nCreateStorageLensGroupRequest.builder()\\n.storageLensGroup(storageLensGroup)\\n.tags(resourceTag1, resourceTag2)\\n.accountId(accountId).build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();\\ns3ControlClient.createStorageLensGroup(createStorageLensGroupRequest);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you want to view aggregated metrics for your Storage Lens group, you must attach it\\nto your Storage Lens dashboard. For examples of Storage Lens group JSON configuration\\nfiles, see S3 Storage Lens example configuration with Storage Lens groups in JSON.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nBy default, Advanced metrics is also selected. However, you can also deselect this\\nsetting as it's not required to aggregate Storage Lens groups data.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIt will take up to 48 hours for your dashboard to reflect the configuration updates.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nYou can't apply a Storage Lens group filter along with a prefix filter, or the reverse. You also\\ncan't further analyze a Storage Lens group by using a prefix filter.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAfter you permanently delete an AWS resource tag, it can’t be restored.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-storage-lens-group --account-id 111122223333 \\\\\\n--region us-east-1 --name marketing-department', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control update-storage-lens-group --account-id 111122223333 \\\\\\n--region us-east-1 --storage-lens-group=file://./marketing-department.json', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.GetStorageLensGroupRequest;\\nimport software.amazon.awssdk.services.s3control.model.GetStorageLensGroupResponse;\\npublic class GetStorageLensGroup {\\npublic static void main(String[] args) {\\nString storageLensGroupName = \"Marketing-Department\";\\nString accountId = \"111122223333\";\\ntry {\\nGetStorageLensGroupRequest getRequest =\\nGetStorageLensGroupRequest.builder()', '']]]\n",
      "[[['', \".name(storageLensGroupName)\\n.accountId(accountId).build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();\\nGetStorageLensGroupResponse response =\\ns3ControlClient.getStorageLensGroup(getRequest);\\nSystem.out.println(response);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.StorageLensGroup;\\nimport software.amazon.awssdk.services.s3control.model.StorageLensGroupFilter;\\nimport software.amazon.awssdk.services.s3control.model.UpdateStorageLensGroupRequest;\\npublic class UpdateStorageLensGroup {\\npublic static void main(String[] args) {\\nString storageLensGroupName = \"Marketing-Department\";\\nString accountId = \"111122223333\";\\ntry {', '']]]\n",
      "[[['', '// Create updated filter.\\nStorageLensGroupFilter suffixFilter = StorageLensGroupFilter.builder()\\n.matchAnySuffix(\".png\", \".gif\", \".jpg\", \".jpeg\")\\n.build();\\nStorageLensGroup storageLensGroup = StorageLensGroup.builder()\\n.name(storageLensGroupName)\\n.filter(suffixFilter)\\n.build();\\nUpdateStorageLensGroupRequest updateStorageLensGroupRequest =\\nUpdateStorageLensGroupRequest.builder()\\n.name(storageLensGroupName)\\n.storageLensGroup(storageLensGroup)\\n.accountId(accountId)\\n.build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();\\ns3ControlClient.updateStorageLensGroup(updateStorageLensGroupRequest);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nAdding a new tag with the same key as an existing tag overwrites the previous tag\\nvalue.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control tag-resource --account-id 111122223333 \\\\\\n--resource-arn arn:aws:s3:us-east-1:111122223333:storage-lens-group/marketing-\\ndepartment \\\\\\n--region us-east-1 --tags Key=k1,Value=v1 Key=k2,Value=v2', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.Tag;\\nimport software.amazon.awssdk.services.s3control.model.TagResourceRequest;\\npublic class TagResource {\\npublic static void main(String[] args) {\\nString resourceARN = \"Resource_ARN\";\\nString accountId = \"111122223333\";\\ntry {\\nTag resourceTag1 = Tag.builder()\\n.key(\"resource-tag-key-1\")\\n.value(\"resource-tag-value-1\")\\n.build();\\nTag resourceTag2 = Tag.builder()\\n.key(\"resource-tag-key-2\")\\n.value(\"resource-tag-value-2\")\\n.build();\\nTagResourceRequest tagResourceRequest = TagResourceRequest.builder()\\n.resourceArn(resourceARN)\\n.tags(resourceTag1, resourceTag2)\\n.accountId(accountId)\\n.build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();\\ns3ControlClient.tagResource(tagResourceRequest);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}', '']]]\n",
      "[[['', '}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nAdding a new tag with the same key as an existing tag overwrites the previous tag\\nvalue.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control tag-resource --account-id 111122223333 \\\\\\n--resource-arn arn:aws:s3:us-east-1:111122223333:storage-lens-group/marketing-\\ndepartment \\\\\\n--region us-east-1 --tags Key=k1,Value=v3 Key=k2,Value=v4', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.Tag;\\nimport software.amazon.awssdk.services.s3control.model.TagResourceRequest;\\npublic class UpdateTagsForResource {\\npublic static void main(String[] args) {\\nString resourceARN = \"Resource_ARN\";\\nString accountId = \"111122223333\";\\ntry {\\nTag updatedResourceTag1 = Tag.builder()\\n.key(\"resource-tag-key-1\")\\n.value(\"resource-tag-updated-value-1\")\\n.build();\\nTag updatedResourceTag2 = Tag.builder()\\n.key(\"resource-tag-key-2\")\\n.value(\"resource-tag-updated-value-2\")\\n.build();\\nTagResourceRequest tagResourceRequest = TagResourceRequest.builder()\\n.resourceArn(resourceARN)\\n.tags(updatedResourceTag1, updatedResourceTag2)\\n.accountId(accountId)\\n.build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();', '']]]\n",
      "[[['', \"s3ControlClient.tagResource(tagResourceRequest);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf tags are used to control access, proceeding with this action can affect related\\nresources. After you permanently delete a tag, it can't be restored.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control untag-resource --account-id 111122223333 \\\\\\n--resource-arn arn:aws:s3:us-east-1:111122223333:storage-lens-group/Marketing-\\nDepartment \\\\\\n--region us-east-1 --tag-keys k1 k2', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.UntagResourceRequest;\\npublic class UntagResource {\\npublic static void main(String[] args) {\\nString resourceARN = \"Resource_ARN\";\\nString accountId = \"111122223333\";\\ntry {\\nString tagKey1 = \"resource-tag-key-1\";\\nString tagKey2 = \"resource-tag-key-2\";\\nUntagResourceRequest untagResourceRequest = UntagResourceRequest.builder()\\n.resourceArn(resourceARN)\\n.tagKeys(tagKey1, tagKey2)\\n.accountId(accountId)\\n.build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())', '']]]\n",
      "[[['', \".build();\\ns3ControlClient.untagResource(untagResourceRequest);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3control list-tags-for-resource --account-id 111122223333 \\\\\\n--resource-arn arn:aws:s3:us-east-1:111122223333:storage-lens-group/marketing-\\ndepartment \\\\', '']]]\n",
      "[[['', '--region us-east-1', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.ListTagsForResourceRequest;\\nimport software.amazon.awssdk.services.s3control.model.ListTagsForResourceResponse;\\npublic class ListTagsForResource {\\npublic static void main(String[] args) {\\nString resourceARN = \"Resource_ARN\";\\nString accountId = \"111122223333\";\\ntry {\\nListTagsForResourceRequest listTagsForResourceRequest =\\nListTagsForResourceRequest.builder()\\n.resourceArn(resourceARN)\\n.accountId(accountId)\\n.build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();\\nListTagsForResourceResponse response =\\ns3ControlClient.listTagsForResource(listTagsForResourceRequest);\\nSystem.out.println(response);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.', '']]]\n",
      "[[['', 'e.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control list-storage-lens-groups --account-id 111122223333 \\\\\\n--region us-east-1', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;', '']]]\n",
      "[[['', 'import software.amazon.awssdk.services.s3control.model.ListStorageLensGroupsRequest;\\nimport software.amazon.awssdk.services.s3control.model.ListStorageLensGroupsResponse;\\npublic class ListStorageLensGroups {\\npublic static void main(String[] args) {\\nString accountId = \"111122223333\";\\ntry {\\nListStorageLensGroupsRequest listStorageLensGroupsRequest =\\nListStorageLensGroupsRequest.builder()\\n.accountId(accountId)\\n.build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();\\nListStorageLensGroupsResponse response =\\ns3ControlClient.listStorageLensGroups(listStorageLensGroupsRequest);\\nSystem.out.println(response);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-storage-lens-group --account-id 111122223333 \\\\\\n--region us-east-1 --name marketing-department', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.GetStorageLensGroupRequest;\\nimport software.amazon.awssdk.services.s3control.model.GetStorageLensGroupResponse;\\npublic class GetStorageLensGroup {\\npublic static void main(String[] args) {\\nString storageLensGroupName = \"Marketing-Department\";\\nString accountId = \"111122223333\";\\ntry {\\nGetStorageLensGroupRequest getRequest =\\nGetStorageLensGroupRequest.builder()\\n.name(storageLensGroupName)\\n.accountId(accountId).build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()', '']]]\n",
      "[[['', \".region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();\\nGetStorageLensGroupResponse response =\\ns3ControlClient.getStorageLensGroup(getRequest);\\nSystem.out.println(response);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it and returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nAfter you delete a Storage Lens group, it can't be restored.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control delete-storage-lens-group --account-id 111122223333 \\\\\\n--region us-east-1 --name marketing-department', ''], ['', '', '']], [['', '', ''], ['', 'package aws.example.s3control;\\nimport com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.auth.credentials.ProfileCredentialsProvider;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.DeleteStorageLensGroupRequest;\\npublic class DeleteStorageLensGroup {\\npublic static void main(String[] args) {\\nString storageLensGroupName = \"Marketing-Department\";\\nString accountId = \"111122223333\";\\ntry {\\nDeleteStorageLensGroupRequest deleteStorageLensGroupRequest =\\nDeleteStorageLensGroupRequest.builder()\\n.name(storageLensGroupName)\\n.accountId(accountId).build();\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(Region.US_WEST_2)\\n.credentialsProvider(ProfileCredentialsProvider.create())\\n.build();\\ns3ControlClient.deleteStorageLensGroup(deleteStorageLensGroupRequest);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it and returned an error response.', '']]]\n",
      "[[['', \"e.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nYou can use the AWS Amplify Console to host a single-page web app. The AWS Amplify\\nConsole supports single-page apps built with single-page app frameworks (for example,\\nReact JS, Vue JS, Angular JS, and Nuxt) and static site generators (for example, Gatsby\\nJS, React-static, Jekyll, and Hugo). For more information, see Getting Started in the AWS\\nAmplify Console User Guide.\\nAmazon S3 website endpoints do not support HTTPS. If you want to use HTTPS, you\\ncan use Amazon CloudFront to serve a static website hosted on Amazon S3. For more\\ninformation, see How do I use CloudFront to serve HTTPS requests for my Amazon S3\\nbucket? To use HTTPS with a custom domain, see Configuring a static website using a\\ncustom domain registered with Route 53.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo augment the security of your Amazon S3 static websites, the Amazon S3 website\\nendpoint domains (for example, s3-website-us-east-1.amazonaws.com or s3-website.ap-\\nsouth-1.amazonaws.com) are registered in the Public Suffix List (PSL). For further security,\\nwe recommend that you use cookies with a __Host- prefix if you ever need to set sensitive\\ncookies in the domain name for your Amazon S3 static websites. This practice will help\\nto defend your domain against cross-site request forgery attempts (CSRF). For more\\ninformation see the Set-Cookie page in the Mozilla Developer Network.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nAmazon S3 website endpoints do not support HTTPS or access points. If you want to use\\nHTTPS, you can use Amazon CloudFront to serve a static website hosted on Amazon S3. For\\nmore information, see How do I use CloudFront to serve HTTPS requests for my Amazon\\nS3 bucket? To use HTTPS with a custom domain, see Configuring a static website using a\\ncustom domain registered with Route 53.', '']]]\n",
      "[[['', 'Requester Pays buckets do not allow access through a website endpoint. Any request to\\nsuch a bucket receives a 403 Access Denied response. For more information, see Using\\nRequester Pays buckets for storage transfers and usage.', ''], ['', '', '']], [['', '', ''], ['', 'http://bucket-name.s3-website.Region.amazonaws.com/object-name', ''], ['', '', '']], [['', '', ''], ['', 'http://example-bucket.s3-website.us-west-2.amazonaws.com/photo.jpg', ''], ['', '', '']], [['', '', ''], ['', 'http://bucket-name.s3-website.Region.amazonaws.com/folder-name/object-name', ''], ['', '', '']], [['', '', ''], ['', 'http://example-bucket.s3-website.us-west-2.amazonaws.com/docs/doc1.html', ''], ['', '', '']]]\n",
      "[[['Key difference', 'REST API endpoint', 'Website endpoint'], ['Access control', 'Supports both public and private\\ncontent', 'Supports only publicly readable\\ncontent'], ['Error message\\nhandling', 'Returns an XML-formatted error\\nresponse', 'Returns an HTML document'], ['Redirection\\nsupport', 'Not applicable', 'Supports both object-level and\\nbucket-level redirects']]]\n",
      "[[['Key difference', 'REST API endpoint', 'Website endpoint'], ['Requests\\nsupported', 'Supports all bucket and object\\noperations', 'Supports only GET and HEAD\\nrequests on objects'], ['Responses to\\nGET and HEAD\\nrequests at the\\nroot of a bucket', 'Returns a list of the object keys in\\nthe bucket', 'Returns the index document that is\\nspecified in the website configurati\\non'], ['Secure Sockets\\nLayer (SSL)\\nsupport', 'Supports SSL connections', 'Does not support SSL connections']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass WebsiteConfigTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\nprivate const string indexDocumentSuffix = \"*** index object key ***\"; //\\nFor example, index.html.', '']]]\n",
      "[[['', 'private const string errorDocument = \"*** error object key ***\"; // For\\nexample, error.html.\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nclient = new AmazonS3Client(bucketRegion);\\nAddWebsiteConfigurationAsync(bucketName, indexDocumentSuffix,\\nerrorDocument).Wait();\\n}\\nstatic async Task AddWebsiteConfigurationAsync(string bucketName,\\nstring indexDocumentSuffix,\\nstring errorDocument)\\n{\\ntry\\n{\\n// 1. Put the website configuration.\\nPutBucketWebsiteRequest putRequest = new PutBucketWebsiteRequest()\\n{\\nBucketName = bucketName,\\nWebsiteConfiguration = new WebsiteConfiguration()\\n{\\nIndexDocumentSuffix = indexDocumentSuffix,\\nErrorDocument = errorDocument\\n}\\n};\\nPutBucketWebsiteResponse response = await\\nclient.PutBucketWebsiteAsync(putRequest);\\n// 2. Get the website configuration.\\nGetBucketWebsiteRequest getRequest = new GetBucketWebsiteRequest()\\n{\\nBucketName = bucketName\\n};\\nGetBucketWebsiteResponse getResponse = await\\nclient.GetBucketWebsiteAsync(getRequest);\\nConsole.WriteLine(\"Index document: {0}\",\\ngetResponse.WebsiteConfiguration.IndexDocumentSuffix);\\nConsole.WriteLine(\"Error document: {0}\",\\ngetResponse.WebsiteConfiguration.ErrorDocument);\\n}', '']]]\n",
      "[[['', 'catch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"require 'vendor/autoload.php';\\nuse Aws\\\\S3\\\\S3Client;\\n$bucket = '*** Your Bucket Name ***';\\n$s3 = new S3Client([\\n'version' => 'latest',\\n'region' => 'us-east-1'\\n]);\\n// Add the website configuration.\\n$s3->putBucketWebsite([\\n'Bucket' => $bucket,\\n'WebsiteConfiguration' => [\\n'IndexDocument' => ['Suffix' => 'index.html'],\\n'ErrorDocument' => ['Key' => 'error.html']\\n]\", '']]]\n",
      "[[['', \"]);\\n// Retrieve the website configuration.\\n$result = $s3->getBucketWebsite([\\n'Bucket' => $bucket\\n]);\\necho $result->getPath('IndexDocument/Suffix');\\n// Delete the website configuration.\\n$s3->deleteBucketWebsite([\\n'Bucket' => $bucket\\n]);\", ''], ['', '', '']], [['', '', ''], ['', 'http://example-bucket.s3-website.Region.amazonaws.com/', '']]]\n",
      "[[['', 'http://example-bucket.s3-website.Region.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'http://bucket-name.s3-website.Region.amazonaws.com/photos/', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIn a versioning-enabled bucket, you may upload multiple copies of the index.html\\nbut only the newest version will be resolved to. For more information about using S3\\nVersioning see, Using versioning in S3 buckets.', ''], ['', '', '']], [['', '', ''], ['', '<html xmlns=\"http://www.w3.org/1999/xhtml\" >\\n<head>\\n<title>My Website Home Page</title>\\n</head>\\n<body>\\n<h1>Welcome to my website</h1>\\n<p>Now hosted on Amazon S3!</p>\\n</body>\\n</html>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nSome browsers display their own error message when an error occurs, ignoring the error\\ndocument that Amazon S3 returns. For example, when an HTTP 404 Not Found error\\noccurs, Google Chrome might ignore the error document that Amazon S3 returns and\\ndisplay its own error.', ''], ['', '', '']]]\n",
      "[[['HTTP error code', 'Description'], ['301 Moved\\nPermanently', 'When a user sends a request directly to the Amazon S3 website endpoint\\n(http://s3-website. Region.amazonaws.com/ ), Amazon S3\\nreturns a 301 Moved Permanently response and redirects those requests\\nto h ttps://aws.amazon.com/s3/ .'], ['302 Found', 'When Amazon S3 receives a request for a key x, http://bucket-na\\nme .s3-website. Region.amazonaws.com/x , without a trailing\\nslash, it first looks for the object with the key name x. If the object is not\\nfound, Amazon S3 determines that the request is for subfolder x and redir\\nects the request by adding a slash at the end, and returns 302 Found.'], ['304 Not\\nModified', 'Amazon S3 uses request headers If-Modified-Since , If-Unmodi\\nfied-Since , If-Match and/or If-None-Match to determine\\nwhether the requested object is same as the cached copy held by the\\nclient. If the object is the same, the website endpoint returns a 3 04 Not\\nModified response.'], ['400 Malformed\\nRequest', 'The website endpoint responds with a 400 Malformed Request when a\\nuser attempts to access a bucket through the incorrect regional endpoint.'], ['403 Forbidden', 'The website endpoint responds with a 403 Forbidden when a user request\\ntranslates to an object that is not publicly readable. The object owner must\\nmake the object publicly readable using a bucket policy or an ACL.'], ['404 Not Found', '']]]\n",
      "[[['HTTP error code', 'Description'], ['', \"The website endpoint responds with 404 Not Found for the following\\nreasons:\\n•\\nAmazon S3 determines that the URL of the website refers to an object\\nkey that does not exist.\\n•\\nAmazon S3 infers that the request is for an index document that does\\nnot exist.\\n•\\nA bucket specified in the URL does not exist.\\n•\\nA bucket specified in the URL exists, but isn't configured as a website.\\nYou can create a custom document that is returned for 4 04 Not Found.\\nMake sure that the document is uploaded to the bucket configured as\\na website, and that the website hosting configuration is set to use the\\ndocument.\\nFor information on how Amazon S3 interprets the URL as a request for an\\nobject or an index document, see Configuring an index document.\"], ['500 Service Error', 'The website endpoint responds with a 500 Service Error when an internal\\nserver error occurs.'], ['503 Service\\nUnavailable', 'The website endpoint responds with a 503 Service Unavailable when\\nAmazon S3 determines that you need to reduce your request rate.']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nOn the website endpoint, if a user requests an object that doesn't exist, Amazon S3 returns\\nHTTP response code 404 (Not Found). If the object exists but you haven't granted\\nread permission on it, the website endpoint returns HTTP response code 403 (Access\", '']]]\n",
      "[[['', \"Denied). The user can use the response code to infer whether a specific object exists. If\\nyou don't want this behavior, you should not enable website support for your bucket.\", ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nBefore you complete this step, review Blocking public access to your Amazon S3 storage to\\nensure that you understand and accept the risks involved with allowing public access. When\\nyou turn off block public access settings to make your bucket public, anyone on the internet\\ncan access your bucket. We recommend that you block all public access to your buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Warning\\nBefore you complete this step, review Blocking public access to your Amazon S3\\nstorage to ensure you understand and accept the risks involved with allowing public\\naccess. When you turn off block public access settings to make your bucket public,\\nanyone on the internet can access your bucket. We recommend that you block all\\npublic access to your buckets.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nThe following policy is an example only and allows full access to the contents of your\\nbucket. Before you proceed with this step, review How can I secure the files in my Amazon\\nS3 bucket? to ensure that you understand the best practices for securing the files in your S3\\nbucket and risks involved in granting public access.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"PublicReadGetObject\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": [\\n\"s3:GetObject\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::Bucket-Name/*\"', '']]]\n",
      "[[['', ']\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nIf your bucket uses the Bucket owner enforced setting for S3 Object Ownership, you must\\nuse policies to grant access to your bucket and the objects in it. With the Bucket owner\\nenforced setting enabled, requests to set access control lists (ACLs) or update ACLs fail and\\nreturn the AccessControlListNotSupported error code. Requests to read ACLs are still\\nsupported.', ''], ['', '', '']], [['', '', ''], ['', '<Grant>\\n<Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\nxsi:type=\"Group\">\\n<URI>http://acs.amazonaws.com/groups/global/AllUsers</URI>\\n</Grantee>\\n<Permission>READ</Permission>\\n</Grant>', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nTo create redirection rules in the new Amazon S3 console, you must use JSON. For JSON\\nexamples, see Redirection rules examples.', ''], ['', '', '']], [['', '', ''], ['', '[\\n{\\n\"Condition\": {\\n\"HttpErrorCodeReturnedEquals\": \"string\",\\n\"KeyPrefixEquals\": \"string\"\\n},', '']]]\n",
      "[[['', '\"Redirect\": {\\n\"HostName\": \"string\",\\n\"HttpRedirectCode\": \"string\",\\n\"Protocol\": \"http\"|\"https\",\\n\"ReplaceKeyPrefixWith\": \"string\",\\n\"ReplaceKeyWith\": \"string\"\\n}\\n}\\n]\\nNote: Redirect must each have at least one child element. You can have either\\nReplaceKeyPrefix with or ReplaceKeyWith but not both.', ''], ['', '', '']], [['', '', ''], ['', '<RoutingRules> =\\n<RoutingRules>\\n<RoutingRule>...</RoutingRule>\\n[<RoutingRule>...</RoutingRule>\\n...]\\n</RoutingRules>\\n<RoutingRule> =\\n<RoutingRule>\\n[ <Condition>...</Condition> ]\\n<Redirect>...</Redirect>\\n</RoutingRule>\\n<Condition> =\\n<Condition>\\n[ <KeyPrefixEquals>...</KeyPrefixEquals> ]\\n[ <HttpErrorCodeReturnedEquals>...</HttpErrorCodeReturnedEquals> ]\\n</Condition>\\nNote: <Condition> must have at least one child element.\\n<Redirect> =\\n<Redirect>\\n[ <HostName>...</HostName> ]\\n[ <Protocol>...</Protocol> ]\\n[ <ReplaceKeyPrefixWith>...</ReplaceKeyPrefixWith> ]\\n[ <ReplaceKeyWith>...</ReplaceKeyWith> ]\\n[ <HttpRedirectCode>...</HttpRedirectCode> ]\\n</Redirect>', '']]]\n",
      "[[['', 'Note: <Redirect> must have at least one child element. You can have either\\nReplaceKeyPrefix with or ReplaceKeyWith but not both.', ''], ['', '', '']], [['Name', 'Description'], ['RoutingRules', 'Container for a collection of RoutingRule elements.'], ['RoutingRule', 'A rule that identifies a condition and the redirect that is applied\\nwhen the condition is met.\\nCondition:\\n•\\nA RoutingRules container must contain at least one\\nrouting rule.'], ['Condition', 'Container for describing a condition that must be met for the\\nspecified redirect to be applied. If the routing rule does not\\ninclude a condition, the rule is applied to all requests.'], ['KeyPrefixEquals', 'The prefix of the object key name from which requests are redi\\nrected.\\nKeyPrefixEquals is required if H ttpErrorCodeRetur\\nnedEquals is not specified. If both KeyPrefixEquals\\nand H ttpErrorCodeReturnedEquals are specified, both\\nmust be true for the condition to be met.'], ['HttpErrorCodeRetur\\nnedEquals', 'The HTTP error code that must match for the redirect to apply\\n. If an error occurs, and if the error code meets this value, then\\nthe specified redirect applies.']]]\n",
      "[[['Name', 'Description'], ['', 'HttpErrorCodeReturnedEquals is required if\\nKeyPrefixEquals is not specified. If both KeyPrefix\\nEquals and H ttpErrorCodeReturnedEquals are\\nspecified, both must be true for the condition to be met.'], ['Redirect', 'Container element that provides instructions for redirecting\\nthe request. You can redirect requests to another host or\\nanother page, or you can specify another protocol to use. A\\nRoutingRule must have a R edirect element. A Redirect\\nelement must contain at least one of the following sibling\\nelements: Protocol, HostName, ReplaceKeyPrefixWith ,\\nReplaceKeyWith , or HttpRedirectCode .'], ['Protocol', 'The protocol, http or https, to be used in the Location\\nheader that is returned in the response.\\nIf one of its siblings is supplied, Protocol is not required.'], ['HostName', 'The hostname to be used in the Location header that is\\nreturned in the response.\\nIf one of its siblings is supplied, HostName is not required.'], ['ReplaceKeyPrefixWi\\nth', 'The prefix of the object key name that replaces the value of\\nKeyPrefixEquals in the redirect request.\\nIf one of its siblings is supplied, ReplaceKeyPrefixWith\\nis not required. It can be supplied only if ReplaceKeyWith is\\nnot supplied.']]]\n",
      "[[['Name', 'Description'], ['ReplaceKeyWith', 'The object key to be used in the Location header that is\\nreturned in the response.\\nIf one of its siblings is supplied, ReplaceKeyWith is not\\nrequired. It can be supplied only if ReplaceKeyPrefixWith\\nis not supplied.'], ['HttpRedirectCode', 'The HTTP redirect code to be used in the Location header that\\nis returned in the response.\\nIf one of its siblings is supplied, HttpRedirectCode is not\\nrequired.']], [['', '', ''], ['', 'Important\\nTo create redirection rules in the new Amazon S3 console, you must use JSON.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '[\\n{\\n\"Condition\": {\\n\"KeyPrefixEquals\": \"docs/\"\\n},\\n\"Redirect\": {\\n\"ReplaceKeyPrefixWith\": \"documents/\"\\n}\\n}\\n]', ''], ['', '', '']], [['', '', ''], ['', '<RoutingRules>\\n<RoutingRule>\\n<Condition>\\n<KeyPrefixEquals>docs/</KeyPrefixEquals>\\n</Condition>\\n<Redirect>\\n<ReplaceKeyPrefixWith>documents/</ReplaceKeyPrefixWith>\\n</Redirect>\\n</RoutingRule>\\n</RoutingRules>', ''], ['', '', '']], [['', '', ''], ['', '[\\n{\\n\"Condition\": {\\n\"KeyPrefixEquals\": \"images/\"\\n},\\n\"Redirect\": {\\n\"ReplaceKeyWith\": \"folderdeleted.html\"', '']]]\n",
      "[[['', '}\\n}\\n]', ''], ['', '', '']], [['', '', ''], ['', '<RoutingRules>\\n<RoutingRule>\\n<Condition>\\n<KeyPrefixEquals>images/</KeyPrefixEquals>\\n</Condition>\\n<Redirect>\\n<ReplaceKeyWith>folderdeleted.html</ReplaceKeyWith>\\n</Redirect>\\n</RoutingRule>\\n</RoutingRules>', ''], ['', '', '']], [['', '', ''], ['', '[\\n{\\n\"Condition\": {\\n\"HttpErrorCodeReturnedEquals\": \"404\"\\n},\\n\"Redirect\": {\\n\"HostName\": \"ec2-11-22-333-44.compute-1.amazonaws.com\",\\n\"ReplaceKeyPrefixWith\": \"report-404/\"', '']]]\n",
      "[[['', '}\\n}\\n]', ''], ['', '', '']], [['', '', ''], ['', '<RoutingRules>\\n<RoutingRule>\\n<Condition>\\n<HttpErrorCodeReturnedEquals>404</HttpErrorCodeReturnedEquals >\\n</Condition>\\n<Redirect>\\n<HostName>ec2-11-22-333-44.compute-1.amazonaws.com</HostName>\\n<ReplaceKeyPrefixWith>report-404/</ReplaceKeyPrefixWith>\\n</Redirect>\\n</RoutingRule>\\n</RoutingRules>', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf you experience issues related to the presence of IPv6 addresses in log files, contact\\nAWS Support.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"IPAllow\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:*\",\\n\"Resource\": \"arn:aws:s3:::examplebucket/*\",\\n\"Condition\": {\\n\"IpAddress\": {\"aws:SourceIp\": \"54.240.143.0/24\"}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"Condition\": {\\n\"IpAddress\": {\\n\"aws:SourceIp\": [\\n\"54.240.143.0/24\",\\n\"2001:DB8:1234:5678::/64\"', '']]]\n",
      "[[['', ']\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'curl -v http://s3.dualstack.us-west-2.amazonaws.com/', ''], ['', '', '']], [['', '', ''], ['', '* About to connect() to s3-us-west-2.amazonaws.com port 80 (#0)\\n* Trying IPv6 address... connected\\n* Connected to s3.dualstack.us-west-2.amazonaws.com (IPv6 address) port 80 (#0)\\n> GET / HTTP/1.1\\n> User-Agent: curl/7.18.1 (x86_64-unknown-linux-gnu) libcurl/7.18.1 OpenSSL/1.0.1t\\nzlib/1.2.3\\n> Host: s3.dualstack.us-west-2.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'ping ipv6.s3.dualstack.us-west-2.amazonaws.com', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nYou can use transfer acceleration with dual-stack endpoints. For more information, see\\nGetting started with Amazon S3 Transfer Acceleration.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThe two types of VPC endpoints to access Amazon S3 (Interface VPC endpoints and\\nGateway VPC endpoints) don't have dual-stack support. For more information about VPC\\nendpoints for Amazon S3, see AWS PrivateLink for Amazon S3.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '$ aws configure set default.s3.use_dualstack_endpoint true\\n$ aws configure set default.s3.addressing_style virtual', ''], ['', '', '']], [['', '', ''], ['', '$ aws s3api list-objects --bucket bucketname --endpoint-url https://s3.dualstack.aws-\\nregion.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen using the AWS CLI you currently cannot use transfer acceleration with dual-stack\\nendpoints. However, support for the AWS CLI is coming soon. For more information, see\\nUsing the AWS CLI.', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;', '']]]\n",
      "[[['', 'import com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\npublic class DualStackEndpoints {\\npublic static void main(String[] args) {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\ntry {\\n// Create an Amazon S3 client with dual-stack endpoints enabled.\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.withDualstackEnabled(true)\\n.build();\\ns3Client.listObjects(bucketName);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'java.net.preferIPv6Addresses=true', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass DualStackEndpointTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion = RegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nvar config = new AmazonS3Config\\n{\\nUseDualstackEndpoint = true,\\nRegionEndpoint = bucketRegion\\n};\\nclient = new AmazonS3Client(config);\\nConsole.WriteLine(\"Listing objects stored in a bucket\");\\nListingObjectsAsync().Wait();\\n}\\nprivate static async Task ListingObjectsAsync()\\n{\\ntry\\n{\\nvar request = new ListObjectsV2Request\\n{\\nBucketName = bucketName,\\nMaxKeys = 10\\n};\\nListObjectsV2Response response;\\ndo\\n{\\nresponse = await client.ListObjectsV2Async(request);\\n// Process the response.\\nforeach (S3Object entry in response.S3Objects)', '']]]\n",
      "[[['', '{\\nConsole.WriteLine(\"key = {0} size = {1}\",\\nentry.Key, entry.Size);\\n}\\nConsole.WriteLine(\"Next Continuation Token: {0}\",\\nresponse.NextContinuationToken);\\nrequest.ContinuationToken = response.NextContinuationToken;\\n} while (response.IsTruncated == true);\\n}\\ncatch (AmazonS3Exception amazonS3Exception)\\n{\\nConsole.WriteLine(\"An AmazonS3Exception was thrown. Exception: \" +\\namazonS3Exception.ToString());\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Exception: \" + e.ToString());\\n}\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '~/.aws', ''], ['', '', '']], [['', '', ''], ['', '%HOMEPATH%\\\\.aws', ''], ['', '', '']], [['', '', ''], ['', '[default]\\naws_access_key_id = your_access_key_id\\naws_secret_access_key = your_secret_access_key', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.ListObjectsRequest;\\nimport com.amazonaws.services.s3.model.ObjectListing;\\nimport com.amazonaws.services.s3.model.S3ObjectSummary;\\nimport java.io.IOException;\\nimport java.util.List;\\npublic class MakingRequests {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Bucket name ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\n// Get a list of objects in the bucket, two at a time, and\\n// print the name and size of each object.\\nListObjectsRequest listRequest = new\\nListObjectsRequest().withBucketName(bucketName).withMaxKeys(2);\\nObjectListing objects = s3Client.listObjects(listRequest);\\nwhile (true) {\\nList<S3ObjectSummary> summaries = objects.getObjectSummaries();', '']]]\n",
      "[[['', 'for (S3ObjectSummary summary : summaries) {\\nSystem.out.printf(\"Object \\\\\"%s\\\\\" retrieved with size %d\\\\n\",\\nsummary.getKey(), summary.getSize());\\n}\\nif (objects.isTruncated()) {\\nobjects = s3Client.listNextBatchOfObjects(objects);\\n} else {\\nbreak;\\n}\\n}\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\n• You can create the AmazonS3Client client without providing your security\\ncredentials. Requests sent using this client are anonymous requests, without a\\nsignature. Amazon S3 returns an error if you send anonymous requests for a resource\\nthat is not publicly available.', '']]]\n",
      "[[['', '• You can create an AWS account and create the required users. You can also manage\\ncredentials for those users. You need these credentials to perform the task in the\\nfollowing example. For more information, see Configure AWS credentials in the AWS\\nSDK for .NET Developer Guide.\\nYou can then also configure your application to actively retrieve profiles and\\ncredentials, and then explicitly use those credentials when creating an AWS service\\nclient. For more information, see Accessing credentials and profiles in an application\\nin the AWS SDK for .NET Developer Guide.', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass MakeS3RequestTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nusing (client = new AmazonS3Client(bucketRegion))\\n{\\nConsole.WriteLine(\"Listing objects stored in a bucket\");\\nListingObjectsAsync().Wait();\\n}\\n}', '']]]\n",
      "[[['', 'static async Task ListingObjectsAsync()\\n{\\ntry\\n{\\nListObjectsRequest request = new ListObjectsRequest\\n{\\nBucketName = bucketName,\\nMaxKeys = 2\\n};\\ndo\\n{\\nListObjectsResponse response = await\\nclient.ListObjectsAsync(request);\\n// Process the response.\\nforeach (S3Object entry in response.S3Objects)\\n{\\nConsole.WriteLine(\"key = {0} size = {1}\",\\nentry.Key, entry.Size);\\n}\\n// If the response is truncated, set the marker to get the next\\n// set of keys.\\nif (response.IsTruncated)\\n{\\nrequest.Marker = response.NextMarker;\\n}\\nelse\\n{\\nrequest = null;\\n}\\n} while (request != null);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\n}', '']]]\n",
      "[[['', '}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'require \\'vendor/autoload.php\\';\\nuse Aws\\\\S3\\\\Exception\\\\S3Exception;\\nuse Aws\\\\S3\\\\S3Client;\\n$bucket = \\'*** Your Bucket Name ***\\';\\n$s3 = new S3Client([\\n\\'region\\' => \\'us-east-1\\',\\n\\'version\\' => \\'latest\\',\\n]);\\n// Retrieve the list of buckets.\\n$result = $s3->listBuckets();\\ntry {\\n// Retrieve a paginator for listing objects.\\n$objects = $s3->getPaginator(\\'ListObjects\\', [\\n\\'Bucket\\' => $bucket\\n]);\\necho \"Keys retrieved!\" . PHP_EOL;\\n// Print the list of objects to the page.', '']]]\n",
      "[[['', \"foreach ($objects as $object) {\\necho $object['Key'] . PHP_EOL;\\n}\\n} catch (S3Exception $e) {\\necho $e->getMessage() . PHP_EOL;\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can create the S3Client client without providing your security credentials.\\nRequests sent using this client are anonymous requests, without a signature. Amazon\\nS3 returns an error if you send anonymous requests for a resource that is not publicly\\navailable. For more information, see Creating Anonymous Clients in the AWS SDK for\\nPHP Documentation.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '# Prerequisites:\\n# - An existing Amazon S3 bucket.\\nrequire \"aws-sdk-s3\"\\n# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.\\n# @param bucket_name [String] The bucket\\'s name.\\n# @return [Boolean] true if all operations succeed; otherwise, false.\\n# @example\\n# s3_client = Aws::S3::Client.new(region: \\'us-west-2\\')\\n# exit 1 unless list_bucket_objects?(s3_client, \\'doc-example-bucket\\')\\ndef list_bucket_objects?(s3_client, bucket_name)\\nputs \"Accessing the bucket named \\'#{bucket_name}\\'...\"\\nobjects = s3_client.list_objects_v2(\\nbucket: bucket_name,\\nmax_keys: 50\\n)\\nif objects.count.positive?\\nputs \"The object keys in this bucket are (first 50 objects):\"\\nobjects.contents.each do |object|\\nputs object.key\\nend\\nelse\\nputs \"No objects found in this bucket.\"\\nend\\nreturn true\\nrescue StandardError => e\\nputs \"Error while accessing the bucket named \\'#{bucket_name}\\': #{e.message}\"\\nreturn false\\nend\\n# Example usage:\\ndef run_me\\nregion = \"us-west-2\"\\nbucket_name = \"BUCKET_NAME\"\\ns3_client = Aws::S3::Client.new(region: region)', '']]]\n",
      "[[['', 'exit 1 unless list_bucket_objects?(s3_client, bucket_name)\\nend\\nrun_me if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', '# Prerequisites:\\n# - An existing Amazon S3 bucket.\\nrequire \"aws-sdk-s3\"\\n# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.\\n# @param bucket_name [String] The bucket\\'s name.\\n# @return [Boolean] true if all operations succeed; otherwise, false.\\n# @example\\n# s3_client = Aws::S3::Client.new(region: \\'us-west-2\\')\\n# exit 1 unless list_bucket_objects?(s3_client, \\'doc-example-bucket\\')\\ndef list_bucket_objects?(s3_client, bucket_name)\\nputs \"Accessing the bucket named \\'#{bucket_name}\\'...\"\\nobjects = s3_client.list_objects_v2(\\nbucket: bucket_name,\\nmax_keys: 50\\n)\\nif objects.count.positive?\\nputs \"The object keys in this bucket are (first 50 objects):\"\\nobjects.contents.each do |object|\\nputs object.key\\nend\\nelse\\nputs \"No objects found in this bucket.\"\\nend', '']]]\n",
      "[[['', 'return true\\nrescue StandardError => e\\nputs \"Error while accessing the bucket named \\'#{bucket_name}\\': #{e.message}\"\\nreturn false\\nend\\n# Example usage:\\ndef run_me\\nregion = \"us-west-2\"\\nbucket_name = \"BUCKET_NAME\"\\ns3_client = Aws::S3::Client.new(region: region)\\nexit 1 unless list_bucket_objects?(s3_client, bucket_name)\\nend\\nrun_me if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', 'package main\\nimport (\\n\"context\"\\n\"fmt\"\\n\"github.com/aws/aws-sdk-go-v2/config\"\\n\"github.com/aws/aws-sdk-go-v2/service/s3\"\\n)\\n// main uses the AWS SDK for Go V2 to create an Amazon Simple Storage Service\\n// (Amazon S3) client and list up to 10 buckets in your account.\\n// This example uses the default settings specified in your shared credentials\\n// and config files.\\nfunc main() {\\nsdkConfig, err := config.LoadDefaultConfig(context.TODO())\\nif err != nil {', '']]]\n",
      "[[['', 'fmt.Println(\"Couldn\\'t load default configuration. Have you set up your AWS\\naccount?\")\\nfmt.Println(err)\\nreturn\\n}\\ns3Client := s3.NewFromConfig(sdkConfig)\\ncount := 10\\nfmt.Printf(\"Let\\'s list up to %v buckets for your account.\\\\n\", count)\\nresult, err := s3Client.ListBuckets(context.TODO(), &s3.ListBucketsInput{})\\nif err != nil {\\nfmt.Printf(\"Couldn\\'t list buckets for your account. Here\\'s why: %v\\\\n\", err)\\nreturn\\n}\\nif len(result.Buckets) == 0 {\\nfmt.Println(\"You don\\'t have any buckets!\")\\n} else {\\nif count > len(result.Buckets) {\\ncount = len(result.Buckets)\\n}\\nfor _, bucket := range result.Buckets[:count] {\\nfmt.Printf(\"\\\\t%v\\\\n\", *bucket.Name)\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf you obtain temporary security credentials using your AWS account security\\ncredentials, the temporary credentials are valid for only one hour. You can specify the\\nsession duration only if you use IAM user credentials to request a session.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.AWSStaticCredentialsProvider;\\nimport com.amazonaws.auth.BasicSessionCredentials;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.ObjectListing;\\nimport com.amazonaws.services.securitytoken.AWSSecurityTokenService;\\nimport com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder;\\nimport com.amazonaws.services.securitytoken.model.AssumeRoleRequest;\\nimport com.amazonaws.services.securitytoken.model.AssumeRoleResult;\\nimport com.amazonaws.services.securitytoken.model.Credentials;\\npublic class MakingRequestsWithIAMTempCredentials {\\npublic static void main(String[] args) {\\nString clientRegion = \"*** Client region ***\";\\nString roleARN = \"*** ARN for role to be assumed ***\";\\nString roleSessionName = \"*** Role session name ***\";\\nString bucketName = \"*** Bucket name ***\";\\ntry {\\n// Creating the STS client is part of your trusted code. It has\\n// the security credentials you use to obtain temporary security\\ncredentials.\\nAWSSecurityTokenService stsClient =\\nAWSSecurityTokenServiceClientBuilder.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();', '']]]\n",
      "[[['', '// Obtain credentials for the IAM role. Note that you cannot assume the\\nrole of\\n// an AWS root account;\\n// Amazon S3 will deny access. You must use credentials for an IAM user\\nor an\\n// IAM role.\\nAssumeRoleRequest roleRequest = new AssumeRoleRequest()\\n.withRoleArn(roleARN)\\n.withRoleSessionName(roleSessionName);\\nAssumeRoleResult roleResponse = stsClient.assumeRole(roleRequest);\\nCredentials sessionCredentials = roleResponse.getCredentials();\\n// Create a BasicSessionCredentials object that contains the credentials\\nyou\\n// just retrieved.\\nBasicSessionCredentials awsCredentials = new BasicSessionCredentials(\\nsessionCredentials.getAccessKeyId(),\\nsessionCredentials.getSecretAccessKey(),\\nsessionCredentials.getSessionToken());\\n// Provide temporary security credentials so that the Amazon S3 client\\n// can send authenticated requests to Amazon S3. You create the client\\n// using the sessionCredentials object.\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new\\nAWSStaticCredentialsProvider(awsCredentials))\\n.withRegion(clientRegion)\\n.build();\\n// Verify that assuming the role worked and the permissions are set\\ncorrectly\\n// by getting a set of object keys from the bucket.\\nObjectListing objects = s3Client.listObjects(bucketName);\\nSystem.out.println(\"No. of Objects: \" +\\nobjects.getObjectSummaries().size());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}', '']]]\n",
      "[[['', '}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you obtain temporary security credentials using your AWS account security\\ncredentials, those credentials are valid for only one hour. You can specify a session\\nduration only if you use IAM user credentials to request a session.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using Amazon;\\nusing Amazon.Runtime;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing Amazon.SecurityToken;\\nusing Amazon.SecurityToken.Model;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass TempCredExplicitSessionStartTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;\\npublic static void Main()\\n{\\nListObjectsAsync().Wait();\\n}\\nprivate static async Task ListObjectsAsync()\\n{\\ntry\\n{\\n// Credentials use the default AWS SDK for .NET credential search\\nchain.\\n// On local development machines, this is your default profile.', '']]]\n",
      "[[['', 'Console.WriteLine(\"Listing objects stored in a bucket\");\\nSessionAWSCredentials tempCredentials = await\\nGetTemporaryCredentialsAsync();\\n// Create a client by providing temporary security credentials.\\nusing (s3Client = new AmazonS3Client(tempCredentials, bucketRegion))\\n{\\nvar listObjectRequest = new ListObjectsRequest\\n{\\nBucketName = bucketName\\n};\\n// Send request to Amazon S3.\\nListObjectsResponse response = await\\ns3Client.ListObjectsAsync(listObjectRequest);\\nList<S3Object> objects = response.S3Objects;\\nConsole.WriteLine(\"Object count = {0}\", objects.Count);\\n}\\n}\\ncatch (AmazonS3Exception s3Exception)\\n{\\nConsole.WriteLine(s3Exception.Message, s3Exception.InnerException);\\n}\\ncatch (AmazonSecurityTokenServiceException stsException)\\n{\\nConsole.WriteLine(stsException.Message,\\nstsException.InnerException);\\n}\\n}\\nprivate static async Task<SessionAWSCredentials>\\nGetTemporaryCredentialsAsync()\\n{\\nusing (var stsClient = new AmazonSecurityTokenServiceClient())\\n{\\nvar getSessionTokenRequest = new GetSessionTokenRequest\\n{\\nDurationSeconds = 7200 // seconds\\n};\\nGetSessionTokenResponse sessionTokenResponse =\\nawait\\nstsClient.GetSessionTokenAsync(getSessionTokenRequest);\\nCredentials credentials = sessionTokenResponse.Credentials;', '']]]\n",
      "[[['', 'var sessionCredentials =\\nnew SessionAWSCredentials(credentials.AccessKeyId,\\ncredentials.SecretAccessKey,\\ncredentials.SessionToken);\\nreturn sessionCredentials;\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you obtain temporary security credentials using your AWS account security\\ncredentials, the temporary security credentials are valid for only one hour. You can\\nspecify the session duration only if you use IAM user credentials to request a session.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'require \\'vendor/autoload.php\\';\\nuse Aws\\\\S3\\\\Exception\\\\S3Exception;\\nuse Aws\\\\S3\\\\S3Client;\\nuse Aws\\\\Sts\\\\StsClient;\\n$bucket = \\'*** Your Bucket Name ***\\';\\n$sts = new StsClient([\\n\\'version\\' => \\'latest\\',\\n\\'region\\' => \\'us-east-1\\'\\n]);\\n$sessionToken = $sts->getSessionToken();\\n$s3 = new S3Client([\\n\\'region\\' => \\'us-east-1\\',\\n\\'version\\' => \\'latest\\',\\n\\'credentials\\' => [\\n\\'key\\' => $sessionToken[\\'Credentials\\'][\\'AccessKeyId\\'],\\n\\'secret\\' => $sessionToken[\\'Credentials\\'][\\'SecretAccessKey\\'],\\n\\'token\\' => $sessionToken[\\'Credentials\\'][\\'SessionToken\\']\\n]\\n]);\\n$result = $s3->listBuckets();\\ntry {\\n// Retrieve a paginator for listing objects.\\n$objects = $s3->getPaginator(\\'ListObjects\\', [\\n\\'Bucket\\' => $bucket\\n]);\\necho \"Keys retrieved!\" . PHP_EOL;\\n// List objects\\nforeach ($objects as $object) {\\necho $object[\\'Key\\'] . PHP_EOL;\\n}', '']]]\n",
      "[[['', '} catch (S3Exception $e) {\\necho $e->getMessage() . PHP_EOL;\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you obtain temporary security credentials using your AWS account security\\ncredentials, the temporary security credentials are valid for only one hour. You can\\nspecify session duration only if you use IAM user credentials to request a session.', ''], ['', '', '']], [['', '', ''], ['', '# Prerequisites:\\n# - A user in AWS Identity and Access Management (IAM). This user must\\n# be able to assume the following IAM role. You must run this code example\\n# within the context of this user.\\n# - An existing role in IAM that allows all of the Amazon S3 actions for all of the\\n# resources in this code example. This role must also trust the preceding IAM\\nuser.\\n# - An existing S3 bucket.\\nrequire \"aws-sdk-core\"\\nrequire \"aws-sdk-s3\"\\nrequire \"aws-sdk-iam\"', '']]]\n",
      "[[['', '# Checks whether a user exists in IAM.\\n#\\n# @param iam [Aws::IAM::Client] An initialized IAM client.\\n# @param user_name [String] The user\\'s name.\\n# @return [Boolean] true if the user exists; otherwise, false.\\n# @example\\n# iam_client = Aws::IAM::Client.new(region: \\'us-west-2\\')\\n# exit 1 unless user_exists?(iam_client, \\'my-user\\')\\ndef user_exists?(iam_client, user_name)\\nresponse = iam_client.get_user(user_name: user_name)\\nreturn true if response.user.user_name\\nrescue Aws::IAM::Errors::NoSuchEntity\\n# User doesn\\'t exist.\\nrescue StandardError => e\\nputs \"Error while determining whether the user \" \\\\\\n\"\\'#{user_name}\\' exists: #{e.message}\"\\nend\\n# Creates a user in IAM.\\n#\\n# @param iam_client [Aws::IAM::Client] An initialized IAM client.\\n# @param user_name [String] The user\\'s name.\\n# @return [AWS:IAM::Types::User] The new user.\\n# @example\\n# iam_client = Aws::IAM::Client.new(region: \\'us-west-2\\')\\n# user = create_user(iam_client, \\'my-user\\')\\n# exit 1 unless user.user_name\\ndef create_user(iam_client, user_name)\\nresponse = iam_client.create_user(user_name: user_name)\\nreturn response.user\\nrescue StandardError => e\\nputs \"Error while creating the user \\'#{user_name}\\': #{e.message}\"\\nend\\n# Gets a user in IAM.\\n#\\n# @param iam_client [Aws::IAM::Client] An initialized IAM client.\\n# @param user_name [String] The user\\'s name.\\n# @return [AWS:IAM::Types::User] The existing user.\\n# @example\\n# iam_client = Aws::IAM::Client.new(region: \\'us-west-2\\')\\n# user = get_user(iam_client, \\'my-user\\')\\n# exit 1 unless user.user_name', '']]]\n",
      "[[['', 'def get_user(iam_client, user_name)\\nresponse = iam_client.get_user(user_name: user_name)\\nreturn response.user\\nrescue StandardError => e\\nputs \"Error while getting the user \\'#{user_name}\\': #{e.message}\"\\nend\\n# Checks whether a role exists in IAM.\\n#\\n# @param iam_client [Aws::IAM::Client] An initialized IAM client.\\n# @param role_name [String] The role\\'s name.\\n# @return [Boolean] true if the role exists; otherwise, false.\\n# @example\\n# iam_client = Aws::IAM::Client.new(region: \\'us-west-2\\')\\n# exit 1 unless role_exists?(iam_client, \\'my-role\\')\\ndef role_exists?(iam_client, role_name)\\nresponse = iam_client.get_role(role_name: role_name)\\nreturn true if response.role.role_name\\nrescue StandardError => e\\nputs \"Error while determining whether the role \" \\\\\\n\"\\'#{role_name}\\' exists: #{e.message}\"\\nend\\n# Gets credentials for a role in IAM.\\n#\\n# @param sts_client [Aws::STS::Client] An initialized AWS STS client.\\n# @param role_arn [String] The role\\'s Amazon Resource Name (ARN).\\n# @param role_session_name [String] A name for this role\\'s session.\\n# @param duration_seconds [Integer] The number of seconds this session is valid.\\n# @return [AWS::AssumeRoleCredentials] The credentials.\\n# @example\\n# sts_client = Aws::STS::Client.new(region: \\'us-west-2\\')\\n# credentials = get_credentials(\\n# sts_client,\\n# \\'arn:aws:iam::123456789012:role/AmazonS3ReadOnly\\',\\n# \\'ReadAmazonS3Bucket\\',\\n# 3600\\n# )\\n# exit 1 if credentials.nil?\\ndef get_credentials(sts_client, role_arn, role_session_name, duration_seconds)\\nAws::AssumeRoleCredentials.new(\\nclient: sts_client,\\nrole_arn: role_arn,\\nrole_session_name: role_session_name,', '']]]\n",
      "[[['', 'duration_seconds: duration_seconds\\n)\\nrescue StandardError => e\\nputs \"Error while getting credentials: #{e.message}\"\\nend\\n# Checks whether a bucket exists in Amazon S3.\\n#\\n# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.\\n# @param bucket_name [String] The name of the bucket.\\n# @return [Boolean] true if the bucket exists; otherwise, false.\\n# @example\\n# s3_client = Aws::S3::Client.new(region: \\'us-west-2\\')\\n# exit 1 unless bucket_exists?(s3_client, \\'doc-example-bucket\\')\\ndef bucket_exists?(s3_client, bucket_name)\\nresponse = s3_client.list_buckets\\nresponse.buckets.each do |bucket|\\nreturn true if bucket.name == bucket_name\\nend\\nrescue StandardError => e\\nputs \"Error while checking whether the bucket \\'#{bucket_name}\\' \" \\\\\\n\"exists: #{e.message}\"\\nend\\n# Lists the keys and ETags for the objects in an Amazon S3 bucket.\\n#\\n# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.\\n# @param bucket_name [String] The bucket\\'s name.\\n# @return [Boolean] true if the objects were listed; otherwise, false.\\n# @example\\n# s3_client = Aws::S3::Client.new(region: \\'us-west-2\\')\\n# exit 1 unless list_objects_in_bucket?(s3_client, \\'doc-example-bucket\\')\\ndef list_objects_in_bucket?(s3_client, bucket_name)\\nputs \"Accessing the contents of the bucket named \\'#{bucket_name}\\'...\"\\nresponse = s3_client.list_objects_v2(\\nbucket: bucket_name,\\nmax_keys: 50\\n)\\nif response.count.positive?\\nputs \"Contents of the bucket named \\'#{bucket_name}\\' (first 50 objects):\"\\nputs \"Name => ETag\"\\nresponse.contents.each do |obj|\\nputs \"#{obj.key} => #{obj.etag}\"', '']]]\n",
      "[[['', 'end\\nelse\\nputs \"No objects in the bucket named \\'#{bucket_name}\\'.\"\\nend\\nreturn true\\nrescue StandardError => e\\nputs \"Error while accessing the bucket named \\'#{bucket_name}\\': #{e.message}\"\\nend', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nBoth the AWS account and an IAM user can request temporary security credentials\\nfor federated users. However, for added security, only an IAM user with the necessary\\npermissions should request these temporary credentials to ensure that the federated user\\ngets at most the permissions of the requesting IAM user. In some applications, you might\\nfind it suitable to create an IAM user with specific permissions for the sole purpose of\\ngranting temporary security credentials to your federated users and applications.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor added security when requesting temporary security credentials for federated\\nusers and applications, we recommend that you use a dedicated IAM user with only\\nthe necessary access permissions. The temporary user you create can never get more\\npermissions than the IAM user who requested the temporary security credentials. For\\nmore information, see AWS Identity and Access Management FAQs .', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Statement\":[{\\n\"Action\":[\"s3:ListBucket\",\\n\"sts:GetFederationToken*\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":\"*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.AWSStaticCredentialsProvider;\\nimport com.amazonaws.auth.BasicSessionCredentials;\\nimport com.amazonaws.auth.policy.Policy;\\nimport com.amazonaws.auth.policy.Resource;\\nimport com.amazonaws.auth.policy.Statement;\\nimport com.amazonaws.auth.policy.Statement.Effect;\\nimport com.amazonaws.auth.policy.actions.S3Actions;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.ObjectListing;\\nimport com.amazonaws.services.securitytoken.AWSSecurityTokenService;\\nimport com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder;\\nimport com.amazonaws.services.securitytoken.model.Credentials;\\nimport com.amazonaws.services.securitytoken.model.GetFederationTokenRequest;\\nimport com.amazonaws.services.securitytoken.model.GetFederationTokenResult;\\nimport java.io.IOException;\\npublic class MakingRequestsWithFederatedTempCredentials {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString bucketName = \"*** Specify bucket name ***\";\\nString federatedUser = \"*** Federated user name ***\";\\nString resourceARN = \"arn:aws:s3:::\" + bucketName;\\ntry {\\nAWSSecurityTokenService stsClient = AWSSecurityTokenServiceClientBuilder\\n.standard()\\n.withCredentials(new ProfileCredentialsProvider())\\n.withRegion(clientRegion)\\n.build();\\nGetFederationTokenRequest getFederationTokenRequest = new\\nGetFederationTokenRequest();\\ngetFederationTokenRequest.setDurationSeconds(7200);\\ngetFederationTokenRequest.setName(federatedUser);', '']]]\n",
      "[[['', '// Define the policy and add it to the request.\\nPolicy policy = new Policy();\\npolicy.withStatements(new Statement(Effect.Allow)\\n.withActions(S3Actions.ListObjects)\\n.withResources(new Resource(resourceARN)));\\ngetFederationTokenRequest.setPolicy(policy.toJson());\\n// Get the temporary security credentials.\\nGetFederationTokenResult federationTokenResult =\\nstsClient.getFederationToken(getFederationTokenRequest);\\nCredentials sessionCredentials = federationTokenResult.getCredentials();\\n// Package the session credentials as a BasicSessionCredentials\\n// object for an Amazon S3 client object to use.\\nBasicSessionCredentials basicSessionCredentials = new\\nBasicSessionCredentials(\\nsessionCredentials.getAccessKeyId(),\\nsessionCredentials.getSecretAccessKey(),\\nsessionCredentials.getSessionToken());\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withCredentials(new\\nAWSStaticCredentialsProvider(basicSessionCredentials))\\n.withRegion(clientRegion)\\n.build();\\n// To verify that the client works, send a listObjects request using\\n// the temporary security credentials.\\nObjectListing objects = s3Client.listObjects(bucketName);\\nSystem.out.println(\"No. of Objects = \" +\\nobjects.getObjectSummaries().size());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen requesting temporary security credentials for federated users and applications,\\nfor added security, we suggest that you use a dedicated IAM user with only the\\nnecessary access permissions. The temporary user you create can never get more\\npermissions than the IAM user who requested the temporary security credentials. For\\nmore information, see AWS Identity and Access Management FAQs .', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Statement\":[{\\n\"Action\":[\"s3:ListBucket\",\\n\"sts:GetFederationToken*\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":\"*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Statement\":[\\n{\\n\"Sid\":\"1\",\\n\"Action\":[\"s3:ListBucket\"],\\n\"Effect\":\"Allow\",\\n\"Resource\":\"arn:aws:s3:::YourBucketName\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using Amazon;\\nusing Amazon.Runtime;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing Amazon.SecurityToken;\\nusing Amazon.SecurityToken.Model;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass TempFederatedCredentialsTest\\n{\\nprivate const string bucketName = \"*** bucket name ***\";\\n// Specify your bucket region (an example region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 client;\\npublic static void Main()\\n{\\nListObjectsAsync().Wait();\\n}\\nprivate static async Task ListObjectsAsync()\\n{\\ntry\\n{\\nConsole.WriteLine(\"Listing objects stored in a bucket\");\\n// Credentials use the default AWS SDK for .NET credential search\\nchain.\\n// On local development machines, this is your default profile.\\nSessionAWSCredentials tempCredentials =\\nawait GetTemporaryFederatedCredentialsAsync();', '']]]\n",
      "[[['', '// Create a client by providing temporary security credentials.\\nusing (client = new AmazonS3Client(bucketRegion))\\n{\\nListObjectsRequest listObjectRequest = new\\nListObjectsRequest();\\nlistObjectRequest.BucketName = bucketName;\\nListObjectsResponse response = await\\nclient.ListObjectsAsync(listObjectRequest);\\nList<S3Object> objects = response.S3Objects;\\nConsole.WriteLine(\"Object count = {0}\", objects.Count);\\nConsole.WriteLine(\"Press any key to continue...\");\\nConsole.ReadKey();\\n}\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered ***. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\'\\nwhen writing an object\", e.Message);\\n}\\n}\\nprivate static async Task<SessionAWSCredentials>\\nGetTemporaryFederatedCredentialsAsync()\\n{\\nAmazonSecurityTokenServiceConfig config = new\\nAmazonSecurityTokenServiceConfig();\\nAmazonSecurityTokenServiceClient stsClient =\\nnew AmazonSecurityTokenServiceClient(\\nconfig);\\nGetFederationTokenRequest federationTokenRequest =\\nnew GetFederationTokenRequest();\\nfederationTokenRequest.DurationSeconds = 7200;\\nfederationTokenRequest.Name = \"User1\";\\nfederationTokenRequest.Policy = @\"{\\n\"\"Statement\"\":\\n[', '']]]\n",
      "[[['', '{\\n\"\"Sid\"\":\"\"Stmt1311212314284\"\",\\n\"\"Action\"\":[\"\"s3:ListBucket\"\"],\\n\"\"Effect\"\":\"\"Allow\"\",\\n\"\"Resource\"\":\"\"arn:aws:s3:::\" + bucketName + @\"\"\"\\n}\\n]\\n}\\n\";\\nGetFederationTokenResponse federationTokenResponse =\\nawait\\nstsClient.GetFederationTokenAsync(federationTokenRequest);\\nCredentials credentials = federationTokenResponse.Credentials;\\nSessionAWSCredentials sessionCredentials =\\nnew SessionAWSCredentials(credentials.AccessKeyId,\\ncredentials.SecretAccessKey,\\ncredentials.SessionToken);\\nreturn sessionCredentials;\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Statement\":[{\\n\"Action\":[\"s3:ListBucket\",\\n\"sts:GetFederationToken*\"\\n],\\n\"Effect\":\"Allow\",\\n\"Resource\":\"*\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Statement\":[\\n{\\n\"Sid\":\"1\",\\n\"Action\":[\"s3:ListBucket\"],\\n\"Effect\":\"Allow\",\\n\"Resource\":\"arn:aws:s3:::YourBucketName\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"require 'vendor/autoload.php';\\nuse Aws\\\\S3\\\\Exception\\\\S3Exception;\\nuse Aws\\\\S3\\\\S3Client;\\nuse Aws\\\\Sts\\\\StsClient;\\n$bucket = '*** Your Bucket Name ***';\\n// In real applications, the following code is part of your trusted code. It has\\n// the security credentials that you use to obtain temporary security credentials.\\n$sts = new StsClient([\\n'version' => 'latest',\\n'region' => 'us-east-1'\\n]);\\n// Fetch the federated credentials.\\n$sessionToken = $sts->getFederationToken([\\n'Name' => 'User1',\\n'DurationSeconds' => '3600',\\n'Policy' => json_encode([\\n'Statement' => [\\n'Sid' => 'randomstatementid' . time(),\\n'Action' => ['s3:ListBucket'],\\n'Effect' => 'Allow',\\n'Resource' => 'arn:aws:s3:::' . $bucket\\n]\\n])\\n]);\", '']]]\n",
      "[[['', \"// The following will be part of your less trusted code. You provide temporary\\n// security credentials so the code can send authenticated requests to Amazon S3.\\n$s3 = new S3Client([\\n'region' => 'us-east-1',\\n'version' => 'latest',\\n'credentials' => [\\n'key' => $sessionToken['Credentials']['AccessKeyId'],\\n'secret' => $sessionToken['Credentials']['SecretAccessKey'],\\n'token' => $sessionToken['Credentials']['SessionToken']\\n]\\n]);\\ntry {\\n$result = $s3->listObjects([\\n'Bucket' => $bucket\\n]);\\n} catch (S3Exception $e) {\\necho $e->getMessage() . PHP_EOL;\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor added security when you request temporary security credentials for federated users\\nand applications, you might want to use a dedicated IAM user with only the necessary\\naccess permissions. The temporary user you create can never get more permissions than\\nthe IAM user who requested the temporary security credentials. For more information,\\nsee AWS Identity and Access Management FAQs .', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '# Prerequisites:\\n# - An existing Amazon S3 bucket.\\nrequire \"aws-sdk-s3\"\\nrequire \"aws-sdk-iam\"\\nrequire \"json\"\\n# Checks to see whether a user exists in IAM; otherwise,\\n# creates the user.\\n#\\n# @param iam [Aws::IAM::Client] An initialized IAM client.\\n# @param user_name [String] The user\\'s name.\\n# @return [Aws::IAM::Types::User] The existing or new user.\\n# @example\\n# iam = Aws::IAM::Client.new(region: \\'us-west-2\\')\\n# user = get_user(iam, \\'my-user\\')\\n# exit 1 unless user.user_name\\n# puts \"User\\'s name: #{user.user_name}\"\\ndef get_user(iam, user_name)\\nputs \"Checking for a user with the name \\'#{user_name}\\'...\"\\nresponse = iam.get_user(user_name: user_name)\\nputs \"A user with the name \\'#{user_name}\\' already exists.\"\\nreturn response.user\\n# If the user doesn\\'t exist, create them.\\nrescue Aws::IAM::Errors::NoSuchEntity\\nputs \"A user with the name \\'#{user_name}\\' doesn\\'t exist. Creating this user...\"\\nresponse = iam.create_user(user_name: user_name)\\niam.wait_until(:user_exists, user_name: user_name)\\nputs \"Created user with the name \\'#{user_name}\\'.\"\\nreturn response.user\\nrescue StandardError => e\\nputs \"Error while accessing or creating the user named \\'#{user_name}\\':\\n#{e.message}\"\\nend\\n# Gets temporary AWS credentials for an IAM user with the specified permissions.\\n#\\n# @param sts [Aws::STS::Client] An initialized AWS STS client.\\n# @param duration_seconds [Integer] The number of seconds for valid credentials.', '']]]\n",
      "[[['', '# @param user_name [String] The user\\'s name.\\n# @param policy [Hash] The access policy.\\n# @return [Aws::STS::Types::Credentials] AWS credentials for API authentication.\\n# @example\\n# sts = Aws::STS::Client.new(region: \\'us-west-2\\')\\n# credentials = get_temporary_credentials(sts, duration_seconds, user_name,\\n# {\\n# \\'Version\\' => \\'2012-10-17\\',\\n# \\'Statement\\' => [\\n# \\'Sid\\' => \\'Stmt1\\',\\n# \\'Effect\\' => \\'Allow\\',\\n# \\'Action\\' => \\'s3:ListBucket\\',\\n# \\'Resource\\' => \\'arn:aws:s3:::doc-example-bucket\\'\\n# ]\\n# }\\n# )\\n# exit 1 unless credentials.access_key_id\\n# puts \"Access key ID: #{credentials.access_key_id}\"\\ndef get_temporary_credentials(sts, duration_seconds, user_name, policy)\\nresponse = sts.get_federation_token(\\nduration_seconds: duration_seconds,\\nname: user_name,\\npolicy: policy.to_json\\n)\\nreturn response.credentials\\nrescue StandardError => e\\nputs \"Error while getting federation token: #{e.message}\"\\nend\\n# Lists the keys and ETags for the objects in an Amazon S3 bucket.\\n#\\n# @param s3_client [Aws::S3::Client] An initialized Amazon S3 client.\\n# @param bucket_name [String] The bucket\\'s name.\\n# @return [Boolean] true if the objects were listed; otherwise, false.\\n# @example\\n# s3_client = Aws::S3::Client.new(region: \\'us-west-2\\')\\n# exit 1 unless list_objects_in_bucket?(s3_client, \\'doc-example-bucket\\')\\ndef list_objects_in_bucket?(s3_client, bucket_name)\\nputs \"Accessing the contents of the bucket named \\'#{bucket_name}\\'...\"\\nresponse = s3_client.list_objects_v2(\\nbucket: bucket_name,\\nmax_keys: 50\\n)', '']]]\n",
      "[[['', 'if response.count.positive?\\nputs \"Contents of the bucket named \\'#{bucket_name}\\' (first 50 objects):\"\\nputs \"Name => ETag\"\\nresponse.contents.each do |obj|\\nputs \"#{obj.key} => #{obj.etag}\"\\nend\\nelse\\nputs \"No objects in the bucket named \\'#{bucket_name}\\'.\"\\nend\\nreturn true\\nrescue StandardError => e\\nputs \"Error while accessing the bucket named \\'#{bucket_name}\\': #{e.message}\"\\nend\\n# Example usage:\\ndef run_me\\nregion = \"us-west-2\"\\nuser_name = \"my-user\"\\nbucket_name = \"doc-example-bucket\"\\niam = Aws::IAM::Client.new(region: region)\\nuser = get_user(iam, user_name)\\nexit 1 unless user.user_name\\nputs \"User\\'s name: #{user.user_name}\"\\nsts = Aws::STS::Client.new(region: region)\\ncredentials = get_temporary_credentials(sts, 3600, user_name,\\n{\\n\"Version\" => \"2012-10-17\",\\n\"Statement\" => [\\n\"Sid\" => \"Stmt1\",\\n\"Effect\" => \"Allow\",\\n\"Action\" => \"s3:ListBucket\",\\n\"Resource\" => \"arn:aws:s3:::#{bucket_name}\"\\n]\\n}\\n)\\nexit 1 unless credentials.access_key_id\\nputs \"Access key ID: #{credentials.access_key_id}\"\\ns3_client = Aws::S3::Client.new(region: region, credentials: credentials)', '']]]\n",
      "[[['', 'exit 1 unless list_objects_in_bucket?(s3_client, bucket_name)\\nend\\nrun_me if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', 's3.Region.amazonaws.com', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'DELETE /puppy.jpg HTTP/1.1\\nHost: examplebucket.s3.us-west-2.amazonaws.com\\nDate: Mon, 11 Apr 2016 12:00:00 GMT\\nx-amz-date: Mon, 11 Apr 2016 12:00:00 GMT\\nAuthorization: authorization string', ''], ['', '', '']], [['', '', ''], ['', 'DELETE /examplebucket/puppy.jpg HTTP/1.1\\nHost: s3.us-west-2.amazonaws.com\\nDate: Mon, 11 Apr 2016 12:00:00 GMT\\nx-amz-date: Mon, 11 Apr 2016 12:00:00 GMT\\nAuthorization: authorization string', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nUpdate (September 23, 2020) – To make sure that customers have the time that they need\\nto transition to virtual-hosted–style URLs, we have decided to delay the deprecation of\\npath-style URLs. For more information, see Amazon S3 Path Deprecation Plan – The Rest of\\nthe Story in the AWS News Blog.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'GET /puppy.jpg HTTP/1.1\\nHost: examplebucket.s3.dualstack.us-west-2.amazonaws.com\\nDate: Mon, 11 Apr 2016 12:00:00 GMT\\nx-amz-date: Mon, 11 Apr 2016 12:00:00 GMT\\nAuthorization: authorization string', ''], ['', '', '']], [['', '', ''], ['', 'GET /examplebucket/puppy.jpg HTTP/1.1\\nHost: s3.dualstack.us-west-2.amazonaws.com\\nDate: Mon, 11 Apr 2016 12:00:00 GMT\\nx-amz-date: Mon, 11 Apr 2016 12:00:00 GMT\\nAuthorization: authorization string', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nWhen you're using virtual-hosted–style buckets with SSL, the SSL wildcard certificate\\nmatches only buckets that do not contain dots (.). To work around this limitation, use\\nHTTP or write your own certificate-verification logic. For more information, see Amazon S3\\nPath Deprecation Plan on the AWS News Blog.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'https://s3.region-code.amazonaws.com/bucket-name/key-name', ''], ['', '', '']], [['', '', ''], ['', 'https://s3.us-west-2.amazonaws.com/DOC-EXAMPLE-BUCKET1/puppy.jpg', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nUpdate (September 23, 2020) – To make sure that customers have the time that they need\\nto transition to virtual-hosted–style URLs, we have decided to delay the deprecation of\\npath-style URLs. For more information, see Amazon S3 Path Deprecation Plan – The Rest of\\nthe Story in the AWS News Blog.', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nWhen hosting website content that will be accessed from a web browser, avoid using path-\\nstyle URLs, which might interfere with the browser same origin security model. To host\\nwebsite content, we recommend that you use either S3 website endpoints or a CloudFront\\ndistribution. For more information, see Website endpoints and Deploy a React-based\\nsingle-page application to Amazon S3 and CloudFront in the AWS Perspective Guidance\\nPatterns.', ''], ['', '', '']], [['', '', ''], ['', 'https://bucket-name.s3.region-code.amazonaws.com/key-name', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'https://DOC-EXAMPLE-BUCKET1.s3.us-west-2.amazonaws.com/puppy.png', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'http://s3.us-east-1.amazonaws.com/example.com/homepage.html', ''], ['', '', '']], [['', '', ''], ['', 'GET /example.com/homepage.html HTTP/1.1\\nHost: s3.us-east-1.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'GET /example.com/homepage.html HTTP/1.0', ''], ['', '', '']], [['', '', ''], ['', 'http://DOC-EXAMPLE-BUCKET1.s3.eu-west-1.amazonaws.com/homepage.html', ''], ['', '', '']], [['', '', ''], ['', 'GET /homepage.html HTTP/1.1\\nHost: DOC-EXAMPLE-BUCKET1.s3.eu-west-1.amazonaws.com', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'http://www.example.com/homepage.html', ''], ['', '', '']], [['', '', ''], ['', 'GET /homepage.html HTTP/1.1\\nHost: www.example.com', ''], ['', '', '']], [['', '', ''], ['', 'images.example.com CNAME images.example.com.s3.us-east-1.amazonaws.com.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'GET / HTTP/1.1\\nHost: www.example.com\\nDate: date\\nAuthorization: signatureValue', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen using custom URLs with CNAMEs, you will need to ensure a matching bucket exists\\nfor any CNAME or alias record you configure. For example, if you create DNS entries for\\nwww.example.com and login.example.com to publish web content using S3, you will\\nneed to create both buckets www.example.com and login.example.com.\\nWhen a CNAME or alias records is configured pointing to an S3 endpoint without a\\nmatching bucket, any AWS user can create that bucket and publish content under the\\nconfigured alias, even if ownership is not the same.\\nFor the same reason, we recommend that you change or remove the corresponding CNAME\\nor alias when deleting a bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nFor request-routing reasons, the CNAME DNS record must be defined exactly as shown\\nin the preceding example. Otherwise, it might appear to operate correctly, but it will\\neventually result in unpredictable behavior.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAlthough you might see legacy endpoints in your logs, we recommend that you always use\\nthe standard endpoint syntax to access your buckets.\\nAmazon S3 virtual-hosted–style URLs use the following format:\\nhttps://bucket-name.s3.region-code.amazonaws.com/key-name\\nIn Amazon S3, path-style URLs use the following format:\\nhttps://s3.region-code.amazonaws.com/bucket-name/key-name', ''], ['', '', '']], [['', '', ''], ['', 'https://bucket-name.s3.region-code.amazonaws.com/key-name', ''], ['', '', '']], [['', '', ''], ['', 'https://s3.region-code.amazonaws.com/bucket-name/key-name', ''], ['', '', '']], [['', '', ''], ['', 'https://bucket-name.s3-region-code.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'https://DOC-EXAMPLE-BUCKET1.s3-us-west-2.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'bucket-name.s3.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'https://DOC-EXAMPLE-BUCKET1.s3.amazonaws.com', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nAccording to RFC 2616, when using Expect: Continue with an unknown HTTP server,\\nyou should not wait an indefinite period before sending the request body. This is because', '']]]\n",
      "[[['', 'some HTTP servers do not recognize 100-continue. However, Amazon S3 does recognize\\nif your request contains an Expect: Continue and will respond with a provisional\\n100-continue status or a final status code. Additionally, no redirect error will occur after\\nreceiving the provisional 100 continue go-ahead. This will help you avoid receiving a\\nredirect response while you are still writing the request body.', ''], ['', '', '']], [['', '', ''], ['', 'PUT /nelson.txt HTTP/1.1\\nHost: quotes.s3.amazonaws.com\\nDate: Mon, 15 Oct 2007 22:18:46 +0000\\nContent-Length: 6\\nExpect: 100-continue', ''], ['', '', '']], [['', '', ''], ['', 'HTTP/1.1 307 Temporary Redirect\\nLocation: http://quotes.s3-4c25d83b.amazonaws.com/nelson.txt?rk=8d47490b\\nContent-Type: application/xml\\nTransfer-Encoding: chunked\\nDate: Mon, 15 Oct 2007 22:18:46 GMT\\nServer: AmazonS3\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error>\\n<Code>TemporaryRedirect</Code>\\n<Message>Please re-send this request to the\\nspecified temporary endpoint. Continue to use the\\noriginal request endpoint for future requests.\\n</Message>\\n<Endpoint>quotes.s3-4c25d83b.amazonaws.com</Endpoint>\\n<Bucket>quotes</Bucket>', '']]]\n",
      "[[['', '</Error>', ''], ['', '', '']], [['', '', ''], ['', 'PUT /nelson.txt?rk=8d47490b HTTP/1.1\\nHost: quotes.s3-4c25d83b.amazonaws.com\\nDate: Mon, 15 Oct 2007 22:18:46 +0000\\nContent-Length: 6\\nExpect: 100-continue', ''], ['', '', '']], [['', '', ''], ['', 'HTTP/1.1 100 Continue', ''], ['', '', '']], [['', '', ''], ['', 'ha ha\\\\n', ''], ['', '', '']], [['', '', ''], ['', 'HTTP/1.1 200 OK\\nDate: Mon, 15 Oct 2007 22:18:48 GMT\\nETag: \"a2c8d6b872054293afd41061e93bc289\"\\nContent-Length: 0\\nServer: AmazonS3', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nServices in AWS, such as Amazon S3, require that you provide credentials when you\\naccess them. The service can then determine whether you have permissions to access the\\nresources that it owns. The console requires your password. You can create access keys for\\nyour AWS account to access the AWS CLI or API. However, we don't recommend that you\\naccess AWS using the credentials for your AWS account. Instead, we recommend that you\\nuse AWS Identity and Access Management (IAM). Create an IAM user, add the user to an IAM\\ngroup with administrative permissions, and then grant administrative permissions to the\\nIAM user that you created. You can then access AWS using a special URL and the credentials\\nof that IAM user. For instructions, go to Creating Your First IAM user and Administrators\\nGroup in the IAM User Guide.\", ''], ['', '', '']], [['', '', ''], ['', '[adminuser]\\naws_access_key_id = adminuser access key ID\\naws_secret_access_key = adminuser secret access key\\nregion = aws-region', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws help', ''], ['', '', '']], [['', '', ''], ['', 'aws s3 ls --profile adminuser', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nYou can use AWS Amplify for end-to-end fullstack development of web and mobile apps.\\nAmplify Storage seamlessly integrates file storage and management capabilities into\\nfrontend web and mobile apps, built on top of Amazon S3. For more information, see\\nStorage in the Amplify user guide.', ''], ['', '', '']], [['SDK documentation', 'Code examples'], ['AWS SDK for C++', 'AWS SDK for C++ code examples'], ['AWS CLI', 'AWS CLI code examples'], ['AWS SDK for Go', 'AWS SDK for Go code examples']]]\n",
      "[[['SDK documentation', 'Code examples'], ['AWS SDK for Java', 'AWS SDK for Java code examples'], ['AWS SDK for JavaScript', 'AWS SDK for JavaScript code examples'], ['AWS SDK for Kotlin', 'AWS SDK for Kotlin code examples'], ['AWS SDK for .NET', 'AWS SDK for .NET code examples'], ['AWS SDK for PHP', 'AWS SDK for PHP code examples'], ['AWS Tools for PowerShell', 'Tools for PowerShell code examples'], ['AWS SDK for Python (Boto3)', 'AWS SDK for Python (Boto3) code examples'], ['AWS SDK for Ruby', 'AWS SDK for Ruby code examples'], ['AWS SDK for Rust', 'AWS SDK for Rust code examples'], ['AWS SDK for SAP ABAP', 'AWS SDK for SAP ABAP code examples'], ['AWS SDK for Swift', 'AWS SDK for Swift code examples']], [['', '', ''], ['', \"Example availability\\nCan't find what you need? Request a code example by using the Provide feedback link at\\nthe bottom of this page.\", ''], ['', '', '']]]\n",
      "[[['SDK', 'Requesting Signature Version 4 for Request Authentication'], ['AWS CLI', 'For the default profile, run the following command:\\n$ aws configure set default.s3.signature_version\\ns3v4\\nFor a custom profile, run the following command:\\n$ aws configure set profile.your_profile_name.s\\n3.signature_version s3v4'], ['Java SDK', 'Add the following in your code:']], [['', '', ''], ['', '$ aws configure set default.s3.signature_version\\ns3v4', ''], ['', '', '']], [['', '', ''], ['', '$ aws configure set profile.your_profile_name.s\\n3.signature_version s3v4', ''], ['', '', '']]]\n",
      "[[['SDK', 'Requesting Signature Version 4 for Request Authentication'], ['', 'System.setProperty(SDKGlobalConfiguration.ENA\\nBLE_S3_SIGV4_SYSTEM_PROPERTY, \"true\");\\nOr, on the command line, specify the following:\\n-Dcom.amazonaws.services.s3.enableV4'], ['JavaScript SDK', \"Set the signatureVersion parameter to v4 when construct\\ning the client:\\nvar s3 = new AWS.S3({signatureVersion: 'v4'});\"], ['PHP SDK', \"Set the signature parameter to v4 when constructing the\\nAmazon S3 service client for PHP SDK v2:\\n<?php\\n$client = S3Client::factory([\\n'region' => 'YOUR-REGION',\\n'version' => 'latest',\\n'signature' => 'v4'\\n]);\\nWhen using the PHP SDK v3, set the signature_version\\nparameter to v4 during construction of the Amazon S3 service\\nclient:\\n<?php\\n$s3 = new Aws\\\\S3\\\\S3Client([\\n'version' => '2006-03-01',\\n'region' => 'YOUR-REGION',\\n'signature_version' => 'v4'\\n]);\"], ['Python-Boto SDK', 'Specify the following in the boto default config file:\\n[s3] use-sigv4 = True']], [['', '', ''], ['', 'System.setProperty(SDKGlobalConfiguration.ENA\\nBLE_S3_SIGV4_SYSTEM_PROPERTY, \"true\");', ''], ['', '', '']], [['', '', ''], ['', '-Dcom.amazonaws.services.s3.enableV4', ''], ['', '', '']], [['', '', ''], ['', \"var s3 = new AWS.S3({signatureVersion: 'v4'});\", ''], ['', '', '']], [['', '', ''], ['', \"<?php\\n$client = S3Client::factory([\\n'region' => 'YOUR-REGION',\\n'version' => 'latest',\\n'signature' => 'v4'\\n]);\", ''], ['', '', '']], [['', '', ''], ['', \"<?php\\n$s3 = new Aws\\\\S3\\\\S3Client([\\n'version' => '2006-03-01',\\n'region' => 'YOUR-REGION',\\n'signature_version' => 'v4'\\n]);\", ''], ['', '', '']], [['', '', ''], ['', '[s3] use-sigv4 = True', ''], ['', '', '']]]\n",
      "[[['SDK', 'Requesting Signature Version 4 for Request Authentication'], ['Ruby SDK', \"Ruby SDK - Version 1: Set the :s3_signature_version\\nparameter to :v4 when constructing the client:\\ns3 = AWS::S3::Client.new(:s3_signature_version\\n=> :v4)\\nRuby SDK - Version 3: Set the signature_version\\nparameter to v4 when constructing the client:\\ns3 = Aws::S3::Client.new(signature_version: 'v4')\"], ['.NET SDK', 'Add the following to the code before creating the Amazon S3\\nclient:\\nAWSConfigsS3.UseSignatureVersion4 = true;\\nOr, add the following to the config file:\\n<appSettings>\\n<add key=\"AWS.S3.UseSignatureVersion4\" value=\"tr\\nue\" />\\n</appSettings>']], [['', '', ''], ['', 's3 = AWS::S3::Client.new(:s3_signature_version\\n=> :v4)', ''], ['', '', '']], [['', '', ''], ['', \"s3 = Aws::S3::Client.new(signature_version: 'v4')\", ''], ['', '', '']], [['', '', ''], ['', 'AWSConfigsS3.UseSignatureVersion4 = true;', ''], ['', '', '']], [['', '', ''], ['', '<appSettings>\\n<add key=\"AWS.S3.UseSignatureVersion4\" value=\"tr\\nue\" />\\n</appSettings>', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['If you use\\nthis SDK/\\nProduct', 'Upgrade\\nto this SDK\\nversion', 'Code change\\nneeded to\\nthe client to\\nuse Sigv4?', 'Link to SDK documentation'], ['AWS SDK for\\nJava v1', 'Upgrade\\nto Java\\n1.11.201+ or\\nv2.', 'Yes', 'Specifying the Signature Version in Request\\nAuthentication'], ['AWS SDK for\\nJava v2', 'No SDK\\nupgrade is\\nneeded.', 'No', 'AWS SDK for Java'], ['AWS SDK\\nfor .NET v1', 'Upgrade to\\n3.1.10 or\\nlater.', 'Yes', 'AWS SDK for .NET'], ['AWS SDK\\nfor .NET v2', 'Upgrade to\\n3.1.10 or\\nlater.', 'No', 'AWS SDK for .NET v2'], ['AWS SDK\\nfor .NET v3', 'Upgrade to\\n3.3.0.0 or\\nlater.', 'Yes', 'AWS SDK for .NET v3'], ['AWS SDK for\\nJavaScript v1', 'Upgrade to\\n2.68.0 or\\nlater.', 'Yes', 'AWS SDK for JavaScript'], ['AWS SDK for\\nJavaScript v2', 'Upgrade to\\n2.68.0 or\\nlater.', 'Yes', 'AWS SDK for JavaScript'], ['AWS SDK for\\nJavaScript v3', 'No action\\nis currently\\nneeded.\\nUpgrade to', 'No', 'AWS SDK for JavaScript']]]\n",
      "[[['If you use\\nthis SDK/\\nProduct', 'Upgrade\\nto this SDK\\nversion', 'Code change\\nneeded to\\nthe client to\\nuse Sigv4?', 'Link to SDK documentation'], ['', 'major version\\nV3 in Q3\\n2019.', '', ''], ['AWS SDK for\\nPHP v1', \"Recommend\\nto upgrade\\nto the most\\nrecent\\nversion of\\nPHP or, at\\nleast to\\nv2.7.4 with\\nthe signature\\nparameter\\nset to v4\\nin the S3\\nclient's\\nconfigura\\ntion.\", 'Yes', 'AWS SDK for PHP']]]\n",
      "[[['If you use\\nthis SDK/\\nProduct', 'Upgrade\\nto this SDK\\nversion', 'Code change\\nneeded to\\nthe client to\\nuse Sigv4?', 'Link to SDK documentation'], ['AWS SDK for\\nPHP v2', \"Recommend\\nto upgrade\\nto the most\\nrecent\\nversion of\\nPHP or, at\\nleast to\\nv2.7.4 with\\nthe signature\\nparameter\\nset to v4\\nin the S3\\nclient's\\nconfigura\\ntion.\", 'No', 'AWS SDK for PHP'], ['AWS SDK for\\nPHP v3', 'No SDK\\nupgrade is\\nneeded.', 'No', 'AWS SDK for PHP'], ['Boto2', 'Upgrade\\nto Boto2\\nv2.49.0.', 'Yes', 'Boto 2 Upgrade'], ['Boto3', 'Upgrade\\nto 1.5.71\\n(Botocore),\\n1.4.6 (Boto3).', 'Yes', 'Boto 3 - AWS SDK for Python'], ['AWS CLI', 'Upgrade to\\n1.11.108.', 'Yes', 'AWS Command Line Interface']]]\n",
      "[[['If you use\\nthis SDK/\\nProduct', 'Upgrade\\nto this SDK\\nversion', 'Code change\\nneeded to\\nthe client to\\nuse Sigv4?', 'Link to SDK documentation'], ['AWS CLI v2\\n(preview)', 'No SDK\\nupgrade is\\nneeded.', 'No', 'AWS Command Line Interface version 2'], ['AWS SDK for\\nRuby v1', 'Upgrade to\\nRuby V3.', 'Yes', 'Ruby V3 for AWS'], ['AWS SDK for\\nRuby v2', 'Upgrade to\\nRuby V3.', 'Yes', 'Ruby V3 for AWS'], ['AWS SDK for\\nRuby v3', 'No SDK\\nupgrade is\\nneeded.', 'No', 'Ruby V3 for AWS'], ['Go', 'No SDK\\nupgrade is\\nneeded.', 'No', 'AWS SDK for Go'], ['C++', 'No SDK\\nupgrade is\\nneeded.', 'No', 'AWS SDK for C++']], [['', '', ''], ['', 'Get-Module –Name AWSPowershell\\nGet-Module –Name AWSPowershell.NetCore', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Update-Module –Name AWSPowershell\\nUpdate-Module –Name AWSPowershell.NetCore', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nTo use this feature, you must have an application that can handle Amazon S3 redirect\\nresponses. The only exception is for applications that work exclusively with buckets that\\nwere created without <CreateBucketConfiguration>. For more information about\\nlocation constraints, see Accessing and listing an Amazon S3 bucket.\\nFor all Regions that launched after March 20, 2019, if a request arrives at the wrong\\nAmazon S3 location, Amazon S3 returns an HTTP 400 Bad Request error.\\nFor more information about enabling or disabling an AWS Region, see AWS Regions and\\nEndpoints in the AWS General Reference.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\nDon't reuse an endpoint provided by a previous redirect response. It might appear to\\nwork (even for long periods of time), but it might provide unpredictable results and will\\neventually fail without notice.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'HTTP/1.1 307 Temporary Redirect\\nLocation: http://awsexamplebucket1.s3-gztb4pa9sq.amazonaws.com/photos/puppy.jpg?\\nrk=e2c69a31\\nContent-Type: application/xml\\nTransfer-Encoding: chunked\\nDate: Fri, 12 Oct 2007 01:12:56 GMT\\nServer: AmazonS3\\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error>\\n<Code>TemporaryRedirect</Code>\\n<Message>Please re-send this request to the specified temporary endpoint.\\nContinue to use the original request endpoint for future requests.</Message>\\n<Endpoint>awsexamplebucket1.s3-gztb4pa9sq.amazonaws.com</Endpoint>\\n</Error>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']], [['', '', ''], ['', '<soapenv:Body>\\n<soapenv:Fault>\\n<Faultcode>soapenv:Client.TemporaryRedirect</Faultcode>\\n<Faultstring>Please re-send this request to the specified temporary endpoint.\\nContinue to use the original request endpoint for future requests.</Faultstring>\\n<Detail>\\n<Bucket>images</Bucket>\\n<Endpoint>s3-gztb4pa9sq.amazonaws.com</Endpoint>\\n</Detail>\\n</soapenv:Fault>\\n</soapenv:Body>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']], [['', '', ''], ['', '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error>\\n<Code>NoSuchKey</Code>\\n<Message>The resource you requested does not exist</Message>\\n<Resource>/mybucket/myfoto.jpg</Resource>\\n<RequestId>4442587FB7D0A2F9</RequestId>\\n</Error>', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']], [['', '', ''], ['', '<soapenv:Body>\\n<soapenv:Fault>\\n<Faultcode>soapenv:Client.NoSuchKey</Faultcode>\\n<Faultstring>The specified key does not exist.</Faultstring>\\n<Detail>\\n<Key>Fred</Key>\\n</Detail>', '']]]\n",
      "[[['', '</soapenv:Fault>\\n</soapenv:Body>', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nSOAP requests, both authenticated and anonymous, must be sent to Amazon S3 using SSL.\\nAmazon S3 returns an error when you send a SOAP request over HTTP.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen making authenticated SOAP requests, temporary security credentials are not\\nsupported. For more information about types of credentials, see Making requests.', ''], ['', '', '']], [['', '', ''], ['', '<CreateBucket xmlns=\"https://doc.s3.amazonaws.com/2006-03-01\">\\n<Bucket>quotes</Bucket>\\n<Acl>private</Acl>', '']]]\n",
      "[[['', '<AWSAccessKeyId>AKIAIOSFODNN7EXAMPLE</AWSAccessKeyId>\\n<Timestamp>2009-01-01T12:00:00.000Z</Timestamp>\\n<Signature>Iuyz3d3P0aTou39dzbqaEXAMPLE=</Signature>\\n</CreateBucket>', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nSOAP requests, both authenticated and anonymous, must be sent to Amazon S3 using SSL.\\nAmazon S3 returns an error when you send a SOAP request over HTTP.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nDue to different interpretations regarding how extra time precision should be\\ndropped, .NET users should take care not to send Amazon S3 overly specific time\\nstamps. This can be accomplished by manually constructing DateTime objects with only\\nmillisecond precision.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<PutObjectInline xmlns=\"https://doc.s3.amazonaws.com/2006-03-01\">\\n<Bucket>quotes</Bucket>\\n<Key>Nelson</Key>\\n<Metadata>\\n<Name>Content-Type</Name>\\n<Value>text/plain</Value>\\n</Metadata>\\n<Data>aGEtaGE=</Data>\\n<ContentLength>5</ContentLength>\\n<AccessControlList>\\n<Grant>\\n<Grantee xsi:type=\"CanonicalUser\">\\n<ID>75cc57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a</ID>\\n<DisplayName>chriscustomer</DisplayName>\\n</Grantee>\\n<Permission>FULL_CONTROL</Permission>\\n</Grant>\\n<Grant>\\n<Grantee xsi:type=\"Group\">\\n<URI>http://acs.amazonaws.com/groups/global/AllUsers<URI>\\n</Grantee>\\n<Permission>READ</Permission>\\n</Grant>\\n</AccessControlList>\\n<AWSAccessKeyId>AKIAIOSFODNN7EXAMPLE</AWSAccessKeyId>\\n<Timestamp>2009-03-01T12:00:00.183Z</Timestamp>\\n<Signature>Iuyz3d3P0aTou39dzbqaEXAMPLE=</Signature>\\n</PutObjectInline>', ''], ['', '', '']], [['', '', ''], ['', '<PutObjectInlineResponse xmlns=\"https://s3.amazonaws.com/doc/2006-03-01\">\\n<PutObjectInlineResponse>\\n<ETag>&quot828ef3fdfa96f00ad9f27c383fc9ac7f&quot</ETag>\\n<LastModified>2009-01-01T12:00:00.000Z</LastModified>\\n</PutObjectInlineResponse>', '']]]\n",
      "[[['', '</PutObjectInlineResponse>', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nThis section describes how to authenticate requests using AWS Signature Version 2.\\nSignature Version 2 is being turned off (deprecated), Amazon S3 will only accept API\\nrequests that are signed using Signature Version 4. For more information, see AWS\\nSignature Version 2 Turned Off (Deprecated) for Amazon S3\\nSignature Version 4 is supported in all AWS Regions, and it is the only version that is\\nsupported for new Regions. For more information, see Authenticating Requests (AWS\\nSignature Version 4) in the Amazon Simple Storage Service API Reference.\\nAmazon S3 offers you the ability to identify what API signature version was used to sign a\\nrequest. It is important to identify if any of your workflows are utilizing Signature Version 2\\nsigning and upgrading them to use Signature Version 4 to prevent impact to your business.\\n• If you are using CloudTrail event logs(recommended option), please see Identifying\\nAmazon S3 Signature Version 2 requests by using CloudTrail on how to query and\\nidentify such requests.\\n• If you are using the Amazon S3 Server Access logs, see Identifying Signature Version 2\\nrequests by using Amazon S3 access logs', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['1', 'Construct a request to AWS.'], ['2', 'Calculate the signature using your secret access key.'], ['3', 'Send the request to Amazon S3. Include your access key ID and the signature in your\\nrequest. Amazon S3 performs the next three steps.']]]\n",
      "[[['4', 'Amazon S3 uses the access key ID to look up your secret access key.'], ['5', 'Amazon S3 calculates a signature from the request data and the secret access key\\nusing the same algorithm that you used to calculate the signature you sent in the\\nrequest.'], ['6', 'If the signature generated by Amazon S3 matches the one you sent in the request,\\nthe request is considered authentic. If the comparison fails, the request is discarded,\\nand Amazon S3 returns an error response.']]]\n",
      "[[['', '', ''], ['', 'Note\\nThis topic explains authenticating requests using Signature Version 2. Amazon S3 now\\nsupports the latest Signature Version 4. This latest signature version is supported in all\\nregions and any new regions after January 30, 2014 will support only Signature Version\\n4. For more information, go to Authenticating Requests (AWS Signature Version 4) in the\\nAmazon Simple Storage Service API Reference.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe content in this section does not apply to HTTP POST. For more information, see\\nBrowser-based uploads using POST (AWS signature version 2).', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'GET /photos/puppy.jpg HTTP/1.1\\nHost: awsexamplebucket1.us-west-1.s3.amazonaws.com\\nDate: Tue, 27 Mar 2007 19:36:42 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:\\nqgk2+6Sv9/oM7G3qLEjTH1a1l1g=', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Authorization: AWS AWSAccessKeyId:Signature', ''], ['', '', '']], [['', '', ''], ['', 'Authorization = \"AWS\" + \" \" + AWSAccessKeyId + \":\" + Signature;\\nSignature = Base64( HMAC-SHA1( UTF-8-Encoding-Of(YourSecretAccessKey), UTF-8-Encoding-\\nOf( StringToSign ) ) );\\nStringToSign = HTTP-Verb + \"\\\\n\" +\\nContent-MD5 + \"\\\\n\" +\\nContent-Type + \"\\\\n\" +\\nDate + \"\\\\n\" +\\nCanonicalizedAmzHeaders +\\nCanonicalizedResource;\\nCanonicalizedResource = [ \"/\" + Bucket ] +\\n<HTTP-Request-URI, from the protocol name up to the query string> +\\n[ subresource, if present. For example \"?acl\", \"?location\", or \"?logging\"];\\nCanonicalizedAmzHeaders = <described below>', ''], ['', '', '']]]\n",
      "[[['1', 'Start with an empty string (\"\").'], ['2', 'If the request specifies a bucket using the HTTP Host header (virtual hosted-style), append\\nthe bucket name preceded by a \"/\" (e.g., \"/bucketname\"). For path-style requests and\\nrequests that don\\'t address a bucket, do nothing. For more information about virtual\\nhosted-style requests, see Virtual hosting of buckets.\\nFor a virtual hosted-style request \"https://awsexamplebucket1.s3.us-west-1.amazo\\nnaws.com/photos/puppy.jpg\", the CanonicalizedResource is \"/awsexamplebucket1\".\\nFor the path-style request, \"https://s3.us-west-1.amazonaws.com/awsexamplebucket1/\\nphotos/puppy.jpg\", the CanonicalizedResource is \"\".'], ['3', 'Append the path part of the un-decoded HTTP Request-URI, up-to but not including the\\nquery string.\\nFor a virtual hosted-style request \"https://awsexamplebucket1.s3.us-west-1.amazo\\nnaws.com/photos/puppy.jpg\", the CanonicalizedResource is \"/awsexamplebucket1/\\nphotos/puppy.jpg\".']]]\n",
      "[[['', 'For a path-style request, \"https://s3.us-west-1.amazonaws.com/awsexamplebucket1/\\nphotos/puppy.jpg\", the CanonicalizedResource is \"/awsexamplebucket1/photos/\\npuppy.jpg\". At this point, the CanonicalizedResource is the same for both the virtual\\nhosted-style and path-style request.\\nFor a request that does not address a bucket, such as GET Service, append \"/\".'], ['4', \"If the request addresses a subresource, such as ?versioning , ?location , ?acl, ?\\nlifecycle , or ?versionid , append the subresource, its value if it has one, and the\\nquestion mark. Note that in case of multiple subresources, subresources must be lexicogra\\nphically sorted by subresource name and separated by '&', e.g., ?acl&versionId=value.\\nThe subresources that must be included when constructing the CanonicalizedResource\\nElement are acl, lifecycle, location, logging, notification, partNumber, policy, requestPa\\nyment, uploadId, uploads, versionId, versioning, versions, and website.\\nIf the request specifies query string parameters overriding the response header values\\n(see Get Object), append the query string parameters and their values. When signing,\\nyou do not encode these values; however, when making the request, you must encode\\nthese parameter values. The query string parameters in a GET request include response-\\ncontent-type , response-content-language , response-expires ,\\nresponse-cache-control , response-content-disposition , and response-\\ncontent-encoding .\\nThe delete query string parameter must be included when you create the Canonical\\nizedResource for a multi-object Delete request.\"]]]\n",
      "[[['1', \"Convert each HTTP header name to lowercase. For example, 'X-Amz-Date ' becomes 'x-\\namz-date '.\"], ['2', 'Sort the collection of headers lexicographically by header name.'], ['3', 'Combine header fields with the same name into one \"header-name:comma-separate\\nd-value-list\" pair as prescribed by RFC 2616, section 4.2, without any spaces between\\nvalues. For example, the two metadata headers \\'x-amz-meta-username: fred \\' and\\n\\'x-amz-meta-username: barney \\' would be combined into the single header \\'x-\\namz-meta-username: fred,barney \\'.'], ['4', '\"Unfold\" long headers that span multiple lines (as allowed by RFC 2616, section 4.2) by\\nreplacing the folding spaces (including new-line) by a single space.'], ['5', \"Trim any spaces around the colon in the header. For example, the header 'x-amz-meta-\\nusername: fred,barney ' would become 'x-amz-meta-username:fred,ba\\nrney '\"], ['6', 'Finally, append a newline character (U+000A) to each canonicalized header in the\\nresulting list. Construct the CanonicalizedResource element by concatenating all headers\\nin this list into a single string.']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe validation constraint on request date applies only to authenticated requests that\\ndo not use query string authentication. For more information, see Query string request\\nauthentication alternative.', ''], ['', '', '']], [['Parameter', 'Value'], ['AWSAccessKeyId', 'AKIAIOSFODNN7EXAMPLE'], ['AWSSecretAccessKey', 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY']]]\n",
      "[[['Request', 'StringToSign'], ['GET /photos/puppy.jpg HTTP/1.1\\nHost: awsexamplebucket1.us-\\nwest-1.s3.amazonaws.com\\nDate: Tue, 27 Mar 2007 19:36:42\\n+0000\\nAuthorization: AWS AKIAIOSFO\\nDNN7EXAMPLE:\\nqgk2+6Sv9/oM7G3qLEjTH1a1l1g=', 'GET\\\\n\\n\\\\n\\n\\\\n\\nTue, 27 Mar 2007 19:36:42 +0000\\\\n\\n/awsexamplebucket1/photos/puppy.jpg']], [['', '', ''], ['', 'GET /photos/puppy.jpg HTTP/1.1\\nHost: awsexamplebucket1.us-\\nwest-1.s3.amazonaws.com\\nDate: Tue, 27 Mar 2007 19:36:42\\n+0000\\nAuthorization: AWS AKIAIOSFO\\nDNN7EXAMPLE:\\nqgk2+6Sv9/oM7G3qLEjTH1a1l1g=', ''], ['', '', '']], [['', '', ''], ['', 'GET\\\\n\\n\\\\n\\n\\\\n\\nTue, 27 Mar 2007 19:36:42 +0000\\\\n\\n/awsexamplebucket1/photos/puppy.jpg', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe following Python script calculates the preceding signature, using the provided\\nparameters. You can use this script to construct your own signatures, replacing the keys and\\nStringToSign as appropriate.\\nimport base64\\nimport hmac\\nfrom hashlib import sha1\\naccess_key = \\'AKIAIOSFODNN7EXAMPLE\\'.encode(\"UTF-8\")\\nsecret_key = \\'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\\'.encode(\"UTF-8\")\\nstring_to_sign = \\'GET\\\\n\\\\n\\\\nTue, 27 Mar 2007 19:36:42 +0000\\\\n/awsexamplebucket1/\\nphotos/puppy.jpg\\'.encode(\"UTF-8\")\\nsignature = base64.b64encode(', '']], [['', '', ''], ['', 'import base64\\nimport hmac\\nfrom hashlib import sha1\\naccess_key = \\'AKIAIOSFODNN7EXAMPLE\\'.encode(\"UTF-8\")\\nsecret_key = \\'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\\'.encode(\"UTF-8\")\\nstring_to_sign = \\'GET\\\\n\\\\n\\\\nTue, 27 Mar 2007 19:36:42 +0000\\\\n/awsexamplebucket1/\\nphotos/puppy.jpg\\'.encode(\"UTF-8\")\\nsignature = base64.b64encode(', '']]]\n",
      "[[['', 'hmac.new(\\nsecret_key, string_to_sign, sha1\\n).digest()\\n).strip()\\nprint(f\"AWS {access_key.decode()}:{signature.decode()}\")', ''], ['', '', '']], [['', 'hmac.new(\\nsecret_key, string_to_sign, sha1\\n).digest()\\n).strip()\\nprint(f\"AWS {access_key.decode()}:{signature.decode()}\")', ''], ['', '', '']], [['Request', 'StringToSign'], ['PUT /photos/puppy.jpg HTTP/1.1\\nContent-Type: image/jpeg\\nContent-Length: 94328\\nHost: awsexamplebucket1.s3.us-wes\\nt-1.amazonaws.com\\nDate: Tue, 27 Mar 2007 21:15:45 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMP\\nLE:\\niqRzw+ileNPu1fhspnRs8nOjjIA=', 'PUT\\\\n\\n\\\\n\\nimage/jpeg\\\\n\\nTue, 27 Mar 2007 21:15:45 +0000\\\\n\\n/awsexamplebucket1/photos/puppy.jpg']], [['', '', ''], ['', 'PUT /photos/puppy.jpg HTTP/1.1\\nContent-Type: image/jpeg\\nContent-Length: 94328\\nHost: awsexamplebucket1.s3.us-wes\\nt-1.amazonaws.com\\nDate: Tue, 27 Mar 2007 21:15:45 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMP\\nLE:\\niqRzw+ileNPu1fhspnRs8nOjjIA=', ''], ['', '', '']], [['', '', ''], ['', 'PUT\\\\n\\n\\\\n\\nimage/jpeg\\\\n\\nTue, 27 Mar 2007 21:15:45 +0000\\\\n\\n/awsexamplebucket1/photos/puppy.jpg', ''], ['', '', '']], [['Request', 'StringToSign'], ['GET /?prefix=photos&max-keys=50&marker=puppy\\nHTTP/1.1\\nUser-Agent: Mozilla/5.0', 'GET\\\\n\\n\\\\n\\n\\\\n']], [['', '', ''], ['', 'GET /?prefix=photos&max-keys=50&marker=puppy\\nHTTP/1.1\\nUser-Agent: Mozilla/5.0', '']], [['', '', ''], ['', 'GET\\\\n\\n\\\\n\\n\\\\n', '']]]\n",
      "[[['Request', 'StringToSign'], ['Host: awsexamplebucket1.s3.us-west-1.amazo\\nnaws.com\\nDate: Tue, 27 Mar 2007 19:42:41 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:\\nm0WP8eCtspQl5Ahe6L1SozdX9YA=', 'Tue, 27 Mar 2007 19:42:41\\n+0000\\\\n\\n/awsexamplebucket1/']], [['', 'Host: awsexamplebucket1.s3.us-west-1.amazo\\nnaws.com\\nDate: Tue, 27 Mar 2007 19:42:41 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:\\nm0WP8eCtspQl5Ahe6L1SozdX9YA=', ''], ['', '', '']], [['', 'Tue, 27 Mar 2007 19:42:41\\n+0000\\\\n\\n/awsexamplebucket1/', ''], ['', '', '']], [['Request', 'StringToSign'], ['GET /?acl HTTP/1.1\\nHost: awsexamplebucket1.s3.us-west-1.amazo\\nnaws.com\\nDate: Tue, 27 Mar 2007 19:44:46 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:\\n82ZHiFIjc+WbcwFKGUVEQspPn+0=', 'GET\\\\n\\n\\\\n\\n\\\\n\\nTue, 27 Mar 2007 19:44:46\\n+0000\\\\n\\n/awsexamplebucket1/?acl']], [['', '', ''], ['', 'GET /?acl HTTP/1.1\\nHost: awsexamplebucket1.s3.us-west-1.amazo\\nnaws.com\\nDate: Tue, 27 Mar 2007 19:44:46 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:\\n82ZHiFIjc+WbcwFKGUVEQspPn+0=', ''], ['', '', '']], [['', '', ''], ['', 'GET\\\\n\\n\\\\n\\n\\\\n\\nTue, 27 Mar 2007 19:44:46\\n+0000\\\\n\\n/awsexamplebucket1/?acl', ''], ['', '', '']], [['Request', 'StringToSign'], ['DELETE /awsexamplebucket1/photos/p\\nuppy.jpg HTTP/1.1\\nUser-Agent: dotnet\\nHost: s3.us-west-1.amazonaws.com\\nDate: Tue, 27 Mar 2007 21:20:27 +0000', 'DELETE\\\\n\\n\\\\n\\n\\\\n\\nTue, 27 Mar 2007 21:20:26 +0000\\\\n']], [['', '', ''], ['', 'DELETE /awsexamplebucket1/photos/p\\nuppy.jpg HTTP/1.1\\nUser-Agent: dotnet\\nHost: s3.us-west-1.amazonaws.com\\nDate: Tue, 27 Mar 2007 21:20:27 +0000', '']], [['', '', ''], ['', 'DELETE\\\\n\\n\\\\n\\n\\\\n\\nTue, 27 Mar 2007 21:20:26 +0000\\\\n', '']]]\n",
      "[[['Request', 'StringToSign'], ['x-amz-date: Tue, 27 Mar 2007 21:20:26\\n+0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMP\\nLE:XbyTlbQdu9Xw5o8P4iMwPktxQd8=', '/awsexamplebucket1/photos/puppy.jpg']], [['', 'x-amz-date: Tue, 27 Mar 2007 21:20:26\\n+0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMP\\nLE:XbyTlbQdu9Xw5o8P4iMwPktxQd8=', ''], ['', '', '']], [['/awsexamplebucket1/photos/puppy.jpg', ''], ['', '']], [['Request', 'StringToSign'], ['PUT /db-backup.dat.gz HTTP/1.1\\nUser-Agent: curl/7.15.5\\nHost: static.example.com:8080\\nDate: Tue, 27 Mar 2007 21:06:08 +0000\\nx-amz-acl: public-read\\ncontent-type: application/x-download\\nContent-MD5: 4gJE4saaMU4BqNR0kLY+lw==\\nX-Amz-Meta-ReviewedBy: joe@example.com\\nX-Amz-Meta-ReviewedBy: jane@exam\\nple.com\\nX-Amz-Meta-FileChecksum: 0x02661779\\nX-Amz-Meta-ChecksumAlgorithm: crc32\\nContent-Disposition: attachment;\\nfilename=database.dat\\nContent-Encoding: gzip\\nContent-Length: 5913339\\nAuthorization: AWS AKIAIOSFODNN7EXAMP\\nLE:\\njtBQa0Aq+DkULFI8qrpwIjGEx0E=', 'PUT\\\\n\\n4gJE4saaMU4BqNR0kLY+lw==\\\\n\\napplication/x-download\\\\n\\nTue, 27 Mar 2007 21:06:08 +0000\\\\n\\nx-amz-acl:public-read\\\\n\\nx-amz-meta-checksumalgorithm:c\\nrc32\\\\n\\nx-amz-meta-filechecksum:0x026\\n61779\\\\n\\nx-amz-meta-reviewedby:\\njoe@example.com,jane@example.com\\n\\\\n\\n/static.example.com/db-backup.dat\\n.gz']], [['', ''], ['', 'PUT /db-backup.dat.gz HTTP/1.1\\nUser-Agent: curl/7.15.5\\nHost: static.example.com:8080\\nDate: Tue, 27 Mar 2007 21:06:08 +0000\\nx-amz-acl: public-read\\ncontent-type: application/x-download\\nContent-MD5: 4gJE4saaMU4BqNR0kLY+lw==\\nX-Amz-Meta-ReviewedBy: joe@example.com\\nX-Amz-Meta-ReviewedBy: jane@exam\\nple.com\\nX-Amz-Meta-FileChecksum: 0x02661779\\nX-Amz-Meta-ChecksumAlgorithm: crc32\\nContent-Disposition: attachment;\\nfilename=database.dat\\nContent-Encoding: gzip\\nContent-Length: 5913339\\nAuthorization: AWS AKIAIOSFODNN7EXAMP\\nLE:\\njtBQa0Aq+DkULFI8qrpwIjGEx0E='], ['', '']], [['', '', ''], ['', 'PUT\\\\n\\n4gJE4saaMU4BqNR0kLY+lw==\\\\n\\napplication/x-download\\\\n\\nTue, 27 Mar 2007 21:06:08 +0000\\\\n\\nx-amz-acl:public-read\\\\n\\nx-amz-meta-checksumalgorithm:c\\nrc32\\\\n\\nx-amz-meta-filechecksum:0x026\\n61779\\\\n\\nx-amz-meta-reviewedby:\\njoe@example.com,jane@example.com\\n\\\\n\\n/static.example.com/db-backup.dat\\n.gz', ''], ['', '', '']]]\n",
      "[[['Request', 'StringToSign'], ['GET / HTTP/1.1\\nHost: s3.us-west-1.amazonaws.com\\nDate: Wed, 28 Mar 2007 01:29:59 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:qGdzdE\\nRIC03wnaRNKh6OqZehG9s=', 'GET\\\\n\\n\\\\n\\n\\\\n\\nWed, 28 Mar 2007 01:29:59\\n+0000\\\\n\\n/']], [['', '', ''], ['', 'GET / HTTP/1.1\\nHost: s3.us-west-1.amazonaws.com\\nDate: Wed, 28 Mar 2007 01:29:59 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMPLE:qGdzdE\\nRIC03wnaRNKh6OqZehG9s=', ''], ['', '', '']], [['', '', ''], ['', 'GET\\\\n\\n\\\\n\\n\\\\n\\nWed, 28 Mar 2007 01:29:59\\n+0000\\\\n\\n/', ''], ['', '', '']], [['Request', 'StringToSign'], ['GET /dictionary/fran%C3%A7ais/pr\\n%c3%a9f%c3%a8re HTTP/1.1\\nHost: s3.us-west-1.amazonaws.com\\nDate: Wed, 28 Mar 2007 01:49:49 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMP\\nLE:DNEZGsoieTZ92F3bUfSPQcbGmlM=', 'GET\\\\n\\n\\\\n\\n\\\\n\\nWed, 28 Mar 2007 01:49:49 +0000\\\\n\\n/dictionary/fran%C3%A7ais/pr\\n%c3%a9f%c3%a8re']], [['', '', ''], ['', 'GET /dictionary/fran%C3%A7ais/pr\\n%c3%a9f%c3%a8re HTTP/1.1\\nHost: s3.us-west-1.amazonaws.com\\nDate: Wed, 28 Mar 2007 01:49:49 +0000\\nAuthorization: AWS AKIAIOSFODNN7EXAMP\\nLE:DNEZGsoieTZ92F3bUfSPQcbGmlM=', ''], ['', '', '']], [['', '', ''], ['', 'GET\\\\n\\n\\\\n\\n\\\\n\\nWed, 28 Mar 2007 01:49:49 +0000\\\\n\\n/dictionary/fran%C3%A7ais/pr\\n%c3%a9f%c3%a8re', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe elements in StringToSign that were derived from the Request-URI are taken literally,\\nincluding URL-Encoding and capitalization.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'GET /photos/puppy.jpg\\n?AWSAccessKeyId=AKIAIOSFODNN7EXAMPLE&Expires=1141889120&Signature=vjbyPxybdZaNmGa\\n%2ByT272YEAiv4%3D HTTP/1.1\\nHost: awsexamplebucket1.s3.us-west-1.amazonaws.com\\nDate: Mon, 26 Mar 2007 19:37:58 +0000', ''], ['', '', '']]]\n",
      "[[['Query string\\nparameter\\nname', 'Example value', 'Description'], ['AWSAccess\\nKeyId', 'AKIAIOSFODNN7EXAMPLE', 'Your AWS access key ID. Specifies\\nthe AWS secret access key used to\\nsign the request and, indirectly, the\\nidentity of the developer making the\\nrequest.'], ['Expires', '1141889120', 'The time when the signature expires,\\nspecified as the number of seconds\\nsince the epoch (00:00:00 UTC on\\nJanuary 1, 1970). A request received\\nafter this time (according to the\\nserver) will be rejected.'], ['Signature', 'vjbyPxybdZaNmGa%2B\\nyT272YEAiv4%3D', 'The URL encoding of the Base64\\nencoding of the HMAC-SHA1 of\\nStringToSign.']], [['', '', ''], ['', 'Signature = URL-Encode( Base64( HMAC-SHA1( YourSecretAccessKey, UTF-8-Encoding-\\nOf( StringToSign ) ) ) );\\nStringToSign = HTTP-VERB + \"\\\\n\" +\\nContent-MD5 + \"\\\\n\" +\\nContent-Type + \"\\\\n\" +\\nExpires + \"\\\\n\" +\\nCanonicalizedAmzHeaders +\\nCanonicalizedResource;', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIn the query string authentication method, you do not use the Date or the x-amz-date\\nrequest header when calculating the string to sign.', ''], ['', '', '']], [['Request', 'StringToSign'], ['GET /photos/puppy.jpg?AWSAccess\\nKeyId=AKIAIOSFODNN7EXAMPLE&\\nSignature=NpgCjnDzrM%2BWFzo\\nENXmpNDUsSn8%3D&\\nExpires=1175139620 HTTP/1.1\\nHost: awsexamplebucket1.s3.us-wes\\nt-1.amazonaws.com', 'GET\\\\n\\n\\\\n\\n\\\\n\\n1175139620\\\\n\\n/awsexamplebucket1/photos/puppy.jpg']], [['', '', ''], ['', 'GET /photos/puppy.jpg?AWSAccess\\nKeyId=AKIAIOSFODNN7EXAMPLE&\\nSignature=NpgCjnDzrM%2BWFzo\\nENXmpNDUsSn8%3D&\\nExpires=1175139620 HTTP/1.1\\nHost: awsexamplebucket1.s3.us-wes\\nt-1.amazonaws.com', ''], ['', '', '']], [['', '', ''], ['', 'GET\\\\n\\n\\\\n\\n\\\\n\\n1175139620\\\\n\\n/awsexamplebucket1/photos/puppy.jpg', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe request authentication discussed in this section is based on AWS Signature Version 2, a\\nprotocol for authenticating inbound API requests to AWS services.\\nAmazon S3 now supports Signature Version 4, a protocol for authenticating inbound API\\nrequests to AWS services, in all AWS Regions. At this time, AWS Regions created before\\nJanuary 30, 2014 will continue to support the previous protocol, Signature Version 2. Any\\nnew regions after January 30, 2014 will support only Signature Version 4 and therefore all\\nrequests to those regions must be made with Signature Version 4. For more information,\\nsee Authenticating Requests in Browser-Based Uploads Using POST (AWS Signature Version\\n4) in the Amazon Simple Storage Service API Reference.', ''], ['', '', '']]]\n",
      "[[['1', 'The user opens a web browser and accesses your web page.'], ['2', 'Your web page contains an HTTP form that contains all the information necessary for\\nthe user to upload content to Amazon S3.'], ['3', 'The user uploads content directly to Amazon S3.']], [['', '', ''], ['', 'Note\\nQuery string authentication is not supported for POST.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nSOAP support over HTTP is deprecated, but SOAP is still available over HTTPS. New\\nAmazon S3 features are not supported for SOAP. Instead of using SOAP, we recommend\\nthat you use either the REST API or the AWS SDKs.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe form data and boundaries (excluding the contents of the file) cannot exceed 20 KB.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe HTML form declaration does not accept query string authentication parameters.', ''], ['', '', '']], [['', '', ''], ['', '<html>\\n<head>\\n...\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\\n...\\n</head>\\n<body>', ''], ['', '', '']], [['', '', ''], ['', 'Content-Type: text/html; charset=UTF-8', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe key name is specified in a form field.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<form action=\"https://awsexamplebucket1.s3.us-west-1.amazonaws.com/\" method=\"post\"\\nenctype=\"multipart/form-data\">', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe variable ${filename} is automatically replaced with the name of the file provided\\nby the user and is recognized by all form fields. If the browser or client provides a full or\\npartial path to the file, only the text following the last slash (/) or backslash (\\\\) will be used.\\nFor example, \"C:\\\\Program Files\\\\directory1\\\\file.txt\" will be interpreted as \"file.txt\". If no file\\nor file name is provided, the variable is replaced with an empty string.', ''], ['', '', '']], [['Field name', 'Description', 'Required'], ['AWSAccessKeyId', 'The AWS Access Key ID of the owner of the\\nbucket who grants an anonymous user\\naccess for a request that satisfies the set of\\nconstraints in the policy. This field is required\\nif the request includes a policy document.', 'Conditional'], ['acl', 'An Amazon S3 access control list (ACL). If\\nan invalid access control list is specified, an\\nerror is generated. For more information on\\nACLs, see Access control lists (ACLs).', 'No']]]\n",
      "[[['Field name', 'Description', 'Required'], ['', 'Type: String\\nDefault: private\\nValid Values: private | public-re\\nad | public-read-write | aws-\\nexec-read | authenticated-read\\n| bucket-owner-read | bucket-ow\\nner-full-control', ''], ['Cache-Control,\\nContent-Type, Content-\\nDisposition, Conten\\nt-Encoding, Expires', 'REST-specific headers. For more information,\\nsee PUT Object.', 'No'], ['key', 'The name of the uploaded key.\\nTo use the filename provided by the user, use\\nthe ${filename} variable. For example, if user\\nBetty uploads the file lolcatz.jpg and you\\nspecify /user/betty/${filename}, the file is\\nstored as /user/betty/lolcatz.jpg.\\nFor more information, see Working with\\nobject metadata.', 'Yes'], ['policy', 'Security policy describing what is permitted\\nin the request. Requests without a securit\\ny policy are considered anonymous and will\\nsucceed only on publicly writable buckets.', 'No']]]\n",
      "[[['Field name', 'Description', 'Required'], ['success_action_red\\nirect, redirect', 'The URL to which the client is redirected\\nupon successful upload. Amazon S3 appends\\nthe bucket, key, and etag values as query\\nstring parameters to the URL.\\nIf success_action_redirect is not specified\\n, Amazon S3 returns the empty document\\ntype specified in the success_action_status\\nfield.\\nIf Amazon S3 cannot interpret the URL, it\\nignores the field.\\nIf the upload fails, Amazon S3 displays an\\nerror and does not redirect the user to a URL.\\nFor more information, see Redirection.\\nNote\\nThe redirect field name is deprecate\\nd and support for the redirect field\\nname will be removed in the future.', 'No']], [['', '', ''], ['', 'Note\\nThe redirect field name is deprecate\\nd and support for the redirect field\\nname will be removed in the future.', ''], ['', '', '']]]\n",
      "[[['Field name', 'Description', 'Required'], ['success_action_status', 'The status code returned to the client upon\\nsuccessful upload if success_action_redirect\\nis not specified.\\nValid values are 200, 201, or 204 (default).\\nIf the value is set to 200 or 204, Amazon S3\\nreturns an empty document with a 200 or\\n204 status code.\\nIf the value is set to 201, Amazon S3 returns\\nan XML document with a 201 status code.\\nFor information about the content of the\\nXML document, see POST Object.\\nIf the value is not set or if it is set to an\\ninvalid value, Amazon S3 returns an empty\\ndocument with a 204 status code.\\nNote\\nSome versions of the Adobe Flash\\nplayer do not properly handle\\nHTTP responses with an empty\\nbody. To support uploads through\\nAdobe Flash, we recommend setting\\nsuccess_action_status to\\n201.', 'No']], [['', '', ''], ['', 'Note\\nSome versions of the Adobe Flash\\nplayer do not properly handle\\nHTTP responses with an empty\\nbody. To support uploads through\\nAdobe Flash, we recommend setting\\nsuccess_action_status to\\n201.', ''], ['', '', '']]]\n",
      "[[['Field name', 'Description', 'Required'], ['signature', 'The HMAC signature constructed by using\\nthe secret access key that corresponds to\\nthe provided AWSAccessKeyId. This field is\\nrequired if a policy document is included\\nwith the request.\\nFor more information, see Identity and Access\\nManagement for Amazon S3.', 'Conditional'], ['x-amz-security-token', 'A security token used by session credentials\\nIf the request is using Amazon DevPay then\\nit requires two x-amz-security-token\\nform fields: one for the product token and\\none for the user token.\\nIf the request is using session credentials,\\nthen it requires one x-amz-security-\\ntoken form. For more information, see\\nTemporary Security Credentials in the IAM\\nUser Guide.', 'No'], ['Other field names prefixed\\nwith x-amz-meta-', 'User-specified metadata.\\nAmazon S3 does not validate or use this data.\\nFor more information, see PUT Object.', 'No']]]\n",
      "[[['Field name', 'Description', 'Required'], ['file', 'File or text content.\\nThe file or content must be the last field in\\nthe form. Any fields below it are ignored.\\nYou cannot upload more than one file at a\\ntime.', 'Yes']], [['', '', ''], ['', 'Note\\nAlthough the policy document is optional, we highly recommend it over making a bucket\\npublicly writable.', ''], ['', '', '']], [['', '', ''], ['', '{ \"expiration\": \"2007-12-01T12:00:00.000Z\",\\n\"conditions\": [\\n{\"acl\": \"public-read\" },', '']]]\n",
      "[[['', '{\"bucket\": \"awsexamplebucket1\" },\\n[\"starts-with\", \"$key\", \"user/eric/\"],\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you have multiple fields with the same name, the values must be separated by commas.\\nFor example, if you have two fields named \"x-amz-meta-tag\" and the first one has a value\\nof \"Ninja\" and second has a value of \"Stallman\", you would set the policy document to\\nNinja,Stallman.\\nAll variables within the form are expanded before the policy is validated. Therefore, all\\ncondition matching should be performed against the expanded fields. For example, if\\nyou set the key field to user/betty/${filename}, your policy might be [ \"starts-\\nwith\", \"$key\", \"user/betty/\" ]. Do not enter [ \"starts-with\", \"$key\",\\n\"user/betty/${filename}\" ]. For more information, see Condition matching.', ''], ['', '', '']]]\n",
      "[[['Element name', 'Description'], ['acl', 'Specifies conditions that the ACL must meet.\\nSupports exact matching and starts-with .'], ['content-length-range', 'Specifies the minimum and maximum allowable size for the\\nuploaded content.\\nSupports range matching.'], ['Cache-Control, Content-Type,\\nContent-Disposition, Content-\\nEncoding, Expires', 'REST-specific headers.\\nSupports exact matching and starts-with .'], ['key', 'The name of the uploaded key.\\nSupports exact matching and starts-with .'], ['success_action_redirect, redirect', 'The URL to which the client is redirected upon successful\\nupload.\\nSupports exact matching and starts-with .'], ['success_action_status', 'The status code returned to the client upon successful\\nupload if success_action_redirect is not specified.\\nSupports exact matching.'], ['x-amz-security-token', 'Amazon DevPay security token.\\nEach request that uses Amazon DevPay requires two\\nx-amz-security-token form fields: one for the\\nproduct token and one for the user token. As a result,\\nthe values must be separated by commas. For example,\\nif the user token is eW91dHViZQ== and the product']]]\n",
      "[[['Element name', 'Description'], ['', 'token is b0hnNVNKWVJIQTA= , you set the policy entry\\nto: { \"x-amz-security-token\": \"eW91dHViZ\\nQ==,b0hnNVNKWVJIQTA=\" } .'], ['Other field names prefixed with\\nx-amz-meta-', 'User-specified metadata.\\nSupports exact matching and starts-with .']], [['', '', ''], ['', \"Note\\nIf your toolkit adds additional fields (e.g., Flash adds filename), you must add them to the\\npolicy document. If you can control this functionality, prefix x-ignore- to the field so\\nAmazon S3 ignores the feature and it won't affect future versions of this feature.\", ''], ['', '', '']], [['Condition', 'Description'], ['Exact Matches', 'Exact matches verify that fields match specific values. This example indicates\\nthat the ACL must be set to public-read:\\n{\"acl\": \"public-read\" }\\nThis example is an alternate way to indicate that the ACL must be set to\\npublic-read:\\n[ \"eq\", \"$acl\", \"public-read\" ]'], ['', '']], [['', '', ''], ['', '{\"acl\": \"public-read\" }', ''], ['', '', '']], [['', '', ''], ['', '[ \"eq\", \"$acl\", \"public-read\" ]', ''], ['', '', '']]]\n",
      "[[['Condition', 'Description'], ['Starts With', 'If the value must start with a certain value, use starts-with. This example\\nindicates that the key must start with user/betty:\\n[\"starts-with\", \"$key\", \"user/betty/\"]'], ['Matching Any\\nContent', 'To configure the policy to allow any content within a field, use starts-with\\nwith an empty value. This example allows any success_action_redirect:\\n[\"starts-with\", \"$success_action_redirect\", \"\"]'], ['Specifying\\nRanges', 'For fields that accept ranges, separate the upper and lower ranges with a\\ncomma. This example allows a file size from 1 to 10 megabytes:\\n[\"content-length-range\", 1048579, 10485760]']], [['', '', ''], ['', '[\"starts-with\", \"$key\", \"user/betty/\"]', ''], ['', '', '']], [['', '', ''], ['', '[\"starts-with\", \"$success_action_redirect\", \"\"]', ''], ['', '', '']], [['', '', ''], ['', '[\"content-length-range\", 1048579, 10485760]', ''], ['', '', '']], [['Escape\\nsequence', 'Description'], ['\\\\\\\\', 'Backslash'], ['\\\\$', 'Dollar sign'], ['\\\\b', 'Backspace'], ['\\\\f', 'Form feed'], ['', '']]]\n",
      "[[['Escape\\nsequence', 'Description'], ['\\\\n', 'New line'], ['\\\\r', 'Carriage return'], ['\\\\t', 'Horizontal tab'], ['\\\\v', 'Vertical tab'], ['\\\\uxxxx', 'All Unicode characters']], [['Step', 'Description'], ['1', 'Encode the policy by using UTF-8.'], ['2', 'Encode those UTF-8 bytes by using Base64.'], ['3', 'Sign the policy with your secret access key by using HMAC SHA-1.'], ['4', 'Encode the SHA-1 signature by using Base64.']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe request authentication discussed in this section is based on AWS Signature Version 2, a\\nprotocol for authenticating inbound API requests to AWS services.\\nAmazon S3 now supports Signature Version 4, a protocol for authenticating inbound API\\nrequests to AWS services, in all AWS Regions. At this time, AWS Regions created before\\nJanuary 30, 2014 will continue to support the previous protocol, Signature Version 2. Any\\nnew regions after January 30, 2014 will support only Signature Version 4 and therefore all\\nrequests to those regions must be made with Signature Version 4. For more information,\\nsee Examples: Browser-Based Upload using HTTP POST (Using AWS Signature Version 4) in\\nthe Amazon Simple Storage Service API Reference.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{ \"expiration\": \"2007-12-01T12:00:00.000Z\",\\n\"conditions\": [\\n{\"bucket\": \"awsexamplebucket1\"},\\n[\"starts-with\", \"$key\", \"user/eric/\"],\\n{\"acl\": \"public-read\"},\\n{\"success_action_redirect\": \"https://awsexamplebucket1.s3.us-west-1.amazonaws.com/\\nsuccessful_upload.html\"},\\n[\"starts-with\", \"$Content-Type\", \"image/\"],\\n{\"x-amz-meta-uuid\": \"14365123651274\"},\\n[\"starts-with\", \"$x-amz-meta-tag\", \"\"]\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'eyAiZXhwaXJhdGlvbiI6ICIyMDA3LTEyLTAxVDEyOjAwOjAwLjAwMFoiLAogICJjb25kaXRpb25zIjogWwogICAg', 'e'], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<html>\\n<head>\\n...\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\\n...\\n</head>\\n<body>\\n...\\n<form action=\"https://DOC-EXAMPLE-BUCKET.s3.us-west-1.amazonaws.com/\" method=\"post\"\\nenctype=\"multipart/form-data\">\\nKey to upload: <input type=\"input\" name=\"key\" value=\"user/eric/\" /><br />\\n<input type=\"hidden\" name=\"acl\" value=\"public-read\" />\\n<input type=\"hidden\" name=\"success_action_redirect\" value=\"https://\\nawsexamplebucket1.s3.us-west-1.amazonaws.com/successful_upload.html\" />\\nContent-Type: <input type=\"input\" name=\"Content-Type\" value=\"image/jpeg\" /><br />\\n<input type=\"hidden\" name=\"x-amz-meta-uuid\" value=\"14365123651274\" />\\nTags for File: <input type=\"input\" name=\"x-amz-meta-tag\" value=\"\" /><br />\\n<input type=\"hidden\" name=\"AWSAccessKeyId\" value=\"AKIAIOSFODNN7EXAMPLE\" />\\n<input type=\"hidden\" name=\"Policy\" value=\"POLICY\" />\\n<input type=\"hidden\" name=\"Signature\" value=\"SIGNATURE\" />\\nFile: <input type=\"file\" name=\"file\" /> <br />\\n<!-- The elements after this will be ignored -->\\n<input type=\"submit\" name=\"submit\" value=\"Upload to Amazon S3\" />\\n</form>\\n...\\n</html>', ''], ['', '', '']], [['', '', ''], ['', 'POST / HTTP/1.1\\nHost: awsexamplebucket1.s3.us-west-1.amazonaws.com\\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.10) Gecko/20071115\\nFirefox/2.0.0.10\\nAccept: text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/\\nplain;q=0.8,image/png,*/*;q=0.5', '']]]\n",
      "[[['', 'Accept-Language: en-us,en;q=0.5\\nAccept-Encoding: gzip,deflate\\nAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7\\nKeep-Alive: 300\\nConnection: keep-alive\\nContent-Type: multipart/form-data; boundary=9431149156168\\nContent-Length: 118698\\n--9431149156168\\nContent-Disposition: form-data; name=\"key\"\\nuser/eric/MyPicture.jpg\\n--9431149156168\\nContent-Disposition: form-data; name=\"acl\"\\npublic-read\\n--9431149156168\\nContent-Disposition: form-data; name=\"success_action_redirect\"\\nhttps://awsexamplebucket1.s3.us-west-1.amazonaws.com/successful_upload.html\\n--9431149156168\\nContent-Disposition: form-data; name=\"Content-Type\"\\nimage/jpeg\\n--9431149156168\\nContent-Disposition: form-data; name=\"x-amz-meta-uuid\"\\n14365123651274\\n--9431149156168\\nContent-Disposition: form-data; name=\"x-amz-meta-tag\"\\nSome,Tag,For,Picture\\n--9431149156168\\nContent-Disposition: form-data; name=\"AWSAccessKeyId\"\\nAKIAIOSFODNN7EXAMPLE\\n--9431149156168\\nContent-Disposition: form-data; name=\"Policy\"\\neyAiZXhwaXJhdGlvbiI6ICIyMDA3LTEyLTAxVDEyOjAwOjAwLjAwMFoiLAogICJjb25kaXRpb25zIjogWwogICAg\\n--9431149156168\\nContent-Disposition: form-data; name=\"Signature\"\\n0RavWzkygo6QX9caELEqKi9kDbU=', 'e']]]\n",
      "[[['', '--9431149156168\\nContent-Disposition: form-data; name=\"file\"; filename=\"MyFilename.jpg\"\\nContent-Type: image/jpeg\\n...file content...\\n--9431149156168\\nContent-Disposition: form-data; name=\"submit\"\\nUpload to Amazon S3\\n--9431149156168--', ''], ['', '', '']], [['', '', ''], ['', 'HTTP/1.1 303 Redirect\\nx-amz-request-id: 1AEE782442F35865\\nx-amz-id-2: cxzFLJRatFHy+NGtaDFRR8YvI9BHmgLxjvJzNiGGICARZ/mVXHj7T+qQKhdpzHFh\\nContent-Type: application/xml\\nDate: Wed, 14 Nov 2007 21:21:33 GMT\\nConnection: close\\nLocation: https://awsexamplebucket1.s3.us-west-1.amazonaws.com/\\nsuccessful_upload.html?bucket=awsexamplebucket1&key=user/eric/\\nMyPicture.jpg&etag=&quot;39d459dfbc0faabbb5e179358dfb94c3&quot;\\nServer: AmazonS3', ''], ['', '', '']], [['', '', ''], ['', '{ \"expiration\": \"2007-12-01T12:00:00.000Z\",\\n\"conditions\": [', '']]]\n",
      "[[['', '{\"bucket\": \"awsexamplebucket1\"},\\n[\"starts-with\", \"$key\", \"user/eric/\"],\\n{\"acl\": \"public-read\"},\\n{\"success_action_redirect\": \"https://awsexamplebucket1.s3.us-west-1.amazonaws.com/\\nnew_post.html\"},\\n[\"eq\", \"$Content-Type\", \"text/html\"],\\n{\"x-amz-meta-uuid\": \"14365123651274\"},\\n[\"starts-with\", \"$x-amz-meta-tag\", \"\"]\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'eyAiZXhwaXJhdGlvbiI6ICIyMDA3LTEyLTAxVDEyOjAwOjAwLjAwMFoiLAogICJjb25kaXR\\npb25zIjogWwogICAgeyJidWNrZXQiOiAiam9obnNtaXRoIn0sCiAgICBbInN0YXJ0cy13aXRoIiwgIiRrZXkiLCA\\nLAogICAgeyJhY2wiOiAicHVibGljLXJlYWQifSwKICAgIHsic3VjY2Vzc19hY3Rpb25fcmVkaXJlY3QiOiAiaHR0\\nC5zMy5hbWF6b25hd3MuY29tL25ld19wb3N0Lmh0bWwifSwKICAgIFsiZXEiLCAiJENvbnRlbnQtVHlwZSIsICJ0Z\\nCAgIHsieC1hbXotbWV0YS11dWlkIjogIjE0MzY1MTIzNjUxMjc0In0sCiAgICBbInN0YXJ0cy13aXRoIiwgIiR4L\\nIsICIiXQogIF0KfQo=', 'i\\nc\\nX\\nW'], ['', '', '']], [['', '', ''], ['', '<html>', '']]]\n",
      "[[['', '<head>\\n...\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\\n...\\n</head>\\n<body>\\n...\\n<form action=\"https://DOC-EXAMPLE-BUCKET.s3.us-west-1.amazonaws.com/\" method=\"post\"\\nenctype=\"multipart/form-data\">\\nKey to upload: <input type=\"input\" name=\"key\" value=\"user/eric/\" /><br />\\n<input type=\"hidden\" name=\"acl\" value=\"public-read\" />\\n<input type=\"hidden\" name=\"success_action_redirect\" value=\"https://\\nawsexamplebucket1.s3.us-west-1.amazonaws.com/new_post.html\" />\\n<input type=\"hidden\" name=\"Content-Type\" value=\"text/html\" />\\n<input type=\"hidden\" name=\"x-amz-meta-uuid\" value=\"14365123651274\" />\\nTags for File: <input type=\"input\" name=\"x-amz-meta-tag\" value=\"\" /><br />\\n<input type=\"hidden\" name=\"AWSAccessKeyId\" value=\"AKIAIOSFODNN7EXAMPLE\" />\\n<input type=\"hidden\" name=\"Policy\" value=\"POLICY\" />\\n<input type=\"hidden\" name=\"Signature\" value=\"SIGNATURE\" />\\nEntry: <textarea name=\"file\" cols=\"60\" rows=\"10\">\\nYour blog post goes here.\\n</textarea><br />\\n<!-- The elements after this will be ignored -->\\n<input type=\"submit\" name=\"submit\" value=\"Upload to Amazon S3\" />\\n</form>\\n...\\n</html>', ''], ['', '', '']], [['', '', ''], ['', 'POST / HTTP/1.1\\nHost: awsexamplebucket1.s3.us-west-1.amazonaws.com\\nUser-Agent: Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.10) Gecko/20071115\\nFirefox/2.0.0.10\\nAccept: text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/\\nplain;q=0.8,image/png,*/*;q=0.5\\nAccept-Language: en-us,en;q=0.5\\nAccept-Encoding: gzip,deflate\\nAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7\\nKeep-Alive: 300', '']]]\n",
      "[[['', 'Connection: keep-alive\\nContent-Type: multipart/form-data; boundary=178521717625888\\nContent-Length: 118635\\n-178521717625888\\nContent-Disposition: form-data; name=\"key\"\\nser/eric/NewEntry.html\\n--178521717625888\\nContent-Disposition: form-data; name=\"acl\"\\npublic-read\\n--178521717625888\\nContent-Disposition: form-data; name=\"success_action_redirect\"\\nhttps://awsexamplebucket1.s3.us-west-1.amazonaws.com/new_post.html\\n--178521717625888\\nContent-Disposition: form-data; name=\"Content-Type\"\\ntext/html\\n--178521717625888\\nContent-Disposition: form-data; name=\"x-amz-meta-uuid\"\\n14365123651274\\n--178521717625888\\nContent-Disposition: form-data; name=\"x-amz-meta-tag\"\\nInteresting Post\\n--178521717625888\\nContent-Disposition: form-data; name=\"AWSAccessKeyId\"\\nAKIAIOSFODNN7EXAMPLE\\n--178521717625888\\nContent-Disposition: form-data; name=\"Policy\"\\neyAiZXhwaXJhdGlvbiI6ICIyMDA3LTEyLTAxVDEyOjAwOjAwLjAwMFoiLAogICJjb25kaXRpb25zIjogWwogICAg\\n--178521717625888\\nContent-Disposition: form-data; name=\"Signature\"\\nqA7FWXKq6VvU68lI9KdveT1cWgF=\\n--178521717625888\\nContent-Disposition: form-data; name=\"file\"\\n...content goes here...', 'e']]]\n",
      "[[['', '--178521717625888\\nContent-Disposition: form-data; name=\"submit\"\\nUpload to Amazon S3\\n--178521717625888--', ''], ['', '', '']], [['', '', ''], ['', 'HTTP/1.1 303 Redirect\\nx-amz-request-id: 1AEE782442F35865\\nx-amz-id-2: cxzFLJRatFHy+NGtaDFRR8YvI9BHmgLxjvJzNiGGICARZ/mVXHj7T+qQKhdpzHFh\\nContent-Type: application/xml\\nDate: Wed, 14 Nov 2007 21:21:33 GMT\\nConnection: close\\nLocation: https://awsexamplebucket1.s3.us-west-1.amazonaws.com/new_post.html?\\nbucket=awsexamplebucket1&key=user/eric/\\nNewEntry.html&etag=40c3271af26b7f1672e41b8a274d28d4\\nServer: AmazonS3', ''], ['', '', '']], [['', '', ''], ['', '<?xml version=\"1.0\"?>\\n<!DOCTYPE cross-domain-policy SYSTEM\\n\"http://www.macromedia.com/xml/dtds/cross-domain-policy.dtd\">\\n<cross-domain-policy>\\n<allow-access-from domain=\"*\" secure=\"false\" />\\n</cross-domain-policy>', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor more information about the Adobe Flash security model, go to the Adobe website.', '']]]\n",
      "[[['', 'Adding the crossdomain.xml file to your bucket allows any Adobe Flash Player to connect\\nto the crossdomain.xml file within your bucket; however, it does not grant access to the\\nactual Amazon S3 bucket.', ''], ['', '', '']], [['', '', ''], ['', \"['starts-with', '$Filename', '']\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor more information about using the Amazon S3 Express One Zone storage class with\\ndirectory buckets, see What is S3 Express One Zone? and Directory buckets.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'arn:aws:s3-outposts:region:account-id:outpost/outpost-id/bucket/bucket-name', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'arn:aws:s3-outposts:region:account-id:outpost/outpost-id/accesspoint/accesspoint-name', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3-outposts:us-west-2:123456789012:outpost/ op-01ac5d28a6a232904/\\nbucket/example-s3-bucket1/object/myobject', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\n• Access control lists (ACLs) are not supported by S3 on Outposts.\\n• S3 on Outposts defaults to the bucket owner as object owner to help ensure that the\\nowner of a bucket can't be prevented from accessing or deleting objects.\\n• S3 on Outposts always has S3 Block Public Access enabled to help ensure that objects\\ncan never have public access.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nYou can't write a policy for an S3 on Outposts bucket that uses a wildcard character (*) in\\nthe Principal element unless the policy also includes a Condition that limits access to a\\nspecific IP address range. This restriction helps ensure that there is no public access to your\\nS3 on Outposts bucket. For an example, see Example policies for S3 on Outposts.\", ''], ['', '', '']]]\n",
      "[[['Amazon S3 on Outposts ARN', 'ARN format', 'Example'], ['Bucket ARN', 'arn:partition :s3-\\noutposts: region:\\naccount_id :outpost\\n/ outpost_id /\\nbucket/bucket_name', 'arn:aws:s3-outpo\\nsts: us-west-2\\n:123456789012 :\\noutpost/ op-01ac5d\\n28a6a232904 /\\nbucket/example-s3-\\nbucket1'], ['Access point ARN', 'arn:partition :s3-\\noutposts: region:\\naccount_id :outpost\\n/ outpost_id /accesspo\\nint/ accesspoint_name', 'arn:aws:s3-outpo\\nsts: us-west-2\\n:123456789012 :\\noutpost/ op-01ac5d\\n28a6a232904 /accesspo\\nint/ access-point-\\nname'], ['Object ARN', 'arn:partition :s3-\\noutposts: region:\\naccount_id :outpost\\n/ outpost_id /\\nbucket/bucket_name /\\nobject/object_key', 'arn:aws:s3-outpo\\nsts: us-west-2\\n:123456789012 :\\noutpost/ op-01ac5d\\n28a6a232904 /\\nbucket/example-s3-\\nbucket1 /object/m\\nyobject'], ['S3 on Outposts access point\\nobject ARN (used in policies)', 'arn:partition :s3-\\noutposts: region:\\naccount_id :outpost\\n/ outpost_id /accesspo\\nint/ accesspoi', 'arn:aws:s3-outpo\\nsts: us-west-2\\n:123456789012 :\\noutpost/ op-01ac5d\\n28a6a232904 /accesspo']]]\n",
      "[[['Amazon S3 on Outposts ARN', 'ARN format', 'Example'], ['', 'nt_name /\\nobject/object_key', 'int/ access-point-\\nname/object/myobject'], ['S3 on Outposts ARN', 'arn:partition :s3-\\noutposts: region:\\naccount_id :outpost\\n/ outpost_id', 'arn:aws:s3-outpo\\nsts: us-west-2\\n:123456789012 :\\noutpost/ op-01ac5d\\n28a6a232904']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Id\":\"ExampleBucketPolicy1\",\\n\"Statement\":[\\n{\\n\"Sid\":\"statement1\",\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"AWS\":\"123456789012\"\\n},\\n\"Action\":\"s3-outposts:*\",\\n\"Resource\":\"arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"ExampleBucketPolicy2\",\\n\"Statement\": [\\n{\\n\"Sid\": \"statement1\",\\n\"Effect\": \"Allow\",\\n\"Principal\": { \"AWS\" : \"*\" },\\n\"Action\":\"s3-outposts:*\",\\n\"Resource\":\"arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket\",\\n\"Condition\" : {\\n\"IpAddress\" : {\\n\"aws:SourceIp\": \"192.0.2.0/24\"\\n},\\n\"NotIpAddress\" : {\\n\"aws:SourceIp\": \"198.51.100.0/24\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\n• For endpoints that use the customer-owned IP address pool (CoIP pool) access type, you\\nalso must have permissions to work with IP addresses from your CoIP pool, as described\\nin the following table.', '']]]\n",
      "[[['', \"• For shared accounts that access S3 on Outposts by using AWS Resource Access Manager,\\nusers in these shared accounts can't create their own endpoints on a shared subnet. If\\na user in a shared account wants to manage their own endpoints, the shared account\\nmust create its own subnet on the Outpost. For more information, see the section called\\n“Sharing S3 on Outposts”.\", ''], ['', '', '']], [['Action', 'IAM permissions'], ['CreateEndpoint', 's3-outposts:CreateEndpoint\\nec2:CreateNetworkInterface\\nec2:DescribeNetworkInterfaces\\nec2:DescribeVpcs\\nec2:DescribeSecurityGroups\\nec2:DescribeSubnets\\nec2:CreateTags\\niam:CreateServiceLinkedRole\\nFor endpoints that are using the on-premises\\ncustomer-owned IP address pool (CoIP pool)\\naccess type, the following additional permissio\\nns are required:\\ns3-outposts:CreateEndpoint\\nec2:DescribeCoipPools\\nec2:GetCoipPoolUsage\\nec2:AllocateAddress\\nec2:AssociateAddress']]]\n",
      "[[['Action', 'IAM permissions'], ['', 'ec2:DescribeAddresses\\nec2:DescribeLocalGatewayRou\\nteTableVpcAssociations'], ['DeleteEndpoint', 's3-outposts:DeleteEndpoint\\nec2:DeleteNetworkInterface\\nec2:DescribeNetworkInterfaces\\nFor endpoints that are using the on-premises\\ncustomer-owned IP address pool (CoIP pool)\\naccess type, the following additional permissio\\nns are required:\\ns3-outposts:DeleteEndpoint\\nec2:DisassociateAddress\\nec2:DescribeAddresses\\nec2:ReleaseAddress'], ['ListEndpoints', 's3-outposts:ListEndpoints']], [['', '', ''], ['', 'Note\\nYou can use resource tags in an IAM policy to manage permissions.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nAvoid including sensitive information such as account numbers in the bucket name.\\nThe bucket name is visible in the URLs that point to the objects in the bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIt can take up to 5 minutes for your Outpost endpoint to be created and your bucket to\\nbe ready to use. To configure additional bucket settings, choose View details.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3control create-bucket --bucket example-outposts-bucket --outpost-\\nid op-01ac5d28a6a232904', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic String createBucket(String bucketName) {\\nCreateBucketRequest reqCreateBucket = new CreateBucketRequest()\\n.withBucket(bucketName)\\n.withOutpostId(OutpostId)\\n.withCreateBucketConfiguration(new CreateBucketConfiguration());\\nCreateBucketResult respCreateBucket =\\ns3ControlClient.createBucket(reqCreateBucket);\\nSystem.out.printf(\"CreateBucket Response: %s%n\", respCreateBucket.toString());\\nreturn respCreateBucket.getBucketArn();\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-access-point --account-id 123456789012\\n--name example-outposts-access-point --bucket \"arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket\" --vpc-configuration VpcId=example-vpc-12345', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic String createAccessPoint(String bucketArn, String accessPointName) {\\nCreateAccessPointRequest reqCreateAP = new CreateAccessPointRequest()\\n.withAccountId(AccountId)\\n.withBucket(bucketArn)\\n.withName(accessPointName)\\n.withVpcConfiguration(new VpcConfiguration().withVpcId(\"vpc-12345\"));\\nCreateAccessPointResult respCreateAP =\\ns3ControlClient.createAccessPoint(reqCreateAP);\\nSystem.out.printf(\"CreateAccessPoint Response: %s%n\", respCreateAP.toString());\\nreturn respCreateAP.getAccessPointArn();', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3outposts create-endpoint --outpost-id op-01ac5d28a6a232904 --subnet-id\\nsubnet-8c7a57c5 --security-group-id sg-ab19e0d1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3outposts create-endpoint --outpost-id op-01ac5d28a6a232904 --subnet-id\\nsubnet-8c7a57c5 --security-group-id sg-ab19e0d1 --access-type CustomerOwnedIp --\\ncustomer-owned-ipv4-pool ipv4pool-coip-12345678901234567', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.services.s3outposts.AmazonS3Outposts;\\nimport com.amazonaws.services.s3outposts.AmazonS3OutpostsClientBuilder;\\nimport com.amazonaws.services.s3outposts.model.CreateEndpointRequest;\\nimport com.amazonaws.services.s3outposts.model.CreateEndpointResult;\\npublic void createEndpoint() {\\nAmazonS3Outposts s3OutpostsClient = AmazonS3OutpostsClientBuilder\\n.standard().build();\\nCreateEndpointRequest createEndpointRequest = new CreateEndpointRequest()\\n.withOutpostId(\"op-0d79779cef3c30a40\")\\n.withSubnetId(\"subnet-8c7a57c5\")\\n.withSecurityGroupId(\"sg-ab19e0d1\")\\n.withAccessType(\"CustomerOwnedIp\")\\n.withCustomerOwnedIpv4Pool(\"ipv4pool-coip-12345678901234567\");\\n// Use .withAccessType and .withCustomerOwnedIpv4Pool only when the access type\\nis\\n// customer-owned IP address pool (CoIP pool)\\nCreateEndpointResult createEndpointResult =\\ns3OutpostsClient.createEndpoint(createEndpointRequest);\\nSystem.out.println(\"Endpoint is created and its ARN is \" +\\ncreateEndpointResult.getEndpointArn());\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'arn:aws:s3-outposts:region:account-id:outpost/outpost-id/bucket/bucket-name', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'arn:aws:s3-outposts:region:account-id:outpost/outpost-id/accesspoint/accesspoint-name', ''], ['', '', '']]]\n",
      "[[['API', 'S3 on Outposts parameter value', ''], ['CreateBucket', 'Bucket name as ARN, Outpost ID', ''], ['ListRegionalBuckets', 'Outpost ID', ''], ['DeleteBucket', 'Bucket name as ARN', ''], ['DeleteBucketLifecy\\ncleConfiguration', 'Bucket name as ARN', ''], ['GetBucketLifecycle\\nConfiguration', 'Bucket name as ARN', ''], ['PutBucketLifecycle\\nConfiguration', 'Bucket name as ARN', ''], ['GetBucketPolicy', 'Bucket name as ARN', ''], ['PutBucketPolicy', 'Bucket name as ARN', ''], ['DeleteBucketPolicy', 'Bucket name as ARN', ''], ['GetBucketTagging', 'Bucket name as ARN', ''], ['PutBucketTagging', 'Bucket name as ARN', ''], ['DeleteBucketTagging', 'Bucket name as ARN', '']]]\n",
      "[[['API', 'S3 on Outposts parameter value', ''], ['CreateAccessPoint', 'Access point name as ARN', ''], ['DeleteAccessPoint', 'Access point name as ARN', ''], ['GetAccessPoint', 'Access point name as ARN', ''], ['GetAccessPoint', 'Access point name as ARN', ''], ['ListAccessPoints', 'Access point name as ARN', ''], ['PutAccessPointPolicy', 'Access point name as ARN', ''], ['GetAccessPointPolicy', 'Access point name as ARN', ''], ['DeleteAccessPointPolicy', 'Access point name as ARN', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe AWS account that creates the bucket owns it and is the only one that can commit\\nactions to it. Buckets have configuration properties, such as Outpost, tag, default\\nencryption, and access point settings. The access point settings include the virtual private\\ncloud (VPC), the access point policy for accessing the objects in the bucket, and other\\nmetadata. For more information, see S3 on Outposts specifications.\\nIf you want to create a bucket that uses AWS PrivateLink to provide bucket and endpoint\\nmanagement access through interface VPC endpoints in your virtual private cloud (VPC), see\\nAWS PrivateLink for S3 on Outposts.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAvoid including sensitive information such as account numbers in the bucket name.\\nThe bucket name is visible in the URLs that point to the objects in the bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIt can take up to 5 minutes for your Outpost endpoint to be created and your bucket to\\nbe ready to use. To configure additional bucket settings, choose View details.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-bucket --bucket example-outposts-bucket --outpost-\\nid op-01ac5d28a6a232904', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic String createBucket(String bucketName) {\\nCreateBucketRequest reqCreateBucket = new CreateBucketRequest()\\n.withBucket(bucketName)\\n.withOutpostId(OutpostId)\\n.withCreateBucketConfiguration(new CreateBucketConfiguration());\\nCreateBucketResult respCreateBucket =\\ns3ControlClient.createBucket(reqCreateBucket);\\nSystem.out.printf(\"CreateBucket Response: %s%n\", respCreateBucket.toString());\\nreturn respCreateBucket.getBucketArn();\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe AWS account that creates the bucket owns it and is the only one that can change its\\ntags.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-bucket-tagging --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket --tagging file://tagging.json\\ntagging.json\\n{\\n\"TagSet\": [\\n{\\n\"Key\": \"organization\",\\n\"Value\": \"marketing\"\\n}', '']]]\n",
      "[[['', ']\\n}', ''], ['', '', '']], [['', '', ''], ['', \"aws s3control put-bucket-tagging --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket --tagging 'TagSet=[{Key=organization,Value=marketing}]'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Id\":\"testBucketPolicy\",\\n\"Statement\":[\\n{\\n\"Sid\":\"st1\",\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"AWS\":\"123456789012\"\\n},\\n\"Action\":\"s3-outposts:*\",\\n\"Resource\":\"arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket\"\\n}\\n]', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-bucket-policy --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket --policy file://policy1.json', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic void putBucketPolicy(String bucketArn) {\\nString policy = \"{\\\\\"Version\\\\\":\\\\\"2012-10-17\\\\\",\\\\\"Id\\\\\":\\\\\"testBucketPolicy\\\\\",\\n\\\\\"Statement\\\\\":[{\\\\\"Sid\\\\\":\\\\\"st1\\\\\",\\\\\"Effect\\\\\":\\\\\"Allow\\\\\",\\\\\"Principal\\\\\":{\\\\\"AWS\\\\\":\\\\\"\" +\\nAccountId+ \"\\\\\"},\\\\\"Action\\\\\":\\\\\"s3-outposts:*\\\\\",\\\\\"Resource\\\\\":\\\\\"\" + bucketArn + \"\\\\\"}]}\";\\nPutBucketPolicyRequest reqPutBucketPolicy = new PutBucketPolicyRequest()\\n.withAccountId(AccountId)\\n.withBucket(bucketArn)\\n.withPolicy(policy);\\nPutBucketPolicyResult respPutBucketPolicy =\\ns3ControlClient.putBucketPolicy(reqPutBucketPolicy);\\nSystem.out.printf(\"PutBucketPolicy Response: %s%n\",\\nrespPutBucketPolicy.toString());\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-bucket-policy --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic void getBucketPolicy(String bucketArn) {\\nGetBucketPolicyRequest reqGetBucketPolicy = new GetBucketPolicyRequest()\\n.withAccountId(AccountId)\\n.withBucket(bucketArn);\\nGetBucketPolicyResult respGetBucketPolicy =\\ns3ControlClient.getBucketPolicy(reqGetBucketPolicy);\\nSystem.out.printf(\"GetBucketPolicy Response: %s%n\",\\nrespGetBucketPolicy.toString());', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control delete-bucket-policy --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen testing s3outposts permissions by using the Amazon S3 console, you must grant\\nadditional permissions that the console requires, such as s3outposts:createendpoint,\\ns3outposts:listendpoints, and so on.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nWhen restricting access to a specific IP address, make sure that you also specify which VPC\\nendpoints, VPC source IP addresses, or external IP addresses can access the S3 on Outposts\\nbucket. Otherwise, you might lose access to the bucket if your policy denies all users from\\nperforming any s3outposts operations on objects in your S3 on Outposts bucket without\\nthe proper permissions already in place.', ''], ['', '', '']], [['', '', ''], ['', \"Warning\\nBefore using this S3 on Outposts policy, replace the 192.0.2.0/24 IP address range in\\nthis example with an appropriate value for your use case. Otherwise, you'll lose the ability\\nto access your bucket.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"S3OutpostsPolicyId1\",\\n\"Statement\": [\\n{\\n\"Sid\": \"IPAllow\",\\n\"Effect\": \"Deny\",', '']]]\n",
      "[[['', '\"Principal\": \"*\",\\n\"Action\": \"s3outposts:*\",\\n\"Resource\": [\\n\"arn:aws:s3-outposts:region:111122223333:outpost/OUTPOSTS-ID/\\naccesspoint/EXAMPLE-ACCESS-POINT-NAME\"\\n\"arn:aws:aws:s3-outposts:region:111122223333:outpost/OUTPOSTS-ID/\\nbucket/DOC-EXAMPLE-BUCKET\"\\n],\\n\"Condition\": {\\n\"NotIpAddress\": {\\n\"aws:SourceIp\": \"192.0.2.0/24\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Warning\\nReplace the IP address ranges in this example with appropriate values for your use case\\nbefore using this S3 on Outposts policy. Otherwise, you might lose the ability to access\\nyour bucket.', ''], ['', '', '']], [['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"Id\": \"S3OutpostsPolicyId2\",\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"AllowIPmix\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3outposts:*\",\\n\"Resource\": [\\n\"arn:aws:aws:s3-outposts:region:111122223333:outpost/OUTPOSTS-ID/\\nbucket/DOC-EXAMPLE-BUCKET\",\\n\"arn:aws:aws:s3-outposts:region:111122223333:outpost/OUTPOSTS-\\nID/bucket/DOC-EXAMPLE-BUCKET/*\"\\n],\\n\"Condition\": {\\n\"IpAddress\": {\\n\"aws:SourceIp\": [\\n\"192.0.2.0/24\",\\n\"2001:DB8:1234:5678::/64\"\\n]\\n},\\n\"NotIpAddress\": {\\n\"aws:SourceIp\": [\\n\"203.0.113.0/24\",\\n\"2001:DB8:1234:5678:ABCD::/80\"\\n]\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control list-regional-buckets --account-id 123456789012 --outpost-\\nid op-01ac5d28a6a232904', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic void listRegionalBuckets() {\\nListRegionalBucketsRequest reqListBuckets = new ListRegionalBucketsRequest()\\n.withAccountId(AccountId)\\n.withOutpostId(OutpostId);\\nListRegionalBucketsResult respListBuckets =\\ns3ControlClient.listRegionalBuckets(reqListBuckets);', '']]]\n",
      "[[['', 'System.out.printf(\"ListRegionalBuckets Response: %s%n\",\\nrespListBuckets.toString());\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nWhen you're working with Amazon S3 on Outposts through the AWS CLI or AWS SDKs, you\\nprovide the access point ARN for the Outpost in place of the bucket name. The access point\\nARN takes the following form, where region is the AWS Region code for the Region that\\nthe Outpost is homed to:\\narn:aws:s3-outposts:region:123456789012:outpost/op-01ac5d28a6a232904/\\naccesspoint/example-outposts-access-point\\nFor more information about S3 on Outposts ARNs, see Resource ARNs for S3 on Outposts.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-bucket --account-id 123456789012 --bucket \"arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket\"', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic void getBucket(String bucketArn) {\\nGetBucketRequest reqGetBucket = new GetBucketRequest()\\n.withBucket(bucketArn)\\n.withAccountId(AccountId);\\nGetBucketResult respGetBucket = s3ControlClient.getBucket(reqGetBucket);\\nSystem.out.printf(\"GetBucket Response: %s%n\", respGetBucket.toString());\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\n• Outposts buckets must be empty before they can be deleted.\\nThe Amazon S3 console doesn't support S3 on Outposts object actions. To delete objects\\nin an S3 on Outposts bucket, you must use the REST API, AWS CLI, or AWS SDKs.\\n• Before you can delete an Outposts bucket, you must delete any Outposts access points\\nfor the bucket. For more information, see Deleting an access point.\\n• You cannot recover a bucket after it has been deleted.\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3control delete-bucket --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe AWS account that creates the Outposts bucket owns it and is the only one that can\\nassign access points to it.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe AWS account that creates the Outposts bucket owns it and is the only one that can\\nassign access points to it.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-access-point --account-id 123456789012\\n--name example-outposts-access-point --bucket \"arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket\" --vpc-configuration VpcId=example-vpc-12345', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic String createAccessPoint(String bucketArn, String accessPointName) {\\nCreateAccessPointRequest reqCreateAP = new CreateAccessPointRequest()\\n.withAccountId(AccountId)\\n.withBucket(bucketArn)\\n.withName(accessPointName)\\n.withVpcConfiguration(new VpcConfiguration().withVpcId(\"vpc-12345\"));\\nCreateAccessPointResult respCreateAP =\\ns3ControlClient.createAccessPoint(reqCreateAP);\\nSystem.out.printf(\"CreateAccessPoint Response: %s%n\", respCreateAP.toString());\\nreturn respCreateAP.getAccessPointArn();\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThe --op-s3 suffix is reserved for access point aliases, we recommend that you don't use\\nit for bucket or access point names. For more information about S3 on Outposts bucket-\\nnaming rules, see Working with S3 on Outposts buckets.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-access-point --bucket example-outposts-bucket --name example-\\noutposts-access-point --account-id 123456789012\\n{\\n\"AccessPointArn\":\\n\"arn:aws:s3-outposts:region:123456789012:outpost/op-01ac5d28a6a232904/\\naccesspoint/example-outposts-access-point\",\\n\"Alias\": \"example-outp-o01ac5d28a6a232904e8xz5w8ijx1qzlbp3i3kuse10--op-s3\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control get-access-point --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket --name example-outposts-access-point --account-id 123456789012\\n{\\n\"Name\": \"example-outposts-access-point\",\\n\"Bucket\": \"example-outposts-bucket\",\\n\"NetworkOrigin\": \"Vpc\",\\n\"VpcConfiguration\": {\\n\"VpcId\": \"vpc-01234567890abcdef\"\\n},\\n\"PublicAccessBlockConfiguration\": {\\n\"BlockPublicAcls\": true,\\n\"IgnorePublicAcls\": true,\\n\"BlockPublicPolicy\": true,', '']]]\n",
      "[[['', '\"RestrictPublicBuckets\": true\\n},\\n\"CreationDate\": \"2022-09-18T17:49:15.584000+00:00\",\\n\"Alias\": \"example-outp-o0b1d075431d83bebde8xz5w8ijx1qzlbp3i3kuse10--op-s3\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control list-access-points --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket\\n{\\n\"AccessPointList\": [\\n{\\n\"Name\": \"example-outposts-access-point\",\\n\"NetworkOrigin\": \"Vpc\",\\n\"VpcConfiguration\": {\\n\"VpcId\": \"vpc-01234567890abcdef\"\\n},\\n\"Bucket\": \"example-outposts-bucket\",\\n\"AccessPointArn\": \"arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-outposts-\\naccess-point\",\\n\"Alias\": \"example-outp-o0b1d075431d83bebde8xz5w8ijx1qzlbp3i3kuse10--op-s3\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object --bucket my-access-po-\\no0b1d075431d83bebde8xz5w8ijx1qzlbp3i3kuse10--op-s3 --key testkey sample-object.rtf', '']]]\n",
      "[[['', '{\\n\"AcceptRanges\": \"bytes\",\\n\"LastModified\": \"2020-01-08T22:16:28+00:00\",\\n\"ContentLength\": 910,\\n\"ETag\": \"\\\\\"00751974dc146b76404bb7290f8f51bb\\\\\"\",\\n\"VersionId\": \"null\",\\n\"ContentType\": \"text/rtf\",\\n\"Metadata\": {}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-access-point --account-id 123456789012 --name arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-outposts-\\naccess-point', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic void getAccessPoint(String accessPointArn) {\\nGetAccessPointRequest reqGetAP = new GetAccessPointRequest()\\n.withAccountId(AccountId)\\n.withName(accessPointArn);\\nGetAccessPointResult respGetAP = s3ControlClient.getAccessPoint(reqGetAP);\\nSystem.out.printf(\"GetAccessPoint Response: %s%n\", respGetAP.toString());\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control list-access-points --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic void listAccessPoints(String bucketArn) {\\nListAccessPointsRequest reqListAPs = new ListAccessPointsRequest()\\n.withAccountId(AccountId)\\n.withBucket(bucketArn);\\nListAccessPointsResult respListAPs = s3ControlClient.listAccessPoints(reqListAPs);\\nSystem.out.printf(\"ListAccessPoints Response: %s%n\", respListAPs.toString());\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control delete-access-point --account-id 123456789012 --name arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-outposts-\\naccess-point', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Id\":\"exampleAccessPointPolicy\",\\n\"Statement\":[\\n{\\n\"Sid\":\"st1\",\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"AWS\":\"123456789012\"\\n},\\n\"Action\":\"s3-outposts:*\",\\n\"Resource\":\"arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-\\noutposts-access-point\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-access-point-policy --account-id 123456789012 --name arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-\\noutposts-access-point --policy file://appolicy1.json', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;', '']]]\n",
      "[[['', 'public void putAccessPointPolicy(String accessPointArn) {\\nString policy = \"{\\\\\"Version\\\\\":\\\\\"2012-10-17\\\\\",\\\\\"Id\\\\\":\\\\\"testAccessPointPolicy\\\\\",\\n\\\\\"Statement\\\\\":[{\\\\\"Sid\\\\\":\\\\\"st1\\\\\",\\\\\"Effect\\\\\":\\\\\"Allow\\\\\",\\\\\"Principal\\\\\":{\\\\\"AWS\\\\\":\\\\\"\" +\\nAccountId + \"\\\\\"},\\\\\"Action\\\\\":\\\\\"s3-outposts:*\\\\\",\\\\\"Resource\\\\\":\\\\\"\" + accessPointArn +\\n\"\\\\\"}]}\";\\nPutAccessPointPolicyRequest reqPutAccessPointPolicy = new\\nPutAccessPointPolicyRequest()\\n.withAccountId(AccountId)\\n.withName(accessPointArn)\\n.withPolicy(policy);\\nPutAccessPointPolicyResult respPutAccessPointPolicy =\\ns3ControlClient.putAccessPointPolicy(reqPutAccessPointPolicy);\\nSystem.out.printf(\"PutAccessPointPolicy Response: %s%n\",\\nrespPutAccessPointPolicy.toString());\\nprintWriter.printf(\"PutAccessPointPolicy Response: %s%n\",\\nrespPutAccessPointPolicy.toString());\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-access-point-policy --account-id 123456789012 --name arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-outposts-\\naccess-point', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic void getAccessPointPolicy(String accessPointArn) {\\nGetAccessPointPolicyRequest reqGetAccessPointPolicy = new\\nGetAccessPointPolicyRequest()\\n.withAccountId(AccountId)\\n.withName(accessPointArn);\\nGetAccessPointPolicyResult respGetAccessPointPolicy =\\ns3ControlClient.getAccessPointPolicy(reqGetAccessPointPolicy);\\nSystem.out.printf(\"GetAccessPointPolicy Response: %s%n\",\\nrespGetAccessPointPolicy.toString());\\nprintWriter.printf(\"GetAccessPointPolicy Response: %s%n\",\\nrespGetAccessPointPolicy.toString());\\n}', ''], ['', '', '']]]\n",
      "[[['API', 'Status', 'Failed Reason Error\\nCode', 'Message - Failed Reason'], ['CreateEnd\\npoint', 'Create_Fa\\niled', 'OutpostNotReachable', 'Endpoint could not be created as\\nthe service link connection to your\\nOutposts home Region is down. Check\\nyour connection, delete the endpoint,\\nand try again.'], ['CreateEnd\\npoint', 'Create_Fa\\niled', 'InternalError', 'Endpoint could not be created due\\nto Internal Error. Please delete the\\nendpoint and create again.'], ['DeleteEnd\\npoint', 'Delete_Fa\\niled', 'OutpostNotReachable', 'Endpoint could not be deleted as\\nthe service link connection to your\\nOutposts home Region is down. Check\\nyour connection and please try again.'], ['DeleteEnd\\npoint', 'Delete_Fa\\niled', 'InternalError', 'Endpoint could not be deleted due to\\nInternal Error. Please try again.']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3outposts create-endpoint --outpost-id op-01ac5d28a6a232904 --subnet-id\\nsubnet-8c7a57c5 --security-group-id sg-ab19e0d1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3outposts create-endpoint --outpost-id op-01ac5d28a6a232904 --subnet-id\\nsubnet-8c7a57c5 --security-group-id sg-ab19e0d1 --access-type CustomerOwnedIp --\\ncustomer-owned-ipv4-pool ipv4pool-coip-12345678901234567', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3outposts.AmazonS3Outposts;\\nimport com.amazonaws.services.s3outposts.AmazonS3OutpostsClientBuilder;\\nimport com.amazonaws.services.s3outposts.model.CreateEndpointRequest;\\nimport com.amazonaws.services.s3outposts.model.CreateEndpointResult;\\npublic void createEndpoint() {\\nAmazonS3Outposts s3OutpostsClient = AmazonS3OutpostsClientBuilder\\n.standard().build();\\nCreateEndpointRequest createEndpointRequest = new CreateEndpointRequest()\\n.withOutpostId(\"op-0d79779cef3c30a40\")\\n.withSubnetId(\"subnet-8c7a57c5\")\\n.withSecurityGroupId(\"sg-ab19e0d1\")\\n.withAccessType(\"CustomerOwnedIp\")\\n.withCustomerOwnedIpv4Pool(\"ipv4pool-coip-12345678901234567\");\\n// Use .withAccessType and .withCustomerOwnedIpv4Pool only when the access type is', '']]]\n",
      "[[['', '// customer-owned IP address pool (CoIP pool)\\nCreateEndpointResult createEndpointResult =\\ns3OutpostsClient.createEndpoint(createEndpointRequest);\\nSystem.out.println(\"Endpoint is created and its ARN is \" +\\ncreateEndpointResult.getEndpointArn());\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3outposts list-endpoints', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3outposts.AmazonS3Outposts;', '']]]\n",
      "[[['', 'import com.amazonaws.services.s3outposts.AmazonS3OutpostsClientBuilder;\\nimport com.amazonaws.services.s3outposts.model.ListEndpointsRequest;\\nimport com.amazonaws.services.s3outposts.model.ListEndpointsResult;\\npublic void listEndpoints() {\\nAmazonS3Outposts s3OutpostsClient = AmazonS3OutpostsClientBuilder\\n.standard().build();\\nListEndpointsRequest listEndpointsRequest = new ListEndpointsRequest();\\nListEndpointsResult listEndpointsResult =\\ns3OutpostsClient.listEndpoints(listEndpointsRequest);\\nSystem.out.println(\"List endpoints result is \" + listEndpointsResult);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3outposts delete-endpoint --endpoint-id example-endpoint-id --outpost-\\nid op-01ac5d28a6a232904', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.arn.Arn;\\nimport com.amazonaws.services.s3outposts.AmazonS3Outposts;\\nimport com.amazonaws.services.s3outposts.AmazonS3OutpostsClientBuilder;\\nimport com.amazonaws.services.s3outposts.model.DeleteEndpointRequest;\\npublic void deleteEndpoint(String endpointArnInput) {\\nString outpostId = \"op-01ac5d28a6a232904\";\\nAmazonS3Outposts s3OutpostsClient = AmazonS3OutpostsClientBuilder\\n.standard().build();\\nArn endpointArn = Arn.fromString(endpointArnInput);\\nString[] resourceParts = endpointArn.getResource().getResource().split(\"/\");\\nString endpointId = resourceParts[resourceParts.length - 1];\\nDeleteEndpointRequest deleteEndpointRequest = new DeleteEndpointRequest()\\n.withEndpointId(endpointId)\\n.withOutpostId(outpostId);\\ns3OutpostsClient.deleteEndpoint(deleteEndpointRequest);\\nSystem.out.println(\"Endpoint with id \" + endpointId + \" is deleted.\");\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'arn:aws:s3-outposts:region:account-id:outpost/outpost-id/accesspoint/accesspoint-name', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3-outposts:us-west-2:123456789012:outpost/ op-01ac5d28a6a232904/\\nbucket/example-s3-bucket1/object/myobject', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'arn:aws:s3-outposts:region:account-id:outpost/outpost-id/accesspoint/accesspoint-name', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket arn:aws:s3-\\noutposts:Region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-\\noutposts-access-point --key sample-object.xml --body sample-object.xml', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.ObjectMetadata;\\nimport com.amazonaws.services.s3.model.PutObjectRequest;\\nimport java.io.File;\\npublic class PutObject {\\npublic static void main(String[] args) {\\nString accessPointArn = \"*** access point ARN ***\";\\nString stringObjKeyName = \"*** String object key name ***\";\\nString fileObjKeyName = \"*** File object key name ***\";\\nString fileName = \"*** Path to file to upload ***\";\\ntry {\\n// This code expects that you have AWS credentials set up per:\\n// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.enableUseArnRegion()\\n.build();\\n// Upload a text string as a new object.\\ns3Client.putObject(accessPointArn, stringObjKeyName, \"Uploaded String\\nObject\");\\n// Upload a file as a new object with ContentType and title specified.\\nPutObjectRequest request = new PutObjectRequest(accessPointArn,\\nfileObjKeyName, new File(fileName));\\nObjectMetadata metadata = new ObjectMetadata();\\nmetadata.setContentType(\"plain/text\");\\nmetadata.addUserMetadata(\"title\", \"someTitle\");\\nrequest.setMetadata(metadata);', '']]]\n",
      "[[['', \"s3Client.putObject(request);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3-outposts:region:account-id:outpost/outpost-id/accesspoint/accesspoint-name', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.CopyObjectRequest;\\npublic class CopyObject {\\npublic static void main(String[] args) {\\nString accessPointArn = \"*** access point ARN ***\";\\nString sourceKey = \"*** Source object key ***\";\\nString destinationKey = \"*** Destination object key ***\";\\ntry {\\n// This code expects that you have AWS credentials set up per:\\n// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.enableUseArnRegion()\\n.build();\\n// Copy the object into a new object in the same bucket.\\nCopyObjectRequest copyObjectRequest = new CopyObjectRequest(accessPointArn,\\nsourceKey, accessPointArn, destinationKey);\\ns3Client.copyObject(copyObjectRequest);\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'arn:aws:s3-outposts:region:account-id:outpost/outpost-id/accesspoint/accesspoint-name', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-outposts-\\naccess-point --key testkey sample-object.xml', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.GetObjectRequest;\\nimport com.amazonaws.services.s3.model.ResponseHeaderOverrides;\\nimport com.amazonaws.services.s3.model.S3Object;\\nimport java.io.BufferedReader;\\nimport java.io.IOException;\\nimport java.io.InputStream;\\nimport java.io.InputStreamReader;\\npublic class GetObject {\\npublic static void main(String[] args) throws IOException {\\nString accessPointArn = \"*** access point ARN ***\";\\nString key = \"*** Object key ***\";\\nS3Object fullObject = null, objectPortion = null, headerOverrideObject = null;\\ntry {\\n// This code expects that you have AWS credentials set up per:\\n// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.enableUseArnRegion()\\n.build();\\n// Get an object and print its contents.\\nSystem.out.println(\"Downloading an object\");\\nfullObject = s3Client.getObject(new GetObjectRequest(accessPointArn, key));\\nSystem.out.println(\"Content-Type: \" +\\nfullObject.getObjectMetadata().getContentType());\\nSystem.out.println(\"Content: \");\\ndisplayTextInputStream(fullObject.getObjectContent());\\n// Get a range of bytes from an object and print the bytes.', '']]]\n",
      "[[['', 'GetObjectRequest rangeObjectRequest = new GetObjectRequest(accessPointArn,\\nkey)\\n.withRange(0, 9);\\nobjectPortion = s3Client.getObject(rangeObjectRequest);\\nSystem.out.println(\"Printing bytes retrieved.\");\\ndisplayTextInputStream(objectPortion.getObjectContent());\\n// Get an entire object, overriding the specified response headers, and\\nprint the object\\'s content.\\nResponseHeaderOverrides headerOverrides = new ResponseHeaderOverrides()\\n.withCacheControl(\"No-cache\")\\n.withContentDisposition(\"attachment; filename=example.txt\");\\nGetObjectRequest getObjectRequestHeaderOverride = new\\nGetObjectRequest(accessPointArn, key)\\n.withResponseHeaders(headerOverrides);\\nheaderOverrideObject = s3Client.getObject(getObjectRequestHeaderOverride);\\ndisplayTextInputStream(headerOverrideObject.getObjectContent());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n} finally {\\n// To ensure that the network connection doesn\\'t remain open, close any\\nopen input streams.\\nif (fullObject != null) {\\nfullObject.close();\\n}\\nif (objectPortion != null) {\\nobjectPortion.close();\\n}\\nif (headerOverrideObject != null) {\\nheaderOverrideObject.close();\\n}\\n}\\n}\\nprivate static void displayTextInputStream(InputStream input) throws IOException {\\n// Read the text input stream one line at a time and display each line.\\nBufferedReader reader = new BufferedReader(new InputStreamReader(input));\\nString line = null;', '']]]\n",
      "[[['', 'while ((line = reader.readLine()) != null) {\\nSystem.out.println(line);\\n}\\nSystem.out.println();\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3-outposts:region:account-id:outpost/outpost-id/accesspoint/accesspoint-name', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nWith Amazon S3 on Outposts, object data is always stored on the Outpost. When AWS\\ninstalls an Outpost rack, your data stays local to your Outpost to meet data-residency\\nrequirements. Your objects never leave your Outpost and are not in an AWS Region.\\nBecause the AWS Management Console is hosted in-Region, you can't use the console\\nto upload or manage objects in your Outpost. However, you can use the REST API, AWS\\nCommand Line Interface (AWS CLI), and AWS SDKs to upload and manage your objects\\nthrough your access points.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api list-objects-v2 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-outposts-\\naccess-point', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen using this action with Amazon S3 on Outposts through the AWS SDKs, you provide\\nthe Outposts access point ARN in place of the bucket name, in the following form:\\narn:aws:s3-outposts:region:123456789012:outpost/op-01ac5d28a6a232904/\\naccesspoint/example-Outposts-Access-Point. For more information about S3 on\\nOutposts ARNs, see Resource ARNs for S3 on Outposts.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nThis example uses ListObjectsV2, which is the latest revision of the ListObjects\\nAPI operation. We recommend that you use this revised API operation for application\\ndevelopment. For backward compatibility, Amazon S3 continues to support the prior\\nversion of this API operation.', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.ListObjectsV2Request;\\nimport com.amazonaws.services.s3.model.ListObjectsV2Result;', '']]]\n",
      "[[['', 'import com.amazonaws.services.s3.model.S3ObjectSummary;\\npublic class ListObjectsV2 {\\npublic static void main(String[] args) {\\nString accessPointArn = \"*** access point ARN ***\";\\ntry {\\n// This code expects that you have AWS credentials set up per:\\n// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.enableUseArnRegion()\\n.build();\\nSystem.out.println(\"Listing objects\");\\n// maxKeys is set to 2 to demonstrate the use of\\n// ListObjectsV2Result.getNextContinuationToken()\\nListObjectsV2Request req = new\\nListObjectsV2Request().withBucketName(accessPointArn).withMaxKeys(2);\\nListObjectsV2Result result;\\ndo {\\nresult = s3Client.listObjectsV2(req);\\nfor (S3ObjectSummary objectSummary : result.getObjectSummaries()) {\\nSystem.out.printf(\" - %s (size: %d)\\\\n\", objectSummary.getKey(),\\nobjectSummary.getSize());\\n}\\n// If there are more than maxKeys keys in the bucket, get a\\ncontinuation token\\n// and list the next objects.\\nString token = result.getNextContinuationToken();\\nSystem.out.println(\"Next Continuation Token: \" + token);\\nreq.setContinuationToken(token);\\n} while (result.isTruncated());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.', '']]]\n",
      "[[['', 'e.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3-outposts:region:account-id:outpost/outpost-id/accesspoint/accesspoint-name', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api delete-object --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-\\noutposts-access-point --key sample-object.xml', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-objects --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-\\noutposts-access-point --delete file://delete.json\\ndelete.json\\n{\\n\"Objects\": [\\n{\\n\"Key\": \"test1.txt\"\\n},\\n{\\n\"Key\": \"sample-object.xml\"\\n}\\n],\\n\"Quiet\": false\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.DeleteObjectRequest;\\npublic class DeleteObject {\\npublic static void main(String[] args) {\\nString accessPointArn = \"*** access point ARN ***\";\\nString keyName = \"*** key name ****\";\\ntry {\\n// This code expects that you have AWS credentials set up per:\\n// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.enableUseArnRegion()\\n.build();\\ns3Client.deleteObject(new DeleteObjectRequest(accessPointArn, keyName));\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.DeleteObjectsRequest;\\nimport com.amazonaws.services.s3.model.DeleteObjectsRequest.KeyVersion;\\nimport com.amazonaws.services.s3.model.DeleteObjectsResult;\\nimport java.util.ArrayList;\\npublic class DeleteObjects {\\npublic static void main(String[] args) {\\nString accessPointArn = \"arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-\\noutposts-access-point\";\\ntry {\\n// This code expects that you have AWS credentials set up per:\\n// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.enableUseArnRegion()\\n.build();\\n// Upload three sample objects.\\nArrayList<KeyVersion> keys = new ArrayList<KeyVersion>();\\nfor (int i = 0; i < 3; i++) {\\nString keyName = \"delete object example \" + i;\\ns3Client.putObject(accessPointArn, keyName, \"Object number \" + i + \"\\nto be deleted.\");\\nkeys.add(new KeyVersion(keyName));\\n}\\nSystem.out.println(keys.size() + \" objects successfully created.\");\\n// Delete the sample objects.', '']]]\n",
      "[[['', 'DeleteObjectsRequest multiObjectDeleteRequest = new\\nDeleteObjectsRequest(accessPointArn)\\n.withKeys(keys)\\n.withQuiet(false);\\n// Verify that the objects were deleted successfully.\\nDeleteObjectsResult delObjRes =\\ns3Client.deleteObjects(multiObjectDeleteRequest);\\nint successfulDeletes = delObjRes.getDeletedObjects().size();\\nSystem.out.println(successfulDeletes + \" objects successfully\\ndeleted.\");\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'arn:aws:s3-outposts:region:account-id:outpost/outpost-id/accesspoint/accesspoint-name', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nWith Amazon S3 on Outposts, object data is always stored on the Outpost. When AWS\\ninstalls an Outpost rack, your data stays local to your Outpost to meet data-residency\\nrequirements. Your objects never leave your Outpost and are not in an AWS Region.\\nBecause the AWS Management Console is hosted in-Region, you can't use the console\\nto upload or manage objects in your Outpost. However, you can use the REST API, AWS\\nCommand Line Interface (AWS CLI), and AWS SDKs to upload and manage your objects\\nthrough your access points.\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api head-bucket --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/accesspoint/example-outposts-\\naccess-point', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.HeadBucketRequest;\\npublic class HeadBucket {', '']]]\n",
      "[[['', 'public static void main(String[] args) {\\nString accessPointArn = \"*** access point ARN ***\";\\ntry {\\n// This code expects that you have AWS credentials set up per:\\n// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.enableUseArnRegion()\\n.build();\\ns3Client.headBucket(new HeadBucketRequest(accessPointArn));\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.*;\\nimport java.util.ArrayList;\\nimport java.util.List;\\npublic class MultipartUploadCopy {\\npublic static void main(String[] args) {\\nString accessPointArn = \"*** Source access point ARN ***\";\\nString sourceObjectKey = \"*** Source object key ***\";\\nString destObjectKey = \"*** Target object key ***\";\\ntry {\\n// This code expects that you have AWS credentials set up per:\\n// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.enableUseArnRegion()\\n.build();\\n// Initiate the multipart upload.\\nInitiateMultipartUploadRequest initRequest = new\\nInitiateMultipartUploadRequest(accessPointArn, destObjectKey);\\nInitiateMultipartUploadResult initResult =\\ns3Client.initiateMultipartUpload(initRequest);\\n// Get the object size to track the end of the copy operation.\\nGetObjectMetadataRequest metadataRequest = new\\nGetObjectMetadataRequest(accessPointArn, sourceObjectKey);\\nObjectMetadata metadataResult =\\ns3Client.getObjectMetadata(metadataRequest);\\nlong objectSize = metadataResult.getContentLength();', '']]]\n",
      "[[['', '// Copy the object using 5 MB parts.\\nlong partSize = 5 * 1024 * 1024;\\nlong bytePosition = 0;\\nint partNum = 1;\\nList<CopyPartResult> copyResponses = new ArrayList<CopyPartResult>();\\nwhile (bytePosition < objectSize) {\\n// The last part might be smaller than partSize, so check to make sure\\n// that lastByte isn\\'t beyond the end of the object.\\nlong lastByte = Math.min(bytePosition + partSize - 1, objectSize - 1);\\n// Copy this part.\\nCopyPartRequest copyRequest = new CopyPartRequest()\\n.withSourceBucketName(accessPointArn)\\n.withSourceKey(sourceObjectKey)\\n.withDestinationBucketName(accessPointArn)\\n.withDestinationKey(destObjectKey)\\n.withUploadId(initResult.getUploadId())\\n.withFirstByte(bytePosition)\\n.withLastByte(lastByte)\\n.withPartNumber(partNum++);\\ncopyResponses.add(s3Client.copyPart(copyRequest));\\nbytePosition += partSize;\\n}\\n// Complete the upload request to concatenate all uploaded parts and make\\nthe copied object available.\\nCompleteMultipartUploadRequest completeRequest = new\\nCompleteMultipartUploadRequest(\\naccessPointArn,\\ndestObjectKey,\\ninitResult.getUploadId(),\\ngetETags(copyResponses));\\ns3Client.completeMultipartUpload(completeRequest);\\nSystem.out.println(\"Multipart copy complete.\");\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}', '']]]\n",
      "[[['', '// This is a helper function to construct a list of ETags.\\nprivate static List<PartETag> getETags(List<CopyPartResult> responses) {\\nList<PartETag> etags = new ArrayList<PartETag>();\\nfor (CopyPartResult response : responses) {\\netags.add(new PartETag(response.getPartNumber(), response.getETag()));\\n}\\nreturn etags;\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.*;\\nimport java.util.ArrayList;\\nimport java.util.List;\\npublic class MultipartUploadCopy {\\npublic static void main(String[] args) {\\nString accessPointArn = \"*** Source access point ARN ***\";\\nString sourceObjectKey = \"*** Source object key ***\";\\nString destObjectKey = \"*** Target object key ***\";\\ntry {\\n// This code expects that you have AWS credentials set up per:\\n// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.enableUseArnRegion()\\n.build();\\n// Initiate the multipart upload.\\nInitiateMultipartUploadRequest initRequest = new\\nInitiateMultipartUploadRequest(accessPointArn, destObjectKey);', '']]]\n",
      "[[['', 'InitiateMultipartUploadResult initResult =\\ns3Client.initiateMultipartUpload(initRequest);\\n// Get the object size to track the end of the copy operation.\\nGetObjectMetadataRequest metadataRequest = new\\nGetObjectMetadataRequest(accessPointArn, sourceObjectKey);\\nObjectMetadata metadataResult =\\ns3Client.getObjectMetadata(metadataRequest);\\nlong objectSize = metadataResult.getContentLength();\\n// Copy the object using 5 MB parts.\\nlong partSize = 5 * 1024 * 1024;\\nlong bytePosition = 0;\\nint partNum = 1;\\nList<CopyPartResult> copyResponses = new ArrayList<CopyPartResult>();\\nwhile (bytePosition < objectSize) {\\n// The last part might be smaller than partSize, so check to make sure\\n// that lastByte isn\\'t beyond the end of the object.\\nlong lastByte = Math.min(bytePosition + partSize - 1, objectSize - 1);\\n// Copy this part.\\nCopyPartRequest copyRequest = new CopyPartRequest()\\n.withSourceBucketName(accessPointArn)\\n.withSourceKey(sourceObjectKey)\\n.withDestinationBucketName(accessPointArn)\\n.withDestinationKey(destObjectKey)\\n.withUploadId(initResult.getUploadId())\\n.withFirstByte(bytePosition)\\n.withLastByte(lastByte)\\n.withPartNumber(partNum++);\\ncopyResponses.add(s3Client.copyPart(copyRequest));\\nbytePosition += partSize;\\n}\\n// Complete the upload request to concatenate all uploaded parts and make\\nthe copied object available.\\nCompleteMultipartUploadRequest completeRequest = new\\nCompleteMultipartUploadRequest(\\naccessPointArn,\\ndestObjectKey,\\ninitResult.getUploadId(),\\ngetETags(copyResponses));\\ns3Client.completeMultipartUpload(completeRequest);\\nSystem.out.println(\"Multipart copy complete.\");', '']]]\n",
      "[[['', \"} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn't process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn't be contacted for a response, or the client\\n// couldn't parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n// This is a helper function to construct a list of ETags.\\nprivate static List<PartETag> getETags(List<CopyPartResult> responses) {\\nList<PartETag> etags = new ArrayList<PartETag>();\\nfor (CopyPartResult response : responses) {\\netags.add(new PartETag(response.getPartNumber(), response.getETag()));\\n}\\nreturn etags;\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.*;\\nimport java.util.List;\\npublic class ListParts {\\npublic static void main(String[] args) {\\nString accessPointArn = \"*** access point ARN ***\";\\nString keyName = \"*** Key name ***\";\\nString uploadId = \"*** Upload ID ***\";\\ntry {\\n// This code expects that you have AWS credentials set up per:', '']]]\n",
      "[[['', '// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.enableUseArnRegion()\\n.build();\\nListPartsRequest listPartsRequest = new ListPartsRequest(accessPointArn,\\nkeyName, uploadId);\\nPartListing partListing = s3Client.listParts(listPartsRequest);\\nList<PartSummary> partSummaries = partListing.getParts();\\nSystem.out.println(partSummaries.size() + \" multipart upload parts\");\\nfor (PartSummary p : partSummaries) {\\nSystem.out.println(\"Upload part: Part number = \\\\\"\" + p.getPartNumber()\\n+ \"\\\\\", ETag = \" + p.getETag());\\n}\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.ListMultipartUploadsRequest;\\nimport com.amazonaws.services.s3.model.MultipartUpload;\\nimport com.amazonaws.services.s3.model.MultipartUploadListing;', '']]]\n",
      "[[['', 'import java.util.List;\\npublic class ListMultipartUploads {\\npublic static void main(String[] args) {\\nString accessPointArn = \"*** access point ARN ***\";\\ntry {\\n// This code expects that you have AWS credentials set up per:\\n// https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-\\ncredentials.html\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.enableUseArnRegion()\\n.build();\\n// Retrieve a list of all in-progress multipart uploads.\\nListMultipartUploadsRequest allMultipartUploadsRequest = new\\nListMultipartUploadsRequest(accessPointArn);\\nMultipartUploadListing multipartUploadListing =\\ns3Client.listMultipartUploads(allMultipartUploadsRequest);\\nList<MultipartUpload> uploads =\\nmultipartUploadListing.getMultipartUploads();\\n// Display information about all in-progress multipart uploads.\\nSystem.out.println(uploads.size() + \" multipart upload(s) in progress.\");\\nfor (MultipartUpload u : uploads) {\\nSystem.out.println(\"Upload in progress: Key = \\\\\"\" + u.getKey() + \"\\\\\",\\nid = \" + u.getUploadId());\\n}\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Deny a presigned URL request if the signature is more than 10\\nminutes old\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {\"AWS\":\"444455556666\"},\\n\"Action\": \"s3-outposts:*\",\\n\"Resource\": \"arn:aws:s3-outposts:us-\\neast-1:111122223333:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket/object/\\n*\",', '']]]\n",
      "[[['', '\"Condition\": {\\n\"NumericGreaterThan\": {\"s3-outposts:signatureAge\": 600000},\\n\"StringEquals\": {\"s3-outposts:authType\": \"REST-QUERY-STRING\"}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Sid\": \"NetworkRestrictionForIAMPrincipal\",\\n\"Effect\": \"Deny\",\\n\"Action\": \"*\",\\n\"Resource\": \"*\",', '']]]\n",
      "[[['', '\"Condition\": {\\n\"NotIpAddressIfExists\": {\"aws:SourceIp\": \"IP-address-range\"},\\n\"BoolIfExists\": {\"aws:ViaAWSService\": \"false\"}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\n• If you created a presigned URL by using a temporary token, the URL expires when the\\ntoken expires, even if you created the URL with a later expiration time.\\n• Because presigned URLs grant access to your S3 on Outposts buckets to whoever has the\\nURL, we recommend that you protect them appropriately. For more information about\\nprotecting presigned URLs, see Limiting presigned URL capabilities.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nWhen you use the AWS SDKs to generate a presigned URL, the maximum expiration time\\nfor a presigned URL is 7 days from the time of creation.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.HttpMethod;\\nimport com.amazonaws.SdkClientException;\\nimport com.amazonaws.auth.profile.ProfileCredentialsProvider;\\nimport com.amazonaws.regions.Regions;\\nimport com.amazonaws.services.s3.AmazonS3;\\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\\nimport com.amazonaws.services.s3.model.GeneratePresignedUrlRequest;\\nimport java.io.IOException;\\nimport java.net.URL;\\nimport java.time.Instant;\\npublic class GeneratePresignedURL {\\npublic static void main(String[] args) throws IOException {\\nRegions clientRegion = Regions.DEFAULT_REGION;\\nString accessPointArn = \"*** access point ARN ***\";\\nString objectKey = \"*** object key ***\";\\ntry {\\nAmazonS3 s3Client = AmazonS3ClientBuilder.standard()\\n.withRegion(clientRegion)\\n.withCredentials(new ProfileCredentialsProvider())\\n.build();\\n// Set the presigned URL to expire after one hour.\\njava.util.Date expiration = new java.util.Date();\\nlong expTimeMillis = Instant.now().toEpochMilli();\\nexpTimeMillis += 1000 * 60 * 60;\\nexpiration.setTime(expTimeMillis);\\n// Generate the presigned URL.\\nSystem.out.println(\"Generating pre-signed URL.\");\\nGeneratePresignedUrlRequest generatePresignedUrlRequest =\\nnew GeneratePresignedUrlRequest(accessPointArn, objectKey)\\n.withMethod(HttpMethod.GET)\\n.withExpiration(expiration);\\nURL url = s3Client.generatePresignedUrl(generatePresignedUrlRequest);', '']]]\n",
      "[[['', 'System.out.println(\"Pre-Signed URL: \" + url.toString());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 couldn\\'t\\nprocess\\n// it, so it returned an error response.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 couldn\\'t be contacted for a response, or the client\\n// couldn\\'t parse the response from Amazon S3.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nnamespace Amazon.DocSamples.S3\\n{\\nclass GenPresignedURLTest\\n{\\nprivate const string accessPointArn = \"*** access point ARN ***\";\\nprivate const string objectKey = \"*** object key ***\";\\n// Specify how long the presigned URL lasts, in hours.\\nprivate const double timeoutDuration = 12;\\n// Specify your bucket Region (an example Region is shown).\\nprivate static readonly RegionEndpoint bucketRegion =\\nRegionEndpoint.USWest2;\\nprivate static IAmazonS3 s3Client;', '']]]\n",
      "[[['', 'public static void Main()\\n{\\ns3Client = new AmazonS3Client(bucketRegion);\\nstring urlString = GeneratePreSignedURL(timeoutDuration);\\n}\\nstatic string GeneratePreSignedURL(double duration)\\n{\\nstring urlString = \"\";\\ntry\\n{\\nGetPreSignedUrlRequest request1 = new GetPreSignedUrlRequest\\n{\\nBucketName = accessPointArn,\\nKey = objectKey,\\nExpires = DateTime.UtcNow.AddHours(duration)\\n};\\nurlString = s3Client.GetPreSignedURL(request1);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when\\nwriting an object\", e.Message);\\n}\\nreturn urlString;\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"import boto3\\nurl = boto3.client('s3').generate_presigned_url(\\nClientMethod='get_object',\", '']]]\n",
      "[[['', \"Params={'Bucket': 'ACCESS_POINT_ARN', 'Key': 'OBJECT_KEY'},\\nExpiresIn=3600)\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen you use the AWS CLI to generate a presigned URL, the maximum expiration time for\\na presigned URL is 7 days from the time of creation.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3 presign s3://arn:aws:s3-outposts:us-\\neast-1:111122223333:outpost/op-01ac5d28a6a232904/accesspoint/example-outpost-access-\\npoint/mydoc.txt --expires-in 604800', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public static void signBucket(S3Presigner presigner, String\\noutpostAccessPointArn, String keyName) {\\ntry {', '']]]\n",
      "[[['', 'PutObjectRequest objectRequest = PutObjectRequest.builder()\\n.bucket(accessPointArn)\\n.key(keyName)\\n.contentType(\"text/plain\")\\n.build();\\nPutObjectPresignRequest presignRequest =\\nPutObjectPresignRequest.builder()\\n.signatureDuration(Duration.ofMinutes(10))\\n.putObjectRequest(objectRequest)\\n.build();\\nPresignedPutObjectRequest presignedRequest =\\npresigner.presignPutObject(presignRequest);\\nString myURL = presignedRequest.url().toString();\\nSystem.out.println(\"Presigned URL to upload a file to: \" +myURL);\\nSystem.out.println(\"Which HTTP method must be used when uploading a\\nfile: \" +\\npresignedRequest.httpRequest().method());\\n// Upload content to the S3 on Outposts bucket by using this URL.\\nURL url = presignedRequest.url();\\n// Create the connection and use it to upload the new object by using\\nthe presigned URL.\\nHttpURLConnection connection = (HttpURLConnection)\\nurl.openConnection();\\nconnection.setDoOutput(true);\\nconnection.setRequestProperty(\"Content-Type\",\"text/plain\");\\nconnection.setRequestMethod(\"PUT\");\\nOutputStreamWriter out = new\\nOutputStreamWriter(connection.getOutputStream());\\nout.write(\"This text was uploaded as an object by using a presigned\\nURL.\");\\nout.close();\\nconnection.getResponseCode();\\nSystem.out.println(\"HTTP response code is \" +\\nconnection.getResponseCode());\\n} catch (S3Exception e) {\\ne.getStackTrace();', '']]]\n",
      "[[['', '} catch (IOException e) {\\ne.getStackTrace();\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import argparse\\nimport logging\\nimport boto3\\nfrom botocore.exceptions import ClientError\\nimport requests\\nlogger = logging.getLogger(__name__)\\ndef generate_presigned_url(s3_client, client_method, method_parameters,\\nexpires_in):\\n\"\"\"\\nGenerate a presigned S3 on Outposts URL that can be used to perform an\\naction.\\n:param s3_client: A Boto3 Amazon S3 client.\\n:param client_method: The name of the client method that the URL performs.\\n:param method_parameters: The parameters of the specified client method.\\n:param expires_in: The number of seconds that the presigned URL is valid for.\\n:return: The presigned URL.\\n\"\"\"\\ntry:\\nurl = s3_client.generate_presigned_url(\\nClientMethod=client_method,\\nParams=method_parameters,\\nExpiresIn=expires_in\\n)\\nlogger.info(\"Got presigned URL: %s\", url)\\nexcept ClientError:', '']]]\n",
      "[[['', 'logger.exception(\\n\"Couldn\\'t get a presigned URL for client method \\'%s\\'.\",\\nclient_method)\\nraise\\nreturn url\\ndef usage_demo():\\nlogging.basicConfig(level=logging.INFO, format=\\'%(levelname)s: %(message)s\\')\\nprint(\\'-\\'*88)\\nprint(\"Welcome to the Amazon S3 on Outposts presigned URL demo.\")\\nprint(\\'-\\'*88)\\nparser = argparse.ArgumentParser()\\nparser.add_argument(\\'accessPointArn\\', help=\"The name of the S3 on Outposts\\naccess point ARN.\")\\nparser.add_argument(\\n\\'key\\', help=\"For a GET operation, the key of the object in S3 on\\nOutposts. For a \"\\n\"PUT operation, the name of a file to upload.\")\\nparser.add_argument(\\n\\'action\\', choices=(\\'get\\', \\'put\\'), help=\"The action to perform.\")\\nargs = parser.parse_args()\\ns3_client = boto3.client(\\'s3\\')\\nclient_action = \\'get_object\\' if args.action == \\'get\\' else \\'put_object\\'\\nurl = generate_presigned_url(\\ns3_client, client_action, {\\'Bucket\\': args.accessPointArn, \\'Key\\':\\nargs.key}, 1000)\\nprint(\"Using the Requests package to send a request to the URL.\")\\nresponse = None\\nif args.action == \\'get\\':\\nresponse = requests.get(url)\\nelif args.action == \\'put\\':\\nprint(\"Putting data to the URL.\")\\ntry:\\nwith open(args.key, \\'r\\') as object_file:\\nobject_text = object_file.read()\\nresponse = requests.put(url, data=object_text)\\nexcept FileNotFoundError:\\nprint(f\"Couldn\\'t find {args.key}. For a PUT operation, the key must\\nbe the \"', '']]]\n",
      "[[['', 'f\"name of a file that exists on your computer.\")\\nif response is not None:\\nprint(\"Got response:\")\\nprint(f\"Status: {response.status_code}\")\\nprint(response.text)\\nprint(\\'-\\'*88)\\nif __name__ == \\'__main__\\':\\nusage_demo()', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '[\\n{\\n\"Classification\": \"core-site\",\\n\"Properties\": {\\n\"fs.s3.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",', '']]]\n",
      "[[['', '\"fs.AbstractFileSystem.s3.impl\": \"org.apache.hadoop.fs.s3a.S3A\"\\n}\\n}\\n]', ''], ['', '', '']], [['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"Version\":\"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Resource\": \"arn:aws:s3-outposts:us-\\nwest-2:111122223333:outpost/op-01ac5d28a6a232904/accesspoint/access-point-name,\\n\"Action\": [\\n\"s3-outposts:*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '[\\n{\\n\"Classification\": \"core-site\",\\n\"Properties\": {\\n\"fs.s3a.bucket.DOC-EXAMPLE-BUCKET.accesspoint.arn\": \"arn:aws:s3-outposts:us-\\nwest-2:111122223333:outpost/op-01ac5d28a6a232904/accesspoint/access-point-name\"', '']]]\n",
      "[[['', '\"fs.s3a.committer.name\": \"magic\",\\n\"fs.s3a.select.enabled\": \"false\"\\n}\\n},\\n{\\n\"Classification\": \"hadoop-env\",\\n\"Configurations\": [\\n{\\n\"Classification\": \"export\",\\n\"Properties\": {\\n\"JAVA_HOME\": \"/usr/lib/jvm/java-11-amazon-corretto.x86_64\"\\n}\\n}\\n],\\n\"Properties\": {}\\n},\\n{\\n\"Classification\": \"spark-env\",\\n\"Configurations\": [\\n{\\n\"Classification\": \"export\",\\n\"Properties\": {\\n\"JAVA_HOME\": \"/usr/lib/jvm/java-11-amazon-corretto.x86_64\"\\n}\\n}\\n],\\n\"Properties\": {}\\n},\\n{\\n\"Classification\": \"spark-defaults\",\\n\"Properties\": {\\n\"spark.executorEnv.JAVA_HOME\": \"/usr/lib/jvm/java-11-amazon-\\ncorretto.x86_64\",\\n\"spark.sql.sources.fastS3PartitionDiscovery.enabled\": \"false\"\\n}\\n}\\n]', ''], ['', '', '']]]\n",
      "[[['Parameter', 'Default value', 'Required value for\\nS3 on Outposts', 'Explanation'], ['fs.s3a.aw\\ns.credent\\nials.provider', 'If not specified, S3A\\nwill look for S3 in\\nRegion bucket with\\nthe Outposts bucket\\nname.', 'The access point\\nARN of the S3 on\\nOutposts bucket', 'Amazon S3 on\\nOutposts supports\\nvirtual private cloud\\n(VPC)-only access\\npoints as the only\\nmeans to access your\\nOutposts buckets.'], ['fs.s3a.co\\nmmitter.name', 'file', 'magic', 'Magic committer is\\nthe only supported\\ncommitter for S3 on\\nOutposts.'], ['fs.s3a.se\\nlect.enabled', 'TRUE', 'FALSE', 'S3 Select is not\\nsupported on\\nOutposts.'], ['JAVA_HOME', '/usr/lib/jvm/\\njava-8', '/usr/lib/jvm/\\njava-11-amazon\\n-corretto\\n.x86_64', 'S3 on Outposts on\\nS3A requires Java\\nversion 11.']]]\n",
      "[[['Parameter', 'Default value', 'Required value for\\nS3 on Outposts', 'Explanation'], ['spark.sql\\n.sources.\\nfastS3Par\\ntitionDis\\ncovery.enabled', 'TRUE', 'FALSE', \"S3 on Outposts\\ndoesn't support fast\\npartition.\"], ['spark.exe\\ncutorEnv.\\nJAVA_HOME', '/usr/lib/jvm/\\njava-8', '/usr/lib/jvm/\\njava-11-amazon\\n-corretto\\n.x86_64', 'S3 on Outposts on\\nS3A requires Java\\nversion 11.']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nS3 on Outposts doesn't support server-side encryption with AWS Key Management Service\\n(AWS KMS) keys (SSE-KMS).\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nS3 on Outposts interface endpoints are resolved from the public DNS domain. S3 on\\nOutposts does not support private DNS. Use the --endpoint-url parameter for all\\nbucket and endpoint management APIs.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control list-regional-buckets --region us-east-1 --endpoint-url\\nhttps://vpce-1a2b3c4d-5e6f.s3-outposts.us-east-1.vpce.amazonaws.com --account-\\nid 111122223333', ''], ['', '', '']], [['', '', ''], ['', \"control_client = session.client(\\nservice_name='s3control',\\nregion_name='us-east-1',\\nendpoint_url='https://vpce-1a2b3c4d-5e6f.s3-outposts.us-east-1.vpce.amazonaws.com'\\n)\", ''], ['', '', '']], [['', '', ''], ['', '// control client\\nRegion region = Region.US_EAST_1;\\ns3ControlClient = S3ControlClient.builder().region(region)\\n.endpointOverride(URI.create(\"https://vpce-1a2b3c4d-5e6f.s3-outposts.us-\\neast-1.vpce.amazonaws.com\"))\\n.build()', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\n• When you apply the example policies for VPC endpoints described in this section, you\\nmight block your access to the bucket without intending to do so. Bucket permissions', '']]]\n",
      "[[['', \"that limit bucket access to connections originating from your VPC endpoint can block\\nall connections to the bucket. For information about how to fix this issue, see My bucket\\npolicy has the wrong VPC or VPC endpoint ID. How can I fix the policy so that I can access\\nthe bucket? in the AWS Support Knowledge Center.\\n• Before using the following example bucket policies, replace the VPC endpoint ID with an\\nappropriate value for your use case. Otherwise, you won't be able to access your bucket.\\n• If your policy only allows access to an S3 on Outposts bucket from a specific VPC\\nendpoint, it disables console access for that bucket because console requests don't\\noriginate from the specified VPC endpoint.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"Policy1415115909151\",\\n\"Statement\": [\\n{ \"Sid\": \"Access-to-specific-bucket-only\",\\n\"Principal\": {\"AWS\":\"111122223333\"},\\n\"Action\": \"s3-outposts:GetBucketPolicy\",\\n\"Effect\": \"Allow\",\\n\"Resource\": \"arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outpost-\\nbucket\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"Policy1415115909152\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Deny-access-to-specific-VPCE\",\\n\"Principal\": {\"AWS\":\"111122223333\"},\\n\"Action\": \"s3-outposts:GetBucketPolicy\",\\n\"Effect\": \"Deny\",\\n\"Resource\": \"arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outpost-\\nbucket\",\\n\"Condition\": {\\n\"StringEquals\": {\"aws:sourceVpce\": \"vpce-1a2b3c4d\"}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['Applicable keys', 'Description'], ['s3-outpos\\nts:authType', 'S3 on Outposts supports various methods of authentication. To restrict\\nincoming requests to use a specific authentication method, you can use\\nthis optional condition key. For example, you can use this condition key\\nto allow only the HTTP Authorization header to be used in request\\nauthentication.\\nValid values:\\nREST-HEADER\\nREST-QUERY-STRING'], ['s3-outpos\\nts:signatur\\neAge', 'The length of time, in milliseconds, that a signature is valid in an\\nauthenticated request.\\nThis condition works only for presigned URLs.\\nIn Signature Version 4, the signing key is valid for up to seven days.\\nTherefore, the signatures are also valid for up to seven days. For more\\ninformation, see Introduction to signing requests in the Amazon Simple\\nStorage Service API Reference. You can use this condition to further limit\\nthe signature age.\\nExample value: 600000'], ['s3-outposts:x-\\namz-content-\\nsha256', 'You can use this condition key to disallow unsigned content in your\\nbucket.\\nWhen you use Signature Version 4, for requests that use the\\nAuthorization header, you add the x-amz-content-sha256\\nheader in the signature calculation and then set its value to the hash\\npayload.\\nYou can use this condition key in your bucket policy to deny any uploads\\nwhere the payloads are not signed. For example:']]]\n",
      "[[['Applicable keys', 'Description'], ['', \"• Deny uploads that use the Authorization header to authentic\\nate requests but don't sign the payload. For more information, see\\nTransferring payload in a single chunk in the Amazon Simple Storage\\nService API Reference.\\n• Deny uploads that use presigned URLs. Presigned URLs always have\\nan UNSIGNED_PAYLOAD . For more information, see Authenticating\\nrequests and Authentication methods in the Amazon Simple Storage\\nService API Reference.\\nValid value: UNSIGNED-PAYLOAD\"]], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Deny a presigned URL request if the signature is more than 10\\nminutes old\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {\"AWS\":\"444455556666\"},\\n\"Action\": \"s3-outposts:*\",\\n\"Resource\": \"arn:aws:s3-outposts:us-\\neast-1:111122223333:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket/object/\\n*\",\\n\"Condition\": {\\n\"NumericGreaterThan\": {\"s3-outposts:signatureAge\": 600000},\\n\"StringEquals\": {\"s3-outposts:authType\": \"REST-QUERY-STRING\"}\\n}', '']]]\n",
      "[[['', '}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Allow only requests that use the Authorization header for\\nrequest authentication. Deny presigned URL requests.\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {\"AWS\":\"111122223333\"},\\n\"Action\": \"s3-outposts:*\",\\n\"Resource\": \"arn:aws:s3-outposts:us-\\neast-1:111122223333:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket/object/\\n*\",\\n\"Condition\": {\\n\"StringNotEquals\": {\\n\"s3-outposts:authType\": \"REST-HEADER\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{', '']]]\n",
      "[[['', '\"Sid\": \"Deny uploads with unsigned payloads.\",\\n\"Effect\": \"Deny\",\\n\"Principal\": {\"AWS\":\"111122223333\"},\\n\"Action\": \"s3-outposts:*\",\\n\"Resource\": \"arn:aws:s3-outposts:us-\\neast-1:111122223333:outpost/op-01ac5d28a6a232904/bucket/example-outpost-bucket/object/\\n*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"s3-outposts:x-amz-content-sha256\": \"UNSIGNED-PAYLOAD\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['Change', 'Description', 'Date'], ['S3 on Outposts added\\nAWSS3OnOutpostsSer\\nviceRolePolicy', 'S3 on Outposts added\\nAWSS3OnOutpostsSer\\nviceRolePolicy as part\\nof the service-linked role\\nAWSServiceRoleForS\\n3OnOutposts , which helps\\nmanage network resources\\nfor you.', 'October 3, 2023'], ['S3 on Outposts started\\ntracking changes', 'S3 on Outposts started\\ntracking changes for its AWS\\nmanaged policies.', 'October 3, 2023']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ec2:DescribeSubnets\",\\n\"ec2:DescribeSecurityGroups\",\\n\"ec2:DescribeNetworkInterfaces\",\\n\"ec2:DescribeVpcs\",\\n\"ec2:DescribeCoipPools\",\\n\"ec2:GetCoipPoolUsage\",\\n\"ec2:DescribeAddresses\",\\n\"ec2:DescribeLocalGatewayRouteTableVpcAssociations\"\\n],\\n\"Resource\": \"*\",\\n\"Sid\": \"DescribeVpcResources\"\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ec2:CreateNetworkInterface\"\\n],\\n\"Resource\": [\\n\"arn:aws:ec2:*:*:subnet/*\",\\n\"arn:aws:ec2:*:*:security-group/*\"', '']]]\n",
      "[[['', '],\\n\"Sid\": \"CreateNetworkInterface\"\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ec2:CreateNetworkInterface\"\\n],\\n\"Resource\": [\\n\"arn:aws:ec2:*:*:network-interface/*\"\\n],\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:RequestTag/CreatedBy\": \"S3 On Outposts\"\\n}\\n},\\n\"Sid\": \"CreateTagsForCreateNetworkInterface\"\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ec2:AllocateAddress\"\\n],\\n\"Resource\": [\\n\"arn:aws:ec2:*:*:ipv4pool-ec2/*\"\\n],\\n\"Sid\": \"AllocateIpAddress\"\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ec2:AllocateAddress\"\\n],\\n\"Resource\": [\\n\"arn:aws:ec2:*:*:elastic-ip/*\"\\n],\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:RequestTag/CreatedBy\": \"S3 On Outposts\"\\n}\\n},\\n\"Sid\": \"CreateTagsForAllocateIpAddress\"\\n},\\n{', '']]]\n",
      "[[['', '\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ec2:ModifyNetworkInterfaceAttribute\",\\n\"ec2:CreateNetworkInterfacePermission\",\\n\"ec2:DeleteNetworkInterface\",\\n\"ec2:DeleteNetworkInterfacePermission\",\\n\"ec2:DisassociateAddress\",\\n\"ec2:ReleaseAddress\",\\n\"ec2:AssociateAddress\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"aws:ResourceTag/CreatedBy\": \"S3 On Outposts\"\\n}\\n},\\n\"Sid\": \"ReleaseVpcResources\"\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"ec2:CreateTags\"\\n],\\n\"Resource\": \"*\",\\n\"Condition\": {\\n\"StringEquals\": {\\n\"ec2:CreateAction\": [\\n\"CreateNetworkInterface\",\\n\"AllocateAddress\"\\n],\\n\"aws:RequestTag/CreatedBy\": [\\n\"S3 On Outposts\"\\n]\\n}\\n},\\n\"Sid\": \"CreateTags\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nIf the S3 on Outposts service is using the role when you try to delete the resources, then\\nthe deletion might fail. If that happens, wait for a few minutes and try the operation again.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nThe AWS account that creates the bucket owns it and is the only one that can commit\\nactions to it. Buckets have configuration properties, such as Outpost, tag, default\\nencryption, and access point settings. The access point settings include the virtual private\\ncloud (VPC), the access point policy for accessing the objects in the bucket, and other\\nmetadata. For more information, see S3 on Outposts specifications.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control put-bucket-versioning --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket --versioning-configuration Status=Enabled', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-bucket-versioning --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/op-01ac5d28a6a232904/bucket/example-outposts-\\nbucket --versioning-configuration Status=Suspended', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe AWS account that creates the bucket owns it and is the only one that can create,\\nenable, disable, or delete a lifecycle rule.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe AWS account that creates the bucket owns it and is the only one that can create,\\nenable, disable, or delete a lifecycle rule.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nSize-based filters can't be used with delete markers and incomplete multipart uploads.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nSize-based filters can't be used with delete markers and incomplete multipart uploads.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThe AWS account that creates the bucket owns it and is the only one that can create,\\nenable, disable, or delete a lifecycle rule.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Rules\": [\\n{\\n\"ID\": \"id-1\",\\n\"Filter\": {', '']]]\n",
      "[[['', '\"And\": {\\n\"Prefix\": \"myprefix\",\\n\"Tags\": [\\n{\\n\"Value\": \"mytagvalue1\",\\n\"Key\": \"mytagkey1\"\\n},\\n{\\n\"Value\": \"mytagvalue2\",\\n\"Key\": \"mytagkey2\"\\n}\\n],\\n\"ObjectSizeGreaterThan\": 1000,\\n\"ObjectSizeLessThan\": 5000\\n}\\n},\\n\"Status\": \"Enabled\",\\n\"Expiration\": {\\n\"Days\": 10\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-bucket-lifecycle-configuration --account-id 123456789012 --\\nbucket arn:aws:s3-outposts:region:123456789012:outpost/op-01ac5d28a6a232904/\\nbucket/example-outposts-bucket --lifecycle-configuration file://lifecycle1.json', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic void putBucketLifecycleConfiguration(String bucketArn) {\\nS3Tag tag1 = new S3Tag().withKey(\"mytagkey1\").withValue(\"mytagkey1\");\\nS3Tag tag2 = new S3Tag().withKey(\"mytagkey2\").withValue(\"mytagkey2\");\\nLifecycleRuleFilter lifecycleRuleFilter = new LifecycleRuleFilter()\\n.withAnd(new LifecycleRuleAndOperator()\\n.withPrefix(\"myprefix\")\\n.withTags(tag1, tag2))\\n.withObjectSizeGreaterThan(1000)\\n.withObjectSizeLessThan(5000);\\nLifecycleExpiration lifecycleExpiration = new LifecycleExpiration()\\n.withExpiredObjectDeleteMarker(false)\\n.withDays(10);\\nLifecycleRule lifecycleRule = new LifecycleRule()\\n.withStatus(\"Enabled\")\\n.withFilter(lifecycleRuleFilter)\\n.withExpiration(lifecycleExpiration)\\n.withID(\"id-1\");\\nLifecycleConfiguration lifecycleConfiguration = new LifecycleConfiguration()\\n.withRules(lifecycleRule);\\nPutBucketLifecycleConfigurationRequest reqPutBucketLifecycle = new\\nPutBucketLifecycleConfigurationRequest()\\n.withAccountId(AccountId)\\n.withBucket(bucketArn)\\n.withLifecycleConfiguration(lifecycleConfiguration);\\nPutBucketLifecycleConfigurationResult respPutBucketLifecycle =\\ns3ControlClient.putBucketLifecycleConfiguration(reqPutBucketLifecycle);\\nSystem.out.printf(\"PutBucketLifecycleConfiguration Response: %s%n\",\\nrespPutBucketLifecycle.toString());\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control get-bucket-lifecycle-configuration --account-id 123456789012 --bucket\\narn:aws:s3-outposts:<your-region>:123456789012:outpost/op-01ac5d28a6a232904/\\nbucket/example-outposts-bucket', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.services.s3control.model.*;\\npublic void getBucketLifecycleConfiguration(String bucketArn) {\\nGetBucketLifecycleConfigurationRequest reqGetBucketLifecycle = new\\nGetBucketLifecycleConfigurationRequest()\\n.withAccountId(AccountId)\\n.withBucket(bucketArn);\\nGetBucketLifecycleConfigurationResult respGetBucketLifecycle =\\ns3ControlClient.getBucketLifecycleConfiguration(reqGetBucketLifecycle);\\nSystem.out.printf(\"GetBucketLifecycleConfiguration Response: %s%n\",\\nrespGetBucketLifecycle.toString());\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<ReplicationConfiguration>\\n<Role>IAM-role-ARN</Role>\\n<Rule>\\n...\\n</Rule>\\n<Rule>\\n...\\n</Rule>\\n...\\n</ReplicationConfiguration>', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nObjects that existed in your bucket before you set up a replication rule aren\\'t replicated\\nautomatically. In other words, Amazon S3 on Outposts doesn\\'t replicate objects\\nretroactively. To replicate objects that were created before your replication configuration,\\nyou can use the CopyObject API operation to copy them to the same bucket. After\\nthe objects are copied, they appear as \"new\" objects in the bucket and the replication\\nconfiguration will apply to them. For more information about copying an object, see\\nCopying an object in an Amazon S3 on Outposts bucket using the AWS SDK for Java and\\nCopyObject in the Amazon Simple Storage Service API Reference.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\":\"s3-outposts.amazonaws.com\"\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3-outposts:GetObjectVersionForReplication\",\\n\"s3-outposts:GetObjectVersionTagging\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3-outposts:region:123456789012:outpost/SOURCE-OUTPOST-ID/\\nbucket/SOURCE-OUTPOSTS-BUCKET/object/*\",\\n\"arn:aws:s3-outposts:region:123456789012:outpost/SOURCE-OUTPOST-ID/\\naccesspoint/SOURCE-OUTPOSTS-BUCKET-ACCESS-POINT/object/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3-outposts:ReplicateObject\",\\n\"s3-outposts:ReplicateDelete\"', '']]]\n",
      "[[['', '],\\n\"Resource\":[\\n\"arn:aws:s3-outposts:region:123456789012:outpost/DESTINATION-OUTPOST-ID/\\nbucket/DESTINATION-OUTPOSTS-BUCKET/object/*\",\\n\"arn:aws:s3-outposts:region:123456789012:outpost/DESTINATION-OUTPOST-ID/\\naccesspoint/DESTINATION-OUTPOSTS-BUCKET-ACCESS-POINT/object/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\n• Permission for the s3-outposts:ReplicateObject action on the DESTINATION-\\nOUTPOSTS-BUCKET bucket (the destination bucket) also allows replication of\\nobject tags. Therefore, you don't need to explicitly grant permission for the s3-\\noutposts:ReplicateTags action.\\n• For cross-account replication, the owner of the destination Outposts\\nbucket must update its bucket policy to grant permission for the s3-\\noutposts:ReplicateObject action on theDESTINATION-OUTPOSTS-BUCKET.\\nThe s3-outposts:ReplicateObject action allows S3 on Outposts to replicate\\nobjects and object tags to the destination Outposts bucket.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Important\\nThe AWS account that owns the IAM role must have permissions for the actions that it\\ngrants to the IAM role.\\nFor example, suppose that the source Outposts bucket contains objects owned by\\nanother AWS account. The owner of the objects must explicitly grant the AWS account\\nthat owns the IAM role the required permissions through the bucket policy and the access\\npoint policy. Otherwise, S3 on Outposts can't access the objects, and replication of the\\nobjects fails.\\nThe permissions described here are related to the minimum replication configuration.\\nIf you choose to add optional replication configurations, you must grant additional\\npermissions to S3 on Outposts.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Id\":\"PolicyForDestinationBucket\",\\n\"Statement\":[\\n{\\n\"Sid\":\"Permissions on objects\",\\n\"Effect\":\"Allow\",\\n\"Principal\":{', '']]]\n",
      "[[['', '\"AWS\":\"arn:aws:iam::SourceBucket-account-ID:role/service-role/source-\\naccount-IAM-role\"\\n},\\n\"Action\":[\\n\"s3-outposts:ReplicateDelete\",\\n\"s3-outposts:ReplicateObject\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3-outposts:region:DestinationBucket-account-\\nID:outpost/DESTINATION-OUTPOST-ID/bucket/DESTINATION-OUTPOSTS-BUCKET/object/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Id\":\"PolicyForDestinationAccessPoint\",\\n\"Statement\":[\\n{\\n\"Sid\":\"Permissions on objects\",\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"AWS\":\"arn:aws:iam::SourceBucket-account-ID:role/service-role/source-\\naccount-IAM-role\"\\n},\\n\"Action\":[\\n\"s3-outposts:ReplicateDelete\",\\n\"s3-outposts:ReplicateObject\"\\n],\\n\"Resource\" :[\\n\"arn:aws:s3-outposts:region:DestinationBucket-account-\\nID:outpost/DESTINATION-OUTPOST-ID/accesspoint/DESTINATION-OUTPOSTS-BUCKET-ACCESS-POINT/\\nobject/*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf objects in the source Outposts bucket are tagged, note the following:', '']]]\n",
      "[[['', 'If the source Outposts bucket owner grants S3 on Outposts permission for the s3-\\noutposts:GetObjectVersionTagging and s3-outposts:ReplicateTags actions to\\nreplicate object tags (through the IAM role), Amazon S3 replicates the tags along with the\\nobjects. For information about the IAM role, see Creating an IAM role.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nObjects that existed in the source Outposts bucket before you set up replication rules\\naren\\'t replicated. In other words, S3 on Outposts doesn\\'t replicate objects retroactively. To\\nreplicate objects that were created before your replication configuration, you can use the\\nCopyObject API operation to copy them to the same bucket. After the objects are copied,\\nthey appear as \"new\" objects in the bucket and the replication configuration will apply to\\nthem. For more information about copying an object, see Copying an object in an Amazon\\nS3 on Outposts bucket using the AWS SDK for Java and CopyObject in the Amazon Simple\\nStorage Service API Reference.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf versioning is not enabled on the destination Outposts bucket, you get a warning that\\ncontains an Enable versioning button. Choose this button to enable versioning on the\\nbucket.', ''], ['', '', '']], [['', '', ''], ['', 'Important\\nWhen you add a replication rule to an S3 on Outposts bucket, you must have the\\niam:CreateRole and iam:PassRole permissions to be able to create and pass the\\nIAM role that grants S3 on Outposts replication permissions. For more information, see\\nGranting a user permissions to pass a role to an AWS service in the IAM User Guide.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Important\\nThe profile that you use for this exercise must have the necessary permissions. For\\nexample, in the replication configuration, you specify the IAM service role that S3\\non Outposts can assume. You can do this only if the profile that you use has the\\niam:CreateRole and iam:PassRole permissions. For more information, see\\nGranting a user permissions to pass a role to an AWS service in the IAM User Guide.\\nIf you use administrator credentials to create a named profile, the named profile will\\nhave the necessary permission to perform all the tasks.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-bucket --bucket SOURCE-OUTPOSTS-BUCKET --outpost-id SOURCE-\\nOUTPOST-ID --profile acctA --region us-east-1', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-bucket-versioning --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/SOURCE-OUTPOST-ID/bucket/SOURCE-OUTPOSTS-\\nBUCKET --versioning-configuration Status=Enabled --profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo set up a replication configuration when both the source and destination Outposts\\nbuckets are in the same AWS account, you use the same named profile. This example\\nuses acctA. To test the replication configuration when the buckets are owned by\\ndifferent AWS accounts, you specify different profiles for each bucket.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-bucket --bucket DESTINATION-OUTPOSTS-BUCKET --create-bucket-\\nconfiguration LocationConstraint=us-west-2 --outpost-id DESTINATION-OUTPOST-ID --\\nprofile acctA --region us-west-2', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-bucket-versioning --account-id 123456789012 --bucket arn:aws:s3-\\noutposts:region:123456789012:outpost/DESTINATION-OUTPOST-ID/bucket/DESTINATION-\\nOUTPOSTS-BUCKET --versioning-configuration Status=Enabled --profile acctA', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Principal\":{\\n\"Service\":\"s3-outposts.amazonaws.com\"\\n},\\n\"Action\":\"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws iam create-role --role-name replicationRole --assume-role-policy-\\ndocument file://s3-on-outposts-role-trust-policy.json --profile acctA', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\":\"2012-10-17\",\\n\"Statement\":[\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3-outposts:GetObjectVersionForReplication\",\\n\"s3-outposts:GetObjectVersionTagging\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3-outposts:region:123456789012:outpost/SOURCE-\\nOUTPOST-ID/bucket/SOURCE-OUTPOSTS-BUCKET/object/*\",\\n\"arn:aws:s3-outposts:region:123456789012:outpost/SOURCE-\\nOUTPOST-ID/accesspoint/SOURCE-OUTPOSTS-BUCKET-ACCESS-POINT/object/*\"\\n]\\n},\\n{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3-outposts:ReplicateObject\",\\n\"s3-outposts:ReplicateDelete\"\\n],\\n\"Resource\":[\\n\"arn:aws:s3-outposts:region:123456789012:outpost/DESTINATION-\\nOUTPOST-ID/bucket/DESTINATION-OUTPOSTS-BUCKET/object/*\",\\n\"arn:aws:s3-outposts:region:123456789012:outpost/DESTINATION-\\nOUTPOST-ID/accesspoint/DESTINATION-OUTPOSTS-BUCKET-ACCESS-POINT/object/*\"\\n]\\n}\\n]', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', 'aws iam put-role-policy --role-name replicationRole --policy-\\ndocument file://s3-on-outposts-role-permissions-policy.json --policy-\\nname replicationRolePolicy --profile acctA', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Role\": \"IAM-role-ARN\",\\n\"Rules\": [\\n{\\n\"Status\": \"Enabled\",\\n\"Priority\": 1,\\n\"DeleteMarkerReplication\": { \"Status\": \"Disabled\" },\\n\"Filter\" : { \"Prefix\": \"Tax\"},\\n\"Destination\": {\\n\"Bucket\":\\n\"arn:aws:s3-outposts:region:123456789012:outpost/DESTINATION-OUTPOST-\\nID/accesspoint/DESTINATION-OUTPOSTS-BUCKET-ACCESS-POINT\"\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control put-bucket-replication --account-id 123456789012 --\\nbucket arn:aws:s3-outposts:region:123456789012:outpost/SOURCE-OUTPOST-', '']]]\n",
      "[[['', 'ID/bucket/SOURCE-OUTPOSTS-BUCKET --replication-configuration file://\\nreplication.json --profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control get-bucket-replication --account-id 123456789012 --bucket\\narn:aws:s3-outposts:region:123456789012:outpost/SOURCE-OUTPOST-ID/\\nbucket/SOURCE-OUTPOSTS-BUCKET --profile acctA', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe amount of time that it takes for S3 on Outposts to replicate an object\\ndepends on the size of the object. For information about how to see the status\\nof replication, see Getting replication status information.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nS3 Replication on Outposts metrics are billed at the same rate as CloudWatch custom\\nmetrics. For more information, see CloudWatch pricing.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf object replication fails after you upload an object, you can't retry replication. You must\\nupload the object again. Objects transition to a FAILED state for issues such as missing\\nreplication role permissions or missing bucket permissions. For temporary failures, such\\nas if a bucket or your Outpost is unavailable, the replication status doesn't transition\\nto FAILED, but remains PENDING. After the resource is back online, S3 on Outposts\\nresumes replicating those objects.\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nBefore deleting an object from a source bucket that has replication enabled, check the\\nobject's replication status to ensure that the object has been replicated.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '\"s3-outposts:GetObjectVersionForReplication\",\\n\"s3-outposts:GetObjectVersionTagging\"', ''], ['', '', '']], [['', '', ''], ['', '\"s3-outposts:ReplicateObject\",\\n\"s3-outposts:ReplicateDelete\",\\n\"s3-outposts:ReplicateTags\"', ''], ['', '', '']]]\n",
      "[[['Event type', 'Description', 'Namespace'], ['Operation\\nFailedRep\\nlication', 'The replication of an object within a\\nreplication rule failed. For more informati\\non about S3 Replication on Outposts\\nfailure reasons, see Using EventBridge to\\nview S3 Replication on Outposts failure\\nreasons.', 's3-outposts']], [['Replication failure reason', 'Description'], ['AssumeRoleNotPermitted', \"S3 on Outposts can't assume the AWS\\nIdentity and Access Management (IAM) role\\nthat's specified in the replication configura\\ntion.\"], ['DstBucketNotFound', \"S3 on Outposts can't find the destination\\nbucket that's specified in the replication\\nconfiguration.\"], ['DstBucketUnversioned', \"Versioning isn't enabled on the Outposts\\ndestination bucket. To replicate objects with\"]]]\n",
      "[[['Replication failure reason', 'Description'], ['', 'S3 Replication on Outposts, you must enable\\nversioning on the destination bucket.'], ['DstDelObjNotPermitted', \"S3 on Outposts can't replicate deletes to\\nthe destination bucket. The s3-outpos\\nts:ReplicateDelete permission\\nmight be missing for the destination bucket.\"], ['DstMultipartCompleteNotPermitted', \"S3 on Outposts can't complete a multipart\\nupload of objects in the destination bucket.\\nThe s3-outposts:ReplicateObject\\npermission might be missing for the\\ndestination bucket.\"], ['DstMultipartInitNotPermitted', \"S3 on Outposts can't initiate a multipart\\nupload of objects to the destination bucket.\\nThe s3-outposts:ReplicateObject\\npermission might be missing for the\\ndestination bucket.\"], ['DstMultipartPartUploadNotPe\\nrmitted', \"S3 on Outposts can't upload multipart\\nobjects in the destination bucket. The\\ns3-outposts:ReplicateObject\\npermission might be missing for the\\ndestination bucket.\"], ['DstOutOfCapacity', \"S3 on Outposts can't replicate to the\\ndestination Outpost because the Outpost is\\nout of S3 storage capacity.\"], ['DstPutObjNotPermitted', \"S3 on Outposts can't replicate objects to\\nthe destination bucket. The s3-outpos\\nts:ReplicateObject permission\\nmight be missing for the destination bucket.\"]]]\n",
      "[[['Replication failure reason', 'Description'], ['DstPutTaggingNotPermitted', \"S3 on Outposts can't replicate object tags\\nto the destination bucket. The s3-outpos\\nts:ReplicateObject permission\\nmight be missing for the destination bucket.\"], ['DstVersionNotFound', \"S3 on Outposts can't find the required object\\nversion in the destination bucket in order to\\nreplicate that object version's metadata.\"], ['SrcBucketReplicationConfigMissing', \"S3 on Outposts can't find a replication\\nconfiguration for the access point that's\\nassociated with the source Outposts bucket.\"], ['SrcGetObjNotPermitted', \"S3 on Outposts can't access the object\\nin the source bucket for replication. The\\ns3-outposts:GetObjectVersio\\nnForReplication permission might be\\nmissing for the source bucket.\"], ['SrcGetTaggingNotPermitted', \"S3 on Outposts can't access the object tag\\ninformation from the source bucket. The\\ns3-outposts:GetObjectVersio\\nnTagging permission might be missing for\\nthe source bucket.\"], ['SrcHeadObjectNotPermitted', \"S3 on Outposts can't retrieve object\\nmetadata from the source bucket. The\\ns3-outposts:GetObjectVersio\\nnForReplication permission might be\\nmissing for the source bucket.\"], ['SrcObjectNotEligible', \"The object isn't eligible for replication. The\\nobject or its object tags don't match the\\nreplication configuration.\"]]]\n",
      "[]\n",
      "[[['Option 1', 'Option 2'], ['S3 on Outposts\\nAllows the user to create buckets on your\\nOutposts and access points and to add\\nobjects to those buckets.\\nSubnets\\nAllows the user to use your virtual private\\ncloud (VPC) and the endpoints that are\\nassociated with your subnet.', 'S3 on Outposts\\nAllows the user to create buckets on your\\nOutposts and access points and to add\\nobjects to those buckets.\\nOutposts\\nAllows the user to see S3 capacity charts\\nand the AWS Outposts console home page.\\nAlso allows users to create subnets on shared\\nOutposts and create endpoints.']]]\n",
      "[[['Option 1', 'Option 2'], ['The Outpost ID\\nThe VPC ID\\nThe subnet ID\\nThe security group ID', 'The Outpost ID']], [['', '', ''], ['', 'Note\\nThe user can confirm that the resources have been shared with them by using the AWS\\nRAM console, the AWS Command Line Interface (AWS CLI), AWS SDKs, or REST API. The\\nuser can view their existing resource shares by using the get-resource-shares CLI command.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3control create-bucket --bucket example-s3-bucket1 --outpost-\\nid op-01ac5d28a6a232904', ''], ['', '', '']], [['Parameter', 'Value'], ['Account ID', '111122223333'], ['Access point name', 'example-outpost-access-point'], ['Outpost ID', 'op-01ac5d28a6a232904'], ['Outpost bucket name', 'example-s3-bucket1'], ['VPC ID', 'vpc-1a2b3c4d5e6f7g8h9']], [['', '', ''], ['', 'Note\\nThe Account ID parameter must be the AWS account ID of the bucket owner, which is the\\nshared user.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3control create-access-point --account-id 111122223333 --name example-outpost-\\naccess-point \\\\\\n--bucket arn:aws:s3-outposts:us-east-1:111122223333:outpost/op-01ac5d28a6a232904/\\nbucket/example-s3-bucket1 \\\\\\n--vpc-configuration VpcId=vpc-1a2b3c4d5e6f7g8h9', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-object --bucket arn:aws:s3-outposts:us-\\neast-1:111122223333:outpost/op-01ac5d28a6a232904/accesspoint/example-outpost-access-\\npoint \\\\\\n--body my_image.jpg --key images/my_image.jpg', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf this operation results in a Resource not found error or is unresponsive, your VPC might\\nnot have a shared endpoint.\\nTo check whether there is a shared endpoint, use the list-shared-endpoints AWS CLI\\ncommand. If there is no shared endpoint, work with the Outpost owner to create one.\\nFor more information, see ListSharedEndpoints in the Amazon Simple Storage Service API\\nReference.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe user can perform this operation only if the resource share includes the Outposts\\nresource.', ''], ['', '', '']], [['', '', ''], ['', 'aws s3outposts create-endpoint --outposts-id op-01ac5d28a6a232904 --subnet-id XXXXXX --\\nsecurity-group-id XXXXXXX', ''], ['', '', '']]]\n",
      "[[['AWS service', 'Description', 'Learn more'], ['Amazon S3', 'All direct S3 on Outposts usage has a matching account\\nand bucket CloudWatch metric.', 'See metrics'], ['Amazon Elastic\\nBlock Store\\n(Amazon EBS)', 'For Amazon EBS on Outposts, you can choose an AWS\\nOutpost as your snapshot destination and store locally\\nin your S3 on Outpost.', 'Learn more'], ['Amazon\\nRelationa\\nl Database\\nService (Amazon\\nRDS)', 'You can use Amazon RDS local backups to store your\\nRDS backups locally on your Outpost.', 'Learn more']]]\n",
      "[[['', '', ''], ['', 'Note\\nS3 on Outposts supports only the following metrics, and no other Amazon S3 metrics.\\nBecause S3 on Outposts has a fixed capacity limit, we recommend creating CloudWatch\\nalarms to notify you when your storage utilization exceeds a certain threshold.', ''], ['', '', '']]]\n",
      "[[['Metric', 'Description', 'Time\\nPeriod', 'Units', 'Type'], ['Outpost\\ntalByte', 'ToT he total provisioned capacity\\nsin bytes for an Outpost.', '5\\nminutes', 'Bytes', 'S3 on Outposts'], ['Outpost\\neeBytes', 'FrT he count of free bytes\\navailable on an Outpost to\\nstore customer data.', '5\\nminutes', 'Bytes', 'S3 on Outposts'], ['BucketU\\ndBytes', 'seT he total size of all objects\\nfor the given bucket.', '5\\nminutes', 'Bytes', 'S3 on Outposts. Direct S3\\nusage only.'], ['Account\\nedBytes', 'UsT he total size of all objects\\nfor the specified Outposts\\naccount.', '5\\nminutes', 'Bytes', 'S3 on Outposts. Direct S3\\nusage only.'], ['BytesPe\\ningRepl\\nation', 'ndT he total number of bytes\\nico f objects that are pending\\nreplication for a given\\nreplication rule. For more\\ninformation about how to\\nenable replication metrics,\\nsee Creating replication rules\\nbetween Outposts.', '5\\nminutes', 'Bytes', 'Optional. For S3 Replication\\non Outposts.'], ['Operati\\nsPendin\\neplicat\\nn', 'onT he total number of\\ngRo perations that are pending\\niore plication for a given\\nreplication rule. For more\\ninformation about how to\\nenable replication metrics,\\nsee Creating replication rules\\nbetween Outposts.', '5\\nminutes', 'Counts', 'Optional. For S3 Replication\\non Outposts.'], ['Replica\\nonLaten', 'tiT he current number of\\ncyseconds delay by which the\\nreplication destination bucket', '5\\nminutes', 'Seconds', 'Optional. For S3 Replication\\non Outposts.']]]\n",
      "[[['Metric', 'Description', 'Time\\nPeriod', 'Units', 'Type'], ['', 'is behind the source bucket\\nfor a given replication rule.\\nFor more information about\\nhow to enable replication\\nmetrics, see Creating replicati\\non rules between Outposts.', '', '', '']], [['', '', ''], ['', 'Note\\nFor S3 on Outposts object operations, AWS API call events sent by CloudTrail will match\\nyour rules only if you have trails (optionally with event selectors) configured to receive\\nthose events. For more information, see Working with CloudTrail log files in the AWS\\nCloudTrail User Guide.', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"source\": [\\n\"aws.s3-outposts\"', '']]]\n",
      "[[['', '],\\n\"detail-type\": [\\n\"AWS API call through CloudTrail\"\\n],\\n\"detail\": {\\n\"eventSource\": [\\n\"s3-outposts.amazonaws.com\"\\n],\\n\"eventName\": [\\n\"DeleteObject\"\\n],\\n\"requestParameters\": {\\n\"bucketName\": [\\n\"example-s3-bucket1\"\\n]\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\n• It's a best practice to create a lifecycle policy for your AWS CloudTrail data event\\nOutposts bucket. Configure the lifecycle policy to periodically remove log files after the\\nperiod of time that you need to audit them. Doing so reduces the amount of data that\\nAmazon Athena analyzes for each query. For more information, see Setting a lifecycle\\nconfiguration on a bucket.\\n• For examples of how to query CloudTrail logs, see the AWS Big Data Blog post Analyze\\nSecurity, Compliance, and Operational Activity Using AWS CloudTrail and Amazon\\nAthena.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nAdditional charges apply for data events. For more information, see AWS CloudTrail pricing.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThe AWS account that creates the bucket owns it and is the only one that can configure S3\\non Outposts data events to be sent to AWS CloudTrail.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\n• When you create a trail and configure data event logging for S3 on Outposts, you\\nmust specify the data event type correctly.\\n• If you use the CloudTrail console, choose S3 Outposts for Data event type. For\\ninformation about how to create trails in the CloudTrail console, see Creating and\\nupdating a trail with the console in the AWS CloudTrail User Guide. For information\\nabout how to configure S3 on Outposts data event logging in the CloudTrail\\nconsole, see Logging data events for Amazon S3 Objects in the AWS CloudTrail\\nUser Guide.\\n• If you use the AWS Command Line Interface (AWS CLI) or the AWS SDKs, set the\\nresources.type field to AWS::S3Outposts::Object. For more information\\nabout how to log S3 on Outposts data events with the AWS CLI, see Log S3 on\\nOutposts events in the AWS CloudTrail User Guide.', '']]]\n",
      "[[['', '• If you use the CloudTrail console or the Amazon S3 console to configure a trail to\\nlog data events for an S3 on Outposts bucket, the Amazon S3 console shows that\\nobject-level logging is enabled for the bucket.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.auth.AWSStaticCredentialsProvider;\\nimport com.amazonaws.auth.BasicAWSCredentials;\\nimport com.amazonaws.services.s3control.AWSS3Control;\\nimport com.amazonaws.services.s3control.AWSS3ControlClient;\\npublic AWSS3Control createS3ControlClient() {\\nString accessKey = AWSAccessKey;\\nString secretKey = SecretAccessKey;\\nBasicAWSCredentials awsCreds = new BasicAWSCredentials(accessKey, secretKey);\\nreturn AWSS3ControlClient.builder().enableUseArnRegion()\\n.withCredentials(new AWSStaticCredentialsProvider(awsCreds))', '']]]\n",
      "[[['', '.build();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nS3 on Outposts object actions (such as PutObject or GetObject) aren’t supported over\\nIPv6 networks.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThis requirement only applies to S3 on Outposts bucket operations and control plane\\nresources across IPv6 networks. Amazon S3 on Outposts object actions aren’t supported\\nacross IPv6 networks.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"IPAllow\",\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3outposts:*\",\\n\"Resource\": \"arn:aws:s3-outposts:region:111122223333:outpost/OUTPOSTS-ID/\\nbucket/DOC-EXAMPLE-BUCKET/*\",\\n\"Condition\": {\\n\"IpAddress\": {\"aws:SourceIp\": \"54.240.143.0/24\"}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '\"Condition\": {\\n\"IpAddress\": {\\n\"aws:SourceIp\": [\\n\"54.240.143.0/24\",\\n\"2001:DB8:1234:5678::/64\"\\n]\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'dig s3-outposts.us-west-2.api.aws AAAA +short', ''], ['', '', '']], [['', '', ''], ['', 'dig s3-outposts.us-west-2.api.aws AAAA +short\\n2600:1f14:2588:4800:b3a9:1460:159f:ebce\\n2600:1f14:2588:4802:6df6:c1fd:ef8a:fc76\\n2600:1f14:2588:4801:d802:8ccf:4e04:817', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo update the supported IP address type from IPv4 to IPv6, see Modify the supported IP\\naddress type in the AWS PrivateLink User Guide.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": \"s3-outposts:*\",\\n\"Resource\": \"*\",\\n\"Principal\": \"*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nTo enable the IPv6 network on your VPC endpoint, you must have IPv6 set for the\\nSupportedIpAddressType filter for S3 on Outposts.', ''], ['', '', '']], [['', '', ''], ['', 'aws ec2 create-vpc-endpoint \\\\\\n--vpc-id vpc-12345678 \\\\\\n--vpc-endpoint-type Interface \\\\\\n--service-name com.amazonaws.us-east-1.s3-outposts \\\\\\n--subnet-id subnet-12345678 \\\\', '']]]\n",
      "[[['', '--security-group-id sg-12345678 \\\\\\n--ip-address-type dualstack \\\\\\n--dns-options \"DnsRecordIpType=dualstack\"', ''], ['', '', '']], [['', '', ''], ['', 'aws ec2 modify-vpc-endpoint \\\\\\n--vpc-endpoint-id vpce-12345678 \\\\\\n--add-subnet-ids subnet-12345678 \\\\\\n--remove-subnet-ids subnet-12345678 \\\\\\n--ip-address-type dualstack \\\\\\n--dns-options \"DnsRecordIpType=dualstack\"', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 's3-outposts-fips.region.api.aws', ''], ['', '', '']], [['', '', ''], ['', 's3-outposts.region.api.aws', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nVirtual hosted-style endpoint names aren't supported in S3 on Outposts.\", ''], ['', '', '']], [['', '', ''], ['', '$ aws s3control list-regional-buckets --endpoint-url https://s3-\\noutposts.region.api.aws', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3control.S3ControlClient;\\nimport software.amazon.awssdk.services.s3control.model.ListRegionalBucketsRequest;\\nimport software.amazon.awssdk.services.s3control.model.ListRegionalBucketsResponse;\\nimport software.amazon.awssdk.services.s3control.model.S3ControlException;\\npublic class DualStackEndpointsExample1 {\\npublic static void main(String[] args) {\\nRegion clientRegion = Region.of(\"us-east-1\");\\nString accountId = \"111122223333\";\\nString navyId = \"9876543210\";\\ntry {\\n// Create an S3ControlClient with dual-stack endpoints enabled.\\nS3ControlClient s3ControlClient = S3ControlClient.builder()\\n.region(clientRegion)\\n.dualstackEnabled(true)\\n.build();\\nListRegionalBucketsRequest listRegionalBucketsRequest =\\nListRegionalBucketsRequest.builder()\\n.accountId(accountId)\\n.outpostId(navyId)\\n.build();', '']]]\n",
      "[[['', 'ListRegionalBucketsResponse listBuckets =\\ns3ControlClient.listRegionalBuckets(listRegionalBucketsRequest);\\nSystem.out.printf(\"ListRegionalBuckets Response: %s%n\",\\nlistBuckets.toString());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 on Outposts\\ncouldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n}\\ncatch (S3ControlException e) {\\n// Unknown exceptions will be thrown as an instance of this type.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 on Outposts couldn\\'t be contacted for a response, or the\\nclient\\n// couldn\\'t parse the response from Amazon S3 on Outposts.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import com.amazonaws.AmazonServiceException;\\nimport com.amazonaws.SdkClientException;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3outposts.S3OutpostsClient;\\nimport software.amazon.awssdk.services.s3outposts.model.ListEndpointsRequest;\\nimport software.amazon.awssdk.services.s3outposts.model.ListEndpointsResponse;\\nimport software.amazon.awssdk.services.s3outposts.model.S3OutpostsException;\\npublic class DualStackEndpointsExample2 {\\npublic static void main(String[] args) {\\nRegion clientRegion = Region.of(\"us-east-1\");\\ntry {\\n// Create an S3OutpostsClient with dual-stack endpoints enabled.\\nS3OutpostsClient s3OutpostsClient = S3OutpostsClient.builder()\\n.region(clientRegion)', '']]]\n",
      "[[['', '.dualstackEnabled(true)\\n.build();\\nListEndpointsRequest listEndpointsRequest =\\nListEndpointsRequest.builder().build();\\nListEndpointsResponse listEndpoints =\\ns3OutpostsClient.listEndpoints(listEndpointsRequest);\\nSystem.out.printf(\"ListEndpoints Response: %s%n\",\\nlistEndpoints.toString());\\n} catch (AmazonServiceException e) {\\n// The call was transmitted successfully, but Amazon S3 on Outposts\\ncouldn\\'t process\\n// it, so it returned an error response.\\ne.printStackTrace();\\n}\\ncatch (S3OutpostsException e) {\\n// Unknown exceptions will be thrown as an instance of this type.\\ne.printStackTrace();\\n} catch (SdkClientException e) {\\n// Amazon S3 on Outposts couldn\\'t be contacted for a response, or the\\nclient\\n// couldn\\'t parse the response from Amazon S3 on Outposts.\\ne.printStackTrace();\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'java.net.preferIPv6Addresses=true', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '# Set the minimum required version of CMake for this project.\\ncmake_minimum_required(VERSION 3.13)\\n# Set the AWS service components used by this project.\\nset(SERVICE_COMPONENTS s3)', '']]]\n",
      "[[['', '# Set this project\\'s name.\\nproject(\"hello_s3\")\\n# Set the C++ standard to use to build this target.\\n# At least C++ 11 is required for the AWS SDK for C++.\\nset(CMAKE_CXX_STANDARD 11)\\n# Use the MSVC variable to determine if this is a Windows build.\\nset(WINDOWS_BUILD ${MSVC})\\nif (WINDOWS_BUILD) # Set the location where CMake can find the installed\\nlibraries for the AWS SDK.\\nstring(REPLACE \";\" \"/aws-cpp-sdk-all;\" SYSTEM_MODULE_PATH\\n\"${CMAKE_SYSTEM_PREFIX_PATH}/aws-cpp-sdk-all\")\\nlist(APPEND CMAKE_PREFIX_PATH ${SYSTEM_MODULE_PATH})\\nendif ()\\n# Find the AWS SDK for C++ package.\\nfind_package(AWSSDK REQUIRED COMPONENTS ${SERVICE_COMPONENTS})\\nif (WINDOWS_BUILD AND AWSSDK_INSTALL_AS_SHARED_LIBS)\\n# Copy relevant AWS SDK for C++ libraries into the current binary directory\\nfor running and debugging.\\n# set(BIN_SUB_DIR \"/Debug\") # if you are building from the command line you\\nmay need to uncomment this\\n# and set the proper subdirectory to the executables\\' location.\\nAWSSDK_CPY_DYN_LIBS(SERVICE_COMPONENTS \"\"\\n${CMAKE_CURRENT_BINARY_DIR}${BIN_SUB_DIR})\\nendif ()\\nadd_executable(${PROJECT_NAME}\\nhello_s3.cpp)\\ntarget_link_libraries(${PROJECT_NAME}\\n${AWSSDK_LINK_LIBRARIES})', ''], ['', '', '']], [['', '', ''], ['', '#include <aws/core/Aws.h>', '']]]\n",
      "[[['', '#include <aws/s3/S3Client.h>\\n#include <iostream>\\n#include <aws/core/auth/AWSCredentialsProviderChain.h>\\nusing namespace Aws;\\nusing namespace Aws::Auth;\\n/*\\n* A \"Hello S3\" starter application which initializes an Amazon Simple Storage\\nService (Amazon S3) client\\n* and lists the Amazon S3 buckets in the selected region.\\n*\\n* main function\\n*\\n* Usage: \\'hello_s3\\'\\n*\\n*/\\nint main(int argc, char **argv) {\\nAws::SDKOptions options;\\n// Optionally change the log level for debugging.\\n// options.loggingOptions.logLevel = Utils::Logging::LogLevel::Debug;\\nAws::InitAPI(options); // Should only be called once.\\nint result = 0;\\n{\\nAws::Client::ClientConfiguration clientConfig;\\n// Optional: Set to the AWS Region (overrides config file).\\n// clientConfig.region = \"us-east-1\";\\n// You don\\'t normally have to test that you are authenticated. But the\\nS3 service permits anonymous requests, thus the s3Client will return \"success\"\\nand 0 buckets even if you are unauthenticated, which can be confusing to a new\\nuser.\\nauto provider =\\nAws::MakeShared<DefaultAWSCredentialsProviderChain>(\"alloc-tag\");\\nauto creds = provider->GetAWSCredentials();\\nif (creds.IsEmpty()) {\\nstd::cerr << \"Failed authentication\" << std::endl;\\n}\\nAws::S3::S3Client s3Client(clientConfig);\\nauto outcome = s3Client.ListBuckets();\\nif (!outcome.IsSuccess()) {', '']]]\n",
      "[[['', 'std::cerr << \"Failed with error: \" << outcome.GetError() <<\\nstd::endl;\\nresult = 1;\\n} else {\\nstd::cout << \"Found \" << outcome.GetResult().GetBuckets().size()\\n<< \" buckets\\\\n\";\\nfor (auto &bucket: outcome.GetResult().GetBuckets()) {\\nstd::cout << bucket.GetName() << std::endl;\\n}\\n}\\n}\\nAws::ShutdownAPI(options); // Should only be called once.\\nreturn result;\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'package main\\nimport (\\n\"context\"\\n\"fmt\"\\n\"github.com/aws/aws-sdk-go-v2/config\"\\n\"github.com/aws/aws-sdk-go-v2/service/s3\"\\n)\\n// main uses the AWS SDK for Go V2 to create an Amazon Simple Storage Service\\n// (Amazon S3) client and list up to 10 buckets in your account.', '']]]\n",
      "[[['', '// This example uses the default settings specified in your shared credentials\\n// and config files.\\nfunc main() {\\nsdkConfig, err := config.LoadDefaultConfig(context.TODO())\\nif err != nil {\\nfmt.Println(\"Couldn\\'t load default configuration. Have you set up your AWS\\naccount?\")\\nfmt.Println(err)\\nreturn\\n}\\ns3Client := s3.NewFromConfig(sdkConfig)\\ncount := 10\\nfmt.Printf(\"Let\\'s list up to %v buckets for your account.\\\\n\", count)\\nresult, err := s3Client.ListBuckets(context.TODO(), &s3.ListBucketsInput{})\\nif err != nil {\\nfmt.Printf(\"Couldn\\'t list buckets for your account. Here\\'s why: %v\\\\n\", err)\\nreturn\\n}\\nif len(result.Buckets) == 0 {\\nfmt.Println(\"You don\\'t have any buckets!\")\\n} else {\\nif count > len(result.Buckets) {\\ncount = len(result.Buckets)\\n}\\nfor _, bucket := range result.Buckets[:count] {\\nfmt.Printf(\"\\\\t%v\\\\n\", *bucket.Name)\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.Bucket;\\nimport software.amazon.awssdk.services.s3.model.ListBucketsResponse;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport java.util.List;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class HelloS3 {\\npublic static void main(String[] args) {\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nlistBuckets(s3);\\n}\\npublic static void listBuckets(S3Client s3) {\\ntry {\\nListBucketsResponse response = s3.listBuckets();\\nList<Bucket> bucketList = response.buckets();\\nbucketList.forEach(bucket -> {\\nSystem.out.println(\"Bucket Name: \" + bucket.name());\\n});\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { ListBucketsCommand, S3Client } from \"@aws-sdk/client-s3\";\\n// When no region or credentials are provided, the SDK will use the\\n// region and credentials from the local AWS config.\\nconst client = new S3Client({});\\nexport const helloS3 = async () => {\\nconst command = new ListBucketsCommand({});\\nconst { Buckets } = await client.send(command);\\nconsole.log(\"Buckets: \");\\nconsole.log(Buckets.map((bucket) => bucket.Name).join(\"\\\\n\"));\\nreturn Buckets;\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'use Aws\\\\S3\\\\S3Client;', '']]]\n",
      "[[['', \"$client = new S3Client(['region' => 'us-west-2']);\\n$results = $client->listBuckets();\\nvar_dump($results);\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import boto3\\ndef hello_s3():\\n\"\"\"\\nUse the AWS SDK for Python (Boto3) to create an Amazon Simple Storage Service\\n(Amazon S3) resource and list the buckets in your account.\\nThis example uses the default settings specified in your shared credentials\\nand config files.\\n\"\"\"\\ns3_resource = boto3.resource(\"s3\")\\nprint(\"Hello, Amazon S3! Let\\'s list your buckets:\")\\nfor bucket in s3_resource.buckets.all():\\nprint(f\"\\\\t{bucket.name}\")\\nif __name__ == \"__main__\":\\nhello_s3()', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'aws s3api abort-multipart-upload \\\\\\n--bucket my-bucket \\\\\\n--key multipart/01 \\\\\\n--upload-id\\ndfRtDYU0WWCCcH43C3WFbkRONycyCpTJJvxu2i5GYkZljF.Yxwh6XG7WfS2vC4to6HiV6Yjlx.cph0gtN', 'B'], ['', '', '']], [['', '', ''], ['', 'Remove-S3MultipartUpload -BucketName test-files -DaysBefore 5', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Remove-S3MultipartUpload -BucketName test-files -InitiatedDate \"Thursday, January\\n02, 2014\"', ''], ['', '', '']], [['', '', ''], ['', 'Remove-S3MultipartUpload -BucketName test-files -InitiatedDate \"2014/01/02\\n10:45:37\"', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Transfer;\\n/// <summary>\\n/// This example shows how to use the Amazon Simple Storage Service\\n/// (Amazon S3) to stop a multi-part upload process using the Amazon S3\\n/// TransferUtility.\\n/// </summary>', '']]]\n",
      "[[['', 'public class AbortMPU\\n{\\npublic static async Task Main()\\n{\\nstring bucketName = \"doc-example-bucket\";\\n// If the AWS Region defined for your default user is different\\n// from the Region where your Amazon S3 bucket is located,\\n// pass the Region name to the S3 client object\\'s constructor.\\n// For example: RegionEndpoint.USWest2.\\nIAmazonS3 client = new AmazonS3Client();\\nawait AbortMPUAsync(client, bucketName);\\n}\\n/// <summary>\\n/// Cancels the multi-part copy process.\\n/// </summary>\\n/// <param name=\"client\">The initialized client object used to create\\n/// the TransferUtility object.</param>\\n/// <param name=\"bucketName\">The name of the S3 bucket where the\\n/// multi-part copy operation is in progress.</param>\\npublic static async Task AbortMPUAsync(IAmazonS3 client, string\\nbucketName)\\n{\\ntry\\n{\\nvar transferUtility = new TransferUtility(client);\\n// Cancel all in-progress uploads initiated before the specified\\ndate.\\nawait transferUtility.AbortMultipartUploadsAsync(\\nbucketName, DateTime.Now.AddDays(-7));\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine($\"Error: {e.Message}\");\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"aws s3api complete-multipart-upload --multipart-upload file://\\nmpustruct --bucket my-bucket --key 'multipart/01' --upload-id\\ndfRtDYU0WWCCcH43C3WFbkRONycyCpTJJvxu2i5GYkZljF.Yxwh6XG7WfS2vC4to6HiV6Yjlx.cph0gtN\", 'B'], ['', '', '']], [['', '', ''], ['', '{\\n\"Parts\": [\\n{\\n\"ETag\": \"e868e0f4719e394144ef36531ee6824c\",', '']]]\n",
      "[[['', '\"PartNumber\": 1\\n},\\n{\\n\"ETag\": \"6bb2b12753d66fe86da4998aa33fffb0\",\\n\"PartNumber\": 2\\n},\\n{\\n\"ETag\": \"d0a0112e841abec9c9ec83406f0159c8\",\\n\"PartNumber\": 3\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"ETag\": \"\\\\\"3944a9f7a4faab7f78788ff6210f63f0-3\\\\\"\",\\n\"Bucket\": \"my-bucket\",\\n\"Location\": \"https://my-bucket.s3.amazonaws.com/multipart%2F01\",\\n\"Key\": \"multipart/01\"\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'let _complete_multipart_upload_res = client\\n.complete_multipart_upload()\\n.bucket(&bucket_name)', '']]]\n",
      "[[['', '.key(&key)\\n.multipart_upload(completed_multipart_upload)\\n.upload_id(upload_id)\\n.send()\\n.await\\n.unwrap();', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\npublic class CopyObject', '']]]\n",
      "[[['', '{\\npublic static async Task Main()\\n{\\n// Specify the AWS Region where your buckets are located if it is\\n// different from the AWS Region of the default user.\\nIAmazonS3 s3Client = new AmazonS3Client();\\n// Remember to change these values to refer to your Amazon S3\\nobjects.\\nstring sourceBucketName = \"doc-example-bucket1\";\\nstring destinationBucketName = \"doc-example-bucket2\";\\nstring sourceObjectKey = \"testfile.txt\";\\nstring destinationObjectKey = \"testfilecopy.txt\";\\nConsole.WriteLine($\"Copying {sourceObjectKey} from {sourceBucketName}\\nto \");\\nConsole.WriteLine($\"{destinationBucketName} as\\n{destinationObjectKey}\");\\nvar response = await CopyingObjectAsync(\\ns3Client,\\nsourceObjectKey,\\ndestinationObjectKey,\\nsourceBucketName,\\ndestinationBucketName);\\nif (response.HttpStatusCode == System.Net.HttpStatusCode.OK)\\n{\\nConsole.WriteLine(\"\\\\nCopy complete.\");\\n}\\n}\\n/// <summary>\\n/// This method calls the AWS SDK for .NET to copy an\\n/// object from one Amazon S3 bucket to another.\\n/// </summary>\\n/// <param name=\"client\">The Amazon S3 client object.</param>\\n/// <param name=\"sourceKey\">The name of the object to be copied.</param>\\n/// <param name=\"destinationKey\">The name under which to save the copy.</\\nparam>\\n/// <param name=\"sourceBucketName\">The name of the Amazon S3 bucket\\n/// where the file is located now.</param>\\n/// <param name=\"destinationBucketName\">The name of the Amazon S3\\n/// bucket where the copy should be saved.</param>', '']]]\n",
      "[[['', '/// <returns>Returns a CopyObjectResponse object with the results from\\n/// the async call.</returns>\\npublic static async Task<CopyObjectResponse> CopyingObjectAsync(\\nIAmazonS3 client,\\nstring sourceKey,\\nstring destinationKey,\\nstring sourceBucketName,\\nstring destinationBucketName)\\n{\\nvar response = new CopyObjectResponse();\\ntry\\n{\\nvar request = new CopyObjectRequest\\n{\\nSourceBucket = sourceBucketName,\\nSourceKey = sourceKey,\\nDestinationBucket = destinationBucketName,\\nDestinationKey = destinationKey,\\n};\\nresponse = await client.CopyObjectAsync(request);\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error copying object: \\'{ex.Message}\\'\");\\n}\\nreturn response;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '###############################################################################\\n# function errecho\\n#\\n# This function outputs everything sent to it to STDERR (standard error output).\\n###############################################################################\\nfunction errecho() {\\nprintf \"%s\\\\n\" \"$*\" 1>&2\\n}\\n###############################################################################\\n# function copy_item_in_bucket\\n#\\n# This function creates a copy of the specified file in the same bucket.\\n#\\n# Parameters:\\n# $1 - The name of the bucket to copy the file from and to.\\n# $2 - The key of the source file to copy.\\n# $3 - The key of the destination file.\\n#\\n# Returns:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction copy_item_in_bucket() {\\nlocal bucket_name=$1\\nlocal source_key=$2\\nlocal destination_key=$3\\nlocal response\\nresponse=$(aws s3api copy-object \\\\\\n--bucket \"$bucket_name\" \\\\\\n--copy-source \"$bucket_name/$source_key\" \\\\\\n--key \"$destination_key\")\\n# shellcheck disable=SC2181\\nif [[ $? -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports s3api copy-object operation failed.\\\\n$response\"\\nreturn 1\\nfi\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::CopyObject(const Aws::String &objectKey, const Aws::String\\n&fromBucket, const Aws::String &toBucket,\\nconst Aws::Client::ClientConfiguration &clientConfig)\\n{\\nAws::S3::S3Client client(clientConfig);\\nAws::S3::Model::CopyObjectRequest request;\\nrequest.WithCopySource(fromBucket + \"/\" + objectKey)\\n.WithKey(objectKey)\\n.WithBucket(toBucket);\\nAws::S3::Model::CopyObjectOutcome outcome = client.CopyObject(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &err = outcome.GetError();\\nstd::cerr << \"Error: CopyObject: \" <<\\nerr.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nstd::cout << \"Successfully copied \" << objectKey << \" from \" <<\\nfromBucket <<\\n\" to \" << toBucket << \".\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api copy-object --copy-source bucket-1/test.txt --key test.txt --bucket\\nbucket-2', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"CopyObjectResult\": {\\n\"LastModified\": \"2015-11-10T01:07:25.000Z\",\\n\"ETag\": \"\\\\\"589c8b79c230a6ecd5a7e1d040a9a030\\\\\"\"\\n},\\n\"VersionId\": \"YdnYvTCVDqRRFA.NFJjy36p0hxifMlkA\"\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3)\\nactions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform\\nbucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}', '']]]\n",
      "[[['', '// CopyToBucket copies an object in a bucket to another bucket.\\nfunc (basics BucketBasics) CopyToBucket(sourceBucket string, destinationBucket\\nstring, objectKey string) error {\\n_, err := basics.S3Client.CopyObject(context.TODO(), &s3.CopyObjectInput{\\nBucket: aws.String(destinationBucket),\\nCopySource: aws.String(fmt.Sprintf(\"%v/%v\", sourceBucket, objectKey)),\\nKey: aws.String(objectKey),\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t copy object from %v:%v to %v:%v. Here\\'s why: %v\\\\n\",\\nsourceBucket, objectKey, destinationBucket, objectKey, err)\\n}\\nreturn err\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.CopyObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.CopyObjectResponse;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\n/**\\n* Before running this Java V2 code example, set up your development', '']]]\n",
      "[[['', '* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class CopyObject {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<objectKey> <fromBucket> <toBucket>\\nWhere:\\nobjectKey - The name of the object (for example, book.pdf).\\nfromBucket - The S3 bucket name that contains the object (for\\nexample, bucket1).\\ntoBucket - The S3 bucket to copy the object to (for example,\\nbucket2).\\n\"\"\";\\nif (args.length != 3) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString objectKey = args[0];\\nString fromBucket = args[1];\\nString toBucket = args[2];\\nSystem.out.format(\"Copying object %s from bucket %s to %s\\\\n\", objectKey,\\nfromBucket, toBucket);\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\ncopyBucketObject(s3, fromBucket, objectKey, toBucket);\\ns3.close();\\n}\\npublic static String copyBucketObject(S3Client s3, String fromBucket, String\\nobjectKey, String toBucket) {', '']]]\n",
      "[[['', 'CopyObjectRequest copyReq = CopyObjectRequest.builder()\\n.sourceBucket(fromBucket)\\n.sourceKey(objectKey)\\n.destinationBucket(toBucket)\\n.destinationKey(objectKey)\\n.build();\\ntry {\\nCopyObjectResponse copyRes = s3.copyObject(copyReq);\\nreturn copyRes.copyObjectResult().toString();\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\nreturn \"\";\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport software.amazon.awssdk.core.sync.RequestBody;\\nimport software.amazon.awssdk.services.s3.model.CopyObjectRequest;\\nimport software.amazon.awssdk.transfer.s3.S3TransferManager;\\nimport software.amazon.awssdk.transfer.s3.model.CompletedCopy;\\nimport software.amazon.awssdk.transfer.s3.model.Copy;\\nimport software.amazon.awssdk.transfer.s3.model.CopyRequest;\\nimport java.util.UUID;\\npublic String copyObject(S3TransferManager transferManager, String\\nbucketName,\\nString key, String destinationBucket, String destinationKey) {\\nCopyObjectRequest copyObjectRequest = CopyObjectRequest.builder()\\n.sourceBucket(bucketName)\\n.sourceKey(key)\\n.destinationBucket(destinationBucket)\\n.destinationKey(destinationKey)\\n.build();', '']]]\n",
      "[[['', 'CopyRequest copyRequest = CopyRequest.builder()\\n.copyObjectRequest(copyObjectRequest)\\n.build();\\nCopy copy = transferManager.copy(copyRequest);\\nCompletedCopy completedCopy = copy.completionFuture().join();\\nreturn completedCopy.response().copyObjectResult().eTag();\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { S3Client, CopyObjectCommand } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new CopyObjectCommand({\\nCopySource: \"SOURCE_BUCKET/SOURCE_OBJECT_KEY\",\\nBucket: \"DESTINATION_BUCKET\",\\nKey: \"NEW_OBJECT_KEY\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);', '']]]\n",
      "[[['', '}\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun copyBucketObject(\\nfromBucket: String,\\nobjectKey: String,\\ntoBucket: String,\\n) {\\nvar encodedUrl = \"\"\\ntry {\\nencodedUrl = URLEncoder.encode(\"$fromBucket/$objectKey\",\\nStandardCharsets.UTF_8.toString())\\n} catch (e: UnsupportedEncodingException) {\\nprintln(\"URL could not be encoded: \" + e.message)\\n}\\nval request =\\nCopyObjectRequest {\\ncopySource = encodedUrl\\nbucket = toBucket\\nkey = objectKey\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\ns3.copyObject(request)\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '$s3client = new Aws\\\\S3\\\\S3Client([\\'region\\' => \\'us-west-2\\']);\\ntry {\\n$folder = \"copied-folder\";\\n$this->s3client->copyObject([\\n\\'Bucket\\' => $this->bucketName,\\n\\'CopySource\\' => \"$this->bucketName/$fileName\",\\n\\'Key\\' => \"$folder/$fileName-copy\",\\n]);\\necho \"Copied $fileName to $folder/$fileName-copy.\\\\n\";\\n} catch (Exception $exception) {\\necho \"Failed to copy $fileName with error: \" . $exception-\\n>getMessage();\\nexit(\"Please fix error with object copying before continuing.\");\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Copy-S3Object -BucketName test-files -Key sample.txt -DestinationKey sample-\\ncopy.txt', ''], ['', '', '']], [['', '', ''], ['', 'Copy-S3Object -BucketName test-files -Key sample.txt -DestinationKey sample-\\ncopy.txt -DestinationBucket backup-files', ''], ['', '', '']], [['', '', ''], ['', 'Copy-S3Object -BucketName test-files -Key sample.txt -LocalFile local-sample.txt', ''], ['', '', '']], [['', '', ''], ['', 'Copy-S3Object -BucketName test-files -Key data/archive.zip -LocalFolder c:\\n\\\\downloads', ''], ['', '', '']], [['', '', ''], ['', 'Copy-S3Object -BucketName test-files -KeyPrefix data -LocalFolder c:\\\\downloads', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'class ObjectWrapper:\\n\"\"\"Encapsulates S3 object actions.\"\"\"\\ndef __init__(self, s3_object):\\n\"\"\"\\n:param s3_object: A Boto3 Object resource. This is a high-level resource\\nin Boto3\\nthat wraps object actions in a class-like structure.\\n\"\"\"\\nself.object = s3_object\\nself.key = self.object.key\\ndef copy(self, dest_object):\\n\"\"\"\\nCopies the object to another bucket.\\n:param dest_object: The destination object initialized with a bucket and\\nkey.\\nThis is a Boto3 Object resource.\\n\"\"\"\\ntry:\\ndest_object.copy_from(\\nCopySource={\"Bucket\": self.object.bucket_name, \"Key\":\\nself.object.key}\\n)\\ndest_object.wait_until_exists()\\nlogger.info(\\n\"Copied object from %s:%s to %s:%s.\",\\nself.object.bucket_name,\\nself.object.key,\\ndest_object.bucket_name,\\ndest_object.key,\\n)\\nexcept ClientError:\\nlogger.exception(\\n\"Couldn\\'t copy object from %s/%s to %s/%s.\",\\nself.object.bucket_name,\\nself.object.key,\\ndest_object.bucket_name,\\ndest_object.key,\\n)\\nraise', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectCopyWrapper\\nattr_reader :source_object\\n# @param source_object [Aws::S3::Object] An existing Amazon S3 object. This is\\nused as the source object for\\n# copy actions.\\ndef initialize(source_object)\\n@source_object = source_object\\nend\\n# Copy the source object to the specified target bucket and rename it with the\\ntarget key.\\n#\\n# @param target_bucket [Aws::S3::Bucket] An existing Amazon S3 bucket where the\\nobject is copied.\\n# @param target_object_key [String] The key to give the copy of the object.\\n# @return [Aws::S3::Object, nil] The copied object when successful; otherwise,\\nnil.\\ndef copy_object(target_bucket, target_object_key)\\n@source_object.copy_to(bucket: target_bucket.name, key: target_object_key)\\ntarget_bucket.object(target_object_key)\\nrescue Aws::Errors::ServiceError => e', '']]]\n",
      "[[['', 'puts \"Couldn\\'t copy #{@source_object.key} to #{target_object_key}. Here\\'s\\nwhy: #{e.message}\"\\nend\\nend\\n# Example usage:\\ndef run_demo\\nsource_bucket_name = \"doc-example-bucket1\"\\nsource_key = \"my-source-file.txt\"\\ntarget_bucket_name = \"doc-example-bucket2\"\\ntarget_key = \"my-target-file.txt\"\\nsource_bucket = Aws::S3::Bucket.new(source_bucket_name)\\nwrapper = ObjectCopyWrapper.new(source_bucket.object(source_key))\\ntarget_bucket = Aws::S3::Bucket.new(target_bucket_name)\\ntarget_object = wrapper.copy_object(target_bucket, target_key)\\nreturn unless target_object\\nputs \"Copied #{source_key} from #{source_bucket_name} to\\n#{target_object.bucket_name}:#{target_object.key}.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectCopyEncryptWrapper\\nattr_reader :source_object\\n# @param source_object [Aws::S3::Object] An existing Amazon S3 object. This is\\nused as the source object for\\n# copy actions.\\ndef initialize(source_object)\\n@source_object = source_object\\nend\\n# Copy the source object to the specified target bucket, rename it with the\\ntarget key, and encrypt it.\\n#', '']]]\n",
      "[[['', '# @param target_bucket [Aws::S3::Bucket] An existing Amazon S3 bucket where the\\nobject is copied.\\n# @param target_object_key [String] The key to give the copy of the object.\\n# @return [Aws::S3::Object, nil] The copied object when successful; otherwise,\\nnil.\\ndef copy_object(target_bucket, target_object_key, encryption)\\n@source_object.copy_to(bucket: target_bucket.name, key: target_object_key,\\nserver_side_encryption: encryption)\\ntarget_bucket.object(target_object_key)\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t copy #{@source_object.key} to #{target_object_key}. Here\\'s\\nwhy: #{e.message}\"\\nend\\nend\\n# Example usage:\\ndef run_demo\\nsource_bucket_name = \"doc-example-bucket1\"\\nsource_key = \"my-source-file.txt\"\\ntarget_bucket_name = \"doc-example-bucket2\"\\ntarget_key = \"my-target-file.txt\"\\ntarget_encryption = \"AES256\"\\nsource_bucket = Aws::S3::Bucket.new(source_bucket_name)\\nwrapper = ObjectCopyEncryptWrapper.new(source_bucket.object(source_key))\\ntarget_bucket = Aws::S3::Bucket.new(target_bucket_name)\\ntarget_object = wrapper.copy_object(target_bucket, target_key,\\ntarget_encryption)\\nreturn unless target_object\\nputs \"Copied #{source_key} from #{source_bucket_name} to\\n#{target_object.bucket_name}:#{target_object.key} and \"\\\\\\n\"encrypted the target with #{target_object.server_side_encryption}\\nencryption.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'pub async fn copy_object(\\nclient: &Client,\\nbucket_name: &str,\\nobject_key: &str,\\ntarget_key: &str,\\n) -> Result<CopyObjectOutput, SdkError<CopyObjectError>> {\\nlet mut source_bucket_and_object: String = \"\".to_owned();\\nsource_bucket_and_object.push_str(bucket_name);\\nsource_bucket_and_object.push(\\'/\\');\\nsource_bucket_and_object.push_str(object_key);\\nclient\\n.copy_object()\\n.copy_source(source_bucket_and_object)\\n.bucket(bucket_name)\\n.key(target_key)\\n.send()\\n.await\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', \"TRY.\\nlo_s3->copyobject(\\niv_bucket = iv_dest_bucket\\niv_key = iv_dest_object\\niv_copysource = |{ iv_src_bucket }/{ iv_src_object }|\\n).\\nMESSAGE 'Object copied to another bucket.' TYPE 'I'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE 'Bucket does not exist.' TYPE 'E'.\\nCATCH /aws1/cx_s3_nosuchkey.\\nMESSAGE 'Object key does not exist.' TYPE 'E'.\\nENDTRY.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis is prerelease documentation for an SDK in preview release. It is subject to\\nchange.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'public func copyFile(from sourceBucket: String, name: String, to destBucket:\\nString) async throws {\\nlet srcUrl = (\"\\\\(sourceBucket)/\\n\\\\(name)\").addingPercentEncoding(withAllowedCharacters: .urlPathAllowed)\\nlet input = CopyObjectInput(\\nbucket: destBucket,\\ncopySource: srcUrl,\\nkey: name\\n)\\n_ = try await client.copyObject(input: input)\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Shows how to create a new Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">An initialized Amazon S3 client object.</param>\\n/// <param name=\"bucketName\">The name of the bucket to create.</param>\\n/// <returns>A boolean value representing the success or failure of\\n/// the bucket creation process.</returns>\\npublic static async Task<bool> CreateBucketAsync(IAmazonS3 client, string\\nbucketName)\\n{\\ntry\\n{\\nvar request = new PutBucketRequest\\n{\\nBucketName = bucketName,\\nUseClientRegion = true,\\n};\\nvar response = await client.PutBucketAsync(request);\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error creating bucket: \\'{ex.Message}\\'\");\\nreturn false;\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '/// <summary>\\n/// Create a new Amazon S3 bucket with object lock actions.\\n/// </summary>\\n/// <param name=\"bucketName\">The name of the bucket to create.</param>\\n/// <param name=\"enableObjectLock\">True to enable object lock on the\\nbucket.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> CreateBucketWithObjectLock(string bucketName, bool\\nenableObjectLock)\\n{\\nConsole.WriteLine($\"\\\\tCreating bucket {bucketName} with object lock\\n{enableObjectLock}.\");\\ntry\\n{\\nvar request = new PutBucketRequest\\n{\\nBucketName = bucketName,\\nUseClientRegion = true,\\nObjectLockEnabledForBucket = enableObjectLock,\\n};\\nvar response = await _amazonS3.PutBucketAsync(request);\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error creating bucket: \\'{ex.Message}\\'\");\\nreturn false;\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '###############################################################################\\n# function iecho\\n#\\n# This function enables the script to display the specified text only if\\n# the global variable $VERBOSE is set to true.\\n###############################################################################\\nfunction iecho() {\\nif [[ $VERBOSE == true ]]; then\\necho \"$@\"\\nfi\\n}\\n###############################################################################\\n# function errecho\\n#\\n# This function outputs everything sent to it to STDERR (standard error output).\\n###############################################################################\\nfunction errecho() {\\nprintf \"%s\\\\n\" \"$*\" 1>&2\\n}\\n###############################################################################\\n# function create-bucket\\n#\\n# This function creates the specified bucket in the specified AWS Region, unless\\n# it already exists.\\n#\\n# Parameters:\\n# -b bucket_name -- The name of the bucket to create.\\n# -r region_code -- The code for an AWS Region in which to\\n# create the bucket.\\n#', '']]]\n",
      "[[['', '# Returns:\\n# The URL of the bucket that was created.\\n# And:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction create_bucket() {\\nlocal bucket_name region_code response\\nlocal option OPTARG # Required to use getopts command in a function.\\n# bashsupport disable=BP5008\\nfunction usage() {\\necho \"function create_bucket\"\\necho \"Creates an Amazon S3 bucket. You must supply a bucket name:\"\\necho \" -b bucket_name The name of the bucket. It must be globally\\nunique.\"\\necho \" [-r region_code] The code for an AWS Region in which the bucket is\\ncreated.\"\\necho \"\"\\n}\\n# Retrieve the calling parameters.\\nwhile getopts \"b:r:h\" option; do\\ncase \"${option}\" in\\nb) bucket_name=\"${OPTARG}\" ;;\\nr) region_code=\"${OPTARG}\" ;;\\nh)\\nusage\\nreturn 0\\n;;\\n\\\\?)\\necho \"Invalid parameter\"\\nusage\\nreturn 1\\n;;\\nesac\\ndone\\nif [[ -z \"$bucket_name\" ]]; then\\nerrecho \"ERROR: You must provide a bucket name with the -b parameter.\"\\nusage\\nreturn 1\\nfi', '']]]\n",
      "[[['', 'local bucket_config_arg\\n# A location constraint for \"us-east-1\" returns an error.\\nif [[ -n \"$region_code\" ]] && [[ \"$region_code\" != \"us-east-1\" ]]; then\\nbucket_config_arg=\"--create-bucket-configuration LocationConstraint=\\n$region_code\"\\nfi\\niecho \"Parameters:\\\\n\"\\niecho \" Bucket name: $bucket_name\"\\niecho \" Region code: $region_code\"\\niecho \"\"\\n# If the bucket already exists, we don\\'t want to try to create it.\\nif (bucket_exists \"$bucket_name\"); then\\nerrecho \"ERROR: A bucket with that name already exists. Try again.\"\\nreturn 1\\nfi\\n# shellcheck disable=SC2086\\nresponse=$(aws s3api create-bucket \\\\\\n--bucket \"$bucket_name\" \\\\\\n$bucket_config_arg)\\n# shellcheck disable=SC2181\\nif [[ ${?} -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports create-bucket operation failed.\\\\n$response\"\\nreturn 1\\nfi\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'bool AwsDoc::S3::CreateBucket(const Aws::String &bucketName,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client client(clientConfig);\\nAws::S3::Model::CreateBucketRequest request;\\nrequest.SetBucket(bucketName);\\n//TODO(user): Change the bucket location constraint enum to your target\\nRegion.\\nif (clientConfig.region != \"us-east-1\") {\\nAws::S3::Model::CreateBucketConfiguration createBucketConfig;\\ncreateBucketConfig.SetLocationConstraint(\\nAws::S3::Model::BucketLocationConstraintMapper::GetBucketLocationConstraintForNam\\nclientConfig.region));\\nrequest.SetCreateBucketConfiguration(createBucketConfig);\\n}\\nAws::S3::Model::CreateBucketOutcome outcome = client.CreateBucket(request);\\nif (!outcome.IsSuccess()) {\\nauto err = outcome.GetError();\\nstd::cerr << \"Error: CreateBucket: \" <<\\nerr.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nstd::cout << \"Created bucket \" << bucketName <<\\n\" in the specified AWS Region.\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}', 'e'], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api create-bucket \\\\\\n--bucket my-bucket \\\\\\n--region us-east-1', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Location\": \"/my-bucket\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api create-bucket \\\\\\n--bucket my-bucket \\\\\\n--region us-east-1 \\\\\\n--object-ownership BucketOwnerEnforced', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Location\": \"/my-bucket\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api create-bucket \\\\\\n--bucket my-bucket \\\\\\n--region eu-west-1 \\\\\\n--create-bucket-configuration LocationConstraint=eu-west-1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Location\": \"http://my-bucket.s3.amazonaws.com/\"\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3)\\nactions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform\\nbucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}\\n// CreateBucket creates a bucket with the specified name in the specified Region.\\nfunc (basics BucketBasics) CreateBucket(name string, region string) error {\\n_, err := basics.S3Client.CreateBucket(context.TODO(), &s3.CreateBucketInput{\\nBucket: aws.String(name),\\nCreateBucketConfiguration: &types.CreateBucketConfiguration{\\nLocationConstraint: types.BucketLocationConstraint(region),\\n},\\n})\\nif err != nil {', '']]]\n",
      "[[['', 'log.Printf(\"Couldn\\'t create bucket %v in Region %v. Here\\'s why: %v\\\\n\",\\nname, region, err)\\n}\\nreturn err\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.core.waiters.WaiterResponse;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.CreateBucketRequest;\\nimport software.amazon.awssdk.services.s3.model.HeadBucketRequest;\\nimport software.amazon.awssdk.services.s3.model.HeadBucketResponse;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.waiters.S3Waiter;\\nimport java.net.URISyntaxException;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/', '']]]\n",
      "[[['', 'public class CreateBucket {\\npublic static void main(String[] args) throws URISyntaxException {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName>\\\\s\\nWhere:\\nbucketName - The name of the bucket to create. The bucket\\nname must be unique, or an error occurs.\\n\"\"\";\\nif (args.length != 1) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nSystem.out.format(\"Creating a bucket named %s\\\\n\", bucketName);\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\ncreateBucket(s3, bucketName);\\ns3.close();\\n}\\npublic static void createBucket(S3Client s3Client, String bucketName) {\\ntry {\\nS3Waiter s3Waiter = s3Client.waiter();\\nCreateBucketRequest bucketRequest = CreateBucketRequest.builder()\\n.bucket(bucketName)\\n.build();\\ns3Client.createBucket(bucketRequest);\\nHeadBucketRequest bucketRequestWait = HeadBucketRequest.builder()\\n.bucket(bucketName)\\n.build();\\n// Wait until the bucket is created and print out the response.\\nWaiterResponse<HeadBucketResponse> waiterResponse =\\ns3Waiter.waitUntilBucketExists(bucketRequestWait);\\nwaiterResponse.matched().response().ifPresent(System.out::println);', '']]]\n",
      "[[['', 'System.out.println(bucketName + \" is ready\");\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '// Create a new Amazon S3 bucket with object lock options.\\npublic void createBucketWithLockOptions(boolean enableObjectLock, String\\nbucketName) {\\nS3Waiter s3Waiter = getClient().waiter();\\nCreateBucketRequest bucketRequest = CreateBucketRequest.builder()\\n.bucket(bucketName)\\n.objectLockEnabledForBucket(enableObjectLock)\\n.build();\\ngetClient().createBucket(bucketRequest);\\nHeadBucketRequest bucketRequestWait = HeadBucketRequest.builder()\\n.bucket(bucketName)\\n.build();\\n// Wait until the bucket is created and print out the response.\\ns3Waiter.waitUntilBucketExists(bucketRequestWait);\\nSystem.out.println(bucketName + \" is ready\");\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { CreateBucketCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new CreateBucketCommand({\\n// The name of the bucket. Bucket names are unique and have several other\\nconstraints.\\n// See https://docs.aws.amazon.com/AmazonS3/latest/userguide/\\nbucketnamingrules.html\\nBucket: \"bucket-name\",\\n});\\ntry {\\nconst { Location } = await client.send(command);\\nconsole.log(`Bucket created with location ${Location}`);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun createNewBucket(bucketName: String) {\\nval request =\\nCreateBucketRequest {\\nbucket = bucketName\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\ns3.createBucket(request)\\nprintln(\"$bucketName is ready\")\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', \"$s3client = new Aws\\\\S3\\\\S3Client(['region' => 'us-west-2']);\\ntry {\", '']]]\n",
      "[[['', '$this->s3client->createBucket([\\n\\'Bucket\\' => $this->bucketName,\\n\\'CreateBucketConfiguration\\' => [\\'LocationConstraint\\' => $region],\\n]);\\necho \"Created bucket named: $this->bucketName \\\\n\";\\n} catch (Exception $exception) {\\necho \"Failed to create bucket $this->bucketName with error: \" .\\n$exception->getMessage();\\nexit(\"Please fix error with bucket creation before continuing.\");\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef create(self, region_override=None):\\n\"\"\"', '']]]\n",
      "[[['', 'Create an Amazon S3 bucket in the default Region for the account or in\\nthe\\nspecified Region.\\n:param region_override: The Region in which to create the bucket. If this\\nis\\nnot specified, the Region configured in your\\nshared\\ncredentials is used.\\n\"\"\"\\nif region_override is not None:\\nregion = region_override\\nelse:\\nregion = self.bucket.meta.client.meta.region_name\\ntry:\\nself.bucket.create(CreateBucketConfiguration={\"LocationConstraint\":\\nregion})\\nself.bucket.wait_until_exists()\\nlogger.info(\"Created bucket \\'%s\\' in region=%s\", self.bucket.name,\\nregion)\\nexcept ClientError as error:\\nlogger.exception(\\n\"Couldn\\'t create bucket named \\'%s\\' in region=%s.\",\\nself.bucket.name,\\nregion,\\n)\\nraise error', ''], ['', '', '']], [['', '', ''], ['', 'def create_versioned_bucket(bucket_name, prefix):\\n\"\"\"\\nCreates an Amazon S3 bucket, enables it for versioning, and configures a\\nlifecycle\\nthat expires noncurrent object versions after 7 days.\\nAdding a lifecycle configuration to a versioned bucket is a best practice.\\nIt helps prevent objects in the bucket from accumulating a large number of\\nnoncurrent versions, which can slow down request performance.', '']]]\n",
      "[[['', 'Usage is shown in the usage_demo_single_object function at the end of this\\nmodule.\\n:param bucket_name: The name of the bucket to create.\\n:param prefix: Identifies which objects are automatically expired under the\\nconfigured lifecycle rules.\\n:return: The newly created bucket.\\n\"\"\"\\ntry:\\nbucket = s3.create_bucket(\\nBucket=bucket_name,\\nCreateBucketConfiguration={\\n\"LocationConstraint\": s3.meta.client.meta.region_name\\n},\\n)\\nlogger.info(\"Created bucket %s.\", bucket.name)\\nexcept ClientError as error:\\nif error.response[\"Error\"][\"Code\"] == \"BucketAlreadyOwnedByYou\":\\nlogger.warning(\"Bucket %s already exists! Using it.\", bucket_name)\\nbucket = s3.Bucket(bucket_name)\\nelse:\\nlogger.exception(\"Couldn\\'t create bucket %s.\", bucket_name)\\nraise\\ntry:\\nbucket.Versioning().enable()\\nlogger.info(\"Enabled versioning on bucket %s.\", bucket.name)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t enable versioning on bucket %s.\", bucket.name)\\nraise\\ntry:\\nexpiration = 7\\nbucket.LifecycleConfiguration().put(\\nLifecycleConfiguration={\\n\"Rules\": [\\n{\\n\"Status\": \"Enabled\",\\n\"Prefix\": prefix,\\n\"NoncurrentVersionExpiration\": {\"NoncurrentDays\":\\nexpiration},\\n}\\n]\\n}', '']]]\n",
      "[[['', ')\\nlogger.info(\\n\"Configured lifecycle to expire noncurrent versions after %s days \"\\n\"on bucket %s.\",\\nexpiration,\\nbucket.name,\\n)\\nexcept ClientError as error:\\nlogger.warning(\\n\"Couldn\\'t configure lifecycle on bucket %s because %s. \"\\n\"Continuing anyway.\",\\nbucket.name,\\nerror,\\n)\\nreturn bucket', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 bucket actions.\\nclass BucketCreateWrapper\\nattr_reader :bucket\\n# @param bucket [Aws::S3::Bucket] An Amazon S3 bucket initialized with a name.\\nThis is a client-side object until\\n# create is called.\\ndef initialize(bucket)', '']]]\n",
      "[[['', '@bucket = bucket\\nend\\n# Creates an Amazon S3 bucket in the specified AWS Region.\\n#\\n# @param region [String] The Region where the bucket is created.\\n# @return [Boolean] True when the bucket is created; otherwise, false.\\ndef create?(region)\\n@bucket.create(create_bucket_configuration: { location_constraint: region })\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t create bucket. Here\\'s why: #{e.message}\"\\nfalse\\nend\\n# Gets the Region where the bucket is located.\\n#\\n# @return [String] The location of the bucket.\\ndef location\\nif @bucket.nil?\\n\"None. You must create a bucket before you can get its location!\"\\nelse\\n@bucket.client.get_bucket_location(bucket:\\n@bucket.name).location_constraint\\nend\\nrescue Aws::Errors::ServiceError => e\\n\"Couldn\\'t get the location of #{@bucket.name}. Here\\'s why: #{e.message}\"\\nend\\nend\\n# Example usage:\\ndef run_demo\\nregion = \"us-west-2\"\\nwrapper = BucketCreateWrapper.new(Aws::S3::Bucket.new(\"doc-example-bucket-\\n#{Random.uuid}\"))\\nreturn unless wrapper.create?(region)\\nputs \"Created bucket #{wrapper.bucket.name}.\"\\nputs \"Your bucket\\'s region is: #{wrapper.location}\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'pub async fn create_bucket(\\nclient: &Client,\\nbucket_name: &str,\\nregion: &str,\\n) -> Result<CreateBucketOutput, SdkError<CreateBucketError>> {\\nlet constraint = BucketLocationConstraint::from(region);\\nlet cfg = CreateBucketConfiguration::builder()\\n.location_constraint(constraint)\\n.build();\\nclient\\n.create_bucket()\\n.create_bucket_configuration(cfg)\\n.bucket(bucket_name)\\n.send()\\n.await\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', \"TRY.\\nlo_s3->createbucket(\\niv_bucket = iv_bucket_name\\n).\\nMESSAGE 'S3 bucket created.' TYPE 'I'.\\nCATCH /aws1/cx_s3_bucketalrdyexists.\\nMESSAGE 'Bucket name already exists.' TYPE 'E'.\\nCATCH /aws1/cx_s3_bktalrdyownedbyyou.\\nMESSAGE 'Bucket already exists and is owned by you.' TYPE 'E'.\\nENDTRY.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis is prerelease documentation for an SDK in preview release. It is subject to\\nchange.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public func createBucket(name: String) async throws {\\nlet config = S3ClientTypes.CreateBucketConfiguration(\\nlocationConstraint: .usEast2\\n)\\nlet input = CreateBucketInput(\\nbucket: name,\\ncreateBucketConfiguration: config\\n)\\n_ = try await client.createBucket(input: input)\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun createS3ControlClient(): S3ControlClient {\\n// Configure your S3ControlClient to send requests to US West\\n(Oregon).\\nval s3Control = S3ControlClient.fromEnvironment {\\nregion = \"us-west-2\"\\n}', '']]]\n",
      "[[['', 'return s3Control\\n}', ''], ['', '', '']], [['', '', ''], ['', 'suspend fun createMrap(\\ns3Control: S3ControlClient,\\naccountIdParam: String,\\nbucketName1: String,\\nbucketName2: String,\\nmrapName: String,\\n): String {\\nprintln(\"Creating MRAP ...\")\\nval createMrapResponse: CreateMultiRegionAccessPointResponse =\\ns3Control.createMultiRegionAccessPoint {\\naccountId = accountIdParam\\nclientToken = UUID.randomUUID().toString()\\ndetails {\\nname = mrapName\\nregions = listOf(\\nRegion {\\nbucket = bucketName1\\n},\\nRegion {\\nbucket = bucketName2\\n},\\n)\\n}\\n}\\nval requestToken: String? = createMrapResponse.requestTokenArn\\n// Use the request token to check for the status of the\\nCreateMultiRegionAccessPoint operation.\\nif (requestToken != null) {\\nwaitForSucceededStatus(s3Control, requestToken, accountIdParam)\\nprintln(\"MRAP created\")\\n}\\nval getMrapResponse =\\ns3Control.getMultiRegionAccessPoint(\\ninput = GetMultiRegionAccessPointRequest {\\naccountId = accountIdParam', '']]]\n",
      "[[['', 'name = mrapName\\n},\\n)\\nval mrapAlias = getMrapResponse.accessPoint?.alias\\nreturn \"arn:aws:s3::$accountIdParam:accesspoint/$mrapAlias\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'suspend fun waitForSucceededStatus(\\ns3Control: S3ControlClient,\\nrequestToken: String,\\naccountIdParam: String,\\ntimeBetweenChecks: Duration = 1.minutes,\\n) {\\nvar describeResponse: DescribeMultiRegionAccessPointOperationResponse\\ndescribeResponse = s3Control.describeMultiRegionAccessPointOperation(\\ninput = DescribeMultiRegionAccessPointOperationRequest {\\naccountId = accountIdParam\\nrequestTokenArn = requestToken\\n},\\n)\\nvar status: String? = describeResponse.asyncOperation?.requestStatus\\nwhile (status != \"SUCCEEDED\") {\\ndelay(timeBetweenChecks)\\ndescribeResponse =\\ns3Control.describeMultiRegionAccessPointOperation(\\ninput = DescribeMultiRegionAccessPointOperationRequest {\\naccountId = accountIdParam\\nrequestTokenArn = requestToken\\n},\\n)\\nstatus = describeResponse.asyncOperation?.requestStatus\\nprintln(status)\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"aws s3api create-multipart-upload --bucket my-bucket --key 'multipart/01'\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Bucket\": \"my-bucket\",\\n\"UploadId\":\\n\"dfRtDYU0WWCCcH43C3WFbkRONycyCpTJJvxu2i5GYkZljF.Yxwh6XG7WfS2vC4to6HiV6Yjlx.cph0gt\\n\"Key\": \"multipart/01\"\\n}', 'N'], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'let multipart_upload_res: CreateMultipartUploadOutput = client\\n.create_multipart_upload()\\n.bucket(&bucket_name)\\n.key(&key)\\n.send()\\n.await\\n.unwrap();', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Shows how to delete an Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">An initialized Amazon S3 client object.</param>\\n/// <param name=\"bucketName\">The name of the Amazon S3 bucket to\\ndelete.</param>\\n/// <returns>A boolean value that represents the success or failure of\\n/// the delete operation.</returns>\\npublic static async Task<bool> DeleteBucketAsync(IAmazonS3 client, string\\nbucketName)\\n{\\nvar request = new DeleteBucketRequest\\n{\\nBucketName = bucketName,\\n};\\nvar response = await client.DeleteBucketAsync(request);\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '###############################################################################\\n# function errecho\\n#\\n# This function outputs everything sent to it to STDERR (standard error output).\\n###############################################################################\\nfunction errecho() {\\nprintf \"%s\\\\n\" \"$*\" 1>&2\\n}\\n###############################################################################\\n# function delete_bucket\\n#\\n# This function deletes the specified bucket.\\n#\\n# Parameters:\\n# $1 - The name of the bucket.\\n# Returns:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction delete_bucket() {\\nlocal bucket_name=$1\\nlocal response\\nresponse=$(aws s3api delete-bucket \\\\\\n--bucket \"$bucket_name\")\\n# shellcheck disable=SC2181\\nif [[ $? -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports s3api delete-bucket failed.\\\\n$response\"\\nreturn 1', '']]]\n",
      "[[['', 'fi\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::DeleteBucket(const Aws::String &bucketName,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client client(clientConfig);\\nAws::S3::Model::DeleteBucketRequest request;\\nrequest.SetBucket(bucketName);\\nAws::S3::Model::DeleteBucketOutcome outcome =\\nclient.DeleteBucket(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &err = outcome.GetError();\\nstd::cerr << \"Error: DeleteBucket: \" <<\\nerr.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nstd::cout << \"The bucket was deleted\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api delete-bucket --bucket my-bucket --region us-east-1', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3)\\nactions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform\\nbucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}\\n// DeleteBucket deletes a bucket. The bucket must be empty or an error is\\nreturned.\\nfunc (basics BucketBasics) DeleteBucket(bucketName string) error {\\n_, err := basics.S3Client.DeleteBucket(context.TODO(), &s3.DeleteBucketInput{\\nBucket: aws.String(bucketName)})\\nif err != nil {', '']]]\n",
      "[[['', 'log.Printf(\"Couldn\\'t delete bucket %v. Here\\'s why: %v\\\\n\", bucketName, err)\\n}\\nreturn err\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'DeleteBucketRequest deleteBucketRequest = DeleteBucketRequest.builder()\\n.bucket(bucket)\\n.build();\\ns3.deleteBucket(deleteBucketRequest);\\ns3.close();', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import { DeleteBucketCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\n// Delete a bucket.\\nexport const main = async () => {\\nconst command = new DeleteBucketCommand({\\nBucket: \"test-bucket\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '$s3client = new Aws\\\\S3\\\\S3Client([\\'region\\' => \\'us-west-2\\']);\\ntry {\\n$this->s3client->deleteBucket([\\n\\'Bucket\\' => $this->bucketName,\\n]);\\necho \"Deleted bucket $this->bucketName.\\\\n\";', '']]]\n",
      "[[['', '} catch (Exception $exception) {\\necho \"Failed to delete $this->bucketName with error: \" . $exception-\\n>getMessage();\\nexit(\"Please fix error with bucket deletion before continuing.\");\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Remove-S3Bucket -BucketName test-files -DeleteBucketContent', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.', '']]]\n",
      "[[['', '\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef delete(self):\\n\"\"\"\\nDelete the bucket. The bucket must be empty or an error is raised.\\n\"\"\"\\ntry:\\nself.bucket.delete()\\nself.bucket.wait_until_not_exists()\\nlogger.info(\"Bucket %s successfully deleted.\", self.bucket.name)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t delete bucket %s.\", self.bucket.name)\\nraise', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '# Deletes the objects in an Amazon S3 bucket and deletes the bucket.\\n#\\n# @param bucket [Aws::S3::Bucket] The bucket to empty and delete.\\ndef delete_bucket(bucket)\\nputs(\"\\\\nDo you want to delete all of the objects as well as the bucket (y/n)?\\n\")\\nanswer = gets.chomp.downcase\\nif answer == \"y\"\\nbucket.objects.batch_delete!\\nbucket.delete\\nputs(\"Emptied and deleted bucket #{bucket.name}.\\\\n\")', '']]]\n",
      "[[['', 'end\\nrescue Aws::Errors::ServiceError => e\\nputs(\"Couldn\\'t empty and delete bucket #{bucket.name}.\")\\nputs(\"\\\\t#{e.code}: #{e.message}\")\\nraise\\nend', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'pub async fn delete_bucket(client: &Client, bucket_name: &str) -> Result<(),\\nError> {\\nclient.delete_bucket().bucket(bucket_name).send().await?;\\nprintln!(\"Bucket deleted\");\\nOk(())\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"TRY.\\nlo_s3->deletebucket(\\niv_bucket = iv_bucket_name\\n).\\nMESSAGE 'Deleted S3 bucket.' TYPE 'I'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE 'Bucket does not exist.' TYPE 'E'.\\nENDTRY.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis is prerelease documentation for an SDK in preview release. It is subject to\\nchange.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'public func deleteBucket(name: String) async throws {\\nlet input = DeleteBucketInput(\\nbucket: name\\n)\\n_ = try await client.deleteBucket(input: input)\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api delete-bucket-analytics-configuration \\\\\\n--bucket my-bucket \\\\\\n--id 1', ''], ['', '', '']], [['', '', ''], ['', \"Remove-S3BucketAnalyticsConfiguration -BucketName 's3testbucket' -AnalyticsId\\n'testfilter'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Deletes a CORS configuration from an Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used\\n/// to delete the CORS configuration from the bucket.</param>\\nprivate static async Task DeleteCORSConfigurationAsync(AmazonS3Client\\nclient)\\n{\\nDeleteCORSConfigurationRequest request = new\\nDeleteCORSConfigurationRequest()\\n{\\nBucketName = BucketName,\\n};\\nawait client.DeleteCORSConfigurationAsync(request);\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api delete-bucket-cors --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef delete_cors(self):\\n\"\"\"\\nDelete the CORS rules from the bucket.\\n:param bucket_name: The name of the bucket to update.\\n\"\"\"\\ntry:', '']]]\n",
      "[[['', 'self.bucket.Cors().delete()\\nlogger.info(\"Deleted CORS from bucket \\'%s\\'.\", self.bucket.name)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t delete CORS from bucket \\'%s\\'.\",\\nself.bucket.name)\\nraise', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 bucket CORS configuration.\\nclass BucketCorsWrapper\\nattr_reader :bucket_cors\\n# @param bucket_cors [Aws::S3::BucketCors] A bucket CORS object configured with\\nan existing bucket.\\ndef initialize(bucket_cors)\\n@bucket_cors = bucket_cors\\nend\\n# Deletes the CORS configuration of a bucket.\\n#\\n# @return [Boolean] True if the CORS rules were deleted; otherwise, false.\\ndef delete_cors\\n@bucket_cors.delete\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t delete CORS rules for #{@bucket_cors.bucket.name}. Here\\'s why:\\n#{e.message}\"', '']]]\n",
      "[[['', 'false\\nend\\nend', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-bucket-encryption \\\\\\n--bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', \"Remove-S3BucketEncryption -BucketName 's3casetestbucket'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Confirm\\nAre you sure you want to perform this action?\\nPerforming the operation \"Remove-S3BucketEncryption (DeleteBucketEncryption)\" on\\ntarget \"s3casetestbucket\".\\n[Y] Yes [A] Yes to All [N] No [L] No to All [S] Suspend [?] Help (default is\\n\"Y\"): Y', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-bucket-inventory-configuration \\\\\\n--bucket my-bucket \\\\\\n--id 1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Remove-S3BucketInventoryConfiguration -BucketName 's3testbucket' -InventoryId\\n'testInventoryName'\", ''], ['', '', '']], [['', '', ''], ['', 'Confirm\\nAre you sure you want to perform this action?\\nPerforming the operation \"Remove-S3BucketInventoryConfiguration\\n(DeleteBucketInventoryConfiguration)\" on target \"s3testbucket\".\\n[Y] Yes [A] Yes to All [N] No [L] No to All [S] Suspend [?] Help (default is\\n\"Y\"): Y', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '/// <summary>\\n/// This method removes the Lifecycle configuration from the named\\n/// S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">The S3 client object used to call\\n/// the RemoveLifecycleConfigAsync method.</param>\\n/// <param name=\"bucketName\">A string representing the name of the\\n/// S3 bucket from which the configuration will be removed.</param>\\npublic static async Task RemoveLifecycleConfigAsync(IAmazonS3 client,\\nstring bucketName)\\n{\\nvar request = new DeleteLifecycleConfigurationRequest()\\n{\\nBucketName = bucketName,\\n};\\nawait client.DeleteLifecycleConfigurationAsync(request);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-bucket-lifecycle --bucket my-bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef delete_lifecycle_configuration(self):\\n\"\"\"\\nRemove the lifecycle configuration from the specified bucket.\\n\"\"\"\\ntry:\\nself.bucket.LifecycleConfiguration().delete()\\nlogger.info(\\n\"Deleted lifecycle configuration for bucket \\'%s\\'.\",\\nself.bucket.name\\n)\\nexcept ClientError:\\nlogger.exception(\\n\"Couldn\\'t delete lifecycle configuration for bucket \\'%s\\'.\",\\nself.bucket.name,\\n)\\nraise', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api delete-bucket-metrics-configuration \\\\\\n--bucket my-bucket \\\\\\n--id 123', ''], ['', '', '']], [['', '', ''], ['', \"Remove-S3BucketMetricsConfiguration -BucketName 's3testbucket' -MetricsId\\n'testmetrics'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::DeleteBucketPolicy(const Aws::String &bucketName,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client client(clientConfig);\\nAws::S3::Model::DeleteBucketPolicyRequest request;\\nrequest.SetBucket(bucketName);\\nAws::S3::Model::DeleteBucketPolicyOutcome outcome =\\nclient.DeleteBucketPolicy(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &err = outcome.GetError();\\nstd::cerr << \"Error: DeleteBucketPolicy: \" <<\\nerr.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nstd::cout << \"Policy was deleted from the bucket.\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-bucket-policy --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.DeleteBucketPolicyRequest;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class DeleteBucketPolicy {\\npublic static void main(String[] args) {', '']]]\n",
      "[[['', 'final String usage = \"\"\"\\nUsage:\\n<bucketName>\\nWhere:\\nbucketName - The Amazon S3 bucket to delete the policy from\\n(for example, bucket1).\"\"\";\\nif (args.length != 1) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nSystem.out.format(\"Deleting policy from bucket: \\\\\"%s\\\\\"\\\\n\\\\n\", bucketName);\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\ndeleteS3BucketPolicy(s3, bucketName);\\ns3.close();\\n}\\n// Delete the bucket policy.\\npublic static void deleteS3BucketPolicy(S3Client s3, String bucketName) {\\nDeleteBucketPolicyRequest delReq = DeleteBucketPolicyRequest.builder()\\n.bucket(bucketName)\\n.build();\\ntry {\\ns3.deleteBucketPolicy(delReq);\\nSystem.out.println(\"Done!\");\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { DeleteBucketPolicyCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\n// This will remove the policy from the bucket.\\nexport const main = async () => {\\nconst command = new DeleteBucketPolicyCommand({\\nBucket: \"test-bucket\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun deleteS3BucketPolicy(bucketName: String?) {\\nval request =\\nDeleteBucketPolicyRequest {\\nbucket = bucketName\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\ns3.deleteBucketPolicy(request)\\nprintln(\"Done!\")\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Remove-S3BucketPolicy -BucketName 's3testbucket'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef delete_policy(self):\\n\"\"\"\\nDelete the security policy from the bucket.\\n\"\"\"\\ntry:\\nself.bucket.Policy().delete()\\nlogger.info(\"Deleted policy for bucket \\'%s\\'.\", self.bucket.name)\\nexcept ClientError:\\nlogger.exception(\\n\"Couldn\\'t delete policy for bucket \\'%s\\'.\", self.bucket.name\\n)\\nraise', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '# Wraps an Amazon S3 bucket policy.\\nclass BucketPolicyWrapper\\nattr_reader :bucket_policy\\n# @param bucket_policy [Aws::S3::BucketPolicy] A bucket policy object\\nconfigured with an existing bucket.\\ndef initialize(bucket_policy)\\n@bucket_policy = bucket_policy\\nend\\ndef delete_policy\\n@bucket_policy.delete\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t delete the policy from #{@bucket_policy.bucket.name}. Here\\'s\\nwhy: #{e.message}\"\\nfalse\\nend\\nend', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api delete-bucket-replication --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', 'Remove-S3BucketReplication -BucketName mybucket', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-bucket-tagging --bucket my-bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Remove-S3BucketTagging -BucketName 's3testbucket'\", ''], ['', '', '']], [['', '', ''], ['', 'Confirm\\nAre you sure you want to perform this action?\\nPerforming the operation \"Remove-S3BucketTagging (DeleteBucketTagging)\" on target\\n\"s3testbucket\".\\n[Y] Yes [A] Yes to All [N] No [L] No to All [S] Suspend [?] Help (default is\\n\"Y\"): Y', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::DeleteBucketWebsite(const Aws::String &bucketName,', '']]]\n",
      "[[['', 'const Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client client(clientConfig);\\nAws::S3::Model::DeleteBucketWebsiteRequest request;\\nrequest.SetBucket(bucketName);\\nAws::S3::Model::DeleteBucketWebsiteOutcome outcome =\\nclient.DeleteBucketWebsite(request);\\nif (!outcome.IsSuccess()) {\\nauto err = outcome.GetError();\\nstd::cerr << \"Error: DeleteBucketWebsite: \" <<\\nerr.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nstd::cout << \"Website configuration was removed.\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-bucket-website --bucket my-bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.DeleteBucketWebsiteRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class DeleteWebsiteConfiguration {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage: <bucketName>\\nWhere:\\nbucketName - The Amazon S3 bucket to delete the website\\nconfiguration from.\\n\"\"\";\\nif (args.length != 1) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];', '']]]\n",
      "[[['', 'System.out.format(\"Deleting website configuration for Amazon S3 bucket:\\n%s\\\\n\", bucketName);\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\ndeleteBucketWebsiteConfig(s3, bucketName);\\nSystem.out.println(\"Done!\");\\ns3.close();\\n}\\npublic static void deleteBucketWebsiteConfig(S3Client s3, String bucketName)\\n{\\nDeleteBucketWebsiteRequest delReq = DeleteBucketWebsiteRequest.builder()\\n.bucket(bucketName)\\n.build();\\ntry {\\ns3.deleteBucketWebsite(delReq);\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.out.println(\"Failed to delete website configuration!\");\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import { DeleteBucketWebsiteCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\n// Disable static website hosting on the bucket.\\nexport const main = async () => {\\nconst command = new DeleteBucketWebsiteCommand({\\nBucket: \"test-bucket\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Remove-S3BucketWebsite -BucketName 's3testbucket'\", ''], ['', '', '']], [['', '', ''], ['', 'Confirm\\nAre you sure you want to perform this action?\\nPerforming the operation \"Remove-S3BucketWebsite (DeleteBucketWebsite)\" on target\\n\"s3testbucket\".\\n[Y] Yes [A] Yes to All [N] No [L] No to All [S] Suspend [?] Help (default is\\n\"Y\"): Y', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example shows how to delete an object from a non-versioned Amazon\\n/// Simple Storage Service (Amazon S3) bucket.\\n/// </summary>\\npublic class DeleteObject\\n{\\n/// <summary>\\n/// The Main method initializes the necessary variables and then calls\\n/// the DeleteObjectNonVersionedBucketAsync method to delete the object', '']]]\n",
      "[[['', '/// named by the keyName parameter.\\n/// </summary>\\npublic static async Task Main()\\n{\\nconst string bucketName = \"doc-example-bucket\";\\nconst string keyName = \"testfile.txt\";\\n// If the Amazon S3 bucket is located in an AWS Region other than the\\n// Region of the default account, define the AWS Region for the\\n// Amazon S3 bucket in your call to the AmazonS3Client constructor.\\n// For example RegionEndpoint.USWest2.\\nIAmazonS3 client = new AmazonS3Client();\\nawait DeleteObjectNonVersionedBucketAsync(client, bucketName,\\nkeyName);\\n}\\n/// <summary>\\n/// The DeleteObjectNonVersionedBucketAsync takes care of deleting the\\n/// desired object from the named bucket.\\n/// </summary>\\n/// <param name=\"client\">An initialized Amazon S3 client used to delete\\n/// an object from an Amazon S3 bucket.</param>\\n/// <param name=\"bucketName\">The name of the bucket from which the\\n/// object will be deleted.</param>\\n/// <param name=\"keyName\">The name of the object to delete.</param>\\npublic static async Task DeleteObjectNonVersionedBucketAsync(IAmazonS3\\nclient, string bucketName, string keyName)\\n{\\ntry\\n{\\nvar deleteObjectRequest = new DeleteObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\n};\\nConsole.WriteLine($\"Deleting object: {keyName}\");\\nawait client.DeleteObjectAsync(deleteObjectRequest);\\nConsole.WriteLine($\"Object: {keyName} deleted from\\n{bucketName}.\");\\n}\\ncatch (AmazonS3Exception ex)\\n{', '']]]\n",
      "[[['', 'Console.WriteLine($\"Error encountered on server.\\nMessage:\\'{ex.Message}\\' when deleting an object.\");\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example creates an object in an Amazon Simple Storage Service\\n/// (Amazon S3) bucket and then deletes the object version that was\\n/// created.\\n/// </summary>\\npublic class DeleteObjectVersion\\n{\\npublic static async Task Main()\\n{\\nstring bucketName = \"doc-example-bucket\";\\nstring keyName = \"verstioned-object.txt\";\\n// If the AWS Region of the default user is different from the AWS\\n// Region of the Amazon S3 bucket, pass the AWS Region of the\\n// bucket region to the Amazon S3 client object\\'s constructor.\\n// Define it like this:\\n// RegionEndpoint bucketRegion = RegionEndpoint.USWest2;\\nIAmazonS3 client = new AmazonS3Client();\\nawait CreateAndDeleteObjectVersionAsync(client, bucketName, keyName);\\n}\\n/// <summary>\\n/// This method creates and then deletes a versioned object.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\n/// create and delete the object.</param>\\n/// <param name=\"bucketName\">The name of the Amazon S3 bucket where the', '']]]\n",
      "[[['', '/// object will be created and deleted.</param>\\n/// <param name=\"keyName\">The key name of the object to create.</param>\\npublic static async Task CreateAndDeleteObjectVersionAsync(IAmazonS3\\nclient, string bucketName, string keyName)\\n{\\ntry\\n{\\n// Add a sample object.\\nstring versionID = await PutAnObject(client, bucketName,\\nkeyName);\\n// Delete the object by specifying an object key and a version\\nID.\\nDeleteObjectRequest request = new DeleteObjectRequest()\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nVersionId = versionID,\\n};\\nConsole.WriteLine(\"Deleting an object\");\\nawait client.DeleteObjectAsync(request);\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error: {ex.Message}\");\\n}\\n}\\n/// <summary>\\n/// This method is used to create the temporary Amazon S3 object.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 object which will be\\nused\\n/// to create the temporary Amazon S3 object.</param>\\n/// <param name=\"bucketName\">The name of the Amazon S3 bucket where the\\nobject\\n/// will be created.</param>\\n/// <param name=\"objectKey\">The name of the Amazon S3 object co create.</\\nparam>\\n/// <returns>The Version ID of the created object.</returns>\\npublic static async Task<string> PutAnObject(IAmazonS3 client, string\\nbucketName, string objectKey)\\n{', '']]]\n",
      "[[['', 'PutObjectRequest request = new PutObjectRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey,\\nContentBody = \"This is the content body!\",\\n};\\nPutObjectResponse response = await client.PutObjectAsync(request);\\nreturn response.VersionId;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '###############################################################################\\n# function errecho\\n#\\n# This function outputs everything sent to it to STDERR (standard error output).\\n###############################################################################\\nfunction errecho() {\\nprintf \"%s\\\\n\" \"$*\" 1>&2\\n}\\n###############################################################################\\n# function delete_item_in_bucket\\n#\\n# This function deletes the specified file from the specified bucket.\\n#\\n# Parameters:\\n# $1 - The name of the bucket.', '']]]\n",
      "[[['', '# $2 - The key (file name) in the bucket to delete.\\n# Returns:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction delete_item_in_bucket() {\\nlocal bucket_name=$1\\nlocal key=$2\\nlocal response\\nresponse=$(aws s3api delete-object \\\\\\n--bucket \"$bucket_name\" \\\\\\n--key \"$key\")\\n# shellcheck disable=SC2181\\nif [[ $? -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports s3api delete-object operation failed.\\\\n\\n$response\"\\nreturn 1\\nfi\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::DeleteObject(const Aws::String &objectKey,\\nconst Aws::String &fromBucket,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client client(clientConfig);\\nAws::S3::Model::DeleteObjectRequest request;', '']]]\n",
      "[[['', 'request.WithKey(objectKey)\\n.WithBucket(fromBucket);\\nAws::S3::Model::DeleteObjectOutcome outcome =\\nclient.DeleteObject(request);\\nif (!outcome.IsSuccess()) {\\nauto err = outcome.GetError();\\nstd::cerr << \"Error: DeleteObject: \" <<\\nerr.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nstd::cout << \"Successfully deleted the object.\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-object --bucket my-bucket --key test.txt', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"VersionId\": \"9_gKg5vG56F.TTEUdwkxGpJ3tNDlWlGq\",\\n\"DeleteMarker\": true\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { DeleteObjectCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new DeleteObjectCommand({\\nBucket: \"test-bucket\",\\nKey: \"test-key.txt\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class ObjectWrapper:\\n\"\"\"Encapsulates S3 object actions.\"\"\"\\ndef __init__(self, s3_object):\\n\"\"\"\\n:param s3_object: A Boto3 Object resource. This is a high-level resource\\nin Boto3\\nthat wraps object actions in a class-like structure.\\n\"\"\"\\nself.object = s3_object\\nself.key = self.object.key\\ndef delete(self):\\n\"\"\"\\nDeletes the object.\\n\"\"\"\\ntry:\\nself.object.delete()\\nself.object.wait_until_not_exists()\\nlogger.info(\\n\"Deleted object \\'%s\\' from bucket \\'%s\\'.\",\\nself.object.key,\\nself.object.bucket_name,\\n)\\nexcept ClientError:\\nlogger.exception(\\n\"Couldn\\'t delete object \\'%s\\' from bucket \\'%s\\'.\",\\nself.object.key,\\nself.object.bucket_name,\\n)', '']]]\n",
      "[[['', 'raise', ''], ['', '', '']], [['', '', ''], ['', 'def rollback_object(bucket, object_key, version_id):\\n\"\"\"\\nRolls back an object to an earlier version by deleting all versions that\\noccurred after the specified rollback version.\\nUsage is shown in the usage_demo_single_object function at the end of this\\nmodule.\\n:param bucket: The bucket that holds the object to roll back.\\n:param object_key: The object to roll back.\\n:param version_id: The version ID to roll back to.\\n\"\"\"\\n# Versions must be sorted by last_modified date because delete markers are\\n# at the end of the list even when they are interspersed in time.\\nversions = sorted(\\nbucket.object_versions.filter(Prefix=object_key),\\nkey=attrgetter(\"last_modified\"),\\nreverse=True,\\n)\\nlogger.debug(\\n\"Got versions:\\\\n%s\",\\n\"\\\\n\".join(\\n[\\nf\"\\\\t{version.version_id}, last modified {version.last_modified}\"\\nfor version in versions\\n]\\n),\\n)\\nif version_id in [ver.version_id for ver in versions]:\\nprint(f\"Rolling back to version {version_id}\")\\nfor version in versions:\\nif version.version_id != version_id:\\nversion.delete()\\nprint(f\"Deleted version {version.version_id}\")\\nelse:', '']]]\n",
      "[[['', 'break\\nprint(f\"Active version is now {bucket.Object(object_key).version_id}\")\\nelse:\\nraise KeyError(\\nf\"{version_id} was not found in the list of versions for \"\\nf\"{object_key}.\"\\n)', ''], ['', '', '']], [['', '', ''], ['', 'def revive_object(bucket, object_key):\\n\"\"\"\\nRevives a versioned object that was deleted by removing the object\\'s active\\ndelete marker.\\nA versioned object presents as deleted when its latest version is a delete\\nmarker.\\nBy removing the delete marker, we make the previous version the latest\\nversion\\nand the object then presents as *not* deleted.\\nUsage is shown in the usage_demo_single_object function at the end of this\\nmodule.\\n:param bucket: The bucket that contains the object.\\n:param object_key: The object to revive.\\n\"\"\"\\n# Get the latest version for the object.\\nresponse = s3.meta.client.list_object_versions(\\nBucket=bucket.name, Prefix=object_key, MaxKeys=1\\n)\\nif \"DeleteMarkers\" in response:\\nlatest_version = response[\"DeleteMarkers\"][0]\\nif latest_version[\"IsLatest\"]:\\nlogger.info(\\n\"Object %s was indeed deleted on %s. Let\\'s revive it.\",\\nobject_key,\\nlatest_version[\"LastModified\"],\\n)', '']]]\n",
      "[[['', 'obj = bucket.Object(object_key)\\nobj.Version(latest_version[\"VersionId\"]).delete()\\nlogger.info(\\n\"Revived %s, active version is now %s with body \\'%s\\'\",\\nobject_key,\\nobj.version_id,\\nobj.get()[\"Body\"].read(),\\n)\\nelse:\\nlogger.warning(\\n\"Delete marker is not the latest version for %s!\", object_key\\n)\\nelif \"Versions\" in response:\\nlogger.warning(\"Got an active version for %s, nothing to do.\",\\nobject_key)\\nelse:\\nlogger.error(\"Couldn\\'t get any version info for %s.\", object_key)', ''], ['', '', '']], [['', '', ''], ['', 'import logging\\nfrom urllib import parse\\nimport boto3\\nfrom botocore.exceptions import ClientError\\nlogger = logging.getLogger(__name__)\\nlogger.setLevel(\"INFO\")\\ns3 = boto3.client(\"s3\")\\ndef lambda_handler(event, context):\\n\"\"\"\\nRemoves a delete marker from the specified versioned object.\\n:param event: The S3 batch event that contains the ID of the delete marker\\nto remove.\\n:param context: Context about the event.', '']]]\n",
      "[[['', ':return: A result structure that Amazon S3 uses to interpret the result of\\nthe\\noperation. When the result code is TemporaryFailure, S3 retries the\\noperation.\\n\"\"\"\\n# Parse job parameters from Amazon S3 batch operations\\ninvocation_id = event[\"invocationId\"]\\ninvocation_schema_version = event[\"invocationSchemaVersion\"]\\nresults = []\\nresult_code = None\\nresult_string = None\\ntask = event[\"tasks\"][0]\\ntask_id = task[\"taskId\"]\\ntry:\\nobj_key = parse.unquote(task[\"s3Key\"], encoding=\"utf-8\")\\nobj_version_id = task[\"s3VersionId\"]\\nbucket_name = task[\"s3BucketArn\"].split(\":\")[-1]\\nlogger.info(\\n\"Got task: remove delete marker %s from object %s.\", obj_version_id,\\nobj_key\\n)\\ntry:\\n# If this call does not raise an error, the object version is not a\\ndelete\\n# marker and should not be deleted.\\nresponse = s3.head_object(\\nBucket=bucket_name, Key=obj_key, VersionId=obj_version_id\\n)\\nresult_code = \"PermanentFailure\"\\nresult_string = (\\nf\"Object {obj_key}, ID {obj_version_id} is not \" f\"a delete\\nmarker.\"\\n)\\nlogger.debug(response)\\nlogger.warning(result_string)\\nexcept ClientError as error:\\ndelete_marker = error.response[\"ResponseMetadata\"]\\n[\"HTTPHeaders\"].get(', '']]]\n",
      "[[['', '\"x-amz-delete-marker\", \"false\"\\n)\\nif delete_marker == \"true\":\\nlogger.info(\\n\"Object %s, version %s is a delete marker.\", obj_key,\\nobj_version_id\\n)\\ntry:\\ns3.delete_object(\\nBucket=bucket_name, Key=obj_key, VersionId=obj_version_id\\n)\\nresult_code = \"Succeeded\"\\nresult_string = (\\nf\"Successfully removed delete marker \"\\nf\"{obj_version_id} from object {obj_key}.\"\\n)\\nlogger.info(result_string)\\nexcept ClientError as error:\\n# Mark request timeout as a temporary failure so it will be\\nretried.\\nif error.response[\"Error\"][\"Code\"] == \"RequestTimeout\":\\nresult_code = \"TemporaryFailure\"\\nresult_string = (\\nf\"Attempt to remove delete marker from \"\\nf\"object {obj_key} timed out.\"\\n)\\nlogger.info(result_string)\\nelse:\\nraise\\nelse:\\nraise ValueError(\\nf\"The x-amz-delete-marker header is either not \"\\nf\"present or is not \\'true\\'.\"\\n)\\nexcept Exception as error:\\n# Mark all other exceptions as permanent failures.\\nresult_code = \"PermanentFailure\"\\nresult_string = str(error)\\nlogger.exception(error)\\nfinally:\\nresults.append(\\n{\\n\"taskId\": task_id,\\n\"resultCode\": result_code,', '']]]\n",
      "[[['', '\"resultString\": result_string,\\n}\\n)\\nreturn {\\n\"invocationSchemaVersion\": invocation_schema_version,\\n\"treatMissingKeysAs\": \"PermanentFailure\",\\n\"invocationId\": invocation_id,\\n\"results\": results,\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'async fn remove_object(client: &Client, bucket: &str, key: &str) -> Result<(),\\nError> {\\nclient\\n.delete_object()\\n.bucket(bucket)\\n.key(key)\\n.send()\\n.await?;\\nprintln!(\"Object deleted.\");\\nOk(())\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', \"TRY.\\nlo_s3->deleteobject(\\niv_bucket = iv_bucket_name\\niv_key = iv_object_key\\n).\\nMESSAGE 'Object deleted from S3 bucket.' TYPE 'I'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE 'Bucket does not exist.' TYPE 'E'.\\nENDTRY.\", ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis is prerelease documentation for an SDK in preview release. It is subject to\\nchange.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public func deleteFile(bucket: String, key: String) async throws {\\nlet input = DeleteObjectInput(\\nbucket: bucket,\\nkey: key\\n)\\ndo {\\n_ = try await client.deleteObject(input: input)\\n} catch {\\nthrow error\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-object-tagging \\\\\\n--bucket my-bucket \\\\\\n--key doc1.rtf', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Remove-S3ObjectTagSet -Key 'testfile.txt' -BucketName 's3testbucket' -Select\\n'^Key'\", ''], ['', '', '']], [['', '', ''], ['', 'Confirm\\nAre you sure you want to perform this action?\\nPerforming the operation \"Remove-S3ObjectTagSet (DeleteObjectTagging)\" on target\\n\"testfile.txt\".\\n[Y] Yes [A] Yes to All [N] No [L] No to All [S] Suspend [?] Help (default is\\n\"Y\"): Y\\ntestfile.txt', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Delete all of the objects stored in an existing Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">An initialized Amazon S3 client object.</param>\\n/// <param name=\"bucketName\">The name of the bucket from which the\\n/// contents will be deleted.</param>\\n/// <returns>A boolean value that represents the success or failure of\\n/// deleting all of the objects in the bucket.</returns>\\npublic static async Task<bool> DeleteBucketContentsAsync(IAmazonS3\\nclient, string bucketName)\\n{\\n// Iterate over the contents of the bucket and delete all objects.\\nvar request = new ListObjectsV2Request\\n{\\nBucketName = bucketName,\\n};\\ntry\\n{\\nListObjectsV2Response response;\\ndo\\n{\\nresponse = await client.ListObjectsV2Async(request);\\nresponse.S3Objects\\n.ForEach(async obj => await\\nclient.DeleteObjectAsync(bucketName, obj.Key));\\n// If the response is truncated, set the request\\nContinuationToken', '']]]\n",
      "[[['', '// from the NextContinuationToken property of the response.\\nrequest.ContinuationToken = response.NextContinuationToken;\\n}\\nwhile (response.IsTruncated);\\nreturn true;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error deleting objects: {ex.Message}\");\\nreturn false;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example shows how to delete multiple objects from an Amazon Simple\\n/// Storage Service (Amazon S3) bucket.\\n/// </summary>\\npublic class DeleteMultipleObjects\\n{\\n/// <summary>\\n/// The Main method initializes the Amazon S3 client and the name of\\n/// the bucket and then passes those values to MultiObjectDeleteAsync.\\n/// </summary>\\npublic static async Task Main()\\n{\\nconst string bucketName = \"doc-example-bucket\";\\n// If the Amazon S3 bucket from which you wish to delete objects is\\nnot\\n// located in the same AWS Region as the default user, define the\\n// AWS Region for the Amazon S3 bucket as a parameter to the client\\n// constructor.', '']]]\n",
      "[[['', 'IAmazonS3 s3Client = new AmazonS3Client();\\nawait MultiObjectDeleteAsync(s3Client, bucketName);\\n}\\n/// <summary>\\n/// This method uses the passed Amazon S3 client to first create and then\\n/// delete three files from the named bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// Amazon S3 methods.</param>\\n/// <param name=\"bucketName\">The name of the Amazon S3 bucket where\\nobjects\\n/// will be created and then deleted.</param>\\npublic static async Task MultiObjectDeleteAsync(IAmazonS3 client, string\\nbucketName)\\n{\\n// Create three sample objects which we will then delete.\\nvar keysAndVersions = await PutObjectsAsync(client, 3, bucketName);\\n// Now perform the multi-object delete, passing the key names and\\n// version IDs. Since we are working with a non-versioned bucket,\\n// the object keys collection includes null version IDs.\\nDeleteObjectsRequest multiObjectDeleteRequest = new\\nDeleteObjectsRequest\\n{\\nBucketName = bucketName,\\nObjects = keysAndVersions,\\n};\\n// You can add a specific object key to the delete request using the\\n// AddKey method of the multiObjectDeleteRequest.\\ntry\\n{\\nDeleteObjectsResponse response = await\\nclient.DeleteObjectsAsync(multiObjectDeleteRequest);\\nConsole.WriteLine(\"Successfully deleted all the {0} items\",\\nresponse.DeletedObjects.Count);\\n}\\ncatch (DeleteObjectsException e)\\n{\\nPrintDeletionErrorStatus(e);\\n}', '']]]\n",
      "[[['', '}\\n/// <summary>\\n/// Prints the list of errors raised by the call to DeleteObjectsAsync.\\n/// </summary>\\n/// <param name=\"ex\">A collection of exceptions returned by the call to\\n/// DeleteObjectsAsync.</param>\\npublic static void PrintDeletionErrorStatus(DeleteObjectsException ex)\\n{\\nDeleteObjectsResponse errorResponse = ex.Response;\\nConsole.WriteLine(\"x {0}\", errorResponse.DeletedObjects.Count);\\nConsole.WriteLine($\"Successfully deleted\\n{errorResponse.DeletedObjects.Count}.\");\\nConsole.WriteLine($\"No. of objects failed to delete =\\n{errorResponse.DeleteErrors.Count}\");\\nConsole.WriteLine(\"Printing error data...\");\\nforeach (DeleteError deleteError in errorResponse.DeleteErrors)\\n{\\nConsole.WriteLine($\"Object Key:\\n{deleteError.Key}\\\\t{deleteError.Code}\\\\t{deleteError.Message}\");\\n}\\n}\\n/// <summary>\\n/// This method creates simple text file objects that can be used in\\n/// the delete method.\\n/// </summary>\\n/// <param name=\"client\">The Amazon S3 client used to call\\nPutObjectAsync.</param>\\n/// <param name=\"number\">The number of objects to create.</param>\\n/// <param name=\"bucketName\">The name of the bucket where the objects\\n/// will be created.</param>\\n/// <returns>A list of keys (object keys) and versions that the calling\\n/// method will use to delete the newly created files.</returns>\\npublic static async Task<List<KeyVersion>> PutObjectsAsync(IAmazonS3\\nclient, int number, string bucketName)\\n{\\nList<KeyVersion> keys = new List<KeyVersion>();\\nfor (int i = 0; i < number; i++)\\n{\\nstring key = \"ExampleObject-\" + new System.Random().Next();\\nPutObjectRequest request = new PutObjectRequest', '']]]\n",
      "[[['', '{\\nBucketName = bucketName,\\nKey = key,\\nContentBody = \"This is the content body!\",\\n};\\nPutObjectResponse response = await\\nclient.PutObjectAsync(request);\\n// For non-versioned bucket operations, we only need the\\n// object key.\\nKeyVersion keyVersion = new KeyVersion\\n{\\nKey = key,\\n};\\nkeys.Add(keyVersion);\\n}\\nreturn keys;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example shows how to delete objects in a version-enabled Amazon\\n/// Simple StorageService (Amazon S3) bucket.\\n/// </summary>\\npublic class DeleteMultipleObjects\\n{\\npublic static async Task Main()\\n{\\nstring bucketName = \"doc-example-bucket\";\\n// If the AWS Region for your Amazon S3 bucket is different from', '']]]\n",
      "[[['', '// the AWS Region of the default user, define the AWS Region for\\n// the Amazon S3 bucket and pass it to the client constructor\\n// like this:\\n// RegionEndpoint bucketRegion = RegionEndpoint.USWest2;\\nIAmazonS3 s3Client;\\ns3Client = new AmazonS3Client();\\nawait DeleteMultipleObjectsFromVersionedBucketAsync(s3Client,\\nbucketName);\\n}\\n/// <summary>\\n/// This method removes multiple versions and objects from a\\n/// version-enabled Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// DeleteObjectVersionsAsync, DeleteObjectsAsync, and\\n/// RemoveDeleteMarkersAsync.</param>\\n/// <param name=\"bucketName\">The name of the bucket from which to delete\\n/// objects.</param>\\npublic static async Task\\nDeleteMultipleObjectsFromVersionedBucketAsync(IAmazonS3 client, string\\nbucketName)\\n{\\n// Delete objects (specifying object version in the request).\\nawait DeleteObjectVersionsAsync(client, bucketName);\\n// Delete objects (without specifying object version in the request).\\nvar deletedObjects = await DeleteObjectsAsync(client, bucketName);\\n// Additional exercise - remove the delete markers Amazon S3 returned\\nfrom\\n// the preceding response. This results in the objects reappearing\\n// in the bucket (you can verify the appearance/disappearance of\\n// objects in the console).\\nawait RemoveDeleteMarkersAsync(client, bucketName, deletedObjects);\\n}\\n/// <summary>\\n/// Creates and then deletes non-versioned Amazon S3 objects and then\\ndeletes\\n/// them again. The method returns a list of the Amazon S3 objects\\ndeleted.', '']]]\n",
      "[[['', '/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// PubObjectsAsync and NonVersionedDeleteAsync.</param>\\n/// <param name=\"bucketName\">The name of the bucket where the objects\\n/// will be created and then deleted.</param>\\n/// <returns>A list of DeletedObjects.</returns>\\npublic static async Task<List<DeletedObject>>\\nDeleteObjectsAsync(IAmazonS3 client, string bucketName)\\n{\\n// Upload the sample objects.\\nvar keysAndVersions2 = await PutObjectsAsync(client, bucketName, 3);\\n// Delete objects using only keys. Amazon S3 creates a delete marker\\nand\\n// returns its version ID in the response.\\nList<DeletedObject> deletedObjects = await\\nNonVersionedDeleteAsync(client, bucketName, keysAndVersions2);\\nreturn deletedObjects;\\n}\\n/// <summary>\\n/// This method creates several temporary objects and then deletes them.\\n/// </summary>\\n/// <param name=\"client\">The S3 client.</param>\\n/// <param name=\"bucketName\">Name of the bucket.</param>\\n/// <returns>Async task.</returns>\\npublic static async Task DeleteObjectVersionsAsync(IAmazonS3 client,\\nstring bucketName)\\n{\\n// Upload the sample objects.\\nvar keysAndVersions1 = await PutObjectsAsync(client, bucketName, 3);\\n// Delete the specific object versions.\\nawait VersionedDeleteAsync(client, bucketName, keysAndVersions1);\\n}\\n/// <summary>\\n/// Displays the list of information about deleted files to the console.\\n/// </summary>\\n/// <param name=\"e\">Error information from the delete process.</param>\\nprivate static void DisplayDeletionErrors(DeleteObjectsException e)\\n{\\nvar errorResponse = e.Response;', '']]]\n",
      "[[['', 'Console.WriteLine($\"No. of objects successfully deleted =\\n{errorResponse.DeletedObjects.Count}\");\\nConsole.WriteLine($\"No. of objects failed to delete =\\n{errorResponse.DeleteErrors.Count}\");\\nConsole.WriteLine(\"Printing error data...\");\\nforeach (var deleteError in errorResponse.DeleteErrors)\\n{\\nConsole.WriteLine($\"Object Key:\\n{deleteError.Key}\\\\t{deleteError.Code}\\\\t{deleteError.Message}\");\\n}\\n}\\n/// <summary>\\n/// Delete multiple objects from a version-enabled bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// DeleteObjectVersionsAsync, DeleteObjectsAsync, and\\n/// RemoveDeleteMarkersAsync.</param>\\n/// <param name=\"bucketName\">The name of the bucket from which to delete\\n/// objects.</param>\\n/// <param name=\"keys\">A list of key names for the objects to delete.</\\nparam>\\nprivate static async Task VersionedDeleteAsync(IAmazonS3 client, string\\nbucketName, List<KeyVersion> keys)\\n{\\nvar multiObjectDeleteRequest = new DeleteObjectsRequest\\n{\\nBucketName = bucketName,\\nObjects = keys, // This includes the object keys and specific\\nversion IDs.\\n};\\ntry\\n{\\nConsole.WriteLine(\"Executing VersionedDelete...\");\\nDeleteObjectsResponse response = await\\nclient.DeleteObjectsAsync(multiObjectDeleteRequest);\\nConsole.WriteLine($\"Successfully deleted all the\\n{response.DeletedObjects.Count} items\");\\n}\\ncatch (DeleteObjectsException ex)\\n{\\nDisplayDeletionErrors(ex);', '']]]\n",
      "[[['', '}\\n}\\n/// <summary>\\n/// Deletes multiple objects from a non-versioned Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// DeleteObjectVersionsAsync, DeleteObjectsAsync, and\\n/// RemoveDeleteMarkersAsync.</param>\\n/// <param name=\"bucketName\">The name of the bucket from which to delete\\n/// objects.</param>\\n/// <param name=\"keys\">A list of key names for the objects to delete.</\\nparam>\\n/// <returns>A list of the deleted objects.</returns>\\nprivate static async Task<List<DeletedObject>>\\nNonVersionedDeleteAsync(IAmazonS3 client, string bucketName, List<KeyVersion>\\nkeys)\\n{\\n// Create a request that includes only the object key names.\\nDeleteObjectsRequest multiObjectDeleteRequest = new\\nDeleteObjectsRequest();\\nmultiObjectDeleteRequest.BucketName = bucketName;\\nforeach (var key in keys)\\n{\\nmultiObjectDeleteRequest.AddKey(key.Key);\\n}\\n// Execute DeleteObjectsAsync.\\n// The DeleteObjectsAsync method adds a delete marker for each\\n// object deleted. You can verify that the objects were removed\\n// using the Amazon S3 console.\\nDeleteObjectsResponse response;\\ntry\\n{\\nConsole.WriteLine(\"Executing NonVersionedDelete...\");\\nresponse = await\\nclient.DeleteObjectsAsync(multiObjectDeleteRequest);\\nConsole.WriteLine(\"Successfully deleted all the {0} items\",\\nresponse.DeletedObjects.Count);\\n}\\ncatch (DeleteObjectsException ex)\\n{', '']]]\n",
      "[[['', 'DisplayDeletionErrors(ex);\\nthrow; // Some deletions failed. Investigate before continuing.\\n}\\n// This response contains the DeletedObjects list which we use to\\ndelete the delete markers.\\nreturn response.DeletedObjects;\\n}\\n/// <summary>\\n/// Deletes the markers left after deleting the temporary objects.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// DeleteObjectVersionsAsync, DeleteObjectsAsync, and\\n/// RemoveDeleteMarkersAsync.</param>\\n/// <param name=\"bucketName\">The name of the bucket from which to delete\\n/// objects.</param>\\n/// <param name=\"deletedObjects\">A list of the objects that were\\ndeleted.</param>\\nprivate static async Task RemoveDeleteMarkersAsync(IAmazonS3 client,\\nstring bucketName, List<DeletedObject> deletedObjects)\\n{\\nvar keyVersionList = new List<KeyVersion>();\\nforeach (var deletedObject in deletedObjects)\\n{\\nKeyVersion keyVersion = new KeyVersion\\n{\\nKey = deletedObject.Key,\\nVersionId = deletedObject.DeleteMarkerVersionId,\\n};\\nkeyVersionList.Add(keyVersion);\\n}\\n// Create another request to delete the delete markers.\\nvar multiObjectDeleteRequest = new DeleteObjectsRequest\\n{\\nBucketName = bucketName,\\nObjects = keyVersionList,\\n};\\n// Now, delete the delete marker to bring your objects back to the\\nbucket.', '']]]\n",
      "[[['', 'try\\n{\\nConsole.WriteLine(\"Removing the delete markers .....\");\\nvar deleteObjectResponse = await\\nclient.DeleteObjectsAsync(multiObjectDeleteRequest);\\nConsole.WriteLine($\"Successfully deleted the\\n{deleteObjectResponse.DeletedObjects.Count} delete markers\");\\n}\\ncatch (DeleteObjectsException ex)\\n{\\nDisplayDeletionErrors(ex);\\n}\\n}\\n/// <summary>\\n/// Create temporary Amazon S3 objects to show how object deletion wors\\nin an\\n/// Amazon S3 bucket with versioning enabled.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// PutObjectAsync to create temporary objects for the example.</param>\\n/// <param name=\"bucketName\">A string representing the name of the S3\\n/// bucket where we will create the temporary objects.</param>\\n/// <param name=\"number\">The number of temporary objects to create.</\\nparam>\\n/// <returns>A list of the KeyVersion objects.</returns>\\nprivate static async Task<List<KeyVersion>> PutObjectsAsync(IAmazonS3\\nclient, string bucketName, int number)\\n{\\nvar keys = new List<KeyVersion>();\\nfor (var i = 0; i < number; i++)\\n{\\nstring key = \"ObjectToDelete-\" + new System.Random().Next();\\nPutObjectRequest request = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = key,\\nContentBody = \"This is the content body!\",\\n};\\nvar response = await client.PutObjectAsync(request);\\nKeyVersion keyVersion = new KeyVersion', '']]]\n",
      "[[['', '{\\nKey = key,\\nVersionId = response.VersionId,\\n};\\nkeys.Add(keyVersion);\\n}\\nreturn keys;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '###############################################################################\\n# function errecho\\n#\\n# This function outputs everything sent to it to STDERR (standard error output).\\n###############################################################################\\nfunction errecho() {\\nprintf \"%s\\\\n\" \"$*\" 1>&2\\n}\\n###############################################################################\\n# function delete_items_in_bucket\\n#\\n# This function deletes the specified list of keys from the specified bucket.\\n#\\n# Parameters:\\n# $1 - The name of the bucket.', '']]]\n",
      "[[['', '# $2 - A list of keys in the bucket to delete.\\n# Returns:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction delete_items_in_bucket() {\\nlocal bucket_name=$1\\nlocal keys=$2\\nlocal response\\n# Create the JSON for the items to delete.\\nlocal delete_items\\ndelete_items=\"{\\\\\"Objects\\\\\":[\"\\nfor key in $keys; do\\ndelete_items=\"$delete_items{\\\\\"Key\\\\\": \\\\\"$key\\\\\"},\"\\ndone\\ndelete_items=${delete_items%?} # Remove the final comma.\\ndelete_items=\"$delete_items]}\"\\nresponse=$(aws s3api delete-objects \\\\\\n--bucket \"$bucket_name\" \\\\\\n--delete \"$delete_items\")\\n# shellcheck disable=SC2181\\nif [[ $? -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports s3api delete-object operation failed.\\\\n\\n$response\"\\nreturn 1\\nfi\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::DeleteObjects(const std::vector<Aws::String> &objectKeys,\\nconst Aws::String &fromBucket,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client client(clientConfig);\\nAws::S3::Model::DeleteObjectsRequest request;\\nAws::S3::Model::Delete deleteObject;\\nfor (const Aws::String& objectKey : objectKeys)\\n{\\ndeleteObject.AddObjects(Aws::S3::Model::ObjectIdentifier().WithKey(objectKey));\\n}\\nrequest.SetDelete(deleteObject);\\nrequest.SetBucket(fromBucket);\\nAws::S3::Model::DeleteObjectsOutcome outcome =\\nclient.DeleteObjects(request);\\nif (!outcome.IsSuccess()) {\\nauto err = outcome.GetError();\\nstd::cerr << \"Error deleting objects. \" <<\\nerr.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nstd::cout << \"Successfully deleted the objects.\";\\nfor (size_t i = 0; i < objectKeys.size(); ++i)\\n{\\nstd::cout << objectKeys[i];\\nif (i < objectKeys.size() - 1)', '']]]\n",
      "[[['', '{\\nstd::cout << \", \";\\n}\\n}\\nstd::cout << \" from bucket \" << fromBucket << \".\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-objects --bucket my-bucket --delete file://delete.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Objects\": [\\n{\\n\"Key\": \"test1.txt\"\\n}\\n],\\n\"Quiet\": false\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Deleted\": [\\n{\\n\"DeleteMarkerVersionId\": \"mYAT5Mc6F7aeUL8SS7FAAqUPO1koHwzU\",\\n\"Key\": \"test1.txt\",\\n\"DeleteMarker\": true', '']]]\n",
      "[[['', '}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3)\\nactions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform\\nbucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}\\n// DeleteObjects deletes a list of objects from a bucket.\\nfunc (basics BucketBasics) DeleteObjects(bucketName string, objectKeys []string)\\nerror {\\nvar objectIds []types.ObjectIdentifier\\nfor _, key := range objectKeys {\\nobjectIds = append(objectIds, types.ObjectIdentifier{Key: aws.String(key)})\\n}\\noutput, err := basics.S3Client.DeleteObjects(context.TODO(),\\n&s3.DeleteObjectsInput{\\nBucket: aws.String(bucketName),\\nDelete: &types.Delete{Objects: objectIds},\\n})\\nif err != nil {', '']]]\n",
      "[[['', 'log.Printf(\"Couldn\\'t delete objects from bucket %v. Here\\'s why: %v\\\\n\",\\nbucketName, err)\\n} else {\\nlog.Printf(\"Deleted %v objects.\\\\n\", len(output.Deleted))\\n}\\nreturn err\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.core.sync.RequestBody;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.PutObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.ObjectIdentifier;\\nimport software.amazon.awssdk.services.s3.model.Delete;\\nimport software.amazon.awssdk.services.s3.model.DeleteObjectsRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport java.util.ArrayList;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/', '']]]\n",
      "[[['', 'public class DeleteMultiObjects {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage: <bucketName>\\nWhere:\\nbucketName - the Amazon S3 bucket name.\\n\"\"\";\\nif (args.length != 1) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\ndeleteBucketObjects(s3, bucketName);\\ns3.close();\\n}\\npublic static void deleteBucketObjects(S3Client s3, String bucketName) {\\n// Upload three sample objects to the specfied Amazon S3 bucket.\\nArrayList<ObjectIdentifier> keys = new ArrayList<>();\\nPutObjectRequest putOb;\\nObjectIdentifier objectId;\\nfor (int i = 0; i < 3; i++) {\\nString keyName = \"delete object example \" + i;\\nobjectId = ObjectIdentifier.builder()\\n.key(keyName)\\n.build();\\nputOb = PutObjectRequest.builder()\\n.bucket(bucketName)\\n.key(keyName)\\n.build();\\ns3.putObject(putOb, RequestBody.fromString(keyName));\\nkeys.add(objectId);', '']]]\n",
      "[[['', '}\\nSystem.out.println(keys.size() + \" objects successfully created.\");\\n// Delete multiple objects in one request.\\nDelete del = Delete.builder()\\n.objects(keys)\\n.build();\\ntry {\\nDeleteObjectsRequest multiObjectDeleteRequest =\\nDeleteObjectsRequest.builder()\\n.bucket(bucketName)\\n.delete(del)\\n.build();\\ns3.deleteObjects(multiObjectDeleteRequest);\\nSystem.out.println(\"Multiple objects are deleted!\");\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { DeleteObjectsCommand, S3Client } from \"@aws-sdk/client-s3\";', '']]]\n",
      "[[['', 'const client = new S3Client({});\\nexport const main = async () => {\\nconst command = new DeleteObjectsCommand({\\nBucket: \"test-bucket\",\\nDelete: {\\nObjects: [{ Key: \"object1.txt\" }, { Key: \"object2.txt\" }],\\n},\\n});\\ntry {\\nconst { Deleted } = await client.send(command);\\nconsole.log(\\n`Successfully deleted ${Deleted.length} objects from S3 bucket. Deleted\\nobjects:`,\\n);\\nconsole.log(Deleted.map((d) => ` • ${d.Key}`).join(\"\\\\n\"));\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun deleteBucketObjects(\\nbucketName: String,\\nobjectName: String,\\n) {\\nval objectId =\\nObjectIdentifier {', '']]]\n",
      "[[['', 'key = objectName\\n}\\nval delOb =\\nDelete {\\nobjects = listOf(objectId)\\n}\\nval request =\\nDeleteObjectsRequest {\\nbucket = bucketName\\ndelete = delOb\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\ns3.deleteObjects(request)\\nprintln(\"$objectName was deleted from $bucketName\")\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', \"$s3client = new Aws\\\\S3\\\\S3Client(['region' => 'us-west-2']);\\ntry {\\n$objects = [];\\nforeach ($contents['Contents'] as $content) {\\n$objects[] = [\\n'Key' => $content['Key'],\", '']]]\n",
      "[[['', '];\\n}\\n$this->s3client->deleteObjects([\\n\\'Bucket\\' => $this->bucketName,\\n\\'Delete\\' => [\\n\\'Objects\\' => $objects,\\n],\\n]);\\n$check = $this->s3client->listObjectsV2([\\n\\'Bucket\\' => $this->bucketName,\\n]);\\nif (count($check) <= 0) {\\nthrow new Exception(\"Bucket wasn\\'t empty.\");\\n}\\necho \"Deleted all objects and folders from $this->bucketName.\\\\n\";\\n} catch (Exception $exception) {\\necho \"Failed to delete $fileName from $this->bucketName with error:\\n\" . $exception->getMessage();\\nexit(\"Please fix error with object deletion before continuing.\");\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Remove-S3Object -BucketName test-files -Key sample.txt', ''], ['', '', '']], [['', '', ''], ['', 'Remove-S3Object -BucketName test-files -Key sample.txt -VersionId\\nHLbxnx6V9omT6AQYVpks8mmFKQcejpqt', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Remove-S3Object -BucketName test-files -KeyCollection @( \"sample1.txt\",\\n\"sample2.txt\", \"sample3.txt\" )', ''], ['', '', '']], [['', '', ''], ['', 'Remove-S3Object -bucketname \"test-files\" -KeyCollection (Get-S3Object \"test-\\nfiles\" -KeyPrefix \"prefix/subprefix\" | select -ExpandProperty Key)', ''], ['', '', '']], [['', '', ''], ['', 'Get-S3Object -BucketName \"test-files\" -KeyPrefix \"prefix/subprefix\" | Remove-\\nS3Object -Force', ''], ['', '', '']], [['', '', ''], ['', '(Get-S3Version -BucketName \"test-files\").Versions | Where {$_.IsDeleteMarker -eq\\n\"True\"} | Remove-S3Object -Force', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '$keyVersions = @()\\n$markers = (Get-S3Version -BucketName $BucketName).Versions | Where\\n{$_.IsDeleteMarker -eq \"True\"}\\nforeach ($marker in $markers) { $keyVersions += @{ Key = $marker.Key; VersionId =\\n$marker.VersionId } }\\nRemove-S3Object -BucketName $BucketName -KeyAndVersionCollection $keyVersions -\\nForce', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class ObjectWrapper:\\n\"\"\"Encapsulates S3 object actions.\"\"\"\\ndef __init__(self, s3_object):\\n\"\"\"\\n:param s3_object: A Boto3 Object resource. This is a high-level resource\\nin Boto3\\nthat wraps object actions in a class-like structure.\\n\"\"\"\\nself.object = s3_object\\nself.key = self.object.key\\n@staticmethod\\ndef delete_objects(bucket, object_keys):\\n\"\"\"\\nRemoves a list of objects from a bucket.\\nThis operation is done as a batch in a single request.', '']]]\n",
      "[[['', ':param bucket: The bucket that contains the objects. This is a Boto3\\nBucket\\nresource.\\n:param object_keys: The list of keys that identify the objects to remove.\\n:return: The response that contains data about which objects were deleted\\nand any that could not be deleted.\\n\"\"\"\\ntry:\\nresponse = bucket.delete_objects(\\nDelete={\"Objects\": [{\"Key\": key} for key in object_keys]}\\n)\\nif \"Deleted\" in response:\\nlogger.info(\\n\"Deleted objects \\'%s\\' from bucket \\'%s\\'.\",\\n[del_obj[\"Key\"] for del_obj in response[\"Deleted\"]],\\nbucket.name,\\n)\\nif \"Errors\" in response:\\nlogger.warning(\\n\"Could not delete objects \\'%s\\' from bucket \\'%s\\'.\",\\n[\\nf\"{del_obj[\\'Key\\']}: {del_obj[\\'Code\\']}\"\\nfor del_obj in response[\"Errors\"]\\n],\\nbucket.name,\\n)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t delete any objects from bucket %s.\",\\nbucket.name)\\nraise\\nelse:\\nreturn response', ''], ['', '', '']], [['', '', ''], ['', 'class ObjectWrapper:\\n\"\"\"Encapsulates S3 object actions.\"\"\"\\ndef __init__(self, s3_object):\\n\"\"\"', '']]]\n",
      "[[['', ':param s3_object: A Boto3 Object resource. This is a high-level resource\\nin Boto3\\nthat wraps object actions in a class-like structure.\\n\"\"\"\\nself.object = s3_object\\nself.key = self.object.key\\n@staticmethod\\ndef empty_bucket(bucket):\\n\"\"\"\\nRemove all objects from a bucket.\\n:param bucket: The bucket to empty. This is a Boto3 Bucket resource.\\n\"\"\"\\ntry:\\nbucket.objects.delete()\\nlogger.info(\"Emptied bucket \\'%s\\'.\", bucket.name)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t empty bucket \\'%s\\'.\", bucket.name)\\nraise', ''], ['', '', '']], [['', '', ''], ['', 'def permanently_delete_object(bucket, object_key):\\n\"\"\"\\nPermanently deletes a versioned object by deleting all of its versions.\\nUsage is shown in the usage_demo_single_object function at the end of this\\nmodule.\\n:param bucket: The bucket that contains the object.\\n:param object_key: The object to delete.\\n\"\"\"\\ntry:\\nbucket.object_versions.filter(Prefix=object_key).delete()\\nlogger.info(\"Permanently deleted all versions of object %s.\", object_key)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t delete all versions of %s.\", object_key)\\nraise', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '# Deletes the objects in an Amazon S3 bucket and deletes the bucket.\\n#\\n# @param bucket [Aws::S3::Bucket] The bucket to empty and delete.\\ndef delete_bucket(bucket)\\nputs(\"\\\\nDo you want to delete all of the objects as well as the bucket (y/n)?\\n\")\\nanswer = gets.chomp.downcase\\nif answer == \"y\"\\nbucket.objects.batch_delete!\\nbucket.delete\\nputs(\"Emptied and deleted bucket #{bucket.name}.\\\\n\")\\nend\\nrescue Aws::Errors::ServiceError => e\\nputs(\"Couldn\\'t empty and delete bucket #{bucket.name}.\")\\nputs(\"\\\\t#{e.code}: #{e.message}\")\\nraise\\nend', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'pub async fn delete_objects(client: &Client, bucket_name: &str) ->\\nResult<Vec<String>, Error> {\\nlet objects = client.list_objects_v2().bucket(bucket_name).send().await?;\\nlet mut delete_objects: Vec<ObjectIdentifier> = vec![];\\nfor obj in objects.contents() {\\nlet obj_id = ObjectIdentifier::builder()\\n.set_key(Some(obj.key().unwrap().to_string()))\\n.build()\\n.map_err(Error::from)?;\\ndelete_objects.push(obj_id);\\n}\\nlet return_keys = delete_objects.iter().map(|o| o.key.clone()).collect();\\nif !delete_objects.is_empty() {\\nclient\\n.delete_objects()\\n.bucket(bucket_name)\\n.delete(\\nDelete::builder()\\n.set_objects(Some(delete_objects))\\n.build()\\n.map_err(Error::from)?,\\n)\\n.send()\\n.await?;\\n}\\nlet objects: ListObjectsV2Output =\\nclient.list_objects_v2().bucket(bucket_name).send().await?;', '']]]\n",
      "[[['', 'eprintln!(\"{objects:?}\");\\nmatch objects.key_count {\\nSome(0) => Ok(return_keys),\\n_ => Err(Error::unhandled(\\n\"There were still objects left in the bucket.\",\\n)),\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis is prerelease documentation for an SDK in preview release. It is subject to\\nchange.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'public func deleteObjects(bucket: String, keys: [String]) async throws {\\nlet input = DeleteObjectsInput(\\nbucket: bucket,\\ndelete: S3ClientTypes.Delete(\\nobjects: keys.map({ S3ClientTypes.ObjectIdentifier(key: $0) }),\\nquiet: true\\n)\\n)\\ndo {\\nlet output = try await client.deleteObjects(input: input)', '']]]\n",
      "[[['', \"// As of the last update to this example, any errors are returned\\n// in the `output` object's `errors` property. If there are any\\n// errors in this array, throw an exception. Once the error\\n// handling is finalized in later updates to the AWS SDK for\\n// Swift, this example will be updated to handle errors better.\\nguard let errors = output.errors else {\\nreturn // No errors.\\n}\\nif errors.count != 0 {\\nthrow ServiceHandlerError.deleteObjectsError\\n}\\n} catch {\\nthrow error\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api delete-public-access-block \\\\\\n--bucket my-bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Remove-S3PublicAccessBlock -BucketName 's3testbucket' -Force -Select\\n'^BucketName'\", ''], ['', '', '']], [['', '', ''], ['', 's3testbucket', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-accelerate-configuration \\\\\\n--bucket my-bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Status\": \"Enabled\"\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Get-S3BucketAccelerateConfiguration -BucketName 's3testbucket'\", ''], ['', '', '']], [['', '', ''], ['', 'Value\\n-----\\nEnabled', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Get the access control list (ACL) for the new bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized client object used to get the\\n/// access control list (ACL) of the bucket.</param>\\n/// <param name=\"newBucketName\">The name of the newly created bucket.</\\nparam>\\n/// <returns>An S3AccessControlList.</returns>\\npublic static async Task<S3AccessControlList>\\nGetACLForBucketAsync(IAmazonS3 client, string newBucketName)\\n{\\n// Retrieve bucket ACL to show that the ACL was properly applied to\\n// the new bucket.\\nGetACLResponse getACLResponse = await client.GetACLAsync(new\\nGetACLRequest\\n{\\nBucketName = newBucketName,\\n});\\nreturn getACLResponse.AccessControlList;\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::GetBucketAcl(const Aws::String &bucketName,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client s3_client(clientConfig);\\nAws::S3::Model::GetBucketAclRequest request;\\nrequest.SetBucket(bucketName);\\nAws::S3::Model::GetBucketAclOutcome outcome =\\ns3_client.GetBucketAcl(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &err = outcome.GetError();\\nstd::cerr << \"Error: GetBucketAcl: \"\\n<< err.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nAws::Vector<Aws::S3::Model::Grant> grants =\\noutcome.GetResult().GetGrants();\\nfor (auto it = grants.begin(); it != grants.end(); it++) {\\nAws::S3::Model::Grant grant = *it;\\nAws::S3::Model::Grantee grantee = grant.GetGrantee();\\nstd::cout << \"For bucket \" << bucketName << \": \"\\n<< std::endl << std::endl;\\nif (grantee.TypeHasBeenSet()) {\\nstd::cout << \"Type: \"\\n<< GetGranteeTypeString(grantee.GetType()) <<\\nstd::endl;', '']]]\n",
      "[[['', '}\\nif (grantee.DisplayNameHasBeenSet()) {\\nstd::cout << \"Display name: \"\\n<< grantee.GetDisplayName() << std::endl;\\n}\\nif (grantee.EmailAddressHasBeenSet()) {\\nstd::cout << \"Email address: \"\\n<< grantee.GetEmailAddress() << std::endl;\\n}\\nif (grantee.IDHasBeenSet()) {\\nstd::cout << \"ID: \"\\n<< grantee.GetID() << std::endl;\\n}\\nif (grantee.URIHasBeenSet()) {\\nstd::cout << \"URI: \"\\n<< grantee.GetURI() << std::endl;\\n}\\nstd::cout << \"Permission: \" <<\\nGetPermissionString(grant.GetPermission()) <<\\nstd::endl << std::endl;\\n}\\n}\\nreturn outcome.IsSuccess();\\n}\\n//! Routine which converts a built-in type enumeration to a human-readable\\nstring.\\n/*!\\n\\\\sa GetGranteeTypeString()\\n\\\\param type Type enumeration.\\n*/\\nAws::String GetGranteeTypeString(const Aws::S3::Model::Type &type) {\\nswitch (type) {\\ncase Aws::S3::Model::Type::AmazonCustomerByEmail:\\nreturn \"Email address of an AWS account\";\\ncase Aws::S3::Model::Type::CanonicalUser:\\nreturn \"Canonical user ID of an AWS account\";', '']]]\n",
      "[[['', 'case Aws::S3::Model::Type::Group:\\nreturn \"Predefined Amazon S3 group\";\\ncase Aws::S3::Model::Type::NOT_SET:\\nreturn \"Not set\";\\ndefault:\\nreturn \"Type unknown\";\\n}\\n}\\n//! Routine which converts a built-in type enumeration to a human-readable\\nstring.\\n/*!\\n\\\\sa GetPermissionString()\\n\\\\param permission Permission enumeration.\\n*/\\nAws::String GetPermissionString(const Aws::S3::Model::Permission &permission) {\\nswitch (permission) {\\ncase Aws::S3::Model::Permission::FULL_CONTROL:\\nreturn \"Can list objects in this bucket, create/overwrite/delete \"\\n\"objects in this bucket, and read/write this \"\\n\"bucket\\'s permissions\";\\ncase Aws::S3::Model::Permission::NOT_SET:\\nreturn \"Permission not set\";\\ncase Aws::S3::Model::Permission::READ:\\nreturn \"Can list objects in this bucket\";\\ncase Aws::S3::Model::Permission::READ_ACP:\\nreturn \"Can read this bucket\\'s permissions\";\\ncase Aws::S3::Model::Permission::WRITE:\\nreturn \"Can create, overwrite, and delete objects in this bucket\";\\ncase Aws::S3::Model::Permission::WRITE_ACP:\\nreturn \"Can write this bucket\\'s permissions\";\\ndefault:\\nreturn \"Permission unknown\";\\n}\\nreturn \"Permission unknown\";\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-acl --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Owner\": {\\n\"DisplayName\": \"my-username\",\\n\"ID\": \"7009a8971cd538e11f6b6606438875e7c86c5b672f46db45460ddcd087d36c32\"\\n},\\n\"Grants\": [\\n{\\n\"Grantee\": {\\n\"DisplayName\": \"my-username\",\\n\"ID\":\\n\"7009a8971cd538e11f6b6606438875e7c86c5b672f46db45460ddcd087d36c32\"\\n},\\n\"Permission\": \"FULL_CONTROL\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.regions.Region;', '']]]\n",
      "[[['', 'import software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.GetObjectAclRequest;\\nimport software.amazon.awssdk.services.s3.model.GetObjectAclResponse;\\nimport software.amazon.awssdk.services.s3.model.Grant;\\nimport java.util.List;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class GetAcl {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <objectKey>\\nWhere:\\nbucketName - The Amazon S3 bucket to get the access control\\nlist (ACL) for.\\nobjectKey - The object to get the ACL for.\\\\s\\n\"\"\";\\nif (args.length != 2) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString objectKey = args[1];\\nSystem.out.println(\"Retrieving ACL for object: \" + objectKey);\\nSystem.out.println(\"in bucket: \" + bucketName);\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\ngetBucketACL(s3, objectKey, bucketName);', '']]]\n",
      "[[['', 's3.close();\\nSystem.out.println(\"Done!\");\\n}\\npublic static String getBucketACL(S3Client s3, String objectKey, String\\nbucketName) {\\ntry {\\nGetObjectAclRequest aclReq = GetObjectAclRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.build();\\nGetObjectAclResponse aclRes = s3.getObjectAcl(aclReq);\\nList<Grant> grants = aclRes.grants();\\nString grantee = \"\";\\nfor (Grant grant : grants) {\\nSystem.out.format(\" %s: %s\\\\n\", grant.grantee().id(),\\ngrant.permission());\\ngrantee = grant.grantee().id();\\n}\\nreturn grantee;\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\nreturn \"\";\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import { GetBucketAclCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new GetBucketAclCommand({\\nBucket: \"test-bucket\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3', '']]]\n",
      "[[['', 'that wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef get_acl(self):\\n\"\"\"\\nGet the ACL of the bucket.\\n:return: The ACL of the bucket.\\n\"\"\"\\ntry:\\nacl = self.bucket.Acl()\\nlogger.info(\\n\"Got ACL for bucket %s. Owner is %s.\", self.bucket.name,\\nacl.owner\\n)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t get ACL for bucket %s.\", self.bucket.name)\\nraise\\nelse:\\nreturn acl', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-analytics-configuration \\\\\\n--bucket my-bucket \\\\\\n--id 1', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"AnalyticsConfiguration\": {\\n\"StorageClassAnalysis\": {},\\n\"Id\": \"1\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Get-S3BucketAnalyticsConfiguration -BucketName 's3testbucket' -AnalyticsId\\n'testfilter'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Retrieve the CORS configuration applied to the Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used\\n/// to retrieve the CORS configuration.</param>\\n/// <returns>The created CORS configuration object.</returns>\\nprivate static async Task<CORSConfiguration>\\nRetrieveCORSConfigurationAsync(AmazonS3Client client)\\n{\\nGetCORSConfigurationRequest request = new\\nGetCORSConfigurationRequest()\\n{\\nBucketName = BucketName,\\n};\\nvar response = await client.GetCORSConfigurationAsync(request);\\nvar configuration = response.Configuration;\\nPrintCORSRules(configuration);\\nreturn configuration;\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-cors --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"CORSRules\": [\\n{\\n\"AllowedHeaders\": [\\n\"*\"\\n],\\n\"ExposeHeaders\": [\\n\"x-amz-server-side-encryption\"\\n],\\n\"AllowedMethods\": [\\n\"PUT\",\\n\"POST\",\\n\"DELETE\"\\n],\\n\"MaxAgeSeconds\": 3000,\\n\"AllowedOrigins\": [\\n\"http://www.example.com\"\\n]\\n},\\n{\\n\"AllowedHeaders\": [\\n\"Authorization\"\\n],\\n\"MaxAgeSeconds\": 3000,\\n\"AllowedMethods\": [\\n\"GET\"\\n],\\n\"AllowedOrigins\": [\\n\"*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { GetBucketCorsCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new GetBucketCorsCommand({\\nBucket: \"test-bucket\",\\n});\\ntry {\\nconst { CORSRules } = await client.send(command);\\nCORSRules.forEach((cr, i) => {\\nconsole.log(\\n`\\\\nCORSRule ${i + 1}`,\\n`\\\\n${\"-\".repeat(10)}`,\\n`\\\\nAllowedHeaders: ${cr.AllowedHeaders.join(\" \")}`,\\n`\\\\nAllowedMethods: ${cr.AllowedMethods.join(\" \")}`,\\n`\\\\nAllowedOrigins: ${cr.AllowedOrigins.join(\" \")}`,\\n`\\\\nExposeHeaders: ${cr.ExposeHeaders.join(\" \")}`,\\n`\\\\nMaxAgeSeconds: ${cr.MaxAgeSeconds}`,\\n);\\n});\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef get_cors(self):\\n\"\"\"\\nGet the CORS rules for the bucket.\\n:return The CORS rules for the specified bucket.\\n\"\"\"\\ntry:\\ncors = self.bucket.Cors()\\nlogger.info(\\n\"Got CORS rules %s for bucket \\'%s\\'.\", cors.cors_rules,\\nself.bucket.name\\n)\\nexcept ClientError:\\nlogger.exception((\"Couldn\\'t get CORS for bucket %s.\",\\nself.bucket.name))\\nraise\\nelse:\\nreturn cors', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 bucket CORS configuration.\\nclass BucketCorsWrapper\\nattr_reader :bucket_cors\\n# @param bucket_cors [Aws::S3::BucketCors] A bucket CORS object configured with\\nan existing bucket.\\ndef initialize(bucket_cors)\\n@bucket_cors = bucket_cors\\nend\\n# Gets the CORS configuration of a bucket.\\n#\\n# @return [Aws::S3::Type::GetBucketCorsOutput, nil] The current CORS\\nconfiguration for the bucket.\\ndef get_cors\\n@bucket_cors.data\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t get CORS configuration for #{@bucket_cors.bucket.name}. Here\\'s\\nwhy: #{e.message}\"\\nnil\\nend\\nend', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-encryption \\\\\\n--bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"ServerSideEncryptionConfiguration\": {\\n\"Rules\": [\\n{\\n\"ApplyServerSideEncryptionByDefault\": {\\n\"SSEAlgorithm\": \"AES256\"\\n}\\n}\\n]\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Get-S3BucketEncryption -BucketName 's3casetestbucket'\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-inventory-configuration \\\\\\n--bucket my-bucket \\\\\\n--id 1', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"InventoryConfiguration\": {\\n\"IsEnabled\": true,\\n\"Destination\": {\\n\"S3BucketDestination\": {', '']]]\n",
      "[[['', '\"Format\": \"ORC\",\\n\"Bucket\": \"arn:aws:s3:::my-bucket\",\\n\"AccountId\": \"123456789012\"\\n}\\n},\\n\"IncludedObjectVersions\": \"Current\",\\n\"Id\": \"1\",\\n\"Schedule\": {\\n\"Frequency\": \"Weekly\"\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Get-S3BucketInventoryConfiguration -BucketName 's3testbucket' -InventoryId\\n'testinventory'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Returns a configuration object for the supplied bucket name.\\n/// </summary>\\n/// <param name=\"client\">The S3 client object used to call\\n/// the GetLifecycleConfigurationAsync method.</param>\\n/// <param name=\"bucketName\">The name of the S3 bucket for which a\\n/// configuration will be created.</param>\\n/// <returns>Returns a new LifecycleConfiguration object.</returns>\\npublic static async Task<LifecycleConfiguration>\\nRetrieveLifecycleConfigAsync(IAmazonS3 client, string bucketName)\\n{\\nvar request = new GetLifecycleConfigurationRequest()\\n{\\nBucketName = bucketName,\\n};\\nvar response = await client.GetLifecycleConfigurationAsync(request);\\nvar configuration = response.Configuration;\\nreturn configuration;\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-lifecycle-configuration --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Rules\": [\\n{\\n\"ID\": \"Move rotated logs to Glacier\",\\n\"Prefix\": \"rotated/\",\\n\"Status\": \"Enabled\",\\n\"Transitions\": [\\n{\\n\"Date\": \"2015-11-10T00:00:00.000Z\",\\n\"StorageClass\": \"GLACIER\"\\n}\\n]\\n},\\n{\\n\"Status\": \"Enabled\",\\n\"Prefix\": \"\",\\n\"NoncurrentVersionTransitions\": [\\n{\\n\"NoncurrentDays\": 0,\\n\"StorageClass\": \"GLACIER\"\\n}\\n],\\n\"ID\": \"Move old versions to Glacier\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef get_lifecycle_configuration(self):\\n\"\"\"\\nGet the lifecycle configuration of the bucket.\\n:return: The lifecycle rules of the specified bucket.\\n\"\"\"\\ntry:\\nconfig = self.bucket.LifecycleConfiguration()\\nlogger.info(\\n\"Got lifecycle rules %s for bucket \\'%s\\'.\",\\nconfig.rules,\\nself.bucket.name,\\n)\\nexcept:\\nlogger.exception(\\n\"Couldn\\'t get lifecycle rules for bucket \\'%s\\'.\", self.bucket.name\\n)\\nraise\\nelse:\\nreturn config.rules', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-location --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"LocationConstraint\": \"us-west-2\"\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Get-S3BucketLocation -BucketName 's3testbucket'\", ''], ['', '', '']], [['', '', ''], ['', 'Value\\n-----\\nap-south-1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'async fn show_buckets(strict: bool, client: &Client, region: &str) -> Result<(),\\nError> {\\nlet resp = client.list_buckets().send().await?;\\nlet buckets = resp.buckets();\\nlet num_buckets = buckets.len();\\nlet mut in_region = 0;\\nfor bucket in buckets {\\nif strict {\\nlet r = client\\n.get_bucket_location()\\n.bucket(bucket.name().unwrap_or_default())\\n.send()\\n.await?;\\nif r.location_constraint().unwrap().as_ref() == region {\\nprintln!(\"{}\", bucket.name().unwrap_or_default());\\nin_region += 1;\\n}\\n} else {\\nprintln!(\"{}\", bucket.name().unwrap_or_default());\\n}\\n}\\nprintln!();\\nif strict {\\nprintln!(\\n\"Found {} buckets in the {} region out of a total of {} buckets.\",\\nin_region, region, num_buckets\\n);\\n} else {', '']]]\n",
      "[[['', 'println!(\"Found {} buckets in all regions.\", num_buckets);\\n}\\nOk(())\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-logging \\\\\\n--bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"LoggingEnabled\": {\\n\"TargetPrefix\": \"\",\\n\"TargetBucket\": \"my-bucket-logs\"\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Get-S3BucketLogging -BucketName 's3testbucket'\", ''], ['', '', '']], [['', '', ''], ['', 'TargetBucketName Grants TargetPrefix\\n---------------- ------ ------------\\ntestbucket1 {} testprefix', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-metrics-configuration \\\\\\n--bucket my-bucket \\\\\\n--id 123', ''], ['', '', '']], [['', '', ''], ['', '{', '']]]\n",
      "[[['', '\"MetricsConfiguration\": {\\n\"Filter\": {\\n\"Prefix\": \"logs\"\\n},\\n\"Id\": \"123\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Get-S3BucketMetricsConfiguration -BucketName 's3testbucket' -MetricsId\\n'testfilter'\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-notification --bucket my-bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"TopicConfiguration\": {\\n\"Topic\": \"arn:aws:sns:us-west-2:123456789012:my-notification-topic\",\\n\"Id\": \"YmQzMmEwM2EjZWVlI0NGItNzVtZjI1MC00ZjgyLWZDBiZWNl\",\\n\"Event\": \"s3:ObjectCreated:*\",\\n\"Events\": [\\n\"s3:ObjectCreated:*\"\\n]\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Get-S3BucketNotification -BucketName kt-tools | select -ExpandProperty\\nTopicConfigurations', ''], ['', '', '']], [['', '', ''], ['', 'Id Topic\\n-- -----\\nmimo arn:aws:sns:eu-west-1:123456789012:topic-1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::GetBucketPolicy(const Aws::String &bucketName,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client s3_client(clientConfig);\\nAws::S3::Model::GetBucketPolicyRequest request;\\nrequest.SetBucket(bucketName);\\nAws::S3::Model::GetBucketPolicyOutcome outcome =\\ns3_client.GetBucketPolicy(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &err = outcome.GetError();\\nstd::cerr << \"Error: GetBucketPolicy: \"\\n<< err.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nAws::StringStream policy_stream;\\nAws::String line;\\noutcome.GetResult().GetPolicy() >> line;\\npolicy_stream << line;\\nstd::cout << \"Retrieve the policy for bucket \\'\" << bucketName << \"\\':\\\\n\\\\n\"\\n<<\\npolicy_stream.str() << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-policy --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Policy\": \"{\\\\\"Version\\\\\":\\\\\"2008-10-17\\\\\",\\\\\"Statement\\\\\":[{\\\\\"Sid\\\\\":\\\\\"\\\\\",\\\\\"Effect\\n\\\\\":\\\\\"Allow\\\\\",\\\\\"Principal\\\\\":\\\\\"*\\\\\",\\\\\"Action\\\\\":\\\\\"s3:GetObject\\\\\",\\\\\"Resource\\\\\":\\n\\\\\"arn:aws:s3:::my-bucket/*\\\\\"},{\\\\\"Sid\\\\\":\\\\\"\\\\\",\\\\\"Effect\\\\\":\\\\\"Deny\\\\\",\\\\\"Principal\\\\\":\\n\\\\\"*\\\\\",\\\\\"Action\\\\\":\\\\\"s3:GetObject\\\\\",\\\\\"Resource\\\\\":\\\\\"arn:aws:s3:::my-bucket/secret/*\\n\\\\\"}]}\"\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-policy --bucket mybucket --query Policy --output text >\\npolicy.json', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-policy --bucket mybucket --policy file://policy.json', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.GetBucketPolicyRequest;\\nimport software.amazon.awssdk.services.s3.model.GetBucketPolicyResponse;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class GetBucketPolicy {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName>\\nWhere:\\nbucketName - The Amazon S3 bucket to get the policy from.\\n\"\"\";\\nif (args.length != 1) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}', '']]]\n",
      "[[['', 'String bucketName = args[0];\\nSystem.out.format(\"Getting policy for bucket: \\\\\"%s\\\\\"\\\\n\\\\n\", bucketName);\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nString polText = getPolicy(s3, bucketName);\\nSystem.out.println(\"Policy Text: \" + polText);\\ns3.close();\\n}\\npublic static String getPolicy(S3Client s3, String bucketName) {\\nString policyText;\\nSystem.out.format(\"Getting policy for bucket: \\\\\"%s\\\\\"\\\\n\\\\n\", bucketName);\\nGetBucketPolicyRequest policyReq = GetBucketPolicyRequest.builder()\\n.bucket(bucketName)\\n.build();\\ntry {\\nGetBucketPolicyResponse policyRes = s3.getBucketPolicy(policyReq);\\npolicyText = policyRes.policy();\\nreturn policyText;\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\nreturn \"\";\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { GetBucketPolicyCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new GetBucketPolicyCommand({\\nBucket: \"test-bucket\",\\n});\\ntry {\\nconst { Policy } = await client.send(command);\\nconsole.log(JSON.parse(Policy));\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun getPolicy(bucketName: String): String? {\\nprintln(\"Getting policy for bucket $bucketName\")\\nval request =\\nGetBucketPolicyRequest {\\nbucket = bucketName\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\nval policyRes = s3.getBucketPolicy(request)\\nreturn policyRes.policy\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Get-S3BucketPolicy -BucketName 's3testbucket'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef get_policy(self):\\n\"\"\"\\nGet the security policy of the bucket.\\n:return: The security policy of the specified bucket, in JSON format.\\n\"\"\"\\ntry:\\npolicy = self.bucket.Policy()\\nlogger.info(\\n\"Got policy %s for bucket \\'%s\\'.\", policy.policy, self.bucket.name\\n)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t get policy for bucket \\'%s\\'.\",\\nself.bucket.name)\\nraise\\nelse:\\nreturn json.loads(policy.policy)', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '# Wraps an Amazon S3 bucket policy.\\nclass BucketPolicyWrapper\\nattr_reader :bucket_policy\\n# @param bucket_policy [Aws::S3::BucketPolicy] A bucket policy object\\nconfigured with an existing bucket.\\ndef initialize(bucket_policy)\\n@bucket_policy = bucket_policy\\nend\\n# Gets the policy of a bucket.\\n#\\n# @return [Aws::S3::GetBucketPolicyOutput, nil] The current bucket policy.\\ndef get_policy\\npolicy = @bucket_policy.data.policy\\npolicy.respond_to?(:read) ? policy.read : policy\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t get the policy for #{@bucket_policy.bucket.name}. Here\\'s why:\\n#{e.message}\"\\nnil\\nend\\nend', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-policy-status \\\\\\n--bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"PolicyStatus\": {\\n\"IsPublic\": false\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Get-S3BucketPolicyStatus -BucketName 's3casetestbucket'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-bucket-replication --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"ReplicationConfiguration\": {\\n\"Rules\": [\\n{\\n\"Status\": \"Enabled\",\\n\"Prefix\": \"\",\\n\"Destination\": {\\n\"Bucket\": \"arn:aws:s3:::my-bucket-backup\",\\n\"StorageClass\": \"STANDARD\"\\n},\\n\"ID\": \"ZmUwNzE4ZmQ4tMjVhOS00MTlkLOGI4NDkzZTIWJjNTUtYTA1\"\\n}\\n],\\n\"Role\": \"arn:aws:iam::123456789012:role/s3-replication-role\"\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Get-S3BucketReplication -BucketName mybucket', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-request-payment \\\\\\n--bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Payer\": \"BucketOwner\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Get-S3BucketRequestPayment -BucketName mybucket', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-tagging --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"TagSet\": [\\n{\\n\"Value\": \"marketing\",\\n\"Key\": \"organization\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Get-S3BucketTagging -BucketName 's3casetestbucket'\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-versioning --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Status\": \"Enabled\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Get-S3BucketVersioning -BucketName 's3testbucket'\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Get the website configuration.\\nGetBucketWebsiteRequest getRequest = new\\nGetBucketWebsiteRequest()\\n{\\nBucketName = bucketName,\\n};\\nGetBucketWebsiteResponse getResponse = await\\nclient.GetBucketWebsiteAsync(getRequest);\\nConsole.WriteLine($\"Index document:\\n{getResponse.WebsiteConfiguration.IndexDocumentSuffix}\");\\nConsole.WriteLine($\"Error document:\\n{getResponse.WebsiteConfiguration.ErrorDocument}\");', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::GetWebsiteConfig(const Aws::String &bucketName,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client s3_client(clientConfig);\\nAws::S3::Model::GetBucketWebsiteRequest request;\\nrequest.SetBucket(bucketName);\\nAws::S3::Model::GetBucketWebsiteOutcome outcome =\\ns3_client.GetBucketWebsite(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &err = outcome.GetError();\\nstd::cerr << \"Error: GetBucketWebsite: \"\\n<< err.GetMessage() << std::endl;\\n}\\nelse {\\nAws::S3::Model::GetBucketWebsiteResult websiteResult =\\noutcome.GetResult();\\nstd::cout << \"Success: GetBucketWebsite: \"\\n<< std::endl << std::endl\\n<< \"For bucket \\'\" << bucketName << \"\\':\"\\n<< std::endl\\n<< \"Index page : \"\\n<< websiteResult.GetIndexDocument().GetSuffix()', '']]]\n",
      "[[['', '<< std::endl\\n<< \"Error page: \"\\n<< websiteResult.GetErrorDocument().GetKey()\\n<< std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-bucket-website --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"IndexDocument\": {\\n\"Suffix\": \"index.html\"\\n},\\n\"ErrorDocument\": {\\n\"Key\": \"error.html\"\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { GetBucketWebsiteCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new GetBucketWebsiteCommand({\\nBucket: \"test-bucket\",\\n});\\ntry {\\nconst { ErrorDocument, IndexDocument } = await client.send(command);\\nconsole.log(\\n`Your bucket is set up to host a website. It has an error document:`,\\n`${ErrorDocument.Key}, and an index document: ${IndexDocument.Suffix}.`,\\n);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Get-S3BucketWebsite -BucketName 's3testbucket'\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Shows how to download an object from an Amazon S3 bucket to the\\n/// local computer.\\n/// </summary>', '']]]\n",
      "[[['', '/// <param name=\"client\">An initialized Amazon S3 client object.</param>\\n/// <param name=\"bucketName\">The name of the bucket where the object is\\n/// currently stored.</param>\\n/// <param name=\"objectName\">The name of the object to download.</param>\\n/// <param name=\"filePath\">The path, including filename, where the\\n/// downloaded object will be stored.</param>\\n/// <returns>A boolean value indicating the success or failure of the\\n/// download process.</returns>\\npublic static async Task<bool> DownloadObjectFromBucketAsync(\\nIAmazonS3 client,\\nstring bucketName,\\nstring objectName,\\nstring filePath)\\n{\\n// Create a GetObject request\\nvar request = new GetObjectRequest\\n{\\nBucketName = bucketName,\\nKey = objectName,\\n};\\n// Issue request and remember to dispose of the response\\nusing GetObjectResponse response = await\\nclient.GetObjectAsync(request);\\ntry\\n{\\n// Save object to local file\\nawait response.WriteResponseStreamToFileAsync($\"{filePath}\\\\\\n\\\\{objectName}\", true, CancellationToken.None);\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error saving {objectName}: {ex.Message}\");\\nreturn false;\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '###############################################################################\\n# function errecho\\n#\\n# This function outputs everything sent to it to STDERR (standard error output).\\n###############################################################################\\nfunction errecho() {\\nprintf \"%s\\\\n\" \"$*\" 1>&2\\n}\\n###############################################################################\\n# function download_object_from_bucket\\n#\\n# This function downloads an object in a bucket to a file.\\n#\\n# Parameters:\\n# $1 - The name of the bucket to download the object from.\\n# $2 - The path and file name to store the downloaded bucket.\\n# $3 - The key (name) of the object in the bucket.\\n#\\n# Returns:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction download_object_from_bucket() {\\nlocal bucket_name=$1\\nlocal destination_file_name=$2\\nlocal object_name=$3\\nlocal response\\nresponse=$(aws s3api get-object \\\\\\n--bucket \"$bucket_name\" \\\\\\n--key \"$object_name\" \\\\', '']]]\n",
      "[[['', '\"$destination_file_name\")\\n# shellcheck disable=SC2181\\nif [[ ${?} -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports put-object operation failed.\\\\n$response\"\\nreturn 1\\nfi\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::GetObject(const Aws::String &objectKey,\\nconst Aws::String &fromBucket,\\nconst Aws::Client::ClientConfiguration &clientConfig)\\n{\\nAws::S3::S3Client client(clientConfig);\\nAws::S3::Model::GetObjectRequest request;\\nrequest.SetBucket(fromBucket);\\nrequest.SetKey(objectKey);\\nAws::S3::Model::GetObjectOutcome outcome =\\nclient.GetObject(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &err = outcome.GetError();\\nstd::cerr << \"Error: GetObject: \" <<\\nerr.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {', '']]]\n",
      "[[['', 'std::cout << \"Successfully retrieved \\'\" << objectKey << \"\\' from \\'\"\\n<< fromBucket << \"\\'.\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object --bucket text-content --key dir/my_images.tar.bz2\\nmy_images.tar.bz2', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object --bucket text-content --key dir/my_data --range\\nbytes=8888-9999 my_data_range', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3)\\nactions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform\\nbucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}\\n// DownloadFile gets an object from a bucket and stores it in a local file.\\nfunc (basics BucketBasics) DownloadFile(bucketName string, objectKey string,\\nfileName string) error {\\nresult, err := basics.S3Client.GetObject(context.TODO(), &s3.GetObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t get object %v:%v. Here\\'s why: %v\\\\n\", bucketName,\\nobjectKey, err)\\nreturn err\\n}\\ndefer result.Body.Close()\\nfile, err := os.Create(fileName)\\nif err != nil {\\nlog.Printf(\"Couldn\\'t create file %v. Here\\'s why: %v\\\\n\", fileName, err)\\nreturn err\\n}\\ndefer file.Close()', '']]]\n",
      "[[['', 'body, err := io.ReadAll(result.Body)\\nif err != nil {\\nlog.Printf(\"Couldn\\'t read object body from %v. Here\\'s why: %v\\\\n\", objectKey,\\nerr)\\n}\\n_, err = file.Write(body)\\nreturn err\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.core.ResponseBytes;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.GetObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.model.GetObjectResponse;\\nimport java.io.File;\\nimport java.io.FileOutputStream;\\nimport java.io.IOException;\\nimport java.io.OutputStream;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*', '']]]\n",
      "[[['', '* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class GetObjectData {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <keyName> <path>\\nWhere:\\nbucketName - The Amazon S3 bucket name.\\\\s\\nkeyName - The key name.\\\\s\\npath - The path where the file is written to.\\\\s\\n\"\"\";\\nif (args.length != 3) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString keyName = args[1];\\nString path = args[2];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\ngetObjectBytes(s3, bucketName, keyName, path);\\n}\\npublic static void getObjectBytes(S3Client s3, String bucketName, String\\nkeyName, String path) {\\ntry {\\nGetObjectRequest objectRequest = GetObjectRequest\\n.builder()\\n.key(keyName)\\n.bucket(bucketName)\\n.build();\\nResponseBytes<GetObjectResponse> objectBytes =\\ns3.getObjectAsBytes(objectRequest);', '']]]\n",
      "[[['', 'byte[] data = objectBytes.asByteArray();\\n// Write the data to a local file.\\nFile myFile = new File(path);\\nOutputStream os = new FileOutputStream(myFile);\\nos.write(data);\\nSystem.out.println(\"Successfully obtained bytes from an S3 object\");\\nos.close();\\n} catch (IOException ex) {\\nex.printStackTrace();\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport software.amazon.awssdk.core.sync.RequestBody;\\nimport software.amazon.awssdk.transfer.s3.S3TransferManager;\\nimport software.amazon.awssdk.transfer.s3.model.CompletedFileDownload;\\nimport software.amazon.awssdk.transfer.s3.model.DownloadFileRequest;\\nimport software.amazon.awssdk.transfer.s3.model.FileDownload;\\nimport software.amazon.awssdk.transfer.s3.progress.LoggingTransferListener;\\nimport java.io.IOException;\\nimport java.net.URISyntaxException;\\nimport java.net.URL;\\nimport java.nio.file.Files;\\nimport java.nio.file.Path;\\nimport java.nio.file.Paths;\\nimport java.util.UUID;\\npublic Long downloadFile(S3TransferManager transferManager, String\\nbucketName,\\nString key, String downloadedFileWithPath) {', '']]]\n",
      "[[['', 'DownloadFileRequest downloadFileRequest = DownloadFileRequest.builder()\\n.getObjectRequest(b -> b.bucket(bucketName).key(key))\\n.destination(Paths.get(downloadedFileWithPath))\\n.build();\\nFileDownload downloadFile =\\ntransferManager.downloadFile(downloadFileRequest);\\nCompletedFileDownload downloadResult =\\ndownloadFile.completionFuture().join();\\nlogger.info(\"Content length [{}]\",\\ndownloadResult.response().contentLength());\\nreturn downloadResult.response().contentLength();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.GetObjectTaggingRequest;\\nimport software.amazon.awssdk.services.s3.model.GetObjectTaggingResponse;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.model.Tag;\\nimport java.util.List;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class GetObjectTags {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <keyName>\\\\s', '']]]\n",
      "[[['', 'Where:\\nbucketName - The Amazon S3 bucket name.\\\\s\\nkeyName - A key name that represents the object.\\\\s\\n\"\"\";\\nif (args.length != 2) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString keyName = args[1];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nlistTags(s3, bucketName, keyName);\\ns3.close();\\n}\\npublic static void listTags(S3Client s3, String bucketName, String keyName) {\\ntry {\\nGetObjectTaggingRequest getTaggingRequest = GetObjectTaggingRequest\\n.builder()\\n.key(keyName)\\n.bucket(bucketName)\\n.build();\\nGetObjectTaggingResponse tags =\\ns3.getObjectTagging(getTaggingRequest);\\nList<Tag> tagSet = tags.tagSet();\\nfor (Tag tag : tagSet) {\\nSystem.out.println(tag.key());\\nSystem.out.println(tag.value());\\n}\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.GetUrlRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport java.net.URL;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class GetObjectUrl {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <keyName>\\\\s\\nWhere:\\nbucketName - The Amazon S3 bucket name.\\nkeyName - A key name that represents the object.\\\\s\\n\"\"\";\\nif (args.length != 2) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString keyName = args[1];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();', '']]]\n",
      "[[['', 'getURL(s3, bucketName, keyName);\\ns3.close();\\n}\\npublic static void getURL(S3Client s3, String bucketName, String keyName) {\\ntry {\\nGetUrlRequest request = GetUrlRequest.builder()\\n.bucket(bucketName)\\n.key(keyName)\\n.build();\\nURL url = s3.utilities().getUrl(request);\\nSystem.out.println(\"The URL for \" + keyName + \" is \" + url);\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import java.io.IOException;\\nimport java.io.InputStream;\\nimport java.io.OutputStream;\\nimport java.net.HttpURLConnection;\\nimport java.time.Duration;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.model.GetObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport\\nsoftware.amazon.awssdk.services.s3.presigner.model.GetObjectPresignRequest;\\nimport\\nsoftware.amazon.awssdk.services.s3.presigner.model.PresignedGetObjectRequest;\\nimport software.amazon.awssdk.services.s3.presigner.S3Presigner;\\nimport software.amazon.awssdk.utils.IoUtils;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.', '']]]\n",
      "[[['', '*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class GetObjectPresignedUrl {\\npublic static void main(String[] args) {\\nfinal String USAGE = \"\"\"\\nUsage:\\n<bucketName> <keyName>\\\\s\\nWhere:\\nbucketName - The Amazon S3 bucket name.\\\\s\\nkeyName - A key name that represents a text file.\\\\s\\n\"\"\";\\nif (args.length != 2) {\\nSystem.out.println(USAGE);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString keyName = args[1];\\nRegion region = Region.US_EAST_1;\\nS3Presigner presigner = S3Presigner.builder()\\n.region(region)\\n.build();\\ngetPresignedUrl(presigner, bucketName, keyName);\\npresigner.close();\\n}\\npublic static void getPresignedUrl(S3Presigner presigner, String bucketName,\\nString keyName) {\\ntry {\\nGetObjectRequest getObjectRequest = GetObjectRequest.builder()\\n.bucket(bucketName)\\n.key(keyName)\\n.build();\\nGetObjectPresignRequest getObjectPresignRequest =\\nGetObjectPresignRequest.builder()', '']]]\n",
      "[[['', '.signatureDuration(Duration.ofMinutes(60))\\n.getObjectRequest(getObjectRequest)\\n.build();\\nPresignedGetObjectRequest presignedGetObjectRequest =\\npresigner.presignGetObject(getObjectPresignRequest);\\nString theUrl = presignedGetObjectRequest.url().toString();\\nSystem.out.println(\"Presigned URL: \" + theUrl);\\nHttpURLConnection connection = (HttpURLConnection)\\npresignedGetObjectRequest.url().openConnection();\\npresignedGetObjectRequest.httpRequest().headers().forEach((header,\\nvalues) -> {\\nvalues.forEach(value -> {\\nconnection.addRequestProperty(header, value);\\n});\\n});\\n// Send any request payload that the service needs (not needed when\\n// isBrowserExecutable is true).\\nif (presignedGetObjectRequest.signedPayload().isPresent()) {\\nconnection.setDoOutput(true);\\ntry (InputStream signedPayload =\\npresignedGetObjectRequest.signedPayload().get().asInputStream();\\nOutputStream httpOutputStream =\\nconnection.getOutputStream()) {\\nIoUtils.copy(signedPayload, httpOutputStream);\\n}\\n}\\n// Download the result of executing the request.\\ntry (InputStream content = connection.getInputStream()) {\\nSystem.out.println(\"Service returned response: \");\\nIoUtils.copy(content, System.out);\\n}\\n} catch (S3Exception | IOException e) {\\ne.getStackTrace();\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import software.amazon.awssdk.core.ResponseBytes;\\nimport software.amazon.awssdk.core.sync.ResponseTransformer;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.GetObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.model.GetObjectResponse;\\nimport java.io.File;\\nimport java.io.FileOutputStream;\\nimport java.io.IOException;\\nimport java.io.OutputStream;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class GetDataResponseTransformer {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <keyName> <path>\\nWhere:\\nbucketName - The Amazon S3 bucket name.\\\\s\\nkeyName - The key name.\\\\s\\npath - The path where the file is written to.\\\\s\\n\"\"\";\\nif (args.length != 3) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString keyName = args[1];', '']]]\n",
      "[[['', 'String path = args[2];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\ngetObjectBytes(s3, bucketName, keyName, path);\\ns3.close();\\n}\\npublic static void getObjectBytes(S3Client s3, String bucketName, String\\nkeyName, String path) {\\ntry {\\nGetObjectRequest objectRequest = GetObjectRequest\\n.builder()\\n.key(keyName)\\n.bucket(bucketName)\\n.build();\\nResponseBytes<GetObjectResponse> objectBytes =\\ns3.getObject(objectRequest, ResponseTransformer.toBytes());\\nbyte[] data = objectBytes.asByteArray();\\n// Write the data to a local file.\\nFile myFile = new File(path);\\nOutputStream os = new FileOutputStream(myFile);\\nos.write(data);\\nSystem.out.println(\"Successfully obtained bytes from an S3 object\");\\nos.close();\\n} catch (IOException ex) {\\nex.printStackTrace();\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { GetObjectCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new GetObjectCommand({\\nBucket: \"test-bucket\",\\nKey: \"hello-s3.txt\",\\n});\\ntry {\\nconst response = await client.send(command);\\n// The Body object also has \\'transformToByteArray\\' and \\'transformToWebStream\\'\\nmethods.\\nconst str = await response.Body.transformToString();\\nconsole.log(str);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun getObjectBytes(\\nbucketName: String,\\nkeyName: String,\\npath: String,\\n) {\\nval request =\\nGetObjectRequest {\\nkey = keyName\\nbucket = bucketName\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\ns3.getObject(request) { resp ->\\nval myFile = File(path)\\nresp.body?.writeToFile(myFile)\\nprintln(\"Successfully read $keyName from $bucketName\")\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '$s3client = new Aws\\\\S3\\\\S3Client([\\'region\\' => \\'us-west-2\\']);\\ntry {\\n$file = $this->s3client->getObject([\\n\\'Bucket\\' => $this->bucketName,\\n\\'Key\\' => $fileName,\\n]);\\n$body = $file->get(\\'Body\\');\\n$body->rewind();\\necho \"Downloaded the file and it begins with: {$body->read(26)}.\\\\n\";\\n} catch (Exception $exception) {\\necho \"Failed to download $fileName from $this->bucketName with error:\\n\" . $exception->getMessage();\\nexit(\"Please fix error with file downloading before continuing.\");\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Read-S3Object -BucketName test-files -Key sample.txt -File local-sample.txt', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Read-S3Object -BucketName test-files -KeyPrefix DIR -Folder Local-DIR', ''], ['', '', '']], [['', '', ''], ['', \"Get-S3Bucket | ? { $_.BucketName -like '*config*' } | Get-S3Object | ? { $_.Key -\\nlike '*.json' } | Read-S3Object -Folder C:\\\\ConfigObjects\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class ObjectWrapper:\\n\"\"\"Encapsulates S3 object actions.\"\"\"\\ndef __init__(self, s3_object):\\n\"\"\"\\n:param s3_object: A Boto3 Object resource. This is a high-level resource\\nin Boto3\\nthat wraps object actions in a class-like structure.\\n\"\"\"\\nself.object = s3_object\\nself.key = self.object.key\\ndef get(self):\\n\"\"\"', '']]]\n",
      "[[['', 'Gets the object.\\n:return: The object data in bytes.\\n\"\"\"\\ntry:\\nbody = self.object.get()[\"Body\"].read()\\nlogger.info(\\n\"Got object \\'%s\\' from bucket \\'%s\\'.\",\\nself.object.key,\\nself.object.bucket_name,\\n)\\nexcept ClientError:\\nlogger.exception(\\n\"Couldn\\'t get object \\'%s\\' from bucket \\'%s\\'.\",\\nself.object.key,\\nself.object.bucket_name,\\n)\\nraise\\nelse:\\nreturn body', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectGetWrapper\\nattr_reader :object', '']]]\n",
      "[[['', '# @param object [Aws::S3::Object] An existing Amazon S3 object.\\ndef initialize(object)\\n@object = object\\nend\\n# Gets the object directly to a file.\\n#\\n# @param target_path [String] The path to the file where the object is\\ndownloaded.\\n# @return [Aws::S3::Types::GetObjectOutput, nil] The retrieved object data if\\nsuccessful; otherwise nil.\\ndef get_object(target_path)\\n@object.get(response_target: target_path)\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t get object #{@object.key}. Here\\'s why: #{e.message}\"\\nend\\nend\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nobject_key = \"my-object.txt\"\\ntarget_path = \"my-object-as-file.txt\"\\nwrapper = ObjectGetWrapper.new(Aws::S3::Object.new(bucket_name, object_key))\\nobj_data = wrapper.get_object(target_path)\\nreturn unless obj_data\\nputs \"Object #{object_key} (#{obj_data.content_length} bytes} downloaded to\\n#{target_path}.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectGetEncryptionWrapper\\nattr_reader :object', '']]]\n",
      "[[['', '# @param object [Aws::S3::Object] An existing Amazon S3 object.\\ndef initialize(object)\\n@object = object\\nend\\n# Gets the object into memory.\\n#\\n# @return [Aws::S3::Types::GetObjectOutput, nil] The retrieved object data if\\nsuccessful; otherwise nil.\\ndef get_object\\n@object.get\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t get object #{@object.key}. Here\\'s why: #{e.message}\"\\nend\\nend\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nobject_key = \"my-object.txt\"\\nwrapper = ObjectGetEncryptionWrapper.new(Aws::S3::Object.new(bucket_name,\\nobject_key))\\nobj_data = wrapper.get_object\\nreturn unless obj_data\\nencryption = obj_data.server_side_encryption.nil? ? \"no\" :\\nobj_data.server_side_encryption\\nputs \"Object #{object_key} uses #{encryption} encryption.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'async fn get_object(client: Client, opt: Opt) -> Result<usize, anyhow::Error> {\\ntrace!(\"bucket: {}\", opt.bucket);\\ntrace!(\"object: {}\", opt.object);\\ntrace!(\"destination: {}\", opt.destination.display());\\nlet mut file = File::create(opt.destination.clone())?;\\nlet mut object = client\\n.get_object()\\n.bucket(opt.bucket)\\n.key(opt.object)\\n.send()\\n.await?;\\nlet mut byte_count = 0_usize;\\nwhile let Some(bytes) = object.body.try_next().await? {\\nlet bytes_len = bytes.len();\\nfile.write_all(&bytes)?;\\ntrace!(\"Intermediate write of {bytes_len}\");\\nbyte_count += bytes_len;\\n}\\nOk(byte_count)\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'TRY.\\noo_result = lo_s3->getobject( \" oo_result is returned for\\ntesting purposes. \"\\niv_bucket = iv_bucket_name\\niv_key = iv_object_key\\n).\\nDATA(lv_object_data) = oo_result->get_body( ).\\nMESSAGE \\'Object retrieved from S3 bucket.\\' TYPE \\'I\\'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE \\'Bucket does not exist.\\' TYPE \\'E\\'.\\nCATCH /aws1/cx_s3_nosuchkey.\\nMESSAGE \\'Object key does not exist.\\' TYPE \\'E\\'.\\nENDTRY.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis is prerelease documentation for an SDK in preview release. It is subject to\\nchange.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', \"public func downloadFile(bucket: String, key: String, to: String) async\\nthrows {\\nlet fileUrl = URL(fileURLWithPath: to).appendingPathComponent(key)\\nlet input = GetObjectInput(\\nbucket: bucket,\\nkey: key\\n)\\nlet output = try await client.getObject(input: input)\\n// Get the data stream object. Return immediately if there isn't one.\\nguard let body = output.body,\\nlet data = try await body.readData() else {\\nreturn\\n}\\ntry data.write(to: fileUrl)\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'public func readFile(bucket: String, key: String) async throws -> Data {\\nlet input = GetObjectInput(\\nbucket: bucket,\\nkey: key\\n)\\nlet output = try await client.getObject(input: input)\\n// Get the stream and return its contents in a `Data` object. If\\n// there is no stream, return an empty `Data` object instead.\\nguard let body = output.body,\\nlet data = try await body.readData() else {\\nreturn \"\".data(using: .utf8)!\\n}', '']]]\n",
      "[[['', 'return data\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::GetObjectAcl(const Aws::String &bucketName,\\nconst Aws::String &objectKey,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client s3_client(clientConfig);\\nAws::S3::Model::GetObjectAclRequest request;\\nrequest.SetBucket(bucketName);\\nrequest.SetKey(objectKey);', '']]]\n",
      "[[['', 'Aws::S3::Model::GetObjectAclOutcome outcome =\\ns3_client.GetObjectAcl(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &err = outcome.GetError();\\nstd::cerr << \"Error: GetObjectAcl: \"\\n<< err.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nAws::Vector<Aws::S3::Model::Grant> grants =\\noutcome.GetResult().GetGrants();\\nfor (auto it = grants.begin(); it != grants.end(); it++) {\\nstd::cout << \"For object \" << objectKey << \": \"\\n<< std::endl << std::endl;\\nAws::S3::Model::Grant grant = *it;\\nAws::S3::Model::Grantee grantee = grant.GetGrantee();\\nif (grantee.TypeHasBeenSet()) {\\nstd::cout << \"Type: \"\\n<< GetGranteeTypeString(grantee.GetType()) <<\\nstd::endl;\\n}\\nif (grantee.DisplayNameHasBeenSet()) {\\nstd::cout << \"Display name: \"\\n<< grantee.GetDisplayName() << std::endl;\\n}\\nif (grantee.EmailAddressHasBeenSet()) {\\nstd::cout << \"Email address: \"\\n<< grantee.GetEmailAddress() << std::endl;\\n}\\nif (grantee.IDHasBeenSet()) {\\nstd::cout << \"ID: \"\\n<< grantee.GetID() << std::endl;\\n}\\nif (grantee.URIHasBeenSet()) {\\nstd::cout << \"URI: \"\\n<< grantee.GetURI() << std::endl;', '']]]\n",
      "[[['', '}\\nstd::cout << \"Permission: \" <<\\nGetPermissionString(grant.GetPermission()) <<\\nstd::endl << std::endl;\\n}\\n}\\nreturn outcome.IsSuccess();\\n}\\n//! Routine which converts a built-in type enumeration to a human-readable\\nstring.\\n/*!\\n\\\\fn GetGranteeTypeString()\\n\\\\param type Type enumeration.\\n*/\\nAws::String GetGranteeTypeString(const Aws::S3::Model::Type &type) {\\nswitch (type) {\\ncase Aws::S3::Model::Type::AmazonCustomerByEmail:\\nreturn \"Email address of an AWS account\";\\ncase Aws::S3::Model::Type::CanonicalUser:\\nreturn \"Canonical user ID of an AWS account\";\\ncase Aws::S3::Model::Type::Group:\\nreturn \"Predefined Amazon S3 group\";\\ncase Aws::S3::Model::Type::NOT_SET:\\nreturn \"Not set\";\\ndefault:\\nreturn \"Type unknown\";\\n}\\n}\\n//! Routine which converts a built-in type enumeration to a human-readable\\nstring.\\n/*!\\n\\\\fn GetPermissionString()\\n\\\\param permission Permission enumeration.\\n*/\\nAws::String GetPermissionString(const Aws::S3::Model::Permission &permission) {\\nswitch (permission) {\\ncase Aws::S3::Model::Permission::FULL_CONTROL:\\nreturn \"Can read this object\\'s data and its metadata, \"', '']]]\n",
      "[[['', '\"and read/write this object\\'s permissions\";\\ncase Aws::S3::Model::Permission::NOT_SET:\\nreturn \"Permission not set\";\\ncase Aws::S3::Model::Permission::READ:\\nreturn \"Can read this object\\'s data and its metadata\";\\ncase Aws::S3::Model::Permission::READ_ACP:\\nreturn \"Can read this object\\'s permissions\";\\n// case Aws::S3::Model::Permission::WRITE // Not applicable.\\ncase Aws::S3::Model::Permission::WRITE_ACP:\\nreturn \"Can write this object\\'s permissions\";\\ndefault:\\nreturn \"Permission unknown\";\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object-acl --bucket my-bucket --key index.html', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Owner\": {\\n\"DisplayName\": \"my-username\",\\n\"ID\": \"7009a8971cd538e11f6b6606438875e7c86c5b672f46db45460ddcd087d36c32\"\\n},\\n\"Grants\": [\\n{\\n\"Grantee\": {\\n\"DisplayName\": \"my-username\",\\n\"ID\":\\n\"7009a8971cd538e11f6b6606438875e7c86c5b672f46db45460ddcd087d36c32\"\\n},', '']]]\n",
      "[[['', '\"Permission\": \"FULL_CONTROL\"\\n},\\n{\\n\"Grantee\": {\\n\"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\"\\n},\\n\"Permission\": \"READ\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun getBucketACL(\\nobjectKey: String,\\nbucketName: String,\\n) {\\nval request =\\nGetObjectAclRequest {\\nbucket = bucketName\\nkey = objectKey\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\nval response = s3.getObjectAcl(request)\\nresponse.grants?.forEach { grant ->\\nprintln(\"Grant permission is ${grant.permission}\")\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class ObjectWrapper:\\n\"\"\"Encapsulates S3 object actions.\"\"\"\\ndef __init__(self, s3_object):\\n\"\"\"\\n:param s3_object: A Boto3 Object resource. This is a high-level resource\\nin Boto3\\nthat wraps object actions in a class-like structure.\\n\"\"\"\\nself.object = s3_object\\nself.key = self.object.key\\ndef get_acl(self):\\n\"\"\"\\nGets the ACL of the object.\\n:return: The ACL of the object.\\n\"\"\"\\ntry:\\nacl = self.object.Acl()\\nlogger.info(\\n\"Got ACL for object %s owned by %s.\",\\nself.object.key,\\nacl.owner[\"DisplayName\"],\\n)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t get ACL for object %s.\", self.object.key)\\nraise\\nelse:', '']]]\n",
      "[[['', 'return acl', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Get the legal hold details for an S3 object.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket of the object.</param>\\n/// <param name=\"objectKey\">The object key.</param>\\n/// <returns>The object legal hold details.</returns>\\npublic async Task<ObjectLockLegalHold> GetObjectLegalHold(string bucketName,\\nstring objectKey)', '']]]\n",
      "[[['', '{\\ntry\\n{\\nvar request = new GetObjectLegalHoldRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey\\n};\\nvar response = await _amazonS3.GetObjectLegalHoldAsync(request);\\nConsole.WriteLine($\"\\\\tObject legal hold for {objectKey} in\\n{bucketName}: \" +\\n$\"\\\\n\\\\tStatus: {response.LegalHold.Status}\");\\nreturn response.LegalHold;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tUnable to fetch legal hold: \\'{ex.Message}\\'\");\\nreturn new ObjectLockLegalHold();\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object-legal-hold \\\\\\n--bucket my-bucket-with-object-lock \\\\\\n--key doc1.rtf', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"LegalHold\": {', '']]]\n",
      "[[['', '\"Status\": \"ON\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Get the legal hold details for an S3 object.\\npublic ObjectLockLegalHold getObjectLegalHold(String bucketName, String\\nobjectKey) {\\ntry {\\nGetObjectLegalHoldRequest legalHoldRequest =\\nGetObjectLegalHoldRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.build();\\nGetObjectLegalHoldResponse response =\\ngetClient().getObjectLegalHold(legalHoldRequest);\\nSystem.out.println(\"Object legal hold for \" + objectKey + \" in \" +\\nbucketName +\\n\":\\\\n\\\\tStatus: \" + response.legalHold().status());\\nreturn response.legalHold();\\n} catch (S3Exception ex) {\\nSystem.out.println(\"\\\\tUnable to fetch legal hold: \\'\" +\\nex.getMessage() + \"\\'\");\\n}\\nreturn null;\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Get the object lock configuration details for an S3 bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket to get details.</param>\\n/// <returns>The bucket\\'s object lock configuration details.</returns>\\npublic async Task<ObjectLockConfiguration>\\nGetBucketObjectLockConfiguration(string bucketName)\\n{\\ntry\\n{\\nvar request = new GetObjectLockConfigurationRequest()\\n{\\nBucketName = bucketName\\n};', '']]]\n",
      "[[['', 'var response = await\\n_amazonS3.GetObjectLockConfigurationAsync(request);\\nConsole.WriteLine($\"\\\\tBucket object lock config for {bucketName} in\\n{bucketName}: \" +\\n$\"\\\\n\\\\tEnabled:\\n{response.ObjectLockConfiguration.ObjectLockEnabled}\" +\\n$\"\\\\n\\\\tRule:\\n{response.ObjectLockConfiguration.Rule?.DefaultRetention}\");\\nreturn response.ObjectLockConfiguration;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tUnable to fetch object lock config:\\n\\'{ex.Message}\\'\");\\nreturn new ObjectLockConfiguration();\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object-lock-configuration \\\\\\n--bucket my-bucket-with-object-lock', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"ObjectLockConfiguration\": {\\n\"ObjectLockEnabled\": \"Enabled\",\\n\"Rule\": {\\n\"DefaultRetention\": {\\n\"Mode\": \"COMPLIANCE\",', '']]]\n",
      "[[['', '\"Days\": 50\\n}\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Get the object lock configuration details for an S3 bucket.\\npublic void getBucketObjectLockConfiguration(String bucketName) {\\nGetObjectLockConfigurationRequest objectLockConfigurationRequest =\\nGetObjectLockConfigurationRequest.builder()\\n.bucket(bucketName)\\n.build();\\nGetObjectLockConfigurationResponse response =\\ngetClient().getObjectLockConfiguration(objectLockConfigurationRequest);\\nSystem.out.println(\"Bucket object lock config for \"+bucketName +\": \");\\nSystem.out.println(\"\\\\tEnabled:\\n\"+response.objectLockConfiguration().objectLockEnabled());\\nSystem.out.println(\"\\\\tRule: \"+\\nresponse.objectLockConfiguration().rule().defaultRetention());\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport {\\nGetObjectLockConfigurationCommand,\\nS3Client,\\n} from \"@aws-sdk/client-s3\";\\n/**\\n* @param {S3Client} client\\n* @param {string} bucketName\\n*/\\nexport const main = async (client, bucketName) => {\\nconst command = new GetObjectLockConfigurationCommand({\\nBucket: bucketName,\\n// Optionally, you can provide additional parameters\\n// ExpectedBucketOwner: \"ACCOUNT_ID\",\\n});\\ntry {\\nconst { ObjectLockConfiguration } = await client.send(command);\\nconsole.log(`Object Lock Configuration: ${ObjectLockConfiguration}`);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};\\n// Invoke main function if this file was run directly.\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\nmain(new S3Client(), \"BUCKET_NAME\");\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Get-S3ObjectLockConfiguration -BucketName 's3buckettesting' -Select\\nObjectLockConfiguration.ObjectLockEnabled\", ''], ['', '', '']], [['', '', ''], ['', 'Value\\n-----\\nEnabled', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Get the retention period for an S3 object.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket of the object.</param>\\n/// <param name=\"objectKey\">The object key.</param>\\n/// <returns>The object retention details.</returns>\\npublic async Task<ObjectLockRetention> GetObjectRetention(string bucketName,\\nstring objectKey)\\n{\\ntry\\n{\\nvar request = new GetObjectRetentionRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey\\n};\\nvar response = await _amazonS3.GetObjectRetentionAsync(request);\\nConsole.WriteLine($\"\\\\tObject retention for {objectKey} in\\n{bucketName}: \" +\\n$\"\\\\n\\\\t{response.Retention.Mode} until\\n{response.Retention.RetainUntilDate:d}.\");\\nreturn response.Retention;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tUnable to fetch object lock retention:\\n\\'{ex.Message}\\'\");\\nreturn new ObjectLockRetention();\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-object-retention \\\\\\n--bucket my-bucket-with-object-lock \\\\\\n--key doc1.rtf', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Retention\": {\\n\"Mode\": \"GOVERNANCE\",\\n\"RetainUntilDate\": \"2025-01-01T00:00:00.000Z\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Get the retention period for an S3 object.\\npublic ObjectLockRetention getObjectRetention(String bucketName, String key){\\ntry {', '']]]\n",
      "[[['', 'GetObjectRetentionRequest retentionRequest =\\nGetObjectRetentionRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.build();\\nGetObjectRetentionResponse response =\\ngetClient().getObjectRetention(retentionRequest);\\nSystem.out.println(\"tObject retention for \"+key +\"\\nin \"+ bucketName +\": \" + response.retention().mode() +\" until \"+\\nresponse.retention().retainUntilDate() +\".\");\\nreturn response.retention();\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nreturn null;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport { GetObjectRetentionCommand, S3Client } from \"@aws-sdk/client-s3\";\\n/**\\n* @param {S3Client} client\\n* @param {string} bucketName\\n* @param {string} objectKey\\n*/', '']]]\n",
      "[[['', 'export const main = async (client, bucketName, objectKey) => {\\nconst command = new GetObjectRetentionCommand({\\nBucket: bucketName,\\nKey: objectKey,\\n// Optionally, you can provide additional parameters\\n// ExpectedBucketOwner: \"ACCOUNT_ID\",\\n// RequestPayer: \"requester\",\\n// VersionId: \"OBJECT_VERSION_ID\",\\n});\\ntry {\\nconst { Retention } = await client.send(command);\\nconsole.log(`Object Retention Settings: ${Retention.Status}`);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};\\n// Invoke main function if this file was run directly.\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\nmain(new S3Client(), \"BUCKET_NAME\", \"OBJECT_KEY\");\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Get-S3ObjectRetention -BucketName 's3buckettesting' -Key 'testfile.txt'\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api get-object-tagging \\\\\\n--bucket my-bucket \\\\\\n--key doc1.rtf', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"TagSet\": [\\n{\\n\"Value\": \"confidential\",\\n\"Key\": \"designation\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object-tagging \\\\\\n--bucket my-bucket \\\\\\n--key doc2.rtf', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"TagSet\": []\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-object-tagging \\\\\\n--bucket my-bucket \\\\\\n--key doc3.rtf', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"TagSet\": [\\n{\\n\"Value\": \"confidential\",\\n\"Key\": \"designation\"\\n},\\n{\\n\"Value\": \"finance\",\\n\"Key\": \"department\"\\n},\\n{\\n\"Value\": \"payroll\",\\n\"Key\": \"team\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Get-S3ObjectTagSet -Key 'testfile.txt' -BucketName 'testbucket123'\", ''], ['', '', '']], [['', '', ''], ['', 'Key Value\\n--- -----\\ntest value', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api get-public-access-block \\\\\\n--bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"PublicAccessBlockConfiguration\": {\\n\"IgnorePublicAcls\": true,\\n\"BlockPublicPolicy\": true,\\n\"BlockPublicAcls\": true,\\n\"RestrictPublicBuckets\": true\\n}', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', \"Get-S3PublicAccessBlock -BucketName 's3testbucket'\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '###############################################################################\\n# function bucket_exists\\n#\\n# This function checks to see if the specified bucket already exists.\\n#', '']]]\n",
      "[[['', '# Parameters:\\n# $1 - The name of the bucket to check.\\n#\\n# Returns:\\n# 0 - If the bucket already exists.\\n# 1 - If the bucket doesn\\'t exist.\\n###############################################################################\\nfunction bucket_exists() {\\nlocal bucket_name\\nbucket_name=$1\\n# Check whether the bucket already exists.\\n# We suppress all output - we\\'re interested only in the return code.\\nif aws s3api head-bucket \\\\\\n--bucket \"$bucket_name\" \\\\\\n>/dev/null 2>&1; then\\nreturn 0 # 0 in Bash script means true.\\nelse\\nreturn 1 # 1 in Bash script means false.\\nfi\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api head-bucket --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', 'A client error (404) occurred when calling the HeadBucket operation: Not Found', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3)\\nactions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform\\nbucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}\\n// BucketExists checks whether a bucket exists in the current account.\\nfunc (basics BucketBasics) BucketExists(bucketName string) (bool, error) {\\n_, err := basics.S3Client.HeadBucket(context.TODO(), &s3.HeadBucketInput{\\nBucket: aws.String(bucketName),\\n})\\nexists := true\\nif err != nil {\\nvar apiError smithy.APIError\\nif errors.As(err, &apiError) {\\nswitch apiError.(type) {\\ncase *types.NotFound:\\nlog.Printf(\"Bucket %v is available.\\\\n\", bucketName)\\nexists = false\\nerr = nil\\ndefault:\\nlog.Printf(\"Either you don\\'t have access to bucket %v or another error\\noccurred. \"+\\n\"Here\\'s what happened: %v\\\\n\", bucketName, err)\\n}', '']]]\n",
      "[[['', '}\\n} else {\\nlog.Printf(\"Bucket %v exists and you already own it.\", bucketName)\\n}\\nreturn exists, err\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef exists(self):\\n\"\"\"\\nDetermine whether the bucket exists and you have access to it.\\n:return: True when the bucket exists; otherwise, False.\\n\"\"\"\\ntry:', '']]]\n",
      "[[['', 'self.bucket.meta.client.head_bucket(Bucket=self.bucket.name)\\nlogger.info(\"Bucket %s exists.\", self.bucket.name)\\nexists = True\\nexcept ClientError:\\nlogger.warning(\\n\"Bucket %s doesn\\'t exist or you don\\'t have access to it.\",\\nself.bucket.name,\\n)\\nexists = False\\nreturn exists', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api head-object --bucket my-bucket --key index.html', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"AcceptRanges\": \"bytes\",\\n\"ContentType\": \"text/html\",\\n\"LastModified\": \"Thu, 16 Apr 2015 18:19:14 GMT\",\\n\"ContentLength\": 77,\\n\"VersionId\": \"null\",\\n\"ETag\": \"\\\\\"30a6ec7e1a9ad79c203d05a589c8b400\\\\\"\",\\n\"Metadata\": {}', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.HeadObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.HeadObjectResponse;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class GetObjectContentType {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <keyName>>\\nWhere:\\nbucketName - The Amazon S3 bucket name.\\\\s\\nkeyName - The key name.\\\\s\\n\"\"\";', '']]]\n",
      "[[['', 'if (args.length != 2) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString keyName = args[1];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\ngetContentType(s3, bucketName, keyName);\\ns3.close();\\n}\\npublic static void getContentType(S3Client s3, String bucketName, String\\nkeyName) {\\ntry {\\nHeadObjectRequest objectRequest = HeadObjectRequest.builder()\\n.key(keyName)\\n.bucket(bucketName)\\n.build();\\nHeadObjectResponse objectHead = s3.headObject(objectRequest);\\nString type = objectHead.contentType();\\nSystem.out.println(\"The object content type is \" + type);\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.HeadObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.HeadObjectResponse;', '']]]\n",
      "[[['', 'import software.amazon.awssdk.services.s3.model.S3Exception;\\npublic class GetObjectRestoreStatus {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <keyName>\\\\s\\nWhere:\\nbucketName - The Amazon S3 bucket name.\\\\s\\nkeyName - A key name that represents the object.\\\\s\\n\"\"\";\\nif (args.length != 2) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString keyName = args[1];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\ncheckStatus(s3, bucketName, keyName);\\ns3.close();\\n}\\npublic static void checkStatus(S3Client s3, String bucketName, String\\nkeyName) {\\ntry {\\nHeadObjectRequest headObjectRequest = HeadObjectRequest.builder()\\n.bucket(bucketName)\\n.key(keyName)\\n.build();\\nHeadObjectResponse response = s3.headObject(headObjectRequest);\\nSystem.out.println(\"The Amazon S3 object restoration status is \" +\\nresponse.restore());\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());', '']]]\n",
      "[[['', 'System.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectExistsWrapper\\nattr_reader :object\\n# @param object [Aws::S3::Object] An Amazon S3 object.\\ndef initialize(object)\\n@object = object\\nend\\n# Checks whether the object exists.\\n#\\n# @return [Boolean] True if the object exists; otherwise false.\\ndef exists?\\n@object.exists?\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t check existence of object\\n#{@object.bucket.name}:#{@object.key}. Here\\'s why: #{e.message}\"\\nfalse\\nend\\nend\\n# Example usage:', '']]]\n",
      "[[['', 'def run_demo\\nbucket_name = \"doc-example-bucket\"\\nobject_key = \"my-object.txt\"\\nwrapper = ObjectExistsWrapper.new(Aws::S3::Object.new(bucket_name, object_key))\\nexists = wrapper.exists?\\nputs \"Object #{object_key} #{exists ? \\'does\\' : \\'does not\\'} exist.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-bucket-analytics-configurations \\\\\\n--bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"AnalyticsConfigurationList\": [\\n{', '']]]\n",
      "[[['', '\"StorageClassAnalysis\": {},\\n\"Id\": \"1\"\\n}\\n],\\n\"IsTruncated\": false\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Get-S3BucketAnalyticsConfigurationList -BucketName 's3casetestbucket'\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-bucket-inventory-configurations \\\\', '']]]\n",
      "[[['', '--bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"InventoryConfigurationList\": [\\n{\\n\"IsEnabled\": true,\\n\"Destination\": {\\n\"S3BucketDestination\": {\\n\"Format\": \"ORC\",\\n\"Bucket\": \"arn:aws:s3:::my-bucket\",\\n\"AccountId\": \"123456789012\"\\n}\\n},\\n\"IncludedObjectVersions\": \"Current\",\\n\"Id\": \"1\",\\n\"Schedule\": {\\n\"Frequency\": \"Weekly\"\\n}\\n},\\n{\\n\"IsEnabled\": true,\\n\"Destination\": {\\n\"S3BucketDestination\": {\\n\"Format\": \"CSV\",\\n\"Bucket\": \"arn:aws:s3:::my-bucket\",\\n\"AccountId\": \"123456789012\"\\n}\\n},\\n\"IncludedObjectVersions\": \"Current\",\\n\"Id\": \"2\",\\n\"Schedule\": {\\n\"Frequency\": \"Daily\"\\n}\\n}\\n],\\n\"IsTruncated\": false\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Get-S3BucketInventoryConfigurationList -BucketName 's3testbucket'\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'namespace ListBucketsExample\\n{\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example uses the AWS SDK for .NET to list the Amazon Simple Storage\\n/// Service (Amazon S3) buckets belonging to the default account.', '']]]\n",
      "[[['', '/// </summary>\\npublic class ListBuckets\\n{\\nprivate static IAmazonS3 _s3Client;\\n/// <summary>\\n/// Get a list of the buckets owned by the default user.\\n/// </summary>\\n/// <param name=\"client\">An initialized Amazon S3 client object.</param>\\n/// <returns>The response from the ListingBuckets call that contains a\\n/// list of the buckets owned by the default user.</returns>\\npublic static async Task<ListBucketsResponse> GetBuckets(IAmazonS3\\nclient)\\n{\\nreturn await client.ListBucketsAsync();\\n}\\n/// <summary>\\n/// This method lists the name and creation date for the buckets in\\n/// the passed List of S3 buckets.\\n/// </summary>\\n/// <param name=\"bucketList\">A List of S3 bucket objects.</param>\\npublic static void DisplayBucketList(List<S3Bucket> bucketList)\\n{\\nbucketList\\n.ForEach(b => Console.WriteLine($\"Bucket name: {b.BucketName},\\ncreated on: {b.CreationDate}\"));\\n}\\npublic static async Task Main()\\n{\\n// The client uses the AWS Region of the default user.\\n// If the Region where the buckets were created is different,\\n// pass the Region to the client constructor. For example:\\n// _s3Client = new AmazonS3Client(RegionEndpoint.USEast1);\\n_s3Client = new AmazonS3Client();\\nvar response = await GetBuckets(_s3Client);\\nDisplayBucketList(response.Buckets);\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::ListBuckets(const Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client client(clientConfig);\\nauto outcome = client.ListBuckets();\\nbool result = true;\\nif (!outcome.IsSuccess()) {\\nstd::cerr << \"Failed with error: \" << outcome.GetError() << std::endl;\\nresult = false;\\n}\\nelse {\\nstd::cout << \"Found \" << outcome.GetResult().GetBuckets().size() << \"\\nbuckets\\\\n\";\\nfor (auto &&b: outcome.GetResult().GetBuckets()) {\\nstd::cout << b.GetName() << std::endl;\\n}\\n}\\nreturn result;\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api list-buckets --query \"Buckets[].Name\"', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3)\\nactions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform\\nbucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}\\n// ListBuckets lists the buckets in the current account.\\nfunc (basics BucketBasics) ListBuckets() ([]types.Bucket, error) {', '']]]\n",
      "[[['', 'result, err := basics.S3Client.ListBuckets(context.TODO(),\\n&s3.ListBucketsInput{})\\nvar buckets []types.Bucket\\nif err != nil {\\nlog.Printf(\"Couldn\\'t list buckets for your account. Here\\'s why: %v\\\\n\", err)\\n} else {\\nbuckets = result.Buckets\\n}\\nreturn buckets, err\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.Bucket;\\nimport software.amazon.awssdk.services.s3.model.ListBucketsResponse;\\nimport java.util.List;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class ListBuckets {\\npublic static void main(String[] args) {', '']]]\n",
      "[[['', 'Region region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nlistAllBuckets(s3);\\n}\\npublic static void listAllBuckets(S3Client s3) {\\nListBucketsResponse response = s3.listBuckets();\\nList<Bucket> bucketList = response.buckets();\\nfor (Bucket bucket: bucketList) {\\nSystem.out.println(\"Bucket name \"+bucket.name());\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { ListBucketsCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new ListBucketsCommand({});\\ntry {\\nconst { Owner, Buckets } = await client.send(command);\\nconsole.log(', '']]]\n",
      "[[['', '`${Owner.DisplayName} owns ${Buckets.length} bucket${\\nBuckets.length === 1 ? \"\" : \"s\"\\n}:`,\\n);\\nconsole.log(`${Buckets.map((b) => ` • ${b.Name}`).join(\"\\\\n\")}`);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', 'Get-S3Bucket', ''], ['', '', '']], [['', '', ''], ['', 'Get-S3Bucket -BucketName test-files', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:', '']]]\n",
      "[[['', '\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\n@staticmethod\\ndef list(s3_resource):\\n\"\"\"\\nGet the buckets in all Regions for the current account.\\n:param s3_resource: A Boto3 S3 resource. This is a high-level resource in\\nBoto3\\nthat contains collections and factory methods to\\ncreate\\nother high-level S3 sub-resources.\\n:return: The list of buckets.\\n\"\"\"\\ntry:\\nbuckets = list(s3_resource.buckets.all())\\nlogger.info(\"Got buckets: %s.\", buckets)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t get buckets.\")\\nraise\\nelse:\\nreturn buckets', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 resource actions.\\nclass BucketListWrapper\\nattr_reader :s3_resource\\n# @param s3_resource [Aws::S3::Resource] An Amazon S3 resource.\\ndef initialize(s3_resource)\\n@s3_resource = s3_resource\\nend\\n# Lists buckets for the current account.\\n#\\n# @param count [Integer] The maximum number of buckets to list.\\ndef list_buckets(count)\\nputs \"Found these buckets:\"\\n@s3_resource.buckets.each do |bucket|\\nputs \"\\\\t#{bucket.name}\"\\ncount -= 1\\nbreak if count.zero?\\nend\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t list buckets. Here\\'s why: #{e.message}\"\\nfalse\\nend\\nend\\n# Example usage:\\ndef run_demo\\nwrapper = BucketListWrapper.new(Aws::S3::Resource.new)\\nwrapper.list_buckets(25)', '']]]\n",
      "[[['', 'end\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'async fn show_buckets(strict: bool, client: &Client, region: &str) -> Result<(),\\nError> {\\nlet resp = client.list_buckets().send().await?;\\nlet buckets = resp.buckets();\\nlet num_buckets = buckets.len();\\nlet mut in_region = 0;\\nfor bucket in buckets {\\nif strict {\\nlet r = client\\n.get_bucket_location()\\n.bucket(bucket.name().unwrap_or_default())\\n.send()\\n.await?;\\nif r.location_constraint().unwrap().as_ref() == region {\\nprintln!(\"{}\", bucket.name().unwrap_or_default());\\nin_region += 1;\\n}\\n} else {\\nprintln!(\"{}\", bucket.name().unwrap_or_default());\\n}\\n}', '']]]\n",
      "[[['', 'println!();\\nif strict {\\nprintln!(\\n\"Found {} buckets in the {} region out of a total of {} buckets.\",\\nin_region, region, num_buckets\\n);\\n} else {\\nprintln!(\"Found {} buckets in all regions.\", num_buckets);\\n}\\nOk(())\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis is prerelease documentation for an SDK in preview release. It is subject to\\nchange.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// Return an array containing information about every available bucket.\\n///\\n/// - Returns: An array of ``S3ClientTypes.Bucket`` objects describing\\n/// each bucket.\\npublic func getAllBuckets() async throws -> [S3ClientTypes.Bucket] {\\nlet output = try await client.listBuckets(input: ListBucketsInput())\\nguard let buckets = output.buckets else {\\nreturn []', '']]]\n",
      "[[['', '}\\nreturn buckets\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-multipart-uploads --bucket my-bucket', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Uploads\": [\\n{\\n\"Initiator\": {\\n\"DisplayName\": \"username\",\\n\"ID\": \"arn:aws:iam::0123456789012:user/username\"\\n},\\n\"Initiated\": \"2015-06-02T18:01:30.000Z\",\\n\"UploadId\":\\n\"dfRtDYU0WWCCcH43C3WFbkRONycyCpTJJvxu2i5GYkZljF.Yxwh6XG7WfS2vC4to6HiV6Yjlx.cph0gt', 'N']]]\n",
      "[[['', '\"StorageClass\": \"STANDARD\",\\n\"Key\": \"multipart/01\",\\n\"Owner\": {\\n\"DisplayName\": \"aws-account-name\",\\n\"ID\":\\n\"100719349fc3b6dcd7c820a124bf7aecd408092c3d7b51b38494939801fc248b\"\\n}\\n}\\n],\\n\"CommonPrefixes\": []\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.ListMultipartUploadsRequest;\\nimport software.amazon.awssdk.services.s3.model.ListMultipartUploadsResponse;\\nimport software.amazon.awssdk.services.s3.model.MultipartUpload;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport java.util.List;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*', '']]]\n",
      "[[['', '* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class ListMultipartUploads {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName>\\\\s\\nWhere:\\nbucketName - The name of the Amazon S3 bucket where an in-\\nprogress multipart upload is occurring.\\n\"\"\";\\nif (args.length != 1) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nlistUploads(s3, bucketName);\\ns3.close();\\n}\\npublic static void listUploads(S3Client s3, String bucketName) {\\ntry {\\nListMultipartUploadsRequest listMultipartUploadsRequest =\\nListMultipartUploadsRequest.builder()\\n.bucket(bucketName)\\n.build();\\nListMultipartUploadsResponse response =\\ns3.listMultipartUploads(listMultipartUploadsRequest);\\nList<MultipartUpload> uploads = response.uploads();\\nfor (MultipartUpload upload : uploads) {\\nSystem.out.println(\"Upload in progress: Key = \\\\\"\" + upload.key()\\n+ \"\\\\\", id = \" + upload.uploadId());\\n}', '']]]\n",
      "[[['', '} catch (S3Exception e) {\\nSystem.err.println(e.getMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>', '']]]\n",
      "[[['', '/// This example lists the versions of the objects in a version enabled\\n/// Amazon Simple Storage Service (Amazon S3) bucket.\\n/// </summary>\\npublic class ListObjectVersions\\n{\\npublic static async Task Main()\\n{\\nstring bucketName = \"doc-example-bucket\";\\n// If the AWS Region where your bucket is defined is different from\\n// the AWS Region where the Amazon S3 bucket is defined, pass the\\nconstant\\n// for the AWS Region to the client constructor like this:\\n// var client = new AmazonS3Client(RegionEndpoint.USWest2);\\nIAmazonS3 client = new AmazonS3Client();\\nawait GetObjectListWithAllVersionsAsync(client, bucketName);\\n}\\n/// <summary>\\n/// This method lists all versions of the objects within an Amazon S3\\n/// version enabled bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized client object used to call\\n/// ListVersionsAsync.</param>\\n/// <param name=\"bucketName\">The name of the version enabled Amazon S3\\nbucket\\n/// for which you want to list the versions of the contained objects.</\\nparam>\\npublic static async Task GetObjectListWithAllVersionsAsync(IAmazonS3\\nclient, string bucketName)\\n{\\ntry\\n{\\n// When you instantiate the ListVersionRequest, you can\\n// optionally specify a key name prefix in the request\\n// if you want a list of object versions of a specific object.\\n// For this example we set a small limit in MaxKeys to return\\n// a small list of versions.\\nListVersionsRequest request = new ListVersionsRequest()\\n{\\nBucketName = bucketName,\\nMaxKeys = 2,\\n};', '']]]\n",
      "[[['', 'do\\n{\\nListVersionsResponse response = await\\nclient.ListVersionsAsync(request);\\n// Process response.\\nforeach (S3ObjectVersion entry in response.Versions)\\n{\\nConsole.WriteLine($\"key: {entry.Key} size:\\n{entry.Size}\");\\n}\\n// If response is truncated, set the marker to get the next\\n// set of keys.\\nif (response.IsTruncated)\\n{\\nrequest.KeyMarker = response.NextKeyMarker;\\nrequest.VersionIdMarker = response.NextVersionIdMarker;\\n}\\nelse\\n{\\nrequest = null;\\n}\\n}\\nwhile (request != null);\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error: \\'{ex.Message}\\'\");\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api list-object-versions --bucket my-bucket --prefix index.html', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"DeleteMarkers\": [\\n{\\n\"Owner\": {\\n\"DisplayName\": \"my-username\",\\n\"ID\":\\n\"7009a8971cd660687538875e7c86c5b672fe116bd438f46db45460ddcd036c32\"\\n},\\n\"IsLatest\": true,\\n\"VersionId\": \"B2VsEK5saUNNHKcOAJj7hIE86RozToyq\",\\n\"Key\": \"index.html\",\\n\"LastModified\": \"2015-11-10T00:57:03.000Z\"\\n},\\n{\\n\"Owner\": {\\n\"DisplayName\": \"my-username\",\\n\"ID\":\\n\"7009a8971cd660687538875e7c86c5b672fe116bd438f46db45460ddcd036c32\"\\n},\\n\"IsLatest\": false,\\n\"VersionId\": \".FLQEZscLIcfxSq.jsFJ.szUkmng2Yw6\",\\n\"Key\": \"index.html\",\\n\"LastModified\": \"2015-11-09T23:32:20.000Z\"\\n}\\n],\\n\"Versions\": [\\n{\\n\"LastModified\": \"2015-11-10T00:20:11.000Z\",\\n\"VersionId\": \"Rb_l2T8UHDkFEwCgJjhlgPOZC0qJ.vpD\",\\n\"ETag\": \"\\\\\"0622528de826c0df5db1258a23b80be5\\\\\"\",\\n\"StorageClass\": \"STANDARD\",\\n\"Key\": \"index.html\",', '']]]\n",
      "[[['', '\"Owner\": {\\n\"DisplayName\": \"my-username\",\\n\"ID\":\\n\"7009a8971cd660687538875e7c86c5b672fe116bd438f46db45460ddcd036c32\"\\n},\\n\"IsLatest\": false,\\n\"Size\": 38\\n},\\n{\\n\"LastModified\": \"2015-11-09T23:26:41.000Z\",\\n\"VersionId\": \"rasWWGpgk9E4s0LyTJgusGeRQKLVIAFf\",\\n\"ETag\": \"\\\\\"06225825b8028de826c0df5db1a23be5\\\\\"\",\\n\"StorageClass\": \"STANDARD\",\\n\"Key\": \"index.html\",\\n\"Owner\": {\\n\"DisplayName\": \"my-username\",\\n\"ID\":\\n\"7009a8971cd660687538875e7c86c5b672fe116bd438f46db45460ddcd036c32\"\\n},\\n\"IsLatest\": false,\\n\"Size\": 38\\n},\\n{\\n\"LastModified\": \"2015-11-09T22:50:50.000Z\",\\n\"VersionId\": \"null\",\\n\"ETag\": \"\\\\\"d1f45267a863c8392e07d24dd592f1b9\\\\\"\",\\n\"StorageClass\": \"STANDARD\",\\n\"Key\": \"index.html\",\\n\"Owner\": {\\n\"DisplayName\": \"my-username\",\\n\"ID\":\\n\"7009a8971cd660687538875e7c86c5b672fe116bd438f46db45460ddcd036c32\"\\n},\\n\"IsLatest\": false,\\n\"Size\": 533823\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'async fn show_versions(client: &Client, bucket: &str) -> Result<(), Error> {\\nlet resp = client.list_object_versions().bucket(bucket).send().await?;\\nfor version in resp.versions() {\\nprintln!(\"{}\", version.key().unwrap_or_default());\\nprintln!(\" version ID: {}\", version.version_id().unwrap_or_default());\\nprintln!();\\n}\\nOk(())\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"aws s3api list-objects --bucket text-content --query 'Contents[].{Key: Key, Size:\\nSize}'\", ''], ['', '', '']], [['', '', ''], ['', 'Get-S3Object -BucketName test-files', ''], ['', '', '']], [['', '', ''], ['', 'Get-S3Object -BucketName test-files -Key sample.txt', ''], ['', '', '']], [['', '', ''], ['', 'Get-S3Object -BucketName test-files -KeyPrefix sample', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Shows how to list the objects in an Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">An initialized Amazon S3 client object.</param>\\n/// <param name=\"bucketName\">The name of the bucket for which to list\\n/// the contents.</param>\\n/// <returns>A boolean value indicating the success or failure of the\\n/// copy operation.</returns>\\npublic static async Task<bool> ListBucketContentsAsync(IAmazonS3 client,\\nstring bucketName)\\n{\\ntry\\n{\\nvar request = new ListObjectsV2Request\\n{\\nBucketName = bucketName,\\nMaxKeys = 5,', '']]]\n",
      "[[['', '};\\nConsole.WriteLine(\"--------------------------------------\");\\nConsole.WriteLine($\"Listing the contents of {bucketName}:\");\\nConsole.WriteLine(\"--------------------------------------\");\\nListObjectsV2Response response;\\ndo\\n{\\nresponse = await client.ListObjectsV2Async(request);\\nresponse.S3Objects\\n.ForEach(obj => Console.WriteLine($\"{obj.Key,-35}\\n{obj.LastModified.ToShortDateString(),10}{obj.Size,10}\"));\\n// If the response is truncated, set the request\\nContinuationToken\\n// from the NextContinuationToken property of the response.\\nrequest.ContinuationToken = response.NextContinuationToken;\\n}\\nwhile (response.IsTruncated);\\nreturn true;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error encountered on server.\\nMessage:\\'{ex.Message}\\' getting list of objects.\");\\nreturn false;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>', '']]]\n",
      "[[['', '/// The following example lists objects in an Amazon Simple Storage\\n/// Service (Amazon S3) bucket.\\n/// </summary>\\npublic class ListObjectsPaginator\\n{\\nprivate const string BucketName = \"doc-example-bucket\";\\npublic static async Task Main()\\n{\\nIAmazonS3 s3Client = new AmazonS3Client();\\nConsole.WriteLine($\"Listing the objects contained in {BucketName}:\\n\\\\n\");\\nawait ListingObjectsAsync(s3Client, BucketName);\\n}\\n/// <summary>\\n/// This method uses a paginator to retrieve the list of objects in an\\n/// an Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">An Amazon S3 client object.</param>\\n/// <param name=\"bucketName\">The name of the S3 bucket whose objects\\n/// you want to list.</param>\\npublic static async Task ListingObjectsAsync(IAmazonS3 client, string\\nbucketName)\\n{\\nvar listObjectsV2Paginator = client.Paginators.ListObjectsV2(new\\nListObjectsV2Request\\n{\\nBucketName = bucketName,\\n});\\nawait foreach (var response in listObjectsV2Paginator.Responses)\\n{\\nConsole.WriteLine($\"HttpStatusCode: {response.HttpStatusCode}\");\\nConsole.WriteLine($\"Number of Keys: {response.KeyCount}\");\\nforeach (var entry in response.S3Objects)\\n{\\nConsole.WriteLine($\"Key = {entry.Key} Size = {entry.Size}\");\\n}\\n}\\n}\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '###############################################################################\\n# function errecho\\n#\\n# This function outputs everything sent to it to STDERR (standard error output).\\n###############################################################################\\nfunction errecho() {\\nprintf \"%s\\\\n\" \"$*\" 1>&2\\n}\\n###############################################################################\\n# function list_items_in_bucket\\n#\\n# This function displays a list of the files in the bucket with each file\\'s\\n# size. The function uses the --query parameter to retrieve only the key and\\n# size fields from the Contents collection.\\n#\\n# Parameters:\\n# $1 - The name of the bucket.\\n#\\n# Returns:\\n# The list of files in text format.\\n# And:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction list_items_in_bucket() {\\nlocal bucket_name=$1\\nlocal response', '']]]\n",
      "[[['', 'response=$(aws s3api list-objects \\\\\\n--bucket \"$bucket_name\" \\\\\\n--output text \\\\\\n--query \\'Contents[].{Key: Key, Size: Size}\\')\\n# shellcheck disable=SC2181\\nif [[ ${?} -eq 0 ]]; then\\necho \"$response\"\\nelse\\nerrecho \"ERROR: AWS reports s3api list-objects operation failed.\\\\n$response\"\\nreturn 1\\nfi\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::ListObjects(const Aws::String &bucketName,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client s3_client(clientConfig);\\nAws::S3::Model::ListObjectsV2Request request;\\nrequest.WithBucket(bucketName);\\nAws::String continuationToken; // Used for pagination.\\nAws::Vector<Aws::S3::Model::Object> allObjects;\\ndo {\\nif (!continuationToken.empty()) {\\nrequest.SetContinuationToken(continuationToken);', '']]]\n",
      "[[['', '}\\nauto outcome = s3_client.ListObjectsV2(request);\\nif (!outcome.IsSuccess()) {\\nstd::cerr << \"Error: ListObjects: \" <<\\noutcome.GetError().GetMessage() << std::endl;\\nreturn false;\\n}\\nelse {\\nAws::Vector<Aws::S3::Model::Object> objects =\\noutcome.GetResult().GetContents();\\nallObjects.insert(allObjects.end(), objects.begin(), objects.end());\\ncontinuationToken = outcome.GetResult().GetNextContinuationToken();\\n}\\n} while (!continuationToken.empty());\\nstd::cout << allObjects.size() << \" object(s) found:\" << std::endl;\\nfor (const auto &object: allObjects) {\\nstd::cout << \" \" << object.GetKey() << std::endl;\\n}\\nreturn true;\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api list-objects-v2 \\\\\\n--bucket my-bucket', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Contents\": [\\n{\\n\"LastModified\": \"2019-11-05T23:11:50.000Z\",\\n\"ETag\": \"\\\\\"621503c373607d548b37cff8778d992c\\\\\"\",\\n\"StorageClass\": \"STANDARD\",\\n\"Key\": \"doc1.rtf\",\\n\"Size\": 391\\n},\\n{\\n\"LastModified\": \"2019-11-05T23:11:50.000Z\",\\n\"ETag\": \"\\\\\"a2cecc36ab7c7fe3a71a273b9d45b1b5\\\\\"\",\\n\"StorageClass\": \"STANDARD\",\\n\"Key\": \"doc2.rtf\",\\n\"Size\": 373\\n},\\n{\\n\"LastModified\": \"2019-11-05T23:11:50.000Z\",\\n\"ETag\": \"\\\\\"08210852f65a2e9cb999972539a64d68\\\\\"\",\\n\"StorageClass\": \"STANDARD\",\\n\"Key\": \"doc3.rtf\",\\n\"Size\": 399\\n},\\n{\\n\"LastModified\": \"2019-11-05T23:11:50.000Z\",\\n\"ETag\": \"\\\\\"d1852dd683f404306569471af106988e\\\\\"\",\\n\"StorageClass\": \"STANDARD\",\\n\"Key\": \"doc4.rtf\",\\n\"Size\": 6225\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3)\\nactions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform\\nbucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}\\n// ListObjects lists the objects in a bucket.\\nfunc (basics BucketBasics) ListObjects(bucketName string) ([]types.Object, error)\\n{\\nresult, err := basics.S3Client.ListObjectsV2(context.TODO(),\\n&s3.ListObjectsV2Input{\\nBucket: aws.String(bucketName),\\n})\\nvar contents []types.Object\\nif err != nil {\\nlog.Printf(\"Couldn\\'t list objects in bucket %v. Here\\'s why: %v\\\\n\", bucketName,\\nerr)\\n} else {\\ncontents = result.Contents\\n}\\nreturn contents, err\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.ListObjectsRequest;\\nimport software.amazon.awssdk.services.s3.model.ListObjectsResponse;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.model.S3Object;\\nimport java.util.List;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class ListObjects {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName>\\\\s\\nWhere:\\nbucketName - The Amazon S3 bucket from which objects are\\nread.\\\\s\\n\"\"\";', '']]]\n",
      "[[['', 'if (args.length != 1) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nlistBucketObjects(s3, bucketName);\\ns3.close();\\n}\\npublic static void listBucketObjects(S3Client s3, String bucketName) {\\ntry {\\nListObjectsRequest listObjects = ListObjectsRequest\\n.builder()\\n.bucket(bucketName)\\n.build();\\nListObjectsResponse res = s3.listObjects(listObjects);\\nList<S3Object> objects = res.contents();\\nfor (S3Object myValue : objects) {\\nSystem.out.print(\"\\\\n The name of the key is \" + myValue.key());\\nSystem.out.print(\"\\\\n The object is \" + calKb(myValue.size()) + \"\\nKBs\");\\nSystem.out.print(\"\\\\n The owner is \" + myValue.owner());\\n}\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n// convert bytes to kbs.\\nprivate static long calKb(Long val) {\\nreturn val / 1024;\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.ListObjectsV2Request;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.paginators.ListObjectsV2Iterable;\\npublic class ListObjectsPaginated {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName>\\\\s\\nWhere:\\nbucketName - The Amazon S3 bucket from which objects are\\nread.\\\\s\\n\"\"\";\\nif (args.length != 1) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nlistBucketObjects(s3, bucketName);\\ns3.close();\\n}\\npublic static void listBucketObjects(S3Client s3, String bucketName) {\\ntry {\\nListObjectsV2Request listReq = ListObjectsV2Request.builder()\\n.bucket(bucketName)\\n.maxKeys(1)\\n.build();\\nListObjectsV2Iterable listRes = s3.listObjectsV2Paginator(listReq);\\nlistRes.stream()', '']]]\n",
      "[[['', '.flatMap(r -> r.contents().stream())\\n.forEach(content -> System.out.println(\" Key: \" +\\ncontent.key() + \" size = \" + content.size()));\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import {\\nS3Client,\\n// This command supersedes the ListObjectsCommand and is the recommended way to\\nlist objects.\\nListObjectsV2Command,\\n} from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new ListObjectsV2Command({\\nBucket: \"my-bucket\",\\n// The default and maximum number of keys returned is 1000. This limits it to\\n// one for demonstration purposes.', '']]]\n",
      "[[['', 'MaxKeys: 1,\\n});\\ntry {\\nlet isTruncated = true;\\nconsole.log(\"Your bucket contains the following objects:\\\\n\");\\nlet contents = \"\";\\nwhile (isTruncated) {\\nconst { Contents, IsTruncated, NextContinuationToken } =\\nawait client.send(command);\\nconst contentsList = Contents.map((c) => ` • ${c.Key}`).join(\"\\\\n\");\\ncontents += contentsList + \"\\\\n\";\\nisTruncated = IsTruncated;\\ncommand.input.ContinuationToken = NextContinuationToken;\\n}\\nconsole.log(contents);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun listBucketObjects(bucketName: String) {\\nval request =\\nListObjectsRequest {\\nbucket = bucketName\\n}', '']]]\n",
      "[[['', 'S3Client { region = \"us-east-1\" }.use { s3 ->\\nval response = s3.listObjects(request)\\nresponse.contents?.forEach { myObject ->\\nprintln(\"The name of the key is ${myObject.key}\")\\nprintln(\"The object is ${myObject.size?.let { calKb(it) }} KBs\")\\nprintln(\"The owner is ${myObject.owner}\")\\n}\\n}\\n}\\nprivate fun calKb(intValue: Long): Long = intValue / 1024', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '$s3client = new Aws\\\\S3\\\\S3Client([\\'region\\' => \\'us-west-2\\']);\\ntry {\\n$contents = $this->s3client->listObjectsV2([\\n\\'Bucket\\' => $this->bucketName,\\n]);\\necho \"The contents of your bucket are: \\\\n\";\\nforeach ($contents[\\'Contents\\'] as $content) {\\necho $content[\\'Key\\'] . \"\\\\n\";\\n}\\n} catch (Exception $exception) {\\necho \"Failed to list objects in $this->bucketName with error: \" .\\n$exception->getMessage();\\nexit(\"Please fix error with listing objects before continuing.\");\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class ObjectWrapper:\\n\"\"\"Encapsulates S3 object actions.\"\"\"\\ndef __init__(self, s3_object):\\n\"\"\"\\n:param s3_object: A Boto3 Object resource. This is a high-level resource\\nin Boto3\\nthat wraps object actions in a class-like structure.\\n\"\"\"\\nself.object = s3_object\\nself.key = self.object.key\\n@staticmethod\\ndef list(bucket, prefix=None):\\n\"\"\"\\nLists the objects in a bucket, optionally filtered by a prefix.\\n:param bucket: The bucket to query. This is a Boto3 Bucket resource.\\n:param prefix: When specified, only objects that start with this prefix\\nare listed.\\n:return: The list of objects.\\n\"\"\"\\ntry:\\nif not prefix:\\nobjects = list(bucket.objects.all())\\nelse:\\nobjects = list(bucket.objects.filter(Prefix=prefix))', '']]]\n",
      "[[['', 'logger.info(\\n\"Got objects %s from bucket \\'%s\\'\", [o.key for o in objects],\\nbucket.name\\n)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t get objects for bucket \\'%s\\'.\",\\nbucket.name)\\nraise\\nelse:\\nreturn objects', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 bucket actions.\\nclass BucketListObjectsWrapper\\nattr_reader :bucket\\n# @param bucket [Aws::S3::Bucket] An existing Amazon S3 bucket.\\ndef initialize(bucket)\\n@bucket = bucket\\nend\\n# Lists object in a bucket.\\n#\\n# @param max_objects [Integer] The maximum number of objects to list.\\n# @return [Integer] The number of objects listed.\\ndef list_objects(max_objects)\\ncount = 0', '']]]\n",
      "[[['', 'puts \"The objects in #{@bucket.name} are:\"\\n@bucket.objects.each do |obj|\\nputs \"\\\\t#{obj.key}\"\\ncount += 1\\nbreak if count == max_objects\\nend\\ncount\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t list objects in bucket #{bucket.name}. Here\\'s why:\\n#{e.message}\"\\n0\\nend\\nend\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nwrapper = BucketListObjectsWrapper.new(Aws::S3::Bucket.new(bucket_name))\\ncount = wrapper.list_objects(25)\\nputs \"Listed #{count} objects.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'pub async fn list_objects(client: &Client, bucket: &str) -> Result<(), Error> {\\nlet mut response = client\\n.list_objects_v2()\\n.bucket(bucket.to_owned())', '']]]\n",
      "[[['', '.max_keys(10) // In this example, go 10 at a time.\\n.into_paginator()\\n.send();\\nwhile let Some(result) = response.next().await {\\nmatch result {\\nOk(output) => {\\nfor object in output.contents() {\\nprintln!(\" - {}\", object.key().unwrap_or(\"Unknown\"));\\n}\\n}\\nErr(err) => {\\neprintln!(\"{err:?}\")\\n}\\n}\\n}\\nOk(())\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'TRY.\\noo_result = lo_s3->listobjectsv2( \" oo_result is returned for\\ntesting purposes. \"\\niv_bucket = iv_bucket_name\\n).\\nMESSAGE \\'Retrieved list of objects in S3 bucket.\\' TYPE \\'I\\'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE \\'Bucket does not exist.\\' TYPE \\'E\\'.\\nENDTRY.', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis is prerelease documentation for an SDK in preview release. It is subject to\\nchange.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'public func listBucketFiles(bucket: String) async throws -> [String] {\\nlet input = ListObjectsV2Input(\\nbucket: bucket\\n)\\nlet output = try await client.listObjectsV2(input: input)\\nvar names: [String] = []\\nguard let objList = output.contents else {\\nreturn []\\n}\\nfor obj in objList {\\nif let objName = obj.key {\\nnames.append(objName)\\n}\\n}\\nreturn names\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// Amazon Simple Storage Service (Amazon S3) Transfer Acceleration is a\\n/// bucket-level feature that enables you to perform faster data transfers\\n/// to Amazon S3. This example shows how to configure Transfer\\n/// Acceleration.\\n/// </summary>\\npublic class TransferAcceleration\\n{\\n/// <summary>\\n/// The main method initializes the client object and sets the\\n/// Amazon Simple Storage Service (Amazon S3) bucket name before\\n/// calling EnableAccelerationAsync.\\n/// </summary>\\npublic static async Task Main()\\n{\\nvar s3Client = new AmazonS3Client();', '']]]\n",
      "[[['', 'const string bucketName = \"doc-example-bucket\";\\nawait EnableAccelerationAsync(s3Client, bucketName);\\n}\\n/// <summary>\\n/// This method sets the configuration to enable transfer acceleration\\n/// for the bucket referred to in the bucketName parameter.\\n/// </summary>\\n/// <param name=\"client\">An Amazon S3 client used to enable the\\n/// acceleration on an Amazon S3 bucket.</param>\\n/// <param name=\"bucketName\">The name of the Amazon S3 bucket for which\\nthe\\n/// method will be enabling acceleration.</param>\\nprivate static async Task EnableAccelerationAsync(AmazonS3Client client,\\nstring bucketName)\\n{\\ntry\\n{\\nvar putRequest = new PutBucketAccelerateConfigurationRequest\\n{\\nBucketName = bucketName,\\nAccelerateConfiguration = new AccelerateConfiguration\\n{\\nStatus = BucketAccelerateStatus.Enabled,\\n},\\n};\\nawait client.PutBucketAccelerateConfigurationAsync(putRequest);\\nvar getRequest = new GetBucketAccelerateConfigurationRequest\\n{\\nBucketName = bucketName,\\n};\\nvar response = await\\nclient.GetBucketAccelerateConfigurationAsync(getRequest);\\nConsole.WriteLine($\"Acceleration state = \\'{response.Status}\\' \");\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error occurred. Message:\\'{ex.Message}\\' when\\nsetting transfer acceleration\");\\n}\\n}', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-accelerate-configuration \\\\\\n--bucket my-bucket \\\\\\n--accelerate-configuration Status=Enabled', ''], ['', '', '']], [['', '', ''], ['', \"$statusVal = New-Object Amazon.S3.BucketAccelerateStatus('Enabled')\\nWrite-S3BucketAccelerateConfiguration -BucketName 's3testbucket' -\\nAccelerateConfiguration_Status $statusVal\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Creates an Amazon S3 bucket with an ACL to control access to the\\n/// bucket and the objects stored in it.\\n/// </summary>\\n/// <param name=\"client\">The initialized client object used to create\\n/// an Amazon S3 bucket, with an ACL applied to the bucket.\\n/// </param>\\n/// <param name=\"region\">The AWS Region where the bucket will be\\ncreated.</param>\\n/// <param name=\"newBucketName\">The name of the bucket to create.</param>\\n/// <returns>A boolean value indicating success or failure.</returns>\\npublic static async Task<bool> CreateBucketUseCannedACLAsync(IAmazonS3\\nclient, S3Region region, string newBucketName)\\n{\\ntry\\n{\\n// Create a new Amazon S3 bucket with Canned ACL.\\nvar putBucketRequest = new PutBucketRequest()\\n{\\nBucketName = newBucketName,\\nBucketRegion = region,', '']]]\n",
      "[[['', 'CannedACL = S3CannedACL.LogDeliveryWrite,\\n};\\nPutBucketResponse putBucketResponse = await\\nclient.PutBucketAsync(putBucketRequest);\\nreturn putBucketResponse.HttpStatusCode ==\\nSystem.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Amazon S3 error: {ex.Message}\");\\n}\\nreturn false;\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::PutBucketAcl(const Aws::String &bucketName,\\nconst Aws::String &ownerID,\\nconst Aws::String &granteePermission,\\nconst Aws::String &granteeType,\\nconst Aws::String &granteeID,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig,\\nconst Aws::String &granteeDisplayName,\\nconst Aws::String &granteeEmailAddress,\\nconst Aws::String &granteeURI) {\\nAws::S3::S3Client s3_client(clientConfig);', '']]]\n",
      "[[['', 'Aws::S3::Model::Owner owner;\\nowner.SetID(ownerID);\\nAws::S3::Model::Grantee grantee;\\ngrantee.SetType(SetGranteeType(granteeType));\\nif (!granteeEmailAddress.empty()) {\\ngrantee.SetEmailAddress(granteeEmailAddress);\\n}\\nif (!granteeID.empty()) {\\ngrantee.SetID(granteeID);\\n}\\nif (!granteeDisplayName.empty()) {\\ngrantee.SetDisplayName(granteeDisplayName);\\n}\\nif (!granteeURI.empty()) {\\ngrantee.SetURI(granteeURI);\\n}\\nAws::S3::Model::Grant grant;\\ngrant.SetGrantee(grantee);\\ngrant.SetPermission(SetGranteePermission(granteePermission));\\nAws::Vector<Aws::S3::Model::Grant> grants;\\ngrants.push_back(grant);\\nAws::S3::Model::AccessControlPolicy acp;\\nacp.SetOwner(owner);\\nacp.SetGrants(grants);\\nAws::S3::Model::PutBucketAclRequest request;\\nrequest.SetAccessControlPolicy(acp);\\nrequest.SetBucket(bucketName);\\nAws::S3::Model::PutBucketAclOutcome outcome =\\ns3_client.PutBucketAcl(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &error = outcome.GetError();', '']]]\n",
      "[[['', 'std::cerr << \"Error: PutBucketAcl: \" << error.GetExceptionName()\\n<< \" - \" << error.GetMessage() << std::endl;\\n}\\nelse {\\nstd::cout << \"Successfully added an ACL to the bucket \\'\" << bucketName\\n<< \"\\'.\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}\\n//! Routine which converts a human-readable string to a built-in type\\nenumeration.\\n/*!\\n\\\\sa SetGranteePermission()\\n\\\\param access Human readable string.\\n*/\\nAws::S3::Model::Permission SetGranteePermission(const Aws::String &access) {\\nif (access == \"FULL_CONTROL\")\\nreturn Aws::S3::Model::Permission::FULL_CONTROL;\\nif (access == \"WRITE\")\\nreturn Aws::S3::Model::Permission::WRITE;\\nif (access == \"READ\")\\nreturn Aws::S3::Model::Permission::READ;\\nif (access == \"WRITE_ACP\")\\nreturn Aws::S3::Model::Permission::WRITE_ACP;\\nif (access == \"READ_ACP\")\\nreturn Aws::S3::Model::Permission::READ_ACP;\\nreturn Aws::S3::Model::Permission::NOT_SET;\\n}\\n//! Routine which converts a human-readable string to a built-in type\\nenumeration.\\n/*!\\n\\\\sa SetGranteeType()\\n\\\\param type Human readable string.\\n*/\\nAws::S3::Model::Type SetGranteeType(const Aws::String &type) {\\nif (type == \"Amazon customer by email\")\\nreturn Aws::S3::Model::Type::AmazonCustomerByEmail;\\nif (type == \"Canonical user\")\\nreturn Aws::S3::Model::Type::CanonicalUser;', '']]]\n",
      "[[['', 'if (type == \"Group\")\\nreturn Aws::S3::Model::Type::Group;\\nreturn Aws::S3::Model::Type::NOT_SET;\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-acl --bucket MyBucket --grant-full-control\\nemailaddress=user1@example.com,emailaddress=user2@example.com --grant-read\\nuri=http://acs.amazonaws.com/groups/global/AllUsers', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.AccessControlPolicy;\\nimport software.amazon.awssdk.services.s3.model.Grant;\\nimport software.amazon.awssdk.services.s3.model.Permission;', '']]]\n",
      "[[['', 'import software.amazon.awssdk.services.s3.model.PutBucketAclRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.model.Type;\\nimport java.util.ArrayList;\\nimport java.util.List;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class SetAcl {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <id>\\\\s\\nWhere:\\nbucketName - The Amazon S3 bucket to grant permissions on.\\\\s\\nid - The ID of the owner of this bucket (you can get this value\\nfrom the AWS Management Console).\\n\"\"\";\\nif (args.length != 2) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString id = args[1];\\nSystem.out.format(\"Setting access \\\\n\");\\nSystem.out.println(\" in bucket: \" + bucketName);\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nsetBucketAcl(s3, bucketName, id);', '']]]\n",
      "[[['', 'System.out.println(\"Done!\");\\ns3.close();\\n}\\npublic static void setBucketAcl(S3Client s3, String bucketName, String id) {\\ntry {\\nGrant ownerGrant = Grant.builder()\\n.grantee(builder -> builder.id(id)\\n.type(Type.CANONICAL_USER))\\n.permission(Permission.FULL_CONTROL)\\n.build();\\nList<Grant> grantList2 = new ArrayList<>();\\ngrantList2.add(ownerGrant);\\nAccessControlPolicy acl = AccessControlPolicy.builder()\\n.owner(builder -> builder.id(id))\\n.grants(grantList2)\\n.build();\\nPutBucketAclRequest putAclReq = PutBucketAclRequest.builder()\\n.bucket(bucketName)\\n.accessControlPolicy(acl)\\n.build();\\ns3.putBucketAcl(putAclReq);\\n} catch (S3Exception e) {\\ne.printStackTrace();\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { PutBucketAclCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\n// Most Amazon S3 use cases don\\'t require the use of access control lists (ACLs).\\n// We recommend that you disable ACLs, except in unusual circumstances where\\n// you need to control access for each object individually.\\n// Consider a policy instead. For more information see https://\\ndocs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html.\\nexport const main = async () => {\\n// Grant a user READ access to a bucket.\\nconst command = new PutBucketAclCommand({\\nBucket: \"test-bucket\",\\nAccessControlPolicy: {\\nGrants: [\\n{\\nGrantee: {\\n// The canonical ID of the user. This ID is an obfuscated form of\\nyour AWS account number.\\n// It\\'s unique to Amazon S3 and can\\'t be found elsewhere.\\n// For more information, see https://docs.aws.amazon.com/AmazonS3/\\nlatest/userguide/finding-canonical-user-id.html.\\nID: \"canonical-id-1\",\\nType: \"CanonicalUser\",\\n},\\n// One of FULL_CONTROL | READ | WRITE | READ_ACP | WRITE_ACP\\n// https://docs.aws.amazon.com/AmazonS3/latest/API/\\nAPI_Grant.html#AmazonS3-Type-Grant-Permission\\nPermission: \"FULL_CONTROL\",\\n},\\n],', '']]]\n",
      "[[['', 'Owner: {\\nID: \"canonical-id-2\",\\n},\\n},\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun setBucketAcl(\\nbucketName: String,\\nidVal: String,\\n) {\\nval myGrant =\\nGrantee {\\nid = idVal\\ntype = Type.CanonicalUser\\n}\\nval ownerGrant =\\nGrant {\\ngrantee = myGrant', '']]]\n",
      "[[['', 'permission = Permission.FullControl\\n}\\nval grantList = mutableListOf<Grant>()\\ngrantList.add(ownerGrant)\\nval ownerOb =\\nOwner {\\nid = idVal\\n}\\nval acl =\\nAccessControlPolicy {\\nowner = ownerOb\\ngrants = grantList\\n}\\nval request =\\nPutBucketAclRequest {\\nbucket = bucketName\\naccessControlPolicy = acl\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\ns3.putBucketAcl(request)\\nprintln(\"An ACL was successfully set on $bucketName\")\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef grant_log_delivery_access(self):\\n\"\"\"\\nGrant the AWS Log Delivery group write access to the bucket so that\\nAmazon S3 can deliver access logs to the bucket. This is the only\\nrecommended\\nuse of an S3 bucket ACL.\\n\"\"\"\\ntry:\\nacl = self.bucket.Acl()\\n# Putting an ACL overwrites the existing ACL. If you want to preserve\\n# existing grants, append new grants to the list of existing grants.\\ngrants = acl.grants if acl.grants else []\\ngrants.append(\\n{\\n\"Grantee\": {\\n\"Type\": \"Group\",\\n\"URI\": \"http://acs.amazonaws.com/groups/s3/LogDelivery\",\\n},\\n\"Permission\": \"WRITE\",\\n}\\n)\\nacl.put(AccessControlPolicy={\"Grants\": grants, \"Owner\": acl.owner})\\nlogger.info(\"Granted log delivery access to bucket \\'%s\\'\",\\nself.bucket.name)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t add ACL to bucket \\'%s\\'.\",\\nself.bucket.name)\\nraise', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Add CORS configuration to the Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used\\n/// to apply the CORS configuration to an Amazon S3 bucket.</param>\\n/// <param name=\"configuration\">The CORS configuration to apply.</param>\\nprivate static async Task PutCORSConfigurationAsync(AmazonS3Client\\nclient, CORSConfiguration configuration)\\n{\\nPutCORSConfigurationRequest request = new\\nPutCORSConfigurationRequest()\\n{\\nBucketName = BucketName,\\nConfiguration = configuration,\\n};\\n_ = await client.PutCORSConfigurationAsync(request);\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-cors --bucket MyBucket --cors-configuration file://cors.json\\ncors.json:\\n{\\n\"CORSRules\": [\\n{\\n\"AllowedOrigins\": [\"http://www.example.com\"],\\n\"AllowedHeaders\": [\"*\"],\\n\"AllowedMethods\": [\"PUT\", \"POST\", \"DELETE\"],\\n\"MaxAgeSeconds\": 3000,\\n\"ExposeHeaders\": [\"x-amz-server-side-encryption\"]\\n},\\n{\\n\"AllowedOrigins\": [\"*\"],\\n\"AllowedHeaders\": [\"Authorization\"],\\n\"AllowedMethods\": [\"GET\"],\\n\"MaxAgeSeconds\": 3000\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport java.util.ArrayList;\\nimport java.util.List;\\nimport software.amazon.awssdk.services.s3.model.GetBucketCorsRequest;\\nimport software.amazon.awssdk.services.s3.model.GetBucketCorsResponse;\\nimport software.amazon.awssdk.services.s3.model.DeleteBucketCorsRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.model.CORSRule;\\nimport software.amazon.awssdk.services.s3.model.CORSConfiguration;\\nimport software.amazon.awssdk.services.s3.model.PutBucketCorsRequest;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class S3Cors {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <accountId>\\\\s\\nWhere:\\nbucketName - The Amazon S3 bucket to upload an object into.\\naccountId - The id of the account that owns the Amazon S3\\nbucket.\\n\"\"\";\\nif (args.length != 2) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString accountId = args[1];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()', '']]]\n",
      "[[['', '.region(region)\\n.build();\\nsetCorsInformation(s3, bucketName, accountId);\\ngetBucketCorsInformation(s3, bucketName, accountId);\\ndeleteBucketCorsInformation(s3, bucketName, accountId);\\ns3.close();\\n}\\npublic static void deleteBucketCorsInformation(S3Client s3, String\\nbucketName, String accountId) {\\ntry {\\nDeleteBucketCorsRequest bucketCorsRequest =\\nDeleteBucketCorsRequest.builder()\\n.bucket(bucketName)\\n.expectedBucketOwner(accountId)\\n.build();\\ns3.deleteBucketCors(bucketCorsRequest);\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\npublic static void getBucketCorsInformation(S3Client s3, String bucketName,\\nString accountId) {\\ntry {\\nGetBucketCorsRequest bucketCorsRequest =\\nGetBucketCorsRequest.builder()\\n.bucket(bucketName)\\n.expectedBucketOwner(accountId)\\n.build();\\nGetBucketCorsResponse corsResponse =\\ns3.getBucketCors(bucketCorsRequest);\\nList<CORSRule> corsRules = corsResponse.corsRules();\\nfor (CORSRule rule : corsRules) {\\nSystem.out.println(\"allowOrigins: \" + rule.allowedOrigins());\\nSystem.out.println(\"AllowedMethod: \" + rule.allowedMethods());\\n}\\n} catch (S3Exception e) {', '']]]\n",
      "[[['', 'System.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\npublic static void setCorsInformation(S3Client s3, String bucketName, String\\naccountId) {\\nList<String> allowMethods = new ArrayList<>();\\nallowMethods.add(\"PUT\");\\nallowMethods.add(\"POST\");\\nallowMethods.add(\"DELETE\");\\nList<String> allowOrigins = new ArrayList<>();\\nallowOrigins.add(\"http://example.com\");\\ntry {\\n// Define CORS rules.\\nCORSRule corsRule = CORSRule.builder()\\n.allowedMethods(allowMethods)\\n.allowedOrigins(allowOrigins)\\n.build();\\nList<CORSRule> corsRules = new ArrayList<>();\\ncorsRules.add(corsRule);\\nCORSConfiguration configuration = CORSConfiguration.builder()\\n.corsRules(corsRules)\\n.build();\\nPutBucketCorsRequest putBucketCorsRequest =\\nPutBucketCorsRequest.builder()\\n.bucket(bucketName)\\n.corsConfiguration(configuration)\\n.expectedBucketOwner(accountId)\\n.build();\\ns3.putBucketCors(putBucketCorsRequest);\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { PutBucketCorsCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\n// By default, Amazon S3 doesn\\'t allow cross-origin requests. Use this command\\n// to explicitly allow cross-origin requests.\\nexport const main = async () => {\\nconst command = new PutBucketCorsCommand({\\nBucket: \"test-bucket\",\\nCORSConfiguration: {\\nCORSRules: [\\n{\\n// Allow all headers to be sent to this bucket.\\nAllowedHeaders: [\"*\"],\\n// Allow only GET and PUT methods to be sent to this bucket.\\nAllowedMethods: [\"GET\", \"PUT\"],\\n// Allow only requests from the specified origin.\\nAllowedOrigins: [\"https://www.example.com\"],\\n// Allow the entity tag (ETag) header to be returned in the response.\\nThe ETag header\\n// The entity tag represents a specific version of the object. The ETag\\nreflects\\n// changes only to the contents of an object, not its metadata.\\nExposeHeaders: [\"ETag\"],\\n// How long the requesting browser should cache the preflight response.\\nAfter', '']]]\n",
      "[[['', '// this time, the preflight request will have to be made again.\\nMaxAgeSeconds: 3600,\\n},\\n],\\n},\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name', '']]]\n",
      "[[['', 'def put_cors(self, cors_rules):\\n\"\"\"\\nApply CORS rules to the bucket. CORS rules specify the HTTP actions that\\nare\\nallowed from other domains.\\n:param cors_rules: The CORS rules to apply.\\n\"\"\"\\ntry:\\nself.bucket.Cors().put(CORSConfiguration={\"CORSRules\": cors_rules})\\nlogger.info(\\n\"Put CORS rules %s for bucket \\'%s\\'.\", cors_rules,\\nself.bucket.name\\n)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t put CORS rules for bucket %s.\",\\nself.bucket.name)\\nraise', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 bucket CORS configuration.\\nclass BucketCorsWrapper\\nattr_reader :bucket_cors\\n# @param bucket_cors [Aws::S3::BucketCors] A bucket CORS object configured with\\nan existing bucket.', '']]]\n",
      "[[['', 'def initialize(bucket_cors)\\n@bucket_cors = bucket_cors\\nend\\n# Sets CORS rules on a bucket.\\n#\\n# @param allowed_methods [Array<String>] The types of HTTP requests to allow.\\n# @param allowed_origins [Array<String>] The origins to allow.\\n# @returns [Boolean] True if the CORS rules were set; otherwise, false.\\ndef set_cors(allowed_methods, allowed_origins)\\n@bucket_cors.put(\\ncors_configuration: {\\ncors_rules: [\\n{\\nallowed_methods: allowed_methods,\\nallowed_origins: allowed_origins,\\nallowed_headers: %w[*],\\nmax_age_seconds: 3600\\n}\\n]\\n}\\n)\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t set CORS rules for #{@bucket_cors.bucket.name}. Here\\'s why:\\n#{e.message}\"\\nfalse\\nend\\nend', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-encryption \\\\\\n--bucket my-bucket \\\\\\n--server-side-encryption-configuration \\'{\"Rules\":\\n[{\"ApplyServerSideEncryptionByDefault\": {\"SSEAlgorithm\": \"AES256\"}}]}\\'', ''], ['', '', '']], [['', '', ''], ['', '$Encryptionconfig = @{ServerSideEncryptionByDefault =\\n@{ServerSideEncryptionAlgorithm = \"AES256\"}}\\nSet-S3BucketEncryption -BucketName \\'s3testbucket\\' -\\nServerSideEncryptionConfiguration_ServerSideEncryptionRule $Encryptionconfig', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Adds lifecycle configuration information to the S3 bucket named in\\n/// the bucketName parameter.\\n/// </summary>\\n/// <param name=\"client\">The S3 client used to call the\\n/// PutLifecycleConfigurationAsync method.</param>\\n/// <param name=\"bucketName\">A string representing the S3 bucket to\\n/// which configuration information will be added.</param>\\n/// <param name=\"configuration\">A LifecycleConfiguration object that\\n/// will be applied to the S3 bucket.</param>\\npublic static async Task AddExampleLifecycleConfigAsync(IAmazonS3 client,\\nstring bucketName, LifecycleConfiguration configuration)\\n{\\nvar request = new PutLifecycleConfigurationRequest()\\n{\\nBucketName = bucketName,\\nConfiguration = configuration,\\n};\\nvar response = await client.PutLifecycleConfigurationAsync(request);\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-lifecycle-configuration --bucket my-bucket --lifecycle-\\nconfiguration file://lifecycle.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Rules\": [\\n{\\n\"ID\": \"Move rotated logs to Glacier\",\\n\"Prefix\": \"rotated/\",\\n\"Status\": \"Enabled\",\\n\"Transitions\": [\\n{\\n\"Date\": \"2015-11-10T00:00:00.000Z\",\\n\"StorageClass\": \"GLACIER\"\\n}\\n]\\n},\\n{\\n\"Status\": \"Enabled\",\\n\"Prefix\": \"\",\\n\"NoncurrentVersionTransitions\": [\\n{\\n\"NoncurrentDays\": 2,\\n\"StorageClass\": \"GLACIER\"\\n}\\n],\\n\"ID\": \"Move old versions to Glacier\"\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.LifecycleRuleFilter;\\nimport software.amazon.awssdk.services.s3.model.Transition;\\nimport\\nsoftware.amazon.awssdk.services.s3.model.GetBucketLifecycleConfigurationRequest;\\nimport\\nsoftware.amazon.awssdk.services.s3.model.GetBucketLifecycleConfigurationResponse;\\nimport software.amazon.awssdk.services.s3.model.DeleteBucketLifecycleRequest;\\nimport software.amazon.awssdk.services.s3.model.TransitionStorageClass;\\nimport software.amazon.awssdk.services.s3.model.LifecycleRule;\\nimport software.amazon.awssdk.services.s3.model.ExpirationStatus;\\nimport software.amazon.awssdk.services.s3.model.BucketLifecycleConfiguration;\\nimport\\nsoftware.amazon.awssdk.services.s3.model.PutBucketLifecycleConfigurationRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport java.util.ArrayList;\\nimport java.util.List;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class LifecycleConfiguration {', '']]]\n",
      "[[['', 'public static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <accountId>\\\\s\\nWhere:\\nbucketName - The Amazon Simple Storage Service\\n(Amazon S3) bucket to upload an object into.\\naccountId - The id of the account that owns the\\nAmazon S3 bucket.\\n\"\"\";\\nif (args.length != 2) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString accountId = args[1];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nsetLifecycleConfig(s3, bucketName, accountId);\\ngetLifecycleConfig(s3, bucketName, accountId);\\ndeleteLifecycleConfig(s3, bucketName, accountId);\\nSystem.out.println(\"You have successfully created, updated, and\\ndeleted a Lifecycle configuration\");\\ns3.close();\\n}\\npublic static void setLifecycleConfig(S3Client s3, String bucketName,\\nString accountId) {\\ntry {\\n// Create a rule to archive objects with the\\n\"glacierobjects/\" prefix to Amazon\\n// S3 Glacier.\\nLifecycleRuleFilter ruleFilter =\\nLifecycleRuleFilter.builder()\\n.prefix(\"glacierobjects/\")\\n.build();', '']]]\n",
      "[[['', 'Transition transition = Transition.builder()\\n.storageClass(TransitionStorageClass.GLACIER)\\n.days(0)\\n.build();\\nLifecycleRule rule1 = LifecycleRule.builder()\\n.id(\"Archive immediately rule\")\\n.filter(ruleFilter)\\n.transitions(transition)\\n.status(ExpirationStatus.ENABLED)\\n.build();\\n// Create a second rule.\\nTransition transition2 = Transition.builder()\\n.storageClass(TransitionStorageClass.GLACIER)\\n.days(0)\\n.build();\\nList<Transition> transitionList = new ArrayList<>();\\ntransitionList.add(transition2);\\nLifecycleRuleFilter ruleFilter2 =\\nLifecycleRuleFilter.builder()\\n.prefix(\"glacierobjects/\")\\n.build();\\nLifecycleRule rule2 = LifecycleRule.builder()\\n.id(\"Archive and then delete rule\")\\n.filter(ruleFilter2)\\n.transitions(transitionList)\\n.status(ExpirationStatus.ENABLED)\\n.build();\\n// Add the LifecycleRule objects to an ArrayList.\\nArrayList<LifecycleRule> ruleList = new ArrayList<>();\\nruleList.add(rule1);\\nruleList.add(rule2);\\nBucketLifecycleConfiguration lifecycleConfiguration =\\nBucketLifecycleConfiguration.builder()\\n.rules(ruleList)\\n.build();', '']]]\n",
      "[[['', 'PutBucketLifecycleConfigurationRequest\\nputBucketLifecycleConfigurationRequest = PutBucketLifecycleConfigurationRequest\\n.builder()\\n.bucket(bucketName)\\n.lifecycleConfiguration(lifecycleConfiguration)\\n.expectedBucketOwner(accountId)\\n.build();\\ns3.putBucketLifecycleConfiguration(putBucketLifecycleConfigurationRequest);\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n// Retrieve the configuration and add a new rule.\\npublic static void getLifecycleConfig(S3Client s3, String bucketName,\\nString accountId) {\\ntry {\\nGetBucketLifecycleConfigurationRequest\\ngetBucketLifecycleConfigurationRequest = GetBucketLifecycleConfigurationRequest\\n.builder()\\n.bucket(bucketName)\\n.expectedBucketOwner(accountId)\\n.build();\\nGetBucketLifecycleConfigurationResponse response = s3\\n.getBucketLifecycleConfiguration(getBucketLifecycleConfigurationRequest);\\nList<LifecycleRule> newList = new ArrayList<>();\\nList<LifecycleRule> rules = response.rules();\\nfor (LifecycleRule rule : rules) {\\nnewList.add(rule);\\n}\\n// Add a new rule with both a prefix predicate and a tag\\npredicate.\\nLifecycleRuleFilter ruleFilter =\\nLifecycleRuleFilter.builder()\\n.prefix(\"YearlyDocuments/\")', '']]]\n",
      "[[['', '.build();\\nTransition transition = Transition.builder()\\n.storageClass(TransitionStorageClass.GLACIER)\\n.days(3650)\\n.build();\\nLifecycleRule rule1 = LifecycleRule.builder()\\n.id(\"NewRule\")\\n.filter(ruleFilter)\\n.transitions(transition)\\n.status(ExpirationStatus.ENABLED)\\n.build();\\n// Add the new rule to the list.\\nnewList.add(rule1);\\nBucketLifecycleConfiguration lifecycleConfiguration =\\nBucketLifecycleConfiguration.builder()\\n.rules(newList)\\n.build();\\nPutBucketLifecycleConfigurationRequest\\nputBucketLifecycleConfigurationRequest = PutBucketLifecycleConfigurationRequest\\n.builder()\\n.bucket(bucketName)\\n.lifecycleConfiguration(lifecycleConfiguration)\\n.expectedBucketOwner(accountId)\\n.build();\\ns3.putBucketLifecycleConfiguration(putBucketLifecycleConfigurationRequest);\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n// Delete the configuration from the Amazon S3 bucket.\\npublic static void deleteLifecycleConfig(S3Client s3, String bucketName,\\nString accountId) {\\ntry {', '']]]\n",
      "[[['', 'DeleteBucketLifecycleRequest deleteBucketLifecycleRequest\\n= DeleteBucketLifecycleRequest\\n.builder()\\n.bucket(bucketName)\\n.expectedBucketOwner(accountId)\\n.build();\\ns3.deleteBucketLifecycle(deleteBucketLifecycleRequest);\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name', '']]]\n",
      "[[['', 'def put_lifecycle_configuration(self, lifecycle_rules):\\n\"\"\"\\nApply a lifecycle configuration to the bucket. The lifecycle\\nconfiguration can\\nbe used to archive or delete the objects in the bucket according to\\nspecified\\nparameters, such as a number of days.\\n:param lifecycle_rules: The lifecycle rules to apply.\\n\"\"\"\\ntry:\\nself.bucket.LifecycleConfiguration().put(\\nLifecycleConfiguration={\"Rules\": lifecycle_rules}\\n)\\nlogger.info(\\n\"Put lifecycle rules %s for bucket \\'%s\\'.\",\\nlifecycle_rules,\\nself.bucket.name,\\n)\\nexcept ClientError:\\nlogger.exception(\\n\"Couldn\\'t put lifecycle rules for bucket \\'%s\\'.\", self.bucket.name\\n)\\nraise', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.IO;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing Microsoft.Extensions.Configuration;\\n/// <summary>\\n/// This example shows how to enable logging on an Amazon Simple Storage\\n/// Service (Amazon S3) bucket. You need to have two Amazon S3 buckets for\\n/// this example. The first is the bucket for which you wish to enable\\n/// logging, and the second is the location where you want to store the\\n/// logs.\\n/// </summary>\\npublic class ServerAccessLogging\\n{\\nprivate static IConfiguration _configuration = null!;\\npublic static async Task Main()\\n{\\nLoadConfig();\\nstring bucketName = _configuration[\"BucketName\"];\\nstring logBucketName = _configuration[\"LogBucketName\"];\\nstring logObjectKeyPrefix = _configuration[\"LogObjectKeyPrefix\"];\\nstring accountId = _configuration[\"AccountId\"];\\n// If the AWS Region defined for your default user is different\\n// from the Region where your Amazon S3 bucket is located,\\n// pass the Region name to the Amazon S3 client object\\'s constructor.\\n// For example: RegionEndpoint.USWest2 or RegionEndpoint.USEast2.\\nIAmazonS3 client = new AmazonS3Client();', '']]]\n",
      "[[['', 'try\\n{\\n// Update bucket policy for target bucket to allow delivery of\\nlogs to it.\\nawait SetBucketPolicyToAllowLogDelivery(\\nclient,\\nbucketName,\\nlogBucketName,\\nlogObjectKeyPrefix,\\naccountId);\\n// Enable logging on the source bucket.\\nawait EnableLoggingAsync(\\nclient,\\nbucketName,\\nlogBucketName,\\nlogObjectKeyPrefix);\\n}\\ncatch (AmazonS3Exception e)\\n{\\nConsole.WriteLine($\"Error: {e.Message}\");\\n}\\n}\\n/// <summary>\\n/// This method grants appropriate permissions for logging to the\\n/// Amazon S3 bucket where the logs will be stored.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client which will be\\nused\\n/// to apply the bucket policy.</param>\\n/// <param name=\"sourceBucketName\">The name of the source bucket.</param>\\n/// <param name=\"logBucketName\">The name of the bucket where logging\\n/// information will be stored.</param>\\n/// <param name=\"logPrefix\">The logging prefix where the logs should be\\ndelivered.</param>\\n/// <param name=\"accountId\">The account id of the account where the\\nsource bucket exists.</param>\\n/// <returns>Async task.</returns>\\npublic static async Task SetBucketPolicyToAllowLogDelivery(\\nIAmazonS3 client,\\nstring sourceBucketName,\\nstring logBucketName,', '']]]\n",
      "[[['', 'string logPrefix,\\nstring accountId)\\n{\\nvar resourceArn = @\"\"\"arn:aws:s3:::\" + logBucketName + \"/\" +\\nlogPrefix + @\"*\"\"\";\\nvar newPolicy = @\"{\\n\"\"Statement\"\":[{\\n\"\"Sid\"\": \"\"S3ServerAccessLogsPolicy\"\",\\n\"\"Effect\"\": \"\"Allow\"\",\\n\"\"Principal\"\": { \"\"Service\"\":\\n\"\"logging.s3.amazonaws.com\"\" },\\n\"\"Action\"\": [\"\"s3:PutObject\"\"],\\n\"\"Resource\"\": [\" + resourceArn + @\"],\\n\"\"Condition\"\": {\\n\"\"ArnLike\"\": { \"\"aws:SourceArn\"\":\\n\"\"arn:aws:s3:::\" + sourceBucketName + @\"\"\" },\\n\"\"StringEquals\"\": { \"\"aws:SourceAccount\"\": \"\"\" +\\naccountId + @\"\"\" }\\n}\\n}]\\n}\";\\nConsole.WriteLine($\"The policy to apply to bucket {logBucketName} to\\nenable logging:\");\\nConsole.WriteLine(newPolicy);\\nPutBucketPolicyRequest putRequest = new PutBucketPolicyRequest\\n{\\nBucketName = logBucketName,\\nPolicy = newPolicy,\\n};\\nawait client.PutBucketPolicyAsync(putRequest);\\nConsole.WriteLine(\"Policy applied.\");\\n}\\n/// <summary>\\n/// This method enables logging for an Amazon S3 bucket. Logs will be\\nstored\\n/// in the bucket you selected for logging. Selected prefix\\n/// will be prepended to each log object.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client which will be\\nused', '']]]\n",
      "[[['', '/// to configure and apply logging to the selected Amazon S3 bucket.</\\nparam>\\n/// <param name=\"bucketName\">The name of the Amazon S3 bucket for which\\nyou\\n/// wish to enable logging.</param>\\n/// <param name=\"logBucketName\">The name of the Amazon S3 bucket where\\nlogging\\n/// information will be stored.</param>\\n/// <param name=\"logObjectKeyPrefix\">The prefix to prepend to each\\n/// object key.</param>\\n/// <returns>Async task.</returns>\\npublic static async Task EnableLoggingAsync(\\nIAmazonS3 client,\\nstring bucketName,\\nstring logBucketName,\\nstring logObjectKeyPrefix)\\n{\\nConsole.WriteLine($\"Enabling logging for bucket {bucketName}.\");\\nvar loggingConfig = new S3BucketLoggingConfig\\n{\\nTargetBucketName = logBucketName,\\nTargetPrefix = logObjectKeyPrefix,\\n};\\nvar putBucketLoggingRequest = new PutBucketLoggingRequest\\n{\\nBucketName = bucketName,\\nLoggingConfig = loggingConfig,\\n};\\nawait client.PutBucketLoggingAsync(putBucketLoggingRequest);\\nConsole.WriteLine($\"Logging enabled.\");\\n}\\n/// <summary>\\n/// Loads configuration from settings files.\\n/// </summary>\\npublic static void LoadConfig()\\n{\\n_configuration = new ConfigurationBuilder()\\n.SetBasePath(Directory.GetCurrentDirectory())\\n.AddJsonFile(\"settings.json\") // Load settings from .json file.\\n.AddJsonFile(\"settings.local.json\", true) // Optionally, load\\nlocal settings.\\n.Build();', '']]]\n",
      "[[['', '}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-policy \\\\\\n--bucket MyBucket \\\\\\n--policy file://policy.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Sid\": \"S3ServerAccessLogsPolicy\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\"Service\": \"logging.s3.amazonaws.com\"},\\n\"Action\": \"s3:PutObject\",\\n\"Resource\": \"arn:aws:s3:::MyBucket/Logs/*\",\\n\"Condition\": {\\n\"ArnLike\": {\"aws:SourceARN\": \"arn:aws:s3:::SOURCE-BUCKET-NAME\"},\\n\"StringEquals\": {\"aws:SourceAccount\": \"SOURCE-AWS-ACCOUNT-ID\"}\\n}\\n}\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-logging \\\\\\n--bucket MyBucket \\\\\\n--bucket-logging-status file://logging.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"LoggingEnabled\": {\\n\"TargetBucket\": \"MyBucket\",\\n\"TargetPrefix\": \"Logs/\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-acl \\\\\\n--bucket MyBucket \\\\\\n--grant-write URI=http://acs.amazonaws.com/groups/s3/LogDelivery \\\\\\n--grant-read-acp URI=http://acs.amazonaws.com/groups/s3/LogDelivery', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-logging \\\\\\n--bucket MyBucket \\\\\\n--bucket-logging-status file://logging.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"LoggingEnabled\": {\\n\"TargetBucket\": \"MyBucket\",\\n\"TargetPrefix\": \"MyBucketLogs/\",', '']]]\n",
      "[[['', '\"TargetGrants\": [\\n{\\n\"Grantee\": {\\n\"Type\": \"AmazonCustomerByEmail\",\\n\"EmailAddress\": \"bob@example.com\"\\n},\\n\"Permission\": \"FULL_CONTROL\"\\n}\\n]\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-notification --bucket my-bucket --notification-configuration\\nfile://notification.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"TopicConfiguration\": {', '']]]\n",
      "[[['', '\"Event\": \"s3:ObjectCreated:*\",\\n\"Topic\": \"arn:aws:sns:us-west-2:123456789012:s3-notification-topic\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2008-10-17\",\\n\"Id\": \"example-ID\",\\n\"Statement\": [\\n{\\n\"Sid\": \"example-statement-ID\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"s3.amazonaws.com\"\\n},\\n\"Action\": [\\n\"SNS:Publish\"\\n],\\n\"Resource\": \"arn:aws:sns:us-west-2:123456789012:my-bucket\",\\n\"Condition\": {\\n\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:*:*:my-bucket\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '$topic = [Amazon.S3.Model.TopicConfiguration] @{\\nId = \"delete-event\"\\nTopic = \"arn:aws:sns:eu-west-1:123456789012:topic-1\"', '']]]\n",
      "[[['', 'Event = [Amazon.S3.EventType]::ObjectRemovedDelete\\n}\\nWrite-S3BucketNotification -BucketName kt-tools -TopicConfiguration $topic', ''], ['', '', '']], [['', '', ''], ['', '$lambdaConfig = [Amazon.S3.Model.LambdaFunctionConfiguration] @{\\nEvents = \"s3:ObjectCreated:*\"\\nFunctionArn = \"arn:aws:lambda:eu-west-1:123456789012:function:rdplock\"\\nId = \"ObjectCreated-Lambda\"\\nFilter = @{\\nS3KeyFilter = @{\\nFilterRules = @(\\n@{Name=\"Prefix\";Value=\"dada\"}\\n@{Name=\"Suffix\";Value=\".pem\"}\\n)\\n}\\n}\\n}\\nWrite-S3BucketNotification -BucketName ssm-editor -LambdaFunctionConfiguration\\n$lambdaConfig', ''], ['', '', '']], [['', '', ''], ['', '#Lambda Config 1\\n$firstLambdaConfig = [Amazon.S3.Model.LambdaFunctionConfiguration] @{\\nEvents = \"s3:ObjectCreated:*\"\\nFunctionArn = \"arn:aws:lambda:eu-west-1:123456789012:function:verifynet\"\\nId = \"ObjectCreated-dada-ps1\"\\nFilter = @{\\nS3KeyFilter = @{\\nFilterRules = @(\\n@{Name=\"Prefix\";Value=\"dada\"}\\n@{Name=\"Suffix\";Value=\".ps1\"}\\n)\\n}\\n}', '']]]\n",
      "[[['', '}\\n#Lambda Config 2\\n$secondlambdaConfig = [Amazon.S3.Model.LambdaFunctionConfiguration] @{\\nEvents = [Amazon.S3.EventType]::ObjectCreatedAll\\nFunctionArn = \"arn:aws:lambda:eu-west-1:123456789012:function:verifyssm\"\\nId = \"ObjectCreated-dada-json\"\\nFilter = @{\\nS3KeyFilter = @{\\nFilterRules = @(\\n@{Name=\"Prefix\";Value=\"dada\"}\\n@{Name=\"Suffix\";Value=\".json\"}\\n)\\n}\\n}\\n}\\nWrite-S3BucketNotification -BucketName ssm-editor -LambdaFunctionConfiguration\\n$firstLambdaConfig,$secondlambdaConfig', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example shows how to enable notifications for an Amazon Simple\\n/// Storage Service (Amazon S3) bucket.\\n/// </summary>\\npublic class EnableNotifications\\n{\\npublic static async Task Main()\\n{\\nconst string bucketName = \"doc-example-bucket1\";\\nconst string snsTopic = \"arn:aws:sns:us-east-2:0123456789ab:bucket-\\nnotify\";\\nconst string sqsQueue = \"arn:aws:sqs:us-\\neast-2:0123456789ab:Example_Queue\";\\nIAmazonS3 client = new AmazonS3Client(Amazon.RegionEndpoint.USEast2);\\nawait EnableNotificationAsync(client, bucketName, snsTopic,\\nsqsQueue);\\n}\\n/// <summary>\\n/// This method makes the call to the PutBucketNotificationAsync method.\\n/// </summary>\\n/// <param name=\"client\">An initialized Amazon S3 client used to call\\n/// the PutBucketNotificationAsync method.</param>\\n/// <param name=\"bucketName\">The name of the bucket for which\\n/// notifications will be turned on.</param>\\n/// <param name=\"snsTopic\">The ARN for the Amazon Simple Notification\\n/// Service (Amazon SNS) topic associated with the S3 bucket.</param>\\n/// <param name=\"sqsQueue\">The ARN of the Amazon Simple Queue Service\\n/// (Amazon SQS) queue to which notifications will be pushed.</param>\\npublic static async Task EnableNotificationAsync(\\nIAmazonS3 client,\\nstring bucketName,\\nstring snsTopic,\\nstring sqsQueue)\\n{\\ntry', '']]]\n",
      "[[['', '{\\n// The bucket for which we are setting up notifications.\\nvar request = new PutBucketNotificationRequest()\\n{\\nBucketName = bucketName,\\n};\\n// Defines the topic to use when sending a notification.\\nvar topicConfig = new TopicConfiguration()\\n{\\nEvents = new List<EventType> { EventType.ObjectCreatedCopy },\\nTopic = snsTopic,\\n};\\nrequest.TopicConfigurations = new List<TopicConfiguration>\\n{\\ntopicConfig,\\n};\\nrequest.QueueConfigurations = new List<QueueConfiguration>\\n{\\nnew QueueConfiguration()\\n{\\nEvents = new List<EventType>\\n{ EventType.ObjectCreatedPut },\\nQueue = sqsQueue,\\n},\\n};\\n// Now apply the notification settings to the bucket.\\nPutBucketNotificationResponse response = await\\nclient.PutBucketNotificationAsync(request);\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error: {ex.Message}\");\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-notification-configuration \\\\\\n--bucket my-bucket \\\\\\n--notification-configuration file://notification.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"TopicConfigurations\": [\\n{\\n\"TopicArn\": \"arn:aws:sns:us-west-2:123456789012:s3-notification-\\ntopic\",\\n\"Events\": [\\n\"s3:ObjectCreated:*\"\\n]\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2008-10-17\",\\n\"Id\": \"example-ID\",\\n\"Statement\": [\\n{\\n\"Sid\": \"example-statement-ID\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"s3.amazonaws.com\"\\n},\\n\"Action\": [', '']]]\n",
      "[[['', '\"SNS:Publish\"\\n],\\n\"Resource\": \"arn:aws:sns:us-west-2:123456789012::s3-notification-\\ntopic\",\\n\"Condition\": {\\n\"ArnLike\": {\\n\"aws:SourceArn\": \"arn:aws:s3:*:*:my-bucket\"\\n}\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.Event;\\nimport software.amazon.awssdk.services.s3.model.NotificationConfiguration;\\nimport\\nsoftware.amazon.awssdk.services.s3.model.PutBucketNotificationConfigurationReques\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.model.TopicConfiguration;\\nimport java.util.ArrayList;\\nimport java.util.List;\\npublic class SetBucketEventBridgeNotification {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName>\\\\s', 't']]]\n",
      "[[['', 'Where:\\nbucketName - The Amazon S3 bucket.\\\\s\\ntopicArn - The Simple Notification Service topic ARN.\\\\s\\nid - An id value used for the topic configuration. This value\\nis displayed in the AWS Management Console.\\\\s\\n\"\"\";\\nif (args.length != 3) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString topicArn = args[1];\\nString id = args[2];\\nRegion region = Region.US_EAST_1;\\nS3Client s3Client = S3Client.builder()\\n.region(region)\\n.build();\\nsetBucketNotification(s3Client, bucketName, topicArn, id);\\ns3Client.close();\\n}\\npublic static void setBucketNotification(S3Client s3Client, String\\nbucketName, String topicArn, String id) {\\ntry {\\nList<Event> events = new ArrayList<>();\\nevents.add(Event.S3_OBJECT_CREATED_PUT);\\nTopicConfiguration config = TopicConfiguration.builder()\\n.topicArn(topicArn)\\n.events(events)\\n.id(id)\\n.build();\\nList<TopicConfiguration> topics = new ArrayList<>();\\ntopics.add(config);\\nNotificationConfiguration configuration =\\nNotificationConfiguration.builder()\\n.topicConfigurations(topics)\\n.build();', '']]]\n",
      "[[['', 'PutBucketNotificationConfigurationRequest configurationRequest =\\nPutBucketNotificationConfigurationRequest\\n.builder()\\n.bucket(bucketName)\\n.notificationConfiguration(configuration)\\n.skipDestinationValidation(true)\\n.build();\\n// Set the bucket notification configuration.\\ns3Client.putBucketNotificationConfiguration(configurationRequest);\\nSystem.out.println(\"Added bucket \" + bucketName + \" with EventBridge\\nevents enabled.\");\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'bool AwsDoc::S3::PutBucketPolicy(const Aws::String &bucketName,\\nconst Aws::String &policyBody,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client s3_client(clientConfig);\\nstd::shared_ptr<Aws::StringStream> request_body =\\nAws::MakeShared<Aws::StringStream>(\"\");\\n*request_body << policyBody;\\nAws::S3::Model::PutBucketPolicyRequest request;\\nrequest.SetBucket(bucketName);\\nrequest.SetBody(request_body);\\nAws::S3::Model::PutBucketPolicyOutcome outcome =\\ns3_client.PutBucketPolicy(request);\\nif (!outcome.IsSuccess()) {\\nstd::cerr << \"Error: PutBucketPolicy: \"\\n<< outcome.GetError().GetMessage() << std::endl;\\n}\\nelse {\\nstd::cout << \"Set the following policy body for the bucket \\'\" <<\\nbucketName << \"\\':\" << std::endl << std::endl;\\nstd::cout << policyBody << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}\\n//! Build a policy JSON string.\\n/*!\\n\\\\sa GetPolicyString()\\n\\\\param userArn Aws user Amazon Resource Name (ARN).\\nFor more information, see https://docs.aws.amazon.com/IAM/latest/UserGuide/\\nreference_identifiers.html#identifiers-arns.\\n\\\\param bucketName Name of a bucket.\\n*/\\nAws::String GetPolicyString(const Aws::String &userArn,\\nconst Aws::String &bucketName) {\\nreturn', '']]]\n",
      "[[['', '\"{\\\\n\"\\n\" \\\\\"Version\\\\\":\\\\\"2012-10-17\\\\\",\\\\n\"\\n\" \\\\\"Statement\\\\\":[\\\\n\"\\n\" {\\\\n\"\\n\" \\\\\"Sid\\\\\": \\\\\"1\\\\\",\\\\n\"\\n\" \\\\\"Effect\\\\\": \\\\\"Allow\\\\\",\\\\n\"\\n\" \\\\\"Principal\\\\\": {\\\\n\"\\n\" \\\\\"AWS\\\\\": \\\\\"\"\\n+ userArn +\\n\"\\\\\"\\\\n\"\" },\\\\n\"\\n\" \\\\\"Action\\\\\": [ \\\\\"s3:GetObject\\\\\" ],\\\\n\"\\n\" \\\\\"Resource\\\\\": [ \\\\\"arn:aws:s3:::\"\\n+ bucketName +\\n\"/*\\\\\" ]\\\\n\"\\n\" }\\\\n\"\\n\" ]\\\\n\"\\n\"}\";\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-policy --bucket MyBucket --policy file://policy.json\\npolicy.json:\\n{\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:GetObject\",\\n\"Resource\": \"arn:aws:s3:::MyBucket/*\"\\n},\\n{', '']]]\n",
      "[[['', '\"Effect\": \"Deny\",\\n\"Principal\": \"*\",\\n\"Action\": \"s3:GetObject\",\\n\"Resource\": \"arn:aws:s3:::MyBucket/MySecretFolder/*\"\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789012:root\"\\n},\\n\"Action\": [\\n\"s3:DeleteObject\",\\n\"s3:PutObject\"\\n],\\n\"Resource\": \"arn:aws:s3:::MyBucket/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.PutBucketPolicyRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.regions.Region;\\nimport java.io.IOException;\\nimport java.nio.charset.StandardCharsets;\\nimport java.nio.file.Files;\\nimport java.nio.file.Paths;\\nimport java.util.List;\\nimport com.fasterxml.jackson.core.JsonParser;\\nimport com.fasterxml.jackson.databind.ObjectMapper;', '']]]\n",
      "[[['', '/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class SetBucketPolicy {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <polFile>\\nWhere:\\nbucketName - The Amazon S3 bucket to set the policy on.\\npolFile - A JSON file containing the policy (see the Amazon\\nS3 Readme for an example).\\\\s\\n\"\"\";\\nif (args.length != 2) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString polFile = args[1];\\nString policyText = getBucketPolicyFromFile(polFile);\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nsetPolicy(s3, bucketName, policyText);\\ns3.close();\\n}\\npublic static void setPolicy(S3Client s3, String bucketName, String\\npolicyText) {\\nSystem.out.println(\"Setting policy:\");\\nSystem.out.println(\"----\");', '']]]\n",
      "[[['', 'System.out.println(policyText);\\nSystem.out.println(\"----\");\\nSystem.out.format(\"On Amazon S3 bucket: \\\\\"%s\\\\\"\\\\n\", bucketName);\\ntry {\\nPutBucketPolicyRequest policyReq = PutBucketPolicyRequest.builder()\\n.bucket(bucketName)\\n.policy(policyText)\\n.build();\\ns3.putBucketPolicy(policyReq);\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\nSystem.out.println(\"Done!\");\\n}\\n// Loads a JSON-formatted policy from a file\\npublic static String getBucketPolicyFromFile(String policyFile) {\\nStringBuilder fileText = new StringBuilder();\\ntry {\\nList<String> lines = Files.readAllLines(Paths.get(policyFile),\\nStandardCharsets.UTF_8);\\nfor (String line : lines) {\\nfileText.append(line);\\n}\\n} catch (IOException e) {\\nSystem.out.format(\"Problem reading file: \\\\\"%s\\\\\"\", policyFile);\\nSystem.out.println(e.getMessage());\\n}\\ntry {\\nfinal JsonParser parser = new\\nObjectMapper().getFactory().createParser(fileText.toString());\\nwhile (parser.nextToken() != null) {\\n}\\n} catch (IOException jpe) {\\njpe.printStackTrace();', '']]]\n",
      "[[['', '}\\nreturn fileText.toString();\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { PutBucketPolicyCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new PutBucketPolicyCommand({\\nPolicy: JSON.stringify({\\nVersion: \"2012-10-17\",\\nStatement: [\\n{\\nSid: \"AllowGetObject\",\\n// Allow this particular user to call GetObject on any object in this\\nbucket.\\nEffect: \"Allow\",\\nPrincipal: {\\nAWS: \"arn:aws:iam::ACCOUNT-ID:user/USERNAME\",\\n},\\nAction: \"s3:GetObject\",\\nResource: \"arn:aws:s3:::BUCKET-NAME/*\",\\n},\\n],\\n}),', '']]]\n",
      "[[['', '// Apply the preceding policy to this bucket.\\nBucket: \"BUCKET-NAME\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef put_policy(self, policy):\\n\"\"\"', '']]]\n",
      "[[['', 'Apply a security policy to the bucket. Policies control users\\' ability\\nto perform specific actions, such as listing the objects in the bucket.\\n:param policy: The policy to apply to the bucket.\\n\"\"\"\\ntry:\\nself.bucket.Policy().put(Policy=json.dumps(policy))\\nlogger.info(\"Put policy %s for bucket \\'%s\\'.\", policy,\\nself.bucket.name)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t apply policy to bucket \\'%s\\'.\",\\nself.bucket.name)\\nraise', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '# Wraps an Amazon S3 bucket policy.\\nclass BucketPolicyWrapper\\nattr_reader :bucket_policy\\n# @param bucket_policy [Aws::S3::BucketPolicy] A bucket policy object\\nconfigured with an existing bucket.\\ndef initialize(bucket_policy)\\n@bucket_policy = bucket_policy\\nend\\n# Sets a policy on a bucket.\\n#\\ndef set_policy(policy)\\n@bucket_policy.put(policy: policy)', '']]]\n",
      "[[['', 'true\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t set the policy for #{@bucket_policy.bucket.name}. Here\\'s why:\\n#{e.message}\"\\nfalse\\nend\\nend', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-replication \\\\\\n--bucket AWSDOC-EXAMPLE-BUCKET1 \\\\\\n--replication-configuration file://replication.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Role\": \"arn:aws:iam::123456789012:role/s3-replication-role\",\\n\"Rules\": [\\n{\\n\"Status\": \"Enabled\",\\n\"Priority\": 1,', '']]]\n",
      "[[['', '\"DeleteMarkerReplication\": { \"Status\": \"Disabled\" },\\n\"Filter\" : { \"Prefix\": \"\"},\\n\"Destination\": {\\n\"Bucket\": \"arn:aws:s3:::AWSDOC-EXAMPLE-BUCKET2\"\\n}\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetReplicationConfiguration\",\\n\"s3:ListBucket\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::AWSDOC-EXAMPLE-BUCKET1\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:GetObjectVersion\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:GetObjectVersionTagging\"\\n],\\n\"Resource\": [\\n\"arn:aws:s3:::AWSDOC-EXAMPLE-BUCKET1/*\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"s3:ReplicateObject\",', '']]]\n",
      "[[['', '\"s3:ReplicateDelete\",\\n\"s3:ReplicateTags\"\\n],\\n\"Resource\": \"arn:aws:s3:::AWSDOC-EXAMPLE-BUCKET2/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Statement\": [\\n{\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"Service\": \"s3.amazonaws.com\"\\n},\\n\"Action\": \"sts:AssumeRole\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', '$rule1 = New-Object Amazon.S3.Model.ReplicationRule\\n$rule1.ID = \"Rule-1\"\\n$rule1.Status = \"Enabled\"\\n$rule1.Prefix = \"TaxDocs\"', '']]]\n",
      "[[['', '$rule1.Destination = @{ BucketArn = \"arn:aws:s3:::exampletargetbucket\" }\\n$params = @{\\nBucketName = \"examplebucket\"\\nConfiguration_Role = \"arn:aws:iam::35667example:role/\\nCrossRegionReplicationRoleForS3\"\\nConfiguration_Rule = $rule1\\n}\\nWrite-S3BucketReplication @params', ''], ['', '', '']], [['', '', ''], ['', '$rule1 = New-Object Amazon.S3.Model.ReplicationRule\\n$rule1.ID = \"Rule-1\"\\n$rule1.Status = \"Enabled\"\\n$rule1.Prefix = \"TaxDocs\"\\n$rule1.Destination = @{ BucketArn = \"arn:aws:s3:::exampletargetbucket\" }\\n$rule2 = New-Object Amazon.S3.Model.ReplicationRule\\n$rule2.ID = \"Rule-2\"\\n$rule2.Status = \"Enabled\"\\n$rule2.Prefix = \"OtherDocs\"\\n$rule2.Destination = @{ BucketArn = \"arn:aws:s3:::exampletargetbucket\" }\\n$params = @{\\nBucketName = \"examplebucket\"\\nConfiguration_Role = \"arn:aws:iam::35667example:role/\\nCrossRegionReplicationRoleForS3\"\\nConfiguration_Rule = $rule1,$rule2\\n}\\nWrite-S3BucketReplication @params', ''], ['', '', '']], [['', '', ''], ['', '$rule1 = New-Object Amazon.S3.Model.ReplicationRule\\n$rule1.ID = \"Rule-1\"', '']]]\n",
      "[[['', '$rule1.Status = \"Disabled\"\\n$rule1.Prefix = \"TaxDocs\"\\n$rule1.Destination = @{ BucketArn = \"arn:aws:s3:::exampletargetbucket\" }\\n$params = @{\\nBucketName = \"examplebucket\"\\nConfiguration_Role = \"arn:aws:iam::35667example:role/\\nCrossRegionReplicationRoleForS3\"\\nConfiguration_Rule = $rule1\\n}\\nWrite-S3BucketReplication @params', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-request-payment \\\\\\n--bucket my-bucket \\\\\\n--request-payment-configuration \\'{\"Payer\":\"Requester\"}\\'', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api put-bucket-request-payment \\\\\\n--bucket my-bucket \\\\\\n--request-payment-configuration \\'{\"Payer\":\"BucketOwner\"}\\'', ''], ['', '', '']], [['', '', ''], ['', 'Write-S3BucketRequestPayment -BucketName mybucket -\\nRequestPaymentConfiguration_Payer Requester', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-tagging --bucket my-bucket --tagging file://tagging.json', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"TagSet\": [\\n{\\n\"Key\": \"organization\",\\n\"Value\": \"marketing\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"aws s3api put-bucket-tagging --bucket my-bucket --tagging\\n'TagSet=[{Key=organization,Value=marketing}]'\", ''], ['', '', '']], [['', '', ''], ['', 'Write-S3BucketTagging -BucketName cloudtrail-test-2018 -TagSet @( @{ Key=\"Stage\";\\nValue=\"Test\" }, @{ Key=\"Environment\"; Value=\"Alpha\" } )', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Get-S3Bucket -BucketName cloudtrail-test-2018 | Write-S3BucketTagging\\n-TagSet @( @{ Key=\"Stage\"; Value=\"Production\" }, @{ Key=\"Department\";\\nValue=\"Finance\" } )', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-versioning --bucket my-bucket --versioning-configuration\\nStatus=Enabled', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-versioning --bucket my-bucket --versioning-configuration\\nStatus=Enabled --mfa \"SERIAL 123456\"', ''], ['', '', '']], [['', '', ''], ['', \"Write-S3BucketVersioning -BucketName 's3testbucket' -VersioningConfig_Status\\nEnabled\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Put the website configuration.\\nPutBucketWebsiteRequest putRequest = new\\nPutBucketWebsiteRequest()\\n{\\nBucketName = bucketName,\\nWebsiteConfiguration = new WebsiteConfiguration()\\n{\\nIndexDocumentSuffix = indexDocumentSuffix,\\nErrorDocument = errorDocument,\\n},\\n};\\nPutBucketWebsiteResponse response = await\\nclient.PutBucketWebsiteAsync(putRequest);', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::PutWebsiteConfig(const Aws::String &bucketName,\\nconst Aws::String &indexPage, const Aws::String\\n&errorPage,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client client(clientConfig);\\nAws::S3::Model::IndexDocument indexDocument;\\nindexDocument.SetSuffix(indexPage);\\nAws::S3::Model::ErrorDocument errorDocument;\\nerrorDocument.SetKey(errorPage);\\nAws::S3::Model::WebsiteConfiguration websiteConfiguration;\\nwebsiteConfiguration.SetIndexDocument(indexDocument);\\nwebsiteConfiguration.SetErrorDocument(errorDocument);\\nAws::S3::Model::PutBucketWebsiteRequest request;\\nrequest.SetBucket(bucketName);\\nrequest.SetWebsiteConfiguration(websiteConfiguration);\\nAws::S3::Model::PutBucketWebsiteOutcome outcome =\\nclient.PutBucketWebsite(request);\\nif (!outcome.IsSuccess()) {\\nstd::cerr << \"Error: PutBucketWebsite: \"\\n<< outcome.GetError().GetMessage() << std::endl;\\n}\\nelse {\\nstd::cout << \"Success: Set website configuration for bucket \\'\"\\n<< bucketName << \"\\'.\" << std::endl;\\n}', '']]]\n",
      "[[['', 'return outcome.IsSuccess();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-bucket-website --bucket my-bucket --website-configuration file://\\nwebsite.json', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"IndexDocument\": {\\n\"Suffix\": \"index.html\"\\n},\\n\"ErrorDocument\": {\\n\"Key\": \"error.html\"\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.IndexDocument;\\nimport software.amazon.awssdk.services.s3.model.PutBucketWebsiteRequest;\\nimport software.amazon.awssdk.services.s3.model.WebsiteConfiguration;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.regions.Region;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class SetWebsiteConfiguration {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage: <bucketName> [indexdoc]\\\\s\\nWhere:\\nbucketName - The Amazon S3 bucket to set the website\\nconfiguration on.\\\\s\\nindexdoc - The index document, ex. \\'index.html\\'\\nIf not specified, \\'index.html\\' will be set.\\n\"\"\";\\nif (args.length != 1) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString indexDoc = \"index.html\";\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nsetWebsiteConfig(s3, bucketName, indexDoc);', '']]]\n",
      "[[['', 's3.close();\\n}\\npublic static void setWebsiteConfig(S3Client s3, String bucketName, String\\nindexDoc) {\\ntry {\\nWebsiteConfiguration websiteConfig = WebsiteConfiguration.builder()\\n.indexDocument(IndexDocument.builder().suffix(indexDoc).build())\\n.build();\\nPutBucketWebsiteRequest pubWebsiteReq =\\nPutBucketWebsiteRequest.builder()\\n.bucket(bucketName)\\n.websiteConfiguration(websiteConfig)\\n.build();\\ns3.putBucketWebsite(pubWebsiteReq);\\nSystem.out.println(\"The call was successful\");\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import { PutBucketWebsiteCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\n// Set up a bucket as a static website.\\n// The bucket needs to be publicly accessible.\\nexport const main = async () => {\\nconst command = new PutBucketWebsiteCommand({\\nBucket: \"test-bucket\",\\nWebsiteConfiguration: {\\nErrorDocument: {\\n// The object key name to use when a 4XX class error occurs.\\nKey: \"error.html\",\\n},\\nIndexDocument: {\\n// A suffix that is appended to a request that is for a directory.\\nSuffix: \"index.html\",\\n},\\n},\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Write-S3BucketWebsite -BucketName 's3testbucket' -\\nWebsiteConfiguration_IndexDocumentSuffix 'index.html' -\\nWebsiteConfiguration_ErrorDocument 'error.html'\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 bucket website actions.\\nclass BucketWebsiteWrapper\\nattr_reader :bucket_website\\n# @param bucket_website [Aws::S3::BucketWebsite] A bucket website object\\nconfigured with an existing bucket.\\ndef initialize(bucket_website)\\n@bucket_website = bucket_website\\nend\\n# Sets a bucket as a static website.\\n#\\n# @param index_document [String] The name of the index document for the\\nwebsite.\\n# @param error_document [String] The name of the error document to show for 4XX\\nerrors.\\n# @return [Boolean] True when the bucket is configured as a website; otherwise,\\nfalse.\\ndef set_website(index_document, error_document)\\n@bucket_website.put(\\nwebsite_configuration: {\\nindex_document: { suffix: index_document },\\nerror_document: { key: error_document }', '']]]\n",
      "[[['', '}\\n)\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t configure #{@bucket_website.bucket.name} as a website. Here\\'s\\nwhy: #{e.message}\"\\nfalse\\nend\\nend\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nindex_document = \"index.html\"\\nerror_document = \"404.html\"\\nwrapper = BucketWebsiteWrapper.new(Aws::S3::BucketWebsite.new(bucket_name))\\nreturn unless wrapper.set_website(index_document, error_document)\\nputs \"Successfully configured bucket #{bucket_name} as a static website.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Shows how to upload a file from the local computer to an Amazon S3\\n/// bucket.\\n/// </summary>\\n/// <param name=\"client\">An initialized Amazon S3 client object.</param>\\n/// <param name=\"bucketName\">The Amazon S3 bucket to which the object\\n/// will be uploaded.</param>\\n/// <param name=\"objectName\">The object to upload.</param>\\n/// <param name=\"filePath\">The path, including file name, of the object\\n/// on the local computer to upload.</param>\\n/// <returns>A boolean value indicating the success or failure of the\\n/// upload procedure.</returns>\\npublic static async Task<bool> UploadFileAsync(\\nIAmazonS3 client,\\nstring bucketName,\\nstring objectName,\\nstring filePath)\\n{\\nvar request = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = objectName,\\nFilePath = filePath,\\n};\\nvar response = await client.PutObjectAsync(request);\\nif (response.HttpStatusCode == System.Net.HttpStatusCode.OK)\\n{\\nConsole.WriteLine($\"Successfully uploaded {objectName} to\\n{bucketName}.\");\\nreturn true;', '']]]\n",
      "[[['', '}\\nelse\\n{\\nConsole.WriteLine($\"Could not upload {objectName} to\\n{bucketName}.\");\\nreturn false;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example shows how to upload an object to an Amazon Simple Storage\\n/// Service (Amazon S3) bucket with server-side encryption enabled.\\n/// </summary>\\npublic class ServerSideEncryption\\n{\\npublic static async Task Main()\\n{\\nstring bucketName = \"doc-example-bucket\";\\nstring keyName = \"samplefile.txt\";\\n// If the AWS Region defined for your default user is different\\n// from the Region where your Amazon S3 bucket is located,\\n// pass the Region name to the Amazon S3 client object\\'s constructor.\\n// For example: RegionEndpoint.USWest2.\\nIAmazonS3 client = new AmazonS3Client();\\nawait WritingAnObjectAsync(client, bucketName, keyName);\\n}\\n/// <summary>\\n/// Upload a sample object include a setting for encryption.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\n/// to upload a file and apply server-side encryption.</param>', '']]]\n",
      "[[['', '/// <param name=\"bucketName\">The name of the Amazon S3 bucket where the\\n/// encrypted object will reside.</param>\\n/// <param name=\"keyName\">The name for the object that you want to\\n/// create in the supplied bucket.</param>\\npublic static async Task WritingAnObjectAsync(IAmazonS3 client, string\\nbucketName, string keyName)\\n{\\ntry\\n{\\nvar putRequest = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nContentBody = \"sample text\",\\nServerSideEncryptionMethod =\\nServerSideEncryptionMethod.AES256,\\n};\\nvar putResponse = await client.PutObjectAsync(putRequest);\\n// Determine the encryption state of an object.\\nGetObjectMetadataRequest metadataRequest = new\\nGetObjectMetadataRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\n};\\nGetObjectMetadataResponse response = await\\nclient.GetObjectMetadataAsync(metadataRequest);\\nServerSideEncryptionMethod objectEncryption =\\nresponse.ServerSideEncryptionMethod;\\nConsole.WriteLine($\"Encryption method used: {0}\",\\nobjectEncryption.ToString());\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error: \\'{ex.Message}\\' when writing an\\nobject\");\\n}\\n}\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '###############################################################################\\n# function errecho\\n#\\n# This function outputs everything sent to it to STDERR (standard error output).\\n###############################################################################\\nfunction errecho() {\\nprintf \"%s\\\\n\" \"$*\" 1>&2\\n}\\n###############################################################################\\n# function copy_file_to_bucket\\n#\\n# This function creates a file in the specified bucket.\\n#\\n# Parameters:\\n# $1 - The name of the bucket to copy the file to.\\n# $2 - The path and file name of the local file to copy to the bucket.\\n# $3 - The key (name) to call the copy of the file in the bucket.\\n#\\n# Returns:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction copy_file_to_bucket() {\\nlocal response bucket_name source_file destination_file_name\\nbucket_name=$1\\nsource_file=$2\\ndestination_file_name=$3', '']]]\n",
      "[[['', 'response=$(aws s3api put-object \\\\\\n--bucket \"$bucket_name\" \\\\\\n--body \"$source_file\" \\\\\\n--key \"$destination_file_name\")\\n# shellcheck disable=SC2181\\nif [[ ${?} -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports put-object operation failed.\\\\n$response\"\\nreturn 1\\nfi\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::PutObject(const Aws::String &bucketName,\\nconst Aws::String &fileName,\\nconst Aws::Client::ClientConfiguration &clientConfig)\\n{\\nAws::S3::S3Client s3_client(clientConfig);\\nAws::S3::Model::PutObjectRequest request;\\nrequest.SetBucket(bucketName);\\n//We are using the name of the file as the key for the object in the bucket.\\n//However, this is just a string and can be set according to your retrieval\\nneeds.\\nrequest.SetKey(fileName);\\nstd::shared_ptr<Aws::IOStream> inputData =\\nAws::MakeShared<Aws::FStream>(\"SampleAllocationTag\",\\nfileName.c_str(),', '']]]\n",
      "[[['', 'std::ios_base::in |\\nstd::ios_base::binary);\\nif (!*inputData) {\\nstd::cerr << \"Error unable to read file \" << fileName << std::endl;\\nreturn false;\\n}\\nrequest.SetBody(inputData);\\nAws::S3::Model::PutObjectOutcome outcome =\\ns3_client.PutObject(request);\\nif (!outcome.IsSuccess()) {\\nstd::cerr << \"Error: PutObject: \" <<\\noutcome.GetError().GetMessage() << std::endl;\\n}\\nelse {\\nstd::cout << \"Added object \\'\" << fileName << \"\\' to bucket \\'\"\\n<< bucketName << \"\\'.\";\\n}\\nreturn outcome.IsSuccess();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket text-content --key dir-1/my_images.tar.bz2 --body\\nmy_images.tar.bz2', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object --bucket text-content --key dir-1/big-video-file.mp4 --body\\ne:\\\\media\\\\videos\\\\f-sharp-3-data-services.mp4', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3)\\nactions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform\\nbucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}\\n// UploadFile reads from a file and puts the data into an object in a bucket.\\nfunc (basics BucketBasics) UploadFile(bucketName string, objectKey string,\\nfileName string) error {\\nfile, err := os.Open(fileName)\\nif err != nil {\\nlog.Printf(\"Couldn\\'t open file %v to upload. Here\\'s why: %v\\\\n\", fileName, err)\\n} else {\\ndefer file.Close()\\n_, err = basics.S3Client.PutObject(context.TODO(), &s3.PutObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\nBody: file,\\n})\\nif err != nil {', '']]]\n",
      "[[['', 'log.Printf(\"Couldn\\'t upload file %v to %v:%v. Here\\'s why: %v\\\\n\",\\nfileName, bucketName, objectKey, err)\\n}\\n}\\nreturn err\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.core.sync.RequestBody;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.PutObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport java.io.File;\\nimport java.util.HashMap;\\nimport java.util.Map;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/', '']]]\n",
      "[[['', 'public class PutObject {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <objectKey> <objectPath>\\\\s\\nWhere:\\nbucketName - The Amazon S3 bucket to upload an object into.\\nobjectKey - The object to upload (for example, book.pdf).\\nobjectPath - The path where the file is located (for example,\\nC:/AWS/book2.pdf).\\\\s\\n\"\"\";\\nif (args.length != 3) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString objectKey = args[1];\\nString objectPath = args[2];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nputS3Object(s3, bucketName, objectKey, objectPath);\\ns3.close();\\n}\\n// This example uses RequestBody.fromFile to avoid loading the whole file\\ninto\\n// memory.\\npublic static void putS3Object(S3Client s3, String bucketName, String\\nobjectKey, String objectPath) {\\ntry {\\nMap<String, String> metadata = new HashMap<>();\\nmetadata.put(\"x-amz-meta-myVal\", \"test\");\\nPutObjectRequest putOb = PutObjectRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.metadata(metadata)\\n.build();', '']]]\n",
      "[[['', 's3.putObject(putOb, RequestBody.fromFile(new File(objectPath)));\\nSystem.out.println(\"Successfully placed \" + objectKey + \" into bucket\\n\" + bucketName);\\n} catch (S3Exception e) {\\nSystem.err.println(e.getMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport software.amazon.awssdk.transfer.s3.S3TransferManager;\\nimport software.amazon.awssdk.transfer.s3.model.CompletedFileUpload;\\nimport software.amazon.awssdk.transfer.s3.model.FileUpload;\\nimport software.amazon.awssdk.transfer.s3.model.UploadFileRequest;\\nimport software.amazon.awssdk.transfer.s3.progress.LoggingTransferListener;\\nimport java.net.URI;\\nimport java.net.URISyntaxException;\\nimport java.net.URL;\\nimport java.nio.file.Paths;\\nimport java.util.UUID;\\npublic String uploadFile(S3TransferManager transferManager, String\\nbucketName,\\nString key, URI filePathURI) {\\nUploadFileRequest uploadFileRequest = UploadFileRequest.builder()\\n.putObjectRequest(b -> b.bucket(bucketName).key(key))\\n.source(Paths.get(filePathURI))\\n.build();\\nFileUpload fileUpload = transferManager.uploadFile(uploadFileRequest);\\nCompletedFileUpload uploadResult = fileUpload.completionFuture().join();\\nreturn uploadResult.response().eTag();\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public static void putS3ObjectTags(S3Client s3, String bucketName, String\\nobjectKey, String objectPath) {\\ntry {\\nTag tag1 = Tag.builder()\\n.key(\"Tag 1\")\\n.value(\"This is tag 1\")\\n.build();\\nTag tag2 = Tag.builder()\\n.key(\"Tag 2\")\\n.value(\"This is tag 2\")\\n.build();\\nList<Tag> tags = new ArrayList<>();\\ntags.add(tag1);\\ntags.add(tag2);\\nTagging allTags = Tagging.builder()\\n.tagSet(tags)\\n.build();\\nPutObjectRequest putOb = PutObjectRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.tagging(allTags)\\n.build();\\ns3.putObject(putOb,\\nRequestBody.fromBytes(getObjectFile(objectPath)));\\n} catch (S3Exception e) {\\nSystem.err.println(e.getMessage());\\nSystem.exit(1);\\n}\\n}\\npublic static void updateObjectTags(S3Client s3, String bucketName, String\\nobjectKey) {\\ntry {\\nGetObjectTaggingRequest taggingRequest =\\nGetObjectTaggingRequest.builder()\\n.bucket(bucketName)', '']]]\n",
      "[[['', '.key(objectKey)\\n.build();\\nGetObjectTaggingResponse getTaggingRes =\\ns3.getObjectTagging(taggingRequest);\\nList<Tag> obTags = getTaggingRes.tagSet();\\nfor (Tag sinTag : obTags) {\\nSystem.out.println(\"The tag key is: \" + sinTag.key());\\nSystem.out.println(\"The tag value is: \" + sinTag.value());\\n}\\n// Replace the object\\'s tags with two new tags.\\nTag tag3 = Tag.builder()\\n.key(\"Tag 3\")\\n.value(\"This is tag 3\")\\n.build();\\nTag tag4 = Tag.builder()\\n.key(\"Tag 4\")\\n.value(\"This is tag 4\")\\n.build();\\nList<Tag> tags = new ArrayList<>();\\ntags.add(tag3);\\ntags.add(tag4);\\nTagging updatedTags = Tagging.builder()\\n.tagSet(tags)\\n.build();\\nPutObjectTaggingRequest taggingRequest1 =\\nPutObjectTaggingRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.tagging(updatedTags)\\n.build();\\ns3.putObjectTagging(taggingRequest1);\\nGetObjectTaggingResponse getTaggingRes2 =\\ns3.getObjectTagging(taggingRequest);\\nList<Tag> modTags = getTaggingRes2.tagSet();\\nfor (Tag sinTag : modTags) {\\nSystem.out.println(\"The tag key is: \" + sinTag.key());\\nSystem.out.println(\"The tag value is: \" + sinTag.value());', '']]]\n",
      "[[['', '}\\n} catch (S3Exception e) {\\nSystem.err.println(e.getMessage());\\nSystem.exit(1);\\n}\\n}\\n// Return a byte array.\\nprivate static byte[] getObjectFile(String filePath) {\\nFileInputStream fileInputStream = null;\\nbyte[] bytesArray = null;\\ntry {\\nFile file = new File(filePath);\\nbytesArray = new byte[(int) file.length()];\\nfileInputStream = new FileInputStream(file);\\nfileInputStream.read(bytesArray);\\n} catch (IOException e) {\\ne.printStackTrace();\\n} finally {\\nif (fileInputStream != null) {\\ntry {\\nfileInputStream.close();\\n} catch (IOException e) {\\ne.printStackTrace();\\n}\\n}\\n}\\nreturn bytesArray;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.core.sync.RequestBody;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.PutObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;', '']]]\n",
      "[[['', 'import java.io.File;\\nimport java.util.HashMap;\\nimport java.util.Map;\\n/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class PutObjectMetadata {\\npublic static void main(String[] args) {\\nfinal String USAGE = \"\"\"\\nUsage:\\n<bucketName> <objectKey> <objectPath>\\\\s\\nWhere:\\nbucketName - The Amazon S3 bucket to upload an object into.\\nobjectKey - The object to upload (for example, book.pdf).\\nobjectPath - The path where the file is located (for example,\\nC:/AWS/book2.pdf).\\\\s\\n\"\"\";\\nif (args.length != 3) {\\nSystem.out.println(USAGE);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString objectKey = args[1];\\nString objectPath = args[2];\\nSystem.out.println(\"Putting object \" + objectKey + \" into bucket \" +\\nbucketName);\\nSystem.out.println(\" in bucket: \" + bucketName);\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nputS3Object(s3, bucketName, objectKey, objectPath);', '']]]\n",
      "[[['', 's3.close();\\n}\\n// This example uses RequestBody.fromFile to avoid loading the whole file\\ninto\\n// memory.\\npublic static void putS3Object(S3Client s3, String bucketName, String\\nobjectKey, String objectPath) {\\ntry {\\nMap<String, String> metadata = new HashMap<>();\\nmetadata.put(\"author\", \"Mary Doe\");\\nmetadata.put(\"version\", \"1.0.0.0\");\\nPutObjectRequest putOb = PutObjectRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.metadata(metadata)\\n.build();\\ns3.putObject(putOb, RequestBody.fromFile(new File(objectPath)));\\nSystem.out.println(\"Successfully placed \" + objectKey + \" into bucket\\n\" + bucketName);\\n} catch (S3Exception e) {\\nSystem.err.println(e.getMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.PutObjectRetentionRequest;\\nimport software.amazon.awssdk.services.s3.model.ObjectLockRetention;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport java.time.Instant;\\nimport java.time.LocalDate;\\nimport java.time.LocalDateTime;\\nimport java.time.ZoneOffset;', '']]]\n",
      "[[['', '/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class PutObjectRetention {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<key> <bucketName>\\\\s\\nWhere:\\nkey - The name of the object (for example, book.pdf).\\\\s\\nbucketName - The Amazon S3 bucket name that contains the\\nobject (for example, bucket1).\\\\s\\n\"\"\";\\nif (args.length != 2) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString key = args[0];\\nString bucketName = args[1];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nsetRentionPeriod(s3, key, bucketName);\\ns3.close();\\n}\\npublic static void setRentionPeriod(S3Client s3, String key, String bucket) {\\ntry {\\nLocalDate localDate = LocalDate.parse(\"2020-07-17\");\\nLocalDateTime localDateTime = localDate.atStartOfDay();\\nInstant instant = localDateTime.toInstant(ZoneOffset.UTC);', '']]]\n",
      "[[['', 'ObjectLockRetention lockRetention = ObjectLockRetention.builder()\\n.mode(\"COMPLIANCE\")\\n.retainUntilDate(instant)\\n.build();\\nPutObjectRetentionRequest retentionRequest =\\nPutObjectRetentionRequest.builder()\\n.bucket(bucket)\\n.key(key)\\n.bypassGovernanceRetention(true)\\n.retention(lockRetention)\\n.build();\\n// To set Retention on an object, the Amazon S3 bucket must support\\nobject\\n// locking, otherwise an exception is thrown.\\ns3.putObjectRetention(retentionRequest);\\nSystem.out.print(\"An object retention configuration was successfully\\nplaced on the object\");\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import { PutObjectCommand, S3Client } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client({});\\nexport const main = async () => {\\nconst command = new PutObjectCommand({\\nBucket: \"test-bucket\",\\nKey: \"hello-s3.txt\",\\nBody: \"Hello S3!\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(response);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun putS3Object(\\nbucketName: String,\\nobjectKey: String,\\nobjectPath: String,\\n) {\\nval metadataVal = mutableMapOf<String, String>()\\nmetadataVal[\"myVal\"] = \"test\"', '']]]\n",
      "[[['', 'val request =\\nPutObjectRequest {\\nbucket = bucketName\\nkey = objectKey\\nmetadata = metadataVal\\nbody = File(objectPath).asByteStream()\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\nval response = s3.putObject(request)\\nprintln(\"Tag information is ${response.eTag}\")\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '$s3client = new Aws\\\\S3\\\\S3Client([\\'region\\' => \\'us-west-2\\']);\\n$fileName = __DIR__ . \"/local-file-\" . uniqid();\\ntry {\\n$this->s3client->putObject([\\n\\'Bucket\\' => $this->bucketName,\\n\\'Key\\' => $fileName,\\n\\'SourceFile\\' => __DIR__ . \\'/testfile.txt\\'\\n]);\\necho \"Uploaded $fileName to $this->bucketName.\\\\n\";\\n} catch (Exception $exception) {\\necho \"Failed to upload $fileName with error: \" . $exception-\\n>getMessage();', '']]]\n",
      "[[['', 'exit(\"Please fix error with file upload before continuing.\");\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Write-S3Object -BucketName test-files -Key \"sample.txt\" -File .\\\\local-sample.txt', ''], ['', '', '']], [['', '', ''], ['', 'Write-S3Object -BucketName test-files -File .\\\\sample.txt', ''], ['', '', '']], [['', '', ''], ['', 'Write-S3Object -BucketName test-files -Key \"prefix/to/sample.txt\" -File .\\\\local-\\nsample.txt', ''], ['', '', '']], [['', '', ''], ['', 'Write-S3Object -BucketName test-files -Folder .\\\\Scripts -KeyPrefix SampleScripts\\\\', ''], ['', '', '']], [['', '', ''], ['', 'Write-S3Object -BucketName test-files -Folder .\\\\Scripts -KeyPrefix SampleScripts\\\\\\n-SearchPattern *.ps1', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Write-S3Object -BucketName test-files -Key \"sample.txt\" -Content \"object\\ncontents\"', ''], ['', '', '']], [['', '', ''], ['', 'Write-S3Object -BucketName test-files -File \"sample.txt\" -TagSet\\n@{Key=\"key1\";Value=\"value1\"},@{Key=\"key2\";Value=\"value2\"}', ''], ['', '', '']], [['', '', ''], ['', 'Write-S3Object -BucketName test-files -Folder . -KeyPrefix \"TaggedFiles\" -Recurse\\n-TagSet @{Key=\"key1\";Value=\"value1\"},@{Key=\"key2\";Value=\"value2\"}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'class ObjectWrapper:\\n\"\"\"Encapsulates S3 object actions.\"\"\"\\ndef __init__(self, s3_object):\\n\"\"\"\\n:param s3_object: A Boto3 Object resource. This is a high-level resource\\nin Boto3\\nthat wraps object actions in a class-like structure.\\n\"\"\"\\nself.object = s3_object', '']]]\n",
      "[[['', 'self.key = self.object.key\\ndef put(self, data):\\n\"\"\"\\nUpload data to the object.\\n:param data: The data to upload. This can either be bytes or a string.\\nWhen this\\nargument is a string, it is interpreted as a file name,\\nwhich is\\nopened in read bytes mode.\\n\"\"\"\\nput_data = data\\nif isinstance(data, str):\\ntry:\\nput_data = open(data, \"rb\")\\nexcept IOError:\\nlogger.exception(\"Expected file name or binary data, got \\'%s\\'.\",\\ndata)\\nraise\\ntry:\\nself.object.put(Body=put_data)\\nself.object.wait_until_exists()\\nlogger.info(\\n\"Put object \\'%s\\' to bucket \\'%s\\'.\",\\nself.object.key,\\nself.object.bucket_name,\\n)\\nexcept ClientError:\\nlogger.exception(\\n\"Couldn\\'t put object \\'%s\\' to bucket \\'%s\\'.\",\\nself.object.key,\\nself.object.bucket_name,\\n)\\nraise\\nfinally:\\nif getattr(put_data, \"close\", None):\\nput_data.close()', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectUploadFileWrapper\\nattr_reader :object\\n# @param object [Aws::S3::Object] An existing Amazon S3 object.\\ndef initialize(object)\\n@object = object\\nend\\n# Uploads a file to an Amazon S3 object by using a managed uploader.\\n#\\n# @param file_path [String] The path to the file to upload.\\n# @return [Boolean] True when the file is uploaded; otherwise false.\\ndef upload_file(file_path)\\n@object.upload_file(file_path)\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t upload file #{file_path} to #{@object.key}. Here\\'s why:\\n#{e.message}\"\\nfalse\\nend\\nend\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nobject_key = \"my-uploaded-file\"\\nfile_path = \"object_upload_file.rb\"', '']]]\n",
      "[[['', 'wrapper = ObjectUploadFileWrapper.new(Aws::S3::Object.new(bucket_name,\\nobject_key))\\nreturn unless wrapper.upload_file(file_path)\\nputs \"File #{file_path} successfully uploaded to #{bucket_name}:#{object_key}.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectPutWrapper\\nattr_reader :object\\n# @param object [Aws::S3::Object] An existing Amazon S3 object.\\ndef initialize(object)\\n@object = object\\nend\\ndef put_object(source_file_path)\\nFile.open(source_file_path, \"rb\") do |file|\\n@object.put(body: file)\\nend\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t put #{source_file_path} to #{object.key}. Here\\'s why:\\n#{e.message}\"\\nfalse\\nend\\nend\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nobject_key = \"my-object-key\"\\nfile_path = \"my-local-file.txt\"\\nwrapper = ObjectPutWrapper.new(Aws::S3::Object.new(bucket_name, object_key))\\nsuccess = wrapper.put_object(file_path)', '']]]\n",
      "[[['', 'return unless success\\nputs \"Put file #{file_path} into #{object_key} in #{bucket_name}.\"\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps Amazon S3 object actions.\\nclass ObjectPutSseWrapper\\nattr_reader :object\\n# @param object [Aws::S3::Object] An existing Amazon S3 object.\\ndef initialize(object)\\n@object = object\\nend\\ndef put_object_encrypted(object_content, encryption)\\n@object.put(body: object_content, server_side_encryption: encryption)\\ntrue\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t put your content to #{object.key}. Here\\'s why: #{e.message}\"\\nfalse\\nend\\nend\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nobject_key = \"my-encrypted-content\"\\nobject_content = \"This is my super-secret content.\"\\nencryption = \"AES256\"\\nwrapper = ObjectPutSseWrapper.new(Aws::S3::Object.new(bucket_name,\\nobject_content))\\nreturn unless wrapper.put_object_encrypted(object_content, encryption)\\nputs \"Put your content into #{bucket_name}:#{object_key} and encrypted it with\\n#{encryption}.\"', '']]]\n",
      "[[['', 'end\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'pub async fn upload_object(\\nclient: &Client,\\nbucket_name: &str,\\nfile_name: &str,\\nkey: &str,\\n) -> Result<PutObjectOutput, SdkError<PutObjectError>> {\\nlet body = ByteStream::from_path(Path::new(file_name)).await;\\nclient\\n.put_object()\\n.bucket(bucket_name)\\n.key(key)\\n.body(body.unwrap())\\n.send()\\n.await\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '\"Get contents of file from application server.\"\\nDATA lv_body TYPE xstring.\\nOPEN DATASET iv_file_name FOR INPUT IN BINARY MODE.\\nREAD DATASET iv_file_name INTO lv_body.\\nCLOSE DATASET iv_file_name.\\n\"Upload/put an object to an S3 bucket.\"\\nTRY.\\nlo_s3->putobject(\\niv_bucket = iv_bucket_name\\niv_key = iv_file_name\\niv_body = lv_body\\n).\\nMESSAGE \\'Object uploaded to S3 bucket.\\' TYPE \\'I\\'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE \\'Bucket does not exist.\\' TYPE \\'E\\'.\\nENDTRY.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nThis is prerelease documentation for an SDK in preview release. It is subject to\\nchange.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'public func uploadFile(bucket: String, key: String, file: String) async\\nthrows {\\nlet fileUrl = URL(fileURLWithPath: file)\\nlet fileData = try Data(contentsOf: fileUrl)\\nlet dataStream = ByteStream.from(data: fileData)\\nlet input = PutObjectInput(\\nbody: dataStream,\\nbucket: bucket,\\nkey: key\\n)\\n_ = try await client.putObject(input: input)\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public func createFile(bucket: String, key: String, withData data: Data)\\nasync throws {\\nlet dataStream = ByteStream.from(data: data)\\nlet input = PutObjectInput(\\nbody: dataStream,', '']]]\n",
      "[[['', 'bucket: bucket,\\nkey: key\\n)\\n_ = try await client.putObject(input: input)\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'bool AwsDoc::S3::PutObjectAcl(const Aws::String &bucketName,\\nconst Aws::String &objectKey,\\nconst Aws::String &ownerID,\\nconst Aws::String &granteePermission,\\nconst Aws::String &granteeType,\\nconst Aws::String &granteeID,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig,', '']]]\n",
      "[[['', 'const Aws::String &granteeDisplayName,\\nconst Aws::String &granteeEmailAddress,\\nconst Aws::String &granteeURI) {\\nAws::S3::S3Client s3_client(clientConfig);\\nAws::S3::Model::Owner owner;\\nowner.SetID(ownerID);\\nAws::S3::Model::Grantee grantee;\\ngrantee.SetType(SetGranteeType(granteeType));\\nif (!granteeEmailAddress.empty()) {\\ngrantee.SetEmailAddress(granteeEmailAddress);\\n}\\nif (!granteeID.empty()) {\\ngrantee.SetID(granteeID);\\n}\\nif (!granteeDisplayName.empty()) {\\ngrantee.SetDisplayName(granteeDisplayName);\\n}\\nif (!granteeURI.empty()) {\\ngrantee.SetURI(granteeURI);\\n}\\nAws::S3::Model::Grant grant;\\ngrant.SetGrantee(grantee);\\ngrant.SetPermission(SetGranteePermission(granteePermission));\\nAws::Vector<Aws::S3::Model::Grant> grants;\\ngrants.push_back(grant);\\nAws::S3::Model::AccessControlPolicy acp;\\nacp.SetOwner(owner);\\nacp.SetGrants(grants);\\nAws::S3::Model::PutObjectAclRequest request;\\nrequest.SetAccessControlPolicy(acp);\\nrequest.SetBucket(bucketName);\\nrequest.SetKey(objectKey);\\nAws::S3::Model::PutObjectAclOutcome outcome =', '']]]\n",
      "[[['', 's3_client.PutObjectAcl(request);\\nif (!outcome.IsSuccess()) {\\nauto error = outcome.GetError();\\nstd::cerr << \"Error: PutObjectAcl: \" << error.GetExceptionName()\\n<< \" - \" << error.GetMessage() << std::endl;\\n}\\nelse {\\nstd::cout << \"Successfully added an ACL to the object \\'\" << objectKey\\n<< \"\\' in the bucket \\'\" << bucketName << \"\\'.\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}\\n//! Routine which converts a human-readable string to a built-in type\\nenumeration.\\n/*!\\n\\\\sa SetGranteePermission()\\n\\\\param access Human readable string.\\n*/\\nAws::S3::Model::Permission SetGranteePermission(const Aws::String &access) {\\nif (access == \"FULL_CONTROL\")\\nreturn Aws::S3::Model::Permission::FULL_CONTROL;\\nif (access == \"WRITE\")\\nreturn Aws::S3::Model::Permission::WRITE;\\nif (access == \"READ\")\\nreturn Aws::S3::Model::Permission::READ;\\nif (access == \"WRITE_ACP\")\\nreturn Aws::S3::Model::Permission::WRITE_ACP;\\nif (access == \"READ_ACP\")\\nreturn Aws::S3::Model::Permission::READ_ACP;\\nreturn Aws::S3::Model::Permission::NOT_SET;\\n}\\n//! Routine which converts a human-readable string to a built-in type\\nenumeration.\\n/*!\\n\\\\sa SetGranteeType()\\n\\\\param type Human readable string.\\n*/\\nAws::S3::Model::Type SetGranteeType(const Aws::String &type) {', '']]]\n",
      "[[['', 'if (type == \"Amazon customer by email\")\\nreturn Aws::S3::Model::Type::AmazonCustomerByEmail;\\nif (type == \"Canonical user\")\\nreturn Aws::S3::Model::Type::CanonicalUser;\\nif (type == \"Group\")\\nreturn Aws::S3::Model::Type::Group;\\nreturn Aws::S3::Model::Type::NOT_SET;\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object-acl --bucket MyBucket --key file.txt --grant-full-control\\nemailaddress=user1@example.com,emailaddress=user2@example.com --grant-read\\nuri=http://acs.amazonaws.com/groups/global/AllUsers', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'class ObjectWrapper:\\n\"\"\"Encapsulates S3 object actions.\"\"\"\\ndef __init__(self, s3_object):\\n\"\"\"\\n:param s3_object: A Boto3 Object resource. This is a high-level resource\\nin Boto3\\nthat wraps object actions in a class-like structure.\\n\"\"\"\\nself.object = s3_object\\nself.key = self.object.key\\ndef put_acl(self, email):\\n\"\"\"\\nApplies an ACL to the object that grants read access to an AWS user\\nidentified\\nby email address.\\n:param email: The email address of the user to grant access.\\n\"\"\"\\ntry:\\nacl = self.object.Acl()\\n# Putting an ACL overwrites the existing ACL, so append new grants\\n# if you want to preserve existing grants.\\ngrants = acl.grants if acl.grants else []\\ngrants.append(\\n{\\n\"Grantee\": {\"Type\": \"AmazonCustomerByEmail\", \"EmailAddress\":\\nemail},\\n\"Permission\": \"READ\",\\n}\\n)\\nacl.put(AccessControlPolicy={\"Grants\": grants, \"Owner\": acl.owner})\\nlogger.info(\"Granted read access to %s.\", email)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t add ACL to object \\'%s\\'.\", self.object.key)\\nraise', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Set or modify a legal hold on an object in an S3 bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket of the object.</param>\\n/// <param name=\"objectKey\">The key of the object.</param>\\n/// <param name=\"holdStatus\">The On or Off status for the legal hold.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> ModifyObjectLegalHold(string bucketName,\\nstring objectKey, ObjectLockLegalHoldStatus holdStatus)\\n{\\ntry\\n{\\nvar request = new PutObjectLegalHoldRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey,\\nLegalHold = new ObjectLockLegalHold()', '']]]\n",
      "[[['', '{\\nStatus = holdStatus\\n}\\n};\\nvar response = await _amazonS3.PutObjectLegalHoldAsync(request);\\nConsole.WriteLine($\"\\\\tModified legal hold for {objectKey} in\\n{bucketName}.\");\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tError modifying legal hold: \\'{ex.Message}\\'\");\\nreturn false;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object-legal-hold \\\\\\n--bucket my-bucket-with-object-lock \\\\\\n--key doc1.rtf \\\\\\n--legal-hold Status=ON', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Set or modify a legal hold on an object in an S3 bucket.\\npublic void modifyObjectLegalHold(String bucketName, String objectKey,\\nboolean legalHoldOn) {\\nObjectLockLegalHold legalHold ;\\nif (legalHoldOn) {\\nlegalHold = ObjectLockLegalHold.builder()\\n.status(ObjectLockLegalHoldStatus.ON)\\n.build();\\n} else {\\nlegalHold = ObjectLockLegalHold.builder()\\n.status(ObjectLockLegalHoldStatus.OFF)\\n.build();\\n}\\nPutObjectLegalHoldRequest legalHoldRequest =\\nPutObjectLegalHoldRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.legalHold(legalHold)\\n.build();\\ngetClient().putObjectLegalHold(legalHoldRequest) ;\\nSystem.out.println(\"Modified legal hold for \"+ objectKey +\" in\\n\"+bucketName +\".\");\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport { PutObjectLegalHoldCommand, S3Client } from \"@aws-sdk/client-s3\";\\n/**\\n* @param {S3Client} client\\n* @param {string} bucketName\\n* @param {string} objectKey\\n*/\\nexport const main = async (client, bucketName, objectKey) => {\\nconst command = new PutObjectLegalHoldCommand({\\nBucket: bucketName,\\nKey: objectKey,\\nLegalHold: {\\n// Set the status to \\'ON\\' to place a legal hold on the object.\\n// Set the status to \\'OFF\\' to remove the legal hold.\\nStatus: \"ON\",\\n},\\n// Optionally, you can provide additional parameters\\n// ChecksumAlgorithm: \"ALGORITHM\",\\n// ContentMD5: \"MD5_HASH\",\\n// ExpectedBucketOwner: \"ACCOUNT_ID\",\\n// RequestPayer: \"requester\",\\n// VersionId: \"OBJECT_VERSION_ID\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(\\n`Object legal hold status: ${response.$metadata.httpStatusCode}`,\\n);', '']]]\n",
      "[[['', '} catch (err) {\\nconsole.error(err);\\n}\\n};\\n// Invoke main function if this file was run directly.\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\nmain(new S3Client(), \"BUCKET_NAME\", \"OBJECT_KEY\");\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Enable object lock on an existing bucket.', '']]]\n",
      "[[['', '/// </summary>\\n/// <param name=\"bucketName\">The name of the bucket to modify.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> EnableObjectLockOnBucket(string bucketName)\\n{\\ntry\\n{\\n// First, enable Versioning on the bucket.\\nawait _amazonS3.PutBucketVersioningAsync(new\\nPutBucketVersioningRequest()\\n{\\nBucketName = bucketName,\\nVersioningConfig = new S3BucketVersioningConfig()\\n{\\nEnableMfaDelete = false,\\nStatus = VersionStatus.Enabled\\n}\\n});\\nvar request = new PutObjectLockConfigurationRequest()\\n{\\nBucketName = bucketName,\\nObjectLockConfiguration = new ObjectLockConfiguration()\\n{\\nObjectLockEnabled = new ObjectLockEnabled(\"Enabled\"),\\n},\\n};\\nvar response = await\\n_amazonS3.PutObjectLockConfigurationAsync(request);\\nConsole.WriteLine($\"\\\\tAdded an object lock policy to bucket\\n{bucketName}.\");\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error modifying object lock: \\'{ex.Message}\\'\");\\nreturn false;\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '/// <summary>\\n/// Set or modify a retention period on an S3 bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket to modify.</param>\\n/// <param name=\"retention\">The retention mode.</param>\\n/// <param name=\"retainUntilDate\">The date for retention until.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> ModifyBucketDefaultRetention(string bucketName, bool\\nenableObjectLock, ObjectLockRetentionMode retention, DateTime retainUntilDate)\\n{\\nvar enabledString = enableObjectLock ? \"Enabled\" : \"Disabled\";\\nvar timeDifference = retainUntilDate.Subtract(DateTime.Now);\\ntry\\n{\\n// First, enable Versioning on the bucket.\\nawait _amazonS3.PutBucketVersioningAsync(new\\nPutBucketVersioningRequest()\\n{\\nBucketName = bucketName,\\nVersioningConfig = new S3BucketVersioningConfig()\\n{\\nEnableMfaDelete = false,\\nStatus = VersionStatus.Enabled\\n}\\n});\\nvar request = new PutObjectLockConfigurationRequest()\\n{\\nBucketName = bucketName,\\nObjectLockConfiguration = new ObjectLockConfiguration()\\n{\\nObjectLockEnabled = new ObjectLockEnabled(enabledString),\\nRule = new ObjectLockRule()\\n{\\nDefaultRetention = new DefaultRetention()\\n{\\nMode = retention,\\nDays = timeDifference.Days // Can be specified in\\ndays or years but not both.\\n}\\n}\\n}\\n};', '']]]\n",
      "[[['', 'var response = await\\n_amazonS3.PutObjectLockConfigurationAsync(request);\\nConsole.WriteLine($\"\\\\tAdded a default retention to bucket\\n{bucketName}.\");\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tError modifying object lock: \\'{ex.Message}\\'\");\\nreturn false;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object-lock-configuration \\\\\\n--bucket my-bucket-with-object-lock \\\\\\n--object-lock-configuration \\'{ \"ObjectLockEnabled\": \"Enabled\", \"Rule\":\\n{ \"DefaultRetention\": { \"Mode\": \"COMPLIANCE\", \"Days\": 50 }}}\\'', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Enable object lock on an existing bucket.\\npublic void enableObjectLockOnBucket(String bucketName) {\\ntry {\\nVersioningConfiguration versioningConfiguration =\\nVersioningConfiguration.builder()\\n.status(BucketVersioningStatus.ENABLED)\\n.build();\\nPutBucketVersioningRequest putBucketVersioningRequest =\\nPutBucketVersioningRequest.builder()\\n.bucket(bucketName)\\n.versioningConfiguration(versioningConfiguration)\\n.build();\\n// Enable versioning on the bucket.\\ngetClient().putBucketVersioning(putBucketVersioningRequest);\\nPutObjectLockConfigurationRequest request =\\nPutObjectLockConfigurationRequest.builder()\\n.bucket(bucketName)\\n.objectLockConfiguration(ObjectLockConfiguration.builder()\\n.objectLockEnabled(ObjectLockEnabled.ENABLED)\\n.build())\\n.build();\\ngetClient().putObjectLockConfiguration(request);\\nSystem.out.println(\"Successfully enabled object lock on\\n\"+bucketName);\\n} catch (S3Exception ex) {\\nSystem.out.println(\"Error modifying object lock: \\'\" + ex.getMessage()\\n+ \"\\'\");', '']]]\n",
      "[[['', '}\\n}', ''], ['', '', '']], [['', '', ''], ['', '// Set or modify a retention period on an S3 bucket.\\npublic void modifyBucketDefaultRetention(String bucketName) {\\nVersioningConfiguration versioningConfiguration =\\nVersioningConfiguration.builder()\\n.mfaDelete(MFADelete.DISABLED)\\n.status(BucketVersioningStatus.ENABLED)\\n.build();\\nPutBucketVersioningRequest versioningRequest =\\nPutBucketVersioningRequest.builder()\\n.bucket(bucketName)\\n.versioningConfiguration(versioningConfiguration)\\n.build();\\ngetClient().putBucketVersioning(versioningRequest);\\nDefaultRetention rention = DefaultRetention.builder()\\n.days(1)\\n.mode(ObjectLockRetentionMode.GOVERNANCE)\\n.build();\\nObjectLockRule lockRule = ObjectLockRule.builder()\\n.defaultRetention(rention)\\n.build();\\nObjectLockConfiguration objectLockConfiguration =\\nObjectLockConfiguration.builder()\\n.objectLockEnabled(ObjectLockEnabled.ENABLED)\\n.rule(lockRule)\\n.build();\\nPutObjectLockConfigurationRequest putObjectLockConfigurationRequest =\\nPutObjectLockConfigurationRequest.builder()\\n.bucket(bucketName)\\n.objectLockConfiguration(objectLockConfiguration)\\n.build();', '']]]\n",
      "[[['', 'getClient().putObjectLockConfiguration(putObjectLockConfigurationRequest) ;\\nSystem.out.println(\"Added a default retention to bucket \"+bucketName\\n+\".\");\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport {\\nPutObjectLockConfigurationCommand,\\nS3Client,\\n} from \"@aws-sdk/client-s3\";\\n/**\\n* @param {S3Client} client\\n* @param {string} bucketName\\n*/\\nexport const main = async (client, bucketName) => {\\nconst command = new PutObjectLockConfigurationCommand({\\nBucket: bucketName,\\n// The Object Lock configuration that you want to apply to the specified\\nbucket.\\nObjectLockConfiguration: {\\nObjectLockEnabled: \"Enabled\",\\n},\\n// Optionally, you can provide additional parameters', '']]]\n",
      "[[['', '// ExpectedBucketOwner: \"ACCOUNT_ID\",\\n// RequestPayer: \"requester\",\\n// Token: \"OPTIONAL_TOKEN\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(\\n`Object Lock Configuration updated: ${response.$metadata.httpStatusCode}`,\\n);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};\\n// Invoke main function if this file was run directly.\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\nmain(new S3Client(), \"BUCKET_NAME\");\\n}', ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport {\\nPutObjectLockConfigurationCommand,\\nS3Client,\\n} from \"@aws-sdk/client-s3\";\\n/**\\n* @param {S3Client} client\\n* @param {string} bucketName\\n*/\\nexport const main = async (client, bucketName) => {\\nconst command = new PutObjectLockConfigurationCommand({\\nBucket: bucketName,\\n// The Object Lock configuration that you want to apply to the specified\\nbucket.\\nObjectLockConfiguration: {\\nObjectLockEnabled: \"Enabled\",\\nRule: {', '']]]\n",
      "[[['', 'DefaultRetention: {\\nMode: \"GOVERNANCE\",\\nYears: 3,\\n},\\n},\\n},\\n// Optionally, you can provide additional parameters\\n// ExpectedBucketOwner: \"ACCOUNT_ID\",\\n// RequestPayer: \"requester\",\\n// Token: \"OPTIONAL_TOKEN\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(\\n`Default Object Lock Configuration updated: ${response.\\n$metadata.httpStatusCode}`,\\n);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};\\n// Invoke main function if this file was run directly.\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\nmain(new S3Client(), \"BUCKET_NAME\");\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Set or modify a retention period on an object in an S3 bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket of the object.</param>\\n/// <param name=\"objectKey\">The key of the object.</param>\\n/// <param name=\"retention\">The retention mode.</param>\\n/// <param name=\"retainUntilDate\">The date retention expires.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> ModifyObjectRetentionPeriod(string bucketName,\\nstring objectKey, ObjectLockRetentionMode retention, DateTime\\nretainUntilDate)\\n{\\ntry\\n{\\nvar request = new PutObjectRetentionRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey,\\nRetention = new ObjectLockRetention()\\n{\\nMode = retention,\\nRetainUntilDate = retainUntilDate\\n}\\n};\\nvar response = await _amazonS3.PutObjectRetentionAsync(request);\\nConsole.WriteLine($\"\\\\tSet retention for {objectKey} in {bucketName}\\nuntil {retainUntilDate:d}.\");\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}', '']]]\n",
      "[[['', 'catch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tError modifying retention period:\\n\\'{ex.Message}\\'\");\\nreturn false;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api put-object-retention \\\\\\n--bucket my-bucket-with-object-lock \\\\\\n--key doc1.rtf \\\\\\n--retention \\'{ \"Mode\": \"GOVERNANCE\", \"RetainUntilDate\":\\n\"2025-01-01T00:00:00\" }\\'', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Set or modify a retention period on an object in an S3 bucket.', '']]]\n",
      "[[['', 'public void modifyObjectRetentionPeriod(String bucketName, String objectKey)\\n{\\n// Calculate the instant one day from now.\\nInstant futureInstant = Instant.now().plus(1, ChronoUnit.DAYS);\\n// Convert the Instant to a ZonedDateTime object with a specific time\\nzone.\\nZonedDateTime zonedDateTime =\\nfutureInstant.atZone(ZoneId.systemDefault());\\n// Define a formatter for human-readable output.\\nDateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\\nHH:mm:ss\");\\n// Format the ZonedDateTime object to a human-readable date string.\\nString humanReadableDate = formatter.format(zonedDateTime);\\n// Print the formatted date string.\\nSystem.out.println(\"Formatted Date: \" + humanReadableDate);\\nObjectLockRetention retention = ObjectLockRetention.builder()\\n.mode(ObjectLockRetentionMode.GOVERNANCE)\\n.retainUntilDate(futureInstant)\\n.build();\\nPutObjectRetentionRequest retentionRequest =\\nPutObjectRetentionRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.retention(retention)\\n.build();\\ngetClient().putObjectRetention(retentionRequest);\\nSystem.out.println(\"Set retention for \"+objectKey +\" in \" +bucketName +\"\\nuntil \"+ humanReadableDate +\".\");\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport { PutObjectRetentionCommand, S3Client } from \"@aws-sdk/client-s3\";\\n/**\\n* @param {S3Client} client\\n* @param {string} bucketName\\n* @param {string} objectKey\\n*/\\nexport const main = async (client, bucketName, objectKey) => {\\nconst command = new PutObjectRetentionCommand({\\nBucket: bucketName,\\nKey: objectKey,\\nBypassGovernanceRetention: false,\\n// ChecksumAlgorithm: \"ALGORITHM\",\\n// ContentMD5: \"MD5_HASH\",\\n// ExpectedBucketOwner: \"ACCOUNT_ID\",\\n// RequestPayer: \"requester\",\\nRetention: {\\nMode: \"GOVERNANCE\", // or \"COMPLIANCE\"\\nRetainUntilDate: new Date(new Date().getTime() + 24 * 60 * 60 * 1000),\\n},\\n// VersionId: \"OBJECT_VERSION_ID\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(\\n`Object Retention settings updated: ${response.$metadata.httpStatusCode}`,\\n);\\n} catch (err) {', '']]]\n",
      "[[['', 'console.error(err);\\n}\\n};\\n// Invoke main function if this file was run directly.\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\nmain(new S3Client(), \"BUCKET_NAME\", \"OBJECT_KEY\");\\n}', ''], ['', '', '']], [['', '', ''], ['', 'Write-S3ObjectRetention -BucketName \\'s3buckettesting\\' -Key \\'testfile.txt\\' -\\nRetention_Mode GOVERNANCE -Retention_RetainUntilDate \"2019-12-31T00:00:00\"', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using System;\\nusing System.Threading.Tasks;\\nusing Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example shows how to restore an archived object in an Amazon\\n/// Simple Storage Service (Amazon S3) bucket.\\n/// </summary>\\npublic class RestoreArchivedObject\\n{\\npublic static void Main()\\n{\\nstring bucketName = \"doc-example-bucket\";\\nstring objectKey = \"archived-object.txt\";\\n// Specify your bucket region (an example region is shown).\\nRegionEndpoint bucketRegion = RegionEndpoint.USWest2;\\nIAmazonS3 client = new AmazonS3Client(bucketRegion);\\nRestoreObjectAsync(client, bucketName, objectKey).Wait();\\n}\\n/// <summary>\\n/// This method restores an archived object from an Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// RestoreObjectAsync.</param>\\n/// <param name=\"bucketName\">A string representing the name of the\\n/// bucket where the object was located before it was archived.</param>\\n/// <param name=\"objectKey\">A string representing the name of the\\n/// archived object to restore.</param>\\npublic static async Task RestoreObjectAsync(IAmazonS3 client, string\\nbucketName, string objectKey)\\n{\\ntry\\n{\\nvar restoreRequest = new RestoreObjectRequest\\n{\\nBucketName = bucketName,\\nKey = objectKey,', '']]]\n",
      "[[['', 'Days = 2,\\n};\\nRestoreObjectResponse response = await\\nclient.RestoreObjectAsync(restoreRequest);\\n// Check the status of the restoration.\\nawait CheckRestorationStatusAsync(client, bucketName, objectKey);\\n}\\ncatch (AmazonS3Exception amazonS3Exception)\\n{\\nConsole.WriteLine($\"Error: {amazonS3Exception.Message}\");\\n}\\n}\\n/// <summary>\\n/// This method retrieves the status of the object\\'s restoration.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// GetObjectMetadataAsync.</param>\\n/// <param name=\"bucketName\">A string representing the name of the Amazon\\n/// S3 bucket which contains the archived object.</param>\\n/// <param name=\"objectKey\">A string representing the name of the\\n/// archived object you want to restore.</param>\\npublic static async Task CheckRestorationStatusAsync(IAmazonS3 client,\\nstring bucketName, string objectKey)\\n{\\nGetObjectMetadataRequest metadataRequest = new\\nGetObjectMetadataRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey,\\n};\\nGetObjectMetadataResponse response = await\\nclient.GetObjectMetadataAsync(metadataRequest);\\nvar restStatus = response.RestoreInProgress ? \"in-progress\" :\\n\"finished or failed\";\\nConsole.WriteLine($\"Restoration status: {restStatus}\");\\n}\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', 'aws s3api restore-object \\\\\\n--bucket my-glacier-bucket \\\\\\n--key doc1.rtf \\\\\\n--restore-request Days=10', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.RestoreRequest;\\nimport software.amazon.awssdk.services.s3.model.GlacierJobParameters;\\nimport software.amazon.awssdk.services.s3.model.RestoreObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.model.Tier;\\n/*', '']]]\n",
      "[[['', '* For more information about restoring an object, see \"Restoring an archived\\nobject\" at\\n* https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html\\n*\\n* Before running this Java V2 code example, set up your development\\nenvironment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*/\\npublic class RestoreObject {\\npublic static void main(String[] args) {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <keyName> <expectedBucketOwner>\\nWhere:\\nbucketName - The Amazon S3 bucket name.\\\\s\\nkeyName - The key name of an object with a Storage class\\nvalue of Glacier.\\\\s\\nexpectedBucketOwner - The account that owns the bucket (you\\ncan obtain this value from the AWS Management Console).\\\\s\\n\"\"\";\\nif (args.length != 3) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString keyName = args[1];\\nString expectedBucketOwner = args[2];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nrestoreS3Object(s3, bucketName, keyName, expectedBucketOwner);\\ns3.close();\\n}', '']]]\n",
      "[[['', 'public static void restoreS3Object(S3Client s3, String bucketName, String\\nkeyName, String expectedBucketOwner) {\\ntry {\\nRestoreRequest restoreRequest = RestoreRequest.builder()\\n.days(10)\\n.glacierJobParameters(GlacierJobParameters.builder().tier(Tier.STANDARD).build())\\n.build();\\nRestoreObjectRequest objectRequest = RestoreObjectRequest.builder()\\n.expectedBucketOwner(expectedBucketOwner)\\n.bucket(bucketName)\\n.key(keyName)\\n.restoreRequest(restoreRequest)\\n.build();\\ns3.restoreObject(objectRequest);\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api select-object-content \\\\\\n--bucket my-bucket \\\\\\n--key my-data-file.csv \\\\\\n--expression \"select * from s3object limit 100\" \\\\\\n--expression-type \\'SQL\\' \\\\\\n--input-serialization \\'{\"CSV\": {}, \"CompressionType\": \"NONE\"}\\' \\\\\\n--output-serialization \\'{\"CSV\": {}}\\' \"output.csv\"', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport software.amazon.awssdk.core.async.AsyncRequestBody;\\nimport software.amazon.awssdk.core.async.BlockingInputStreamAsyncRequestBody;\\nimport software.amazon.awssdk.core.exception.SdkException;\\nimport software.amazon.awssdk.services.s3.S3AsyncClient;\\nimport software.amazon.awssdk.services.s3.model.CSVInput;\\nimport software.amazon.awssdk.services.s3.model.CSVOutput;\\nimport software.amazon.awssdk.services.s3.model.CompressionType;\\nimport software.amazon.awssdk.services.s3.model.ExpressionType;\\nimport software.amazon.awssdk.services.s3.model.FileHeaderInfo;\\nimport software.amazon.awssdk.services.s3.model.InputSerialization;\\nimport software.amazon.awssdk.services.s3.model.JSONInput;', '']]]\n",
      "[[['', 'import software.amazon.awssdk.services.s3.model.JSONOutput;\\nimport software.amazon.awssdk.services.s3.model.JSONType;\\nimport software.amazon.awssdk.services.s3.model.ObjectIdentifier;\\nimport software.amazon.awssdk.services.s3.model.OutputSerialization;\\nimport software.amazon.awssdk.services.s3.model.Progress;\\nimport software.amazon.awssdk.services.s3.model.PutObjectResponse;\\nimport software.amazon.awssdk.services.s3.model.SelectObjectContentRequest;\\nimport\\nsoftware.amazon.awssdk.services.s3.model.SelectObjectContentResponseHandler;\\nimport software.amazon.awssdk.services.s3.model.Stats;\\nimport java.io.IOException;\\nimport java.net.URL;\\nimport java.util.ArrayList;\\nimport java.util.List;\\nimport java.util.UUID;\\nimport java.util.concurrent.CompletableFuture;\\npublic class SelectObjectContentExample {\\nstatic final Logger logger =\\nLoggerFactory.getLogger(SelectObjectContentExample.class);\\nstatic final String BUCKET_NAME = \"select-object-content-\" +\\nUUID.randomUUID();\\nstatic final S3AsyncClient s3AsyncClient = S3AsyncClient.create();\\nstatic String FILE_CSV = \"csv\";\\nstatic String FILE_JSON = \"json\";\\nstatic String URL_CSV = \"https://raw.githubusercontent.com/mledoze/countries/\\nmaster/dist/countries.csv\";\\nstatic String URL_JSON = \"https://raw.githubusercontent.com/mledoze/\\ncountries/master/dist/countries.json\";\\npublic static void main(String[] args) {\\nSelectObjectContentExample selectObjectContentExample = new\\nSelectObjectContentExample();\\ntry {\\nSelectObjectContentExample.setUp();\\nselectObjectContentExample.runSelectObjectContentMethodForJSON();\\nselectObjectContentExample.runSelectObjectContentMethodForCSV();\\n} catch (SdkException e) {\\nlogger.error(e.getMessage(), e);\\nSystem.exit(1);\\n} finally {\\nSelectObjectContentExample.tearDown();\\n}', '']]]\n",
      "[[['', '}\\nEventStreamInfo runSelectObjectContentMethodForJSON() {\\n// Set up request parameters.\\nfinal String queryExpression = \"select * from s3object[*][*] c where\\nc.area < 350000\";\\nfinal String fileType = FILE_JSON;\\nInputSerialization inputSerialization = InputSerialization.builder()\\n.json(JSONInput.builder().type(JSONType.DOCUMENT).build())\\n.compressionType(CompressionType.NONE)\\n.build();\\nOutputSerialization outputSerialization = OutputSerialization.builder()\\n.json(JSONOutput.builder().recordDelimiter(null).build())\\n.build();\\n// Build the SelectObjectContentRequest.\\nSelectObjectContentRequest select = SelectObjectContentRequest.builder()\\n.bucket(BUCKET_NAME)\\n.key(FILE_JSON)\\n.expression(queryExpression)\\n.expressionType(ExpressionType.SQL)\\n.inputSerialization(inputSerialization)\\n.outputSerialization(outputSerialization)\\n.build();\\nEventStreamInfo eventStreamInfo = new EventStreamInfo();\\n// Call the selectObjectContent method with the request and a response\\nhandler.\\n// Supply an EventStreamInfo object to the response handler to gather\\nrecords and information from the response.\\ns3AsyncClient.selectObjectContent(select,\\nbuildResponseHandler(eventStreamInfo)).join();\\n// Log out information gathered while processing the response stream.\\nlong recordCount = eventStreamInfo.getRecords().stream().mapToInt(record\\n->\\nrecord.split(\"\\\\n\").length\\n).sum();\\nlogger.info(\"Total records {}: {}\", fileType, recordCount);\\nlogger.info(\"Visitor onRecords for fileType {} called {} times\",\\nfileType, eventStreamInfo.getCountOnRecordsCalled());', '']]]\n",
      "[[['', 'logger.info(\"Visitor onStats for fileType {}, {}\", fileType,\\neventStreamInfo.getStats());\\nlogger.info(\"Visitor onContinuations for fileType {}, {}\", fileType,\\neventStreamInfo.getCountContinuationEvents());\\nreturn eventStreamInfo;\\n}\\nstatic SelectObjectContentResponseHandler\\nbuildResponseHandler(EventStreamInfo eventStreamInfo) {\\n// Use a Visitor to process the response stream. This visitor logs\\ninformation and gathers details while processing.\\nfinal SelectObjectContentResponseHandler.Visitor visitor =\\nSelectObjectContentResponseHandler.Visitor.builder()\\n.onRecords(r -> {\\nlogger.info(\"Record event received.\");\\neventStreamInfo.addRecord(r.payload().asUtf8String());\\neventStreamInfo.incrementOnRecordsCalled();\\n})\\n.onCont(ce -> {\\nlogger.info(\"Continuation event received.\");\\neventStreamInfo.incrementContinuationEvents();\\n})\\n.onProgress(pe -> {\\nProgress progress = pe.details();\\nlogger.info(\"Progress event received:\\\\n bytesScanned:\\n{}\\\\nbytesProcessed: {}\\\\nbytesReturned:{}\",\\nprogress.bytesScanned(),\\nprogress.bytesProcessed(),\\nprogress.bytesReturned());\\n})\\n.onEnd(ee -> logger.info(\"End event received.\"))\\n.onStats(se -> {\\nlogger.info(\"Stats event received.\");\\neventStreamInfo.addStats(se.details());\\n})\\n.build();\\n// Build the SelectObjectContentResponseHandler with the visitor that\\nprocesses the stream.\\nreturn SelectObjectContentResponseHandler.builder()\\n.subscriber(visitor).build();\\n}', '']]]\n",
      "[[['', '// The EventStreamInfo class is used to store information gathered while\\nprocessing the response stream.\\nstatic class EventStreamInfo {\\nprivate final List<String> records = new ArrayList<>();\\nprivate Integer countOnRecordsCalled = 0;\\nprivate Integer countContinuationEvents = 0;\\nprivate Stats stats;\\nvoid incrementOnRecordsCalled() {\\ncountOnRecordsCalled++;\\n}\\nvoid incrementContinuationEvents() {\\ncountContinuationEvents++;\\n}\\nvoid addRecord(String record) {\\nrecords.add(record);\\n}\\nvoid addStats(Stats stats) {\\nthis.stats = stats;\\n}\\npublic List<String> getRecords() {\\nreturn records;\\n}\\npublic Integer getCountOnRecordsCalled() {\\nreturn countOnRecordsCalled;\\n}\\npublic Integer getCountContinuationEvents() {\\nreturn countContinuationEvents;\\n}\\npublic Stats getStats() {\\nreturn stats;\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'aws s3api upload-part --bucket my-bucket --key \\'multipart/01\\' --part-number 1 --\\nbody part01 --upload-id\\n\"dfRtDYU0WWCCcH43C3WFbkRONycyCpTJJvxu2i5GYkZljF.Yxwh6XG7WfS2vC4to6HiV6Yjlx.cph0gt', 'N'], ['', '', '']], [['', '', ''], ['', '{\\n\"ETag\": \"\\\\\"e868e0f4719e394144ef36531ee6824c\\\\\"\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'let upload_part_res = client\\n.upload_part()\\n.key(&key)\\n.bucket(&bucket_name)\\n.upload_id(upload_id)\\n.body(stream)\\n.part_number(part_number)\\n.send()\\n.await?;\\nupload_parts.push(\\nCompletedPart::builder()\\n.e_tag(upload_part_res.e_tag.unwrap_or_default())\\n.part_number(part_number)\\n.build(),\\n);\\nlet completed_multipart_upload: CompletedMultipartUpload =\\nCompletedMultipartUpload::builder()\\n.set_parts(Some(upload_parts))\\n.build();', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\npublic class GenPresignedUrl\\n{\\npublic static void Main()\\n{\\nconst string bucketName = \"doc-example-bucket\";\\nconst string objectKey = \"sample.txt\";\\n// Specify how long the presigned URL lasts, in hours\\nconst double timeoutDuration = 12;\\n// Specify the AWS Region of your Amazon S3 bucket. If it is\\n// different from the Region defined for the default user,\\n// pass the Region to the constructor for the client. For\\n// example: new AmazonS3Client(RegionEndpoint.USEast1);\\n// If using the Region us-east-1, and server-side encryption with AWS\\nKMS, you must specify Signature Version 4.', '']]]\n",
      "[[['', '// Region us-east-1 defaults to Signature Version 2 unless explicitly\\nset to Version 4 as shown below.\\n// For more details, see https://docs.aws.amazon.com/AmazonS3/latest/\\nuserguide/UsingAWSSDK.html#specify-signature-version\\n// and https://docs.aws.amazon.com/sdkfornet/v3/apidocs/items/Amazon/\\nTAWSConfigsS3.html\\nAWSConfigsS3.UseSignatureVersion4 = true;\\nIAmazonS3 s3Client = new AmazonS3Client(RegionEndpoint.USEast1);\\nstring urlString = GeneratePresignedURL(s3Client, bucketName,\\nobjectKey, timeoutDuration);\\nConsole.WriteLine($\"The generated URL is: {urlString}.\");\\n}\\n/// <summary>\\n/// Generate a presigned URL that can be used to access the file named\\n/// in the objectKey parameter for the amount of time specified in the\\n/// duration parameter.\\n/// </summary>\\n/// <param name=\"client\">An initialized S3 client object used to call\\n/// the GetPresignedUrl method.</param>\\n/// <param name=\"bucketName\">The name of the S3 bucket containing the\\n/// object for which to create the presigned URL.</param>\\n/// <param name=\"objectKey\">The name of the object to access with the\\n/// presigned URL.</param>\\n/// <param name=\"duration\">The length of time for which the presigned\\n/// URL will be valid.</param>\\n/// <returns>A string representing the generated presigned URL.</returns>\\npublic static string GeneratePresignedURL(IAmazonS3 client, string\\nbucketName, string objectKey, double duration)\\n{\\nstring urlString = string.Empty;\\ntry\\n{\\nvar request = new GetPreSignedUrlRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey,\\nExpires = DateTime.UtcNow.AddHours(duration),\\n};\\nurlString = client.GetPreSignedURL(request);\\n}\\ncatch (AmazonS3Exception ex)\\n{', '']]]\n",
      "[[['', 'Console.WriteLine($\"Error:\\'{ex.Message}\\'\");\\n}\\nreturn urlString;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.IO;\\nusing System.Net.Http;\\nusing System.Threading.Tasks;\\nusing Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example shows how to upload an object to an Amazon Simple Storage\\n/// Service (Amazon S3) bucket using a presigned URL. The code first\\n/// creates a presigned URL and then uses it to upload an object to an\\n/// Amazon S3 bucket using that URL.\\n/// </summary>\\npublic class UploadUsingPresignedURL\\n{\\nprivate static HttpClient httpClient = new HttpClient();\\npublic static async Task Main()\\n{\\nstring bucketName = \"doc-example-bucket\";\\nstring keyName = \"samplefile.txt\";\\nstring filePath = $\"source\\\\\\\\{keyName}\";\\n// Specify how long the signed URL will be valid in hours.\\ndouble timeoutDuration = 12;\\n// Specify the AWS Region of your Amazon S3 bucket. If it is\\n// different from the Region defined for the default user,\\n// pass the Region to the constructor for the client. For\\n// example: new AmazonS3Client(RegionEndpoint.USEast1);', '']]]\n",
      "[[['', '// If using the Region us-east-1, and server-side encryption with AWS\\nKMS, you must specify Signature Version 4.\\n// Region us-east-1 defaults to Signature Version 2 unless explicitly\\nset to Version 4 as shown below.\\n// For more details, see https://docs.aws.amazon.com/AmazonS3/latest/\\nuserguide/UsingAWSSDK.html#specify-signature-version\\n// and https://docs.aws.amazon.com/sdkfornet/v3/apidocs/items/Amazon/\\nTAWSConfigsS3.html\\nAWSConfigsS3.UseSignatureVersion4 = true;\\nIAmazonS3 client = new AmazonS3Client(RegionEndpoint.USEast1);\\nvar url = GeneratePreSignedURL(client, bucketName, keyName,\\ntimeoutDuration);\\nvar success = await UploadObject(filePath, url);\\nif (success)\\n{\\nConsole.WriteLine(\"Upload succeeded.\");\\n}\\nelse\\n{\\nConsole.WriteLine(\"Upload failed.\");\\n}\\n}\\n/// <summary>\\n/// Uploads an object to an Amazon S3 bucket using the presigned URL\\npassed in\\n/// the url parameter.\\n/// </summary>\\n/// <param name=\"filePath\">The path (including file name) to the local\\n/// file you want to upload.</param>\\n/// <param name=\"url\">The presigned URL that will be used to upload the\\n/// file to the Amazon S3 bucket.</param>\\n/// <returns>A Boolean value indicating the success or failure of the\\n/// operation, based on the HttpWebResponse.</returns>\\npublic static async Task<bool> UploadObject(string filePath, string url)\\n{\\nusing var streamContent = new StreamContent(\\nnew FileStream(filePath, FileMode.Open, FileAccess.Read));\\nvar response = await httpClient.PutAsync(url, streamContent);\\nreturn response.IsSuccessStatusCode;\\n}', '']]]\n",
      "[[['', '/// <summary>\\n/// Generates a presigned URL which will be used to upload an object to\\n/// an Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// GetPreSignedURL.</param>\\n/// <param name=\"bucketName\">The name of the Amazon S3 bucket to which\\nthe\\n/// presigned URL will point.</param>\\n/// <param name=\"objectKey\">The name of the file that will be uploaded.</\\nparam>\\n/// <param name=\"duration\">How long (in hours) the presigned URL will\\n/// be valid.</param>\\n/// <returns>The generated URL.</returns>\\npublic static string GeneratePreSignedURL(\\nIAmazonS3 client,\\nstring bucketName,\\nstring objectKey,\\ndouble duration)\\n{\\nvar request = new GetPreSignedUrlRequest\\n{\\nBucketName = bucketName,\\nKey = objectKey,\\nVerb = HttpVerb.PUT,\\nExpires = DateTime.UtcNow.AddHours(duration),\\n};\\nstring url = client.GetPreSignedURL(request);\\nreturn url;\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '//! Routine which demonstrates creating a pre-signed URL to download an object\\nfrom an\\n//! Amazon Simple Storage Service (Amazon S3) bucket.\\n/*!\\n\\\\param bucketName: Name of the bucket.\\n\\\\param key: Name of an object key.\\n\\\\param expirationSeconds: Expiration in seconds for pre-signed URL.\\n\\\\param clientConfig: Aws client configuration.\\n\\\\return Aws::String: A pre-signed URL.\\n*/\\nAws::String AwsDoc::S3::GeneratePreSignedGetObjectURL(const Aws::String\\n&bucketName,\\nconst Aws::String &key,\\nuint64_t expirationSeconds,\\nconst\\nAws::Client::ClientConfiguration &clientConfig) {\\nAws::S3::S3Client client(clientConfig);\\nreturn client.GeneratePresignedUrl(bucketName, key,\\nAws::Http::HttpMethod::HTTP_GET,\\nexpirationSeconds);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'static size_t myCurlWriteBack(char *buffer, size_t size, size_t nitems, void\\n*userdata) {\\nAws::StringStream *str = (Aws::StringStream *) userdata;\\nif (nitems > 0) {', '']]]\n",
      "[[['', 'str->write(buffer, size * nitems);\\n}\\nreturn size * nitems;\\n}\\n//! Utility routine to test GetObject with a pre-signed URL.\\n/*!\\n\\\\param presignedURL: A pre-signed URL to get an object from a bucket.\\n\\\\param resultString: A string to hold the result.\\n\\\\return bool: Function succeeded.\\n*/\\nbool AwsDoc::S3::GetObjectWithPresignedObjectURL(const Aws::String &presignedURL,\\nAws::String &resultString) {\\nCURL *curl = curl_easy_init();\\nCURLcode result;\\nstd::stringstream outWriteString;\\nresult = curl_easy_setopt(curl, CURLOPT_WRITEDATA, &outWriteString);\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to set CURLOPT_WRITEDATA \" << std::endl;\\nreturn false;\\n}\\nresult = curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, myCurlWriteBack);\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to set CURLOPT_WRITEFUNCTION\" << std::endl;\\nreturn false;\\n}\\nresult = curl_easy_setopt(curl, CURLOPT_URL, presignedURL.c_str());\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to set CURLOPT_URL\" << std::endl;\\nreturn false;\\n}\\nresult = curl_easy_perform(curl);\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to perform CURL request\" << std::endl;\\nreturn false;', '']]]\n",
      "[[['', '}\\nresultString = outWriteString.str();\\nif (resultString.find( \"<?xml\") == 0)\\n{\\nstd::cerr << \"Failed to get object, response:\\\\n\" << resultString <<\\nstd::endl;\\nreturn false;\\n}\\nreturn true;\\n}', ''], ['', '', '']], [['', '', ''], ['', '//! Routine which demonstrates creating a pre-signed URL to upload an object to\\nan\\n//! Amazon Simple Storage Service (Amazon S3) bucket.\\n/*!\\n\\\\param bucketName: Name of the bucket.\\n\\\\param key: Name of an object key.\\n\\\\param clientConfig: Aws client configuration.\\n\\\\return Aws::String: A pre-signed URL.\\n*/\\nAws::String AwsDoc::S3::GeneratePreSignedPutObjectURL(const Aws::String\\n&bucketName,\\nconst Aws::String &key,\\nuint64_t expirationSeconds,\\nconst\\nAws::Client::ClientConfiguration &clientConfig) {\\nAws::S3::S3Client client(clientConfig);\\nreturn client.GeneratePresignedUrl(bucketName, key,\\nAws::Http::HttpMethod::HTTP_PUT,\\nexpirationSeconds);\\n}', ''], ['', '', '']], [['', '', ''], ['', 'static size_t myCurlReadBack(char *buffer, size_t size, size_t nitems, void\\n*userdata) {', '']]]\n",
      "[[['', 'Aws::StringStream *str = (Aws::StringStream *) userdata;\\nstr->read(buffer, size * nitems);\\nreturn str->gcount();\\n}\\nstatic size_t myCurlWriteBack(char *buffer, size_t size, size_t nitems, void\\n*userdata) {\\nAws::StringStream *str = (Aws::StringStream *) userdata;\\nif (nitems > 0) {\\nstr->write(buffer, size * nitems);\\n}\\nreturn size * nitems;\\n}\\n//! Utility routine to test PutObject with a pre-signed URL.\\n/*!\\n\\\\param presignedURL: A pre-signed URL to put an object in a bucket.\\n\\\\param data: Body of the PutObject request.\\n\\\\return bool: Function succeeded.\\n*/\\nbool AwsDoc::S3::PutStringWithPresignedObjectURL(const Aws::String &presignedURL,\\nconst Aws::String &data) {\\nCURL *curl = curl_easy_init();\\nCURLcode result;\\nAws::StringStream readStringStream;\\nreadStringStream << data;\\nresult = curl_easy_setopt(curl, CURLOPT_READFUNCTION, myCurlReadBack);\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to set CURLOPT_READFUNCTION\" << std::endl;\\nreturn false;\\n}\\nresult = curl_easy_setopt(curl, CURLOPT_READDATA, &readStringStream);\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to set CURLOPT_READDATA\" << std::endl;\\nreturn false;\\n}\\nresult = curl_easy_setopt(curl, CURLOPT_INFILESIZE_LARGE,', '']]]\n",
      "[[['', '(curl_off_t)data.size());\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to set CURLOPT_INFILESIZE_LARGE\" << std::endl;\\nreturn false;\\n}\\nresult = curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, myCurlWriteBack);\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to set CURLOPT_WRITEFUNCTION\" << std::endl;\\nreturn false;\\n}\\nstd::stringstream outWriteString;\\nresult = curl_easy_setopt(curl, CURLOPT_WRITEDATA, &outWriteString);\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to set CURLOPT_WRITEDATA \" << std::endl;\\nreturn false;\\n}\\nresult = curl_easy_setopt(curl, CURLOPT_URL, presignedURL.c_str());\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to set CURLOPT_URL\" << std::endl;\\nreturn false;\\n}\\nresult = curl_easy_setopt(curl, CURLOPT_UPLOAD, 1L);\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to set CURLOPT_PUT\" << std::endl;\\nreturn false;\\n}\\nresult = curl_easy_perform(curl);\\nif (result != CURLE_OK) {\\nstd::cerr << \"Failed to perform CURL request\" << std::endl;\\nreturn false;\\n}', '']]]\n",
      "[[['', 'std::string outString = outWriteString.str();\\nif (outString.empty()) {\\nstd::cout << \"Successfully put object.\" << std::endl;\\nreturn true;\\n}\\nelse {\\nstd::cout << \"A server error was encountered, output:\\\\n\" << outString\\n<< std::endl;\\nreturn false;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Presigner encapsulates the Amazon Simple Storage Service (Amazon S3) presign\\nactions\\n// used in the examples.\\n// It contains PresignClient, a client that is used to presign requests to Amazon\\nS3.\\n// Presigned requests contain temporary credentials and can be made from any HTTP\\nclient.\\ntype Presigner struct {\\nPresignClient *s3.PresignClient\\n}\\n// GetObject makes a presigned request that can be used to get an object from a\\nbucket.\\n// The presigned request is valid for the specified number of seconds.', '']]]\n",
      "[[['', 'func (presigner Presigner) GetObject(\\nbucketName string, objectKey string, lifetimeSecs int64)\\n(*v4.PresignedHTTPRequest, error) {\\nrequest, err := presigner.PresignClient.PresignGetObject(context.TODO(),\\n&s3.GetObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\n}, func(opts *s3.PresignOptions) {\\nopts.Expires = time.Duration(lifetimeSecs * int64(time.Second))\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t get a presigned request to get %v:%v. Here\\'s why: %v\\\\n\",\\nbucketName, objectKey, err)\\n}\\nreturn request, err\\n}\\n// PutObject makes a presigned request that can be used to put an object in a\\nbucket.\\n// The presigned request is valid for the specified number of seconds.\\nfunc (presigner Presigner) PutObject(\\nbucketName string, objectKey string, lifetimeSecs int64)\\n(*v4.PresignedHTTPRequest, error) {\\nrequest, err := presigner.PresignClient.PresignPutObject(context.TODO(),\\n&s3.PutObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\n}, func(opts *s3.PresignOptions) {\\nopts.Expires = time.Duration(lifetimeSecs * int64(time.Second))\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t get a presigned request to put %v:%v. Here\\'s why: %v\\\\n\",\\nbucketName, objectKey, err)\\n}\\nreturn request, err\\n}\\n// DeleteObject makes a presigned request that can be used to delete an object\\nfrom a bucket.', '']]]\n",
      "[[['', 'func (presigner Presigner) DeleteObject(bucketName string, objectKey string)\\n(*v4.PresignedHTTPRequest, error) {\\nrequest, err := presigner.PresignClient.PresignDeleteObject(context.TODO(),\\n&s3.DeleteObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t get a presigned request to delete object %v. Here\\'s why:\\n%v\\\\n\", objectKey, err)\\n}\\nreturn request, err\\n}', ''], ['', '', '']], [['', '', ''], ['', '// RunPresigningScenario is an interactive example that shows you how to get\\npresigned\\n// HTTP requests that you can use to move data into and out of Amazon Simple\\nStorage\\n// Service (Amazon S3). The presigned requests contain temporary credentials and\\ncan\\n// be used by an HTTP client.\\n//\\n// 1. Get a presigned request to put an object in a bucket.\\n// 2. Use the net/http package to use the presigned request to upload a local\\nfile to the bucket.\\n// 3. Get a presigned request to get an object from a bucket.\\n// 4. Use the net/http package to use the presigned request to download the\\nobject to a local file.\\n// 5. Get a presigned request to delete an object from a bucket.\\n// 6. Use the net/http package to use the presigned request to delete the object.\\n//\\n// This example creates an Amazon S3 presign client from the specified sdkConfig\\nso that\\n// you can replace it with a mocked or stubbed config for unit testing.\\n//\\n// It uses a questioner from the `demotools` package to get input during the\\nexample.', '']]]\n",
      "[[['', '// This package can be found in the ..\\\\..\\\\demotools folder of this repo.\\n//\\n// It uses an IHttpRequester interface to abstract HTTP requests so they can be\\nmocked\\n// during testing.\\nfunc RunPresigningScenario(sdkConfig aws.Config, questioner\\ndemotools.IQuestioner, httpRequester IHttpRequester) {\\ndefer func() {\\nif r := recover(); r != nil {\\nfmt.Printf(\"Something went wrong with the demo.\")\\n}\\n}()\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Println(\"Welcome to the Amazon S3 presigning demo.\")\\nlog.Println(strings.Repeat(\"-\", 88))\\ns3Client := s3.NewFromConfig(sdkConfig)\\nbucketBasics := actions.BucketBasics{S3Client: s3Client}\\npresignClient := s3.NewPresignClient(s3Client)\\npresigner := actions.Presigner{PresignClient: presignClient}\\nbucketName := questioner.Ask(\"We\\'ll need a bucket. Enter a name for a bucket \"+\\n\"you own or one you want to create:\", demotools.NotEmpty{})\\nbucketExists, err := bucketBasics.BucketExists(bucketName)\\nif err != nil {\\npanic(err)\\n}\\nif !bucketExists {\\nerr = bucketBasics.CreateBucket(bucketName, sdkConfig.Region)\\nif err != nil {\\npanic(err)\\n} else {\\nlog.Println(\"Bucket created.\")\\n}\\n}\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Printf(\"Let\\'s presign a request to upload a file to your bucket.\")\\nuploadFilename := questioner.Ask(\"Enter the path to a file you want to upload:\",\\ndemotools.NotEmpty{})\\nuploadKey := questioner.Ask(\"What would you like to name the uploaded object?\",\\ndemotools.NotEmpty{})\\nuploadFile, err := os.Open(uploadFilename)', '']]]\n",
      "[[['', 'if err != nil {\\npanic(err)\\n}\\ndefer uploadFile.Close()\\npresignedPutRequest, err := presigner.PutObject(bucketName, uploadKey, 60)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"Got a presigned %v request to URL:\\\\n\\\\t%v\\\\n\",\\npresignedPutRequest.Method,\\npresignedPutRequest.URL)\\nlog.Println(\"Using net/http to send the request...\")\\ninfo, err := uploadFile.Stat()\\nif err != nil {\\npanic(err)\\n}\\nputResponse, err := httpRequester.Put(presignedPutRequest.URL, info.Size(),\\nuploadFile)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"%v object %v with presigned URL returned %v.\",\\npresignedPutRequest.Method,\\nuploadKey, putResponse.StatusCode)\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Printf(\"Let\\'s presign a request to download the object.\")\\nquestioner.Ask(\"Press Enter when you\\'re ready.\")\\npresignedGetRequest, err := presigner.GetObject(bucketName, uploadKey, 60)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"Got a presigned %v request to URL:\\\\n\\\\t%v\\\\n\",\\npresignedGetRequest.Method,\\npresignedGetRequest.URL)\\nlog.Println(\"Using net/http to send the request...\")\\ngetResponse, err := httpRequester.Get(presignedGetRequest.URL)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"%v object %v with presigned URL returned %v.\",\\npresignedGetRequest.Method,\\nuploadKey, getResponse.StatusCode)\\ndefer getResponse.Body.Close()', '']]]\n",
      "[[['', 'downloadBody, err := io.ReadAll(getResponse.Body)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"Downloaded %v bytes. Here are the first 100 of them:\\\\n\",\\nlen(downloadBody))\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Println(string(downloadBody[:100]))\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Println(\"Let\\'s presign a request to delete the object.\")\\nquestioner.Ask(\"Press Enter when you\\'re ready.\")\\npresignedDelRequest, err := presigner.DeleteObject(bucketName, uploadKey)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"Got a presigned %v request to URL:\\\\n\\\\t%v\\\\n\",\\npresignedDelRequest.Method,\\npresignedDelRequest.URL)\\nlog.Println(\"Using net/http to send the request...\")\\ndelResponse, err := httpRequester.Delete(presignedDelRequest.URL)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"%v object %v with presigned URL returned %v.\\\\n\",\\npresignedDelRequest.Method,\\nuploadKey, delResponse.StatusCode)\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Println(\"Thanks for watching!\")\\nlog.Println(strings.Repeat(\"-\", 88))\\n}', ''], ['', '', '']], [['', '', ''], ['', '// IHttpRequester abstracts HTTP requests into an interface so it can be mocked\\nduring\\n// unit testing.\\ntype IHttpRequester interface {\\nGet(url string) (resp *http.Response, err error)', '']]]\n",
      "[[['', 'Put(url string, contentLength int64, body io.Reader) (resp *http.Response, err\\nerror)\\nDelete(url string) (resp *http.Response, err error)\\n}\\n// HttpRequester uses the net/http package to make HTTP requests during the\\nscenario.\\ntype HttpRequester struct{}\\nfunc (httpReq HttpRequester) Get(url string) (resp *http.Response, err error) {\\nreturn http.Get(url)\\n}\\nfunc (httpReq HttpRequester) Put(url string, contentLength int64, body io.Reader)\\n(resp *http.Response, err error) {\\nputRequest, err := http.NewRequest(\"PUT\", url, body)\\nif err != nil {\\nreturn nil, err\\n}\\nputRequest.ContentLength = contentLength\\nreturn http.DefaultClient.Do(putRequest)\\n}\\nfunc (httpReq HttpRequester) Delete(url string) (resp *http.Response, err error)\\n{\\ndelRequest, err := http.NewRequest(\"DELETE\", url, nil)\\nif err != nil {\\nreturn nil, err\\n}\\nreturn http.DefaultClient.Do(delRequest)\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'import com.example.s3.util.PresignUrlUtils;\\nimport org.slf4j.Logger;\\nimport software.amazon.awssdk.http.HttpExecuteRequest;\\nimport software.amazon.awssdk.http.HttpExecuteResponse;\\nimport software.amazon.awssdk.http.SdkHttpClient;\\nimport software.amazon.awssdk.http.SdkHttpMethod;\\nimport software.amazon.awssdk.http.SdkHttpRequest;\\nimport software.amazon.awssdk.http.apache.ApacheHttpClient;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.GetObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.presigner.S3Presigner;\\nimport\\nsoftware.amazon.awssdk.services.s3.presigner.model.GetObjectPresignRequest;\\nimport\\nsoftware.amazon.awssdk.services.s3.presigner.model.PresignedGetObjectRequest;\\nimport software.amazon.awssdk.utils.IoUtils;\\nimport java.io.ByteArrayOutputStream;\\nimport java.io.File;\\nimport java.io.IOException;\\nimport java.io.InputStream;\\nimport java.net.HttpURLConnection;\\nimport java.net.URISyntaxException;\\nimport java.net.URL;\\nimport java.net.http.HttpClient;\\nimport java.net.http.HttpRequest;\\nimport java.net.http.HttpResponse;\\nimport java.nio.file.Paths;\\nimport java.time.Duration;\\nimport java.util.UUID;', ''], ['', '', '']], [['', '', ''], ['', '/* Create a pre-signed URL to download an object in a subsequent GET request.\\n*/\\npublic String createPresignedGetUrl(String bucketName, String keyName) {\\ntry (S3Presigner presigner = S3Presigner.create()) {', '']]]\n",
      "[[['', 'GetObjectRequest objectRequest = GetObjectRequest.builder()\\n.bucket(bucketName)\\n.key(keyName)\\n.build();\\nGetObjectPresignRequest presignRequest =\\nGetObjectPresignRequest.builder()\\n.signatureDuration(Duration.ofMinutes(10)) // The URL will\\nexpire in 10 minutes.\\n.getObjectRequest(objectRequest)\\n.build();\\nPresignedGetObjectRequest presignedRequest =\\npresigner.presignGetObject(presignRequest);\\nlogger.info(\"Presigned URL: [{}]\",\\npresignedRequest.url().toString());\\nlogger.info(\"HTTP method: [{}]\",\\npresignedRequest.httpRequest().method());\\nreturn presignedRequest.url().toExternalForm();\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '/* Use the JDK HttpURLConnection (since v1.1) class to do the download. */\\npublic byte[] useHttpUrlConnectionToGet(String presignedUrlString) {\\nByteArrayOutputStream byteArrayOutputStream = new\\nByteArrayOutputStream(); // Capture the response body to a byte array.\\ntry {\\nURL presignedUrl = new URL(presignedUrlString);\\nHttpURLConnection connection = (HttpURLConnection)\\npresignedUrl.openConnection();\\nconnection.setRequestMethod(\"GET\");\\n// Download the result of executing the request.\\ntry (InputStream content = connection.getInputStream()) {\\nIoUtils.copy(content, byteArrayOutputStream);\\n}\\nlogger.info(\"HTTP response code is \" + connection.getResponseCode());', '']]]\n",
      "[[['', '} catch (S3Exception | IOException e) {\\nlogger.error(e.getMessage(), e);\\n}\\nreturn byteArrayOutputStream.toByteArray();\\n}', ''], ['', '', '']], [['', '', ''], ['', '/* Use the JDK HttpClient (since v11) class to do the download. */\\npublic byte[] useHttpClientToGet(String presignedUrlString) {\\nByteArrayOutputStream byteArrayOutputStream = new\\nByteArrayOutputStream(); // Capture the response body to a byte array.\\nHttpRequest.Builder requestBuilder = HttpRequest.newBuilder();\\nHttpClient httpClient = HttpClient.newHttpClient();\\ntry {\\nURL presignedUrl = new URL(presignedUrlString);\\nHttpResponse<InputStream> response = httpClient.send(requestBuilder\\n.uri(presignedUrl.toURI())\\n.GET()\\n.build(),\\nHttpResponse.BodyHandlers.ofInputStream());\\nIoUtils.copy(response.body(), byteArrayOutputStream);\\nlogger.info(\"HTTP response code is \" + response.statusCode());\\n} catch (URISyntaxException | InterruptedException | IOException e) {\\nlogger.error(e.getMessage(), e);\\n}\\nreturn byteArrayOutputStream.toByteArray();\\n}', ''], ['', '', '']], [['', '', ''], ['', '/* Use the AWS SDK for Java SdkHttpClient class to do the download. */\\npublic byte[] useSdkHttpClientToPut(String presignedUrlString) {\\nByteArrayOutputStream byteArrayOutputStream = new\\nByteArrayOutputStream(); // Capture the response body to a byte array.', '']]]\n",
      "[[['', 'try {\\nURL presignedUrl = new URL(presignedUrlString);\\nSdkHttpRequest request = SdkHttpRequest.builder()\\n.method(SdkHttpMethod.GET)\\n.uri(presignedUrl.toURI())\\n.build();\\nHttpExecuteRequest executeRequest = HttpExecuteRequest.builder()\\n.request(request)\\n.build();\\ntry (SdkHttpClient sdkHttpClient = ApacheHttpClient.create()) {\\nHttpExecuteResponse response =\\nsdkHttpClient.prepareRequest(executeRequest).call();\\nresponse.responseBody().ifPresentOrElse(\\nabortableInputStream -> {\\ntry {\\nIoUtils.copy(abortableInputStream,\\nbyteArrayOutputStream);\\n} catch (IOException e) {\\nthrow new RuntimeException(e);\\n}\\n},\\n() -> logger.error(\"No response body.\"));\\nlogger.info(\"HTTP Response code is {}\",\\nresponse.httpResponse().statusCode());\\n}\\n} catch (URISyntaxException | IOException e) {\\nlogger.error(e.getMessage(), e);\\n}\\nreturn byteArrayOutputStream.toByteArray();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import com.example.s3.util.PresignUrlUtils;\\nimport org.slf4j.Logger;\\nimport software.amazon.awssdk.core.internal.sync.FileContentStreamProvider;\\nimport software.amazon.awssdk.http.HttpExecuteRequest;\\nimport software.amazon.awssdk.http.HttpExecuteResponse;', '']]]\n",
      "[[['', 'import software.amazon.awssdk.http.SdkHttpClient;\\nimport software.amazon.awssdk.http.SdkHttpMethod;\\nimport software.amazon.awssdk.http.SdkHttpRequest;\\nimport software.amazon.awssdk.http.apache.ApacheHttpClient;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.PutObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.presigner.S3Presigner;\\nimport\\nsoftware.amazon.awssdk.services.s3.presigner.model.PresignedPutObjectRequest;\\nimport\\nsoftware.amazon.awssdk.services.s3.presigner.model.PutObjectPresignRequest;\\nimport java.io.File;\\nimport java.io.IOException;\\nimport java.io.OutputStream;\\nimport java.io.RandomAccessFile;\\nimport java.net.HttpURLConnection;\\nimport java.net.URISyntaxException;\\nimport java.net.URL;\\nimport java.net.http.HttpClient;\\nimport java.net.http.HttpRequest;\\nimport java.net.http.HttpResponse;\\nimport java.nio.ByteBuffer;\\nimport java.nio.channels.FileChannel;\\nimport java.nio.file.Path;\\nimport java.nio.file.Paths;\\nimport java.time.Duration;\\nimport java.util.Map;\\nimport java.util.UUID;', ''], ['', '', '']], [['', '', ''], ['', '/* Create a presigned URL to use in a subsequent PUT request */\\npublic String createPresignedUrl(String bucketName, String keyName,\\nMap<String, String> metadata) {\\ntry (S3Presigner presigner = S3Presigner.create()) {\\nPutObjectRequest objectRequest = PutObjectRequest.builder()\\n.bucket(bucketName)\\n.key(keyName)\\n.metadata(metadata)', '']]]\n",
      "[[['', '.build();\\nPutObjectPresignRequest presignRequest =\\nPutObjectPresignRequest.builder()\\n.signatureDuration(Duration.ofMinutes(10)) // The URL\\nexpires in 10 minutes.\\n.putObjectRequest(objectRequest)\\n.build();\\nPresignedPutObjectRequest presignedRequest =\\npresigner.presignPutObject(presignRequest);\\nString myURL = presignedRequest.url().toString();\\nlogger.info(\"Presigned URL to upload a file to: [{}]\", myURL);\\nlogger.info(\"HTTP method: [{}]\",\\npresignedRequest.httpRequest().method());\\nreturn presignedRequest.url().toExternalForm();\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '/* Use the JDK HttpURLConnection (since v1.1) class to do the upload. */\\npublic void useHttpUrlConnectionToPut(String presignedUrlString, File\\nfileToPut, Map<String, String> metadata) {\\nlogger.info(\"Begin [{}] upload\", fileToPut.toString());\\ntry {\\nURL presignedUrl = new URL(presignedUrlString);\\nHttpURLConnection connection = (HttpURLConnection)\\npresignedUrl.openConnection();\\nconnection.setDoOutput(true);\\nmetadata.forEach((k, v) -> connection.setRequestProperty(\"x-amz-\\nmeta-\" + k, v));\\nconnection.setRequestMethod(\"PUT\");\\nOutputStream out = connection.getOutputStream();\\ntry (RandomAccessFile file = new RandomAccessFile(fileToPut, \"r\");\\nFileChannel inChannel = file.getChannel()) {\\nByteBuffer buffer = ByteBuffer.allocate(8192); //Buffer size is\\n8k', '']]]\n",
      "[[['', 'while (inChannel.read(buffer) > 0) {\\nbuffer.flip();\\nfor (int i = 0; i < buffer.limit(); i++) {\\nout.write(buffer.get());\\n}\\nbuffer.clear();\\n}\\n} catch (IOException e) {\\nlogger.error(e.getMessage(), e);\\n}\\nout.close();\\nconnection.getResponseCode();\\nlogger.info(\"HTTP response code is \" + connection.getResponseCode());\\n} catch (S3Exception | IOException e) {\\nlogger.error(e.getMessage(), e);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '/* Use the JDK HttpClient (since v11) class to do the upload. */\\npublic void useHttpClientToPut(String presignedUrlString, File fileToPut,\\nMap<String, String> metadata) {\\nlogger.info(\"Begin [{}] upload\", fileToPut.toString());\\nHttpRequest.Builder requestBuilder = HttpRequest.newBuilder();\\nmetadata.forEach((k, v) -> requestBuilder.header(\"x-amz-meta-\" + k, v));\\nHttpClient httpClient = HttpClient.newHttpClient();\\ntry {\\nfinal HttpResponse<Void> response = httpClient.send(requestBuilder\\n.uri(new URL(presignedUrlString).toURI())\\n.PUT(HttpRequest.BodyPublishers.ofFile(Path.of(fileToPut.toURI())))\\n.build(),\\nHttpResponse.BodyHandlers.discarding());\\nlogger.info(\"HTTP response code is \" + response.statusCode());', '']]]\n",
      "[[['', '} catch (URISyntaxException | InterruptedException | IOException e) {\\nlogger.error(e.getMessage(), e);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '/* Use the AWS SDK for Java V2 SdkHttpClient class to do the upload. */\\npublic void useSdkHttpClientToPut(String presignedUrlString, File fileToPut,\\nMap<String, String> metadata) {\\nlogger.info(\"Begin [{}] upload\", fileToPut.toString());\\ntry {\\nURL presignedUrl = new URL(presignedUrlString);\\nSdkHttpRequest.Builder requestBuilder = SdkHttpRequest.builder()\\n.method(SdkHttpMethod.PUT)\\n.uri(presignedUrl.toURI());\\n// Add headers\\nmetadata.forEach((k, v) -> requestBuilder.putHeader(\"x-amz-meta-\" +\\nk, v));\\n// Finish building the request.\\nSdkHttpRequest request = requestBuilder.build();\\nHttpExecuteRequest executeRequest = HttpExecuteRequest.builder()\\n.request(request)\\n.contentStreamProvider(new\\nFileContentStreamProvider(fileToPut.toPath()))\\n.build();\\ntry (SdkHttpClient sdkHttpClient = ApacheHttpClient.create()) {\\nHttpExecuteResponse response =\\nsdkHttpClient.prepareRequest(executeRequest).call();\\nlogger.info(\"Response code: {}\",\\nresponse.httpResponse().statusCode());\\n}\\n} catch (URISyntaxException | IOException e) {\\nlogger.error(e.getMessage(), e);\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import https from \"https\";\\nimport { PutObjectCommand, S3Client } from \"@aws-sdk/client-s3\";\\nimport { fromIni } from \"@aws-sdk/credential-providers\";\\nimport { HttpRequest } from \"@smithy/protocol-http\";\\nimport {\\ngetSignedUrl,\\nS3RequestPresigner,\\n} from \"@aws-sdk/s3-request-presigner\";\\nimport { parseUrl } from \"@smithy/url-parser\";\\nimport { formatUrl } from \"@aws-sdk/util-format-url\";\\nimport { Hash } from \"@smithy/hash-node\";\\nconst createPresignedUrlWithoutClient = async ({ region, bucket, key }) => {\\nconst url = parseUrl(`https://${bucket}.s3.${region}.amazonaws.com/${key}`);\\nconst presigner = new S3RequestPresigner({\\ncredentials: fromIni(),\\nregion,\\nsha256: Hash.bind(null, \"sha256\"),\\n});\\nconst signedUrlObject = await presigner.presign(\\nnew HttpRequest({ ...url, method: \"PUT\" }),\\n);\\nreturn formatUrl(signedUrlObject);\\n};\\nconst createPresignedUrlWithClient = ({ region, bucket, key }) => {\\nconst client = new S3Client({ region });\\nconst command = new PutObjectCommand({ Bucket: bucket, Key: key });\\nreturn getSignedUrl(client, command, { expiresIn: 3600 });\\n};', '']]]\n",
      "[[['', 'function put(url, data) {\\nreturn new Promise((resolve, reject) => {\\nconst req = https.request(\\nurl,\\n{ method: \"PUT\", headers: { \"Content-Length\": new Blob([data]).size } },\\n(res) => {\\nlet responseBody = \"\";\\nres.on(\"data\", (chunk) => {\\nresponseBody += chunk;\\n});\\nres.on(\"end\", () => {\\nresolve(responseBody);\\n});\\n},\\n);\\nreq.on(\"error\", (err) => {\\nreject(err);\\n});\\nreq.write(data);\\nreq.end();\\n});\\n}\\nexport const main = async () => {\\nconst REGION = \"us-east-1\";\\nconst BUCKET = \"example_bucket\";\\nconst KEY = \"example_file.txt\";\\n// There are two ways to generate a presigned URL.\\n// 1. Use createPresignedUrl without the S3 client.\\n// 2. Use getSignedUrl in conjunction with the S3 client and GetObjectCommand.\\ntry {\\nconst noClientUrl = await createPresignedUrlWithoutClient({\\nregion: REGION,\\nbucket: BUCKET,\\nkey: KEY,\\n});\\nconst clientUrl = await createPresignedUrlWithClient({\\nregion: REGION,\\nbucket: BUCKET,\\nkey: KEY,\\n});', '']]]\n",
      "[[['', '// After you get the presigned URL, you can provide your own file\\n// data. Refer to put() above.\\nconsole.log(\"Calling PUT using presigned URL without client\");\\nawait put(noClientUrl, \"Hello World\");\\nconsole.log(\"Calling PUT using presigned URL with client\");\\nawait put(clientUrl, \"Hello World\");\\nconsole.log(\"\\\\nDone. Check your S3 console.\");\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', 'import { GetObjectCommand, S3Client } from \"@aws-sdk/client-s3\";\\nimport { fromIni } from \"@aws-sdk/credential-providers\";\\nimport { HttpRequest } from \"@smithy/protocol-http\";\\nimport {\\ngetSignedUrl,\\nS3RequestPresigner,\\n} from \"@aws-sdk/s3-request-presigner\";\\nimport { parseUrl } from \"@smithy/url-parser\";\\nimport { formatUrl } from \"@aws-sdk/util-format-url\";\\nimport { Hash } from \"@smithy/hash-node\";\\nconst createPresignedUrlWithoutClient = async ({ region, bucket, key }) => {\\nconst url = parseUrl(`https://${bucket}.s3.${region}.amazonaws.com/${key}`);\\nconst presigner = new S3RequestPresigner({\\ncredentials: fromIni(),\\nregion,\\nsha256: Hash.bind(null, \"sha256\"),\\n});\\nconst signedUrlObject = await presigner.presign(new HttpRequest(url));\\nreturn formatUrl(signedUrlObject);\\n};\\nconst createPresignedUrlWithClient = ({ region, bucket, key }) => {\\nconst client = new S3Client({ region });', '']]]\n",
      "[[['', 'const command = new GetObjectCommand({ Bucket: bucket, Key: key });\\nreturn getSignedUrl(client, command, { expiresIn: 3600 });\\n};\\nexport const main = async () => {\\nconst REGION = \"us-east-1\";\\nconst BUCKET = \"example_bucket\";\\nconst KEY = \"example_file.jpg\";\\ntry {\\nconst noClientUrl = await createPresignedUrlWithoutClient({\\nregion: REGION,\\nbucket: BUCKET,\\nkey: KEY,\\n});\\nconst clientUrl = await createPresignedUrlWithClient({\\nregion: REGION,\\nbucket: BUCKET,\\nkey: KEY,\\n});\\nconsole.log(\"Presigned URL without client\");\\nconsole.log(noClientUrl);\\nconsole.log(\"\\\\n\");\\nconsole.log(\"Presigned URL with client\");\\nconsole.log(clientUrl);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun getObjectPresigned(\\ns3: S3Client,\\nbucketName: String,\\nkeyName: String,\\n): String {\\n// Create a GetObjectRequest.\\nval unsignedRequest =\\nGetObjectRequest {\\nbucket = bucketName\\nkey = keyName\\n}\\n// Presign the GetObject request.\\nval presignedRequest = s3.presignGetObject(unsignedRequest, 24.hours)\\n// Use the URL from the presigned HttpRequest in a subsequent HTTP GET\\nrequest to retrieve the object.\\nval objectContents = URL(presignedRequest.url.toString()).readText()\\nreturn objectContents\\n}', ''], ['', '', '']], [['', '', ''], ['', 'suspend fun getObjectPresignedMoreOptions(\\ns3: S3Client,\\nbucketName: String,\\nkeyName: String,\\n): HttpRequest {', '']]]\n",
      "[[['', '// Create a GetObjectRequest.\\nval unsignedRequest =\\nGetObjectRequest {\\nbucket = bucketName\\nkey = keyName\\n}\\n// Presign the GetObject request.\\nval presignedRequest =\\ns3.presignGetObject(unsignedRequest, signer = CrtAwsSigner) {\\nsigningDate = Instant.now() + 12.hours // Presigned request can be\\nused 12 hours from now.\\nalgorithm = AwsSigningAlgorithm.SIGV4_ASYMMETRIC\\nsignatureType = AwsSignatureType.HTTP_REQUEST_VIA_QUERY_PARAMS\\nexpiresAfter = 8.hours // Presigned request expires 8 hours later.\\n}\\nreturn presignedRequest\\n}', ''], ['', '', '']], [['', '', ''], ['', 'suspend fun putObjectPresigned(\\ns3: S3Client,\\nbucketName: String,\\nkeyName: String,\\ncontent: String,\\n) {\\n// Create a PutObjectRequest.\\nval unsignedRequest =\\nPutObjectRequest {\\nbucket = bucketName\\nkey = keyName\\n}\\n// Presign the request.\\nval presignedRequest = s3.presignPutObject(unsignedRequest, 24.hours)\\n// Use the URL and any headers from the presigned HttpRequest in a subsequent\\nHTTP PUT request to retrieve the object.\\n// Create a PUT request using the OKHttpClient API.\\nval putRequest =\\nRequest', '']]]\n",
      "[[['', '.Builder()\\n.url(presignedRequest.url.toString())\\n.apply {\\npresignedRequest.headers.forEach { key, values ->\\nheader(key, values.joinToString(\", \"))\\n}\\n}.put(content.toRequestBody())\\n.build()\\nval response = OkHttpClient().newCall(putRequest).execute()\\nassert(response.isSuccessful)\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', \"namespace S3;\\nuse Aws\\\\Exception\\\\AwsException;\\nuse AwsUtilities\\\\PrintableLineBreak;\\nuse AwsUtilities\\\\TestableReadline;\\nuse DateTime;\\nrequire 'vendor/autoload.php';\\nclass PresignedURL\\n{\\nuse PrintableLineBreak;\\nuse TestableReadline;\\npublic function run()\\n{\\n$s3Service = new S3Service();\", '']]]\n",
      "[[['', '$expiration = new DateTime(\"+20 minutes\");\\n$linebreak = $this->getLineBreak();\\necho $linebreak;\\necho (\"Welcome to the Amazon S3 presigned URL demo.\\\\n\");\\necho $linebreak;\\n$bucket = $this->testable_readline(\"First, please enter the name of the\\nS3 bucket to use: \");\\n$key = $this->testable_readline(\"Next, provide the key of an object in\\nthe given bucket: \");\\necho $linebreak;\\n$command = $s3Service->getClient()->getCommand(\\'GetObject\\', [\\n\\'Bucket\\' => $bucket,\\n\\'Key\\' => $key,\\n]);\\ntry {\\n$preSignedUrl = $s3Service->preSignedUrl($command, $expiration);\\necho \"Your preSignedUrl is \\\\n$preSignedUrl\\\\nand will be good for the\\nnext 20 minutes.\\\\n\";\\necho $linebreak;\\necho \"Thanks for trying the Amazon S3 presigned URL demo.\\\\n\";\\n} catch (AwsException $exception) {\\necho $linebreak;\\necho \"Something went wrong: $exception\";\\ndie();\\n}\\n}\\n}\\n$runner = new PresignedURL();\\n$runner->run();', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import argparse\\nimport logging\\nimport boto3\\nfrom botocore.exceptions import ClientError\\nimport requests\\nlogger = logging.getLogger(__name__)\\ndef generate_presigned_url(s3_client, client_method, method_parameters,\\nexpires_in):\\n\"\"\"\\nGenerate a presigned Amazon S3 URL that can be used to perform an action.\\n:param s3_client: A Boto3 Amazon S3 client.\\n:param client_method: The name of the client method that the URL performs.\\n:param method_parameters: The parameters of the specified client method.\\n:param expires_in: The number of seconds the presigned URL is valid for.\\n:return: The presigned URL.\\n\"\"\"\\ntry:\\nurl = s3_client.generate_presigned_url(\\nClientMethod=client_method, Params=method_parameters,\\nExpiresIn=expires_in\\n)\\nlogger.info(\"Got presigned URL: %s\", url)\\nexcept ClientError:\\nlogger.exception(\\n\"Couldn\\'t get a presigned URL for client method \\'%s\\'.\", client_method', '']]]\n",
      "[[['', ')\\nraise\\nreturn url\\ndef usage_demo():\\nlogging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\\nprint(\"-\" * 88)\\nprint(\"Welcome to the Amazon S3 presigned URL demo.\")\\nprint(\"-\" * 88)\\nparser = argparse.ArgumentParser()\\nparser.add_argument(\"bucket\", help=\"The name of the bucket.\")\\nparser.add_argument(\\n\"key\",\\nhelp=\"For a GET operation, the key of the object in Amazon S3. For a \"\\n\"PUT operation, the name of a file to upload.\",\\n)\\nparser.add_argument(\"action\", choices=(\"get\", \"put\"), help=\"The action to\\nperform.\")\\nargs = parser.parse_args()\\ns3_client = boto3.client(\"s3\")\\nclient_action = \"get_object\" if args.action == \"get\" else \"put_object\"\\nurl = generate_presigned_url(\\ns3_client, client_action, {\"Bucket\": args.bucket, \"Key\": args.key}, 1000\\n)\\nprint(\"Using the Requests package to send a request to the URL.\")\\nresponse = None\\nif args.action == \"get\":\\nresponse = requests.get(url)\\nelif args.action == \"put\":\\nprint(\"Putting data to the URL.\")\\ntry:\\nwith open(args.key, \"r\") as object_file:\\nobject_text = object_file.read()\\nresponse = requests.put(url, data=object_text)\\nexcept FileNotFoundError:\\nprint(\\nf\"Couldn\\'t find {args.key}. For a PUT operation, the key must be\\nthe \"\\nf\"name of a file that exists on your computer.\"', '']]]\n",
      "[[['', ')\\nif response is not None:\\nprint(\"Got response:\")\\nprint(f\"Status: {response.status_code}\")\\nprint(response.text)\\nprint(\"-\" * 88)\\nif __name__ == \"__main__\":\\nusage_demo()', ''], ['', '', '']], [['', '', ''], ['', 'class BucketWrapper:\\n\"\"\"Encapsulates S3 bucket actions.\"\"\"\\ndef __init__(self, bucket):\\n\"\"\"\\n:param bucket: A Boto3 Bucket resource. This is a high-level resource in\\nBoto3\\nthat wraps bucket actions in a class-like structure.\\n\"\"\"\\nself.bucket = bucket\\nself.name = bucket.name\\ndef generate_presigned_post(self, object_key, expires_in):\\n\"\"\"\\nGenerate a presigned Amazon S3 POST request to upload a file.\\nA presigned POST can be used for a limited time to let someone without an\\nAWS\\naccount upload a file to a bucket.\\n:param object_key: The object key to identify the uploaded object.\\n:param expires_in: The number of seconds the presigned POST is valid.\\n:return: A dictionary that contains the URL and form fields that contain\\nrequired access data.\\n\"\"\"\\ntry:\\nresponse = self.bucket.meta.client.generate_presigned_post(', '']]]\n",
      "[[['', 'Bucket=self.bucket.name, Key=object_key, ExpiresIn=expires_in\\n)\\nlogger.info(\"Got presigned POST URL: %s\", response[\"url\"])\\nexcept ClientError:\\nlogger.exception(\\n\"Couldn\\'t get a presigned POST URL for bucket \\'%s\\' and object\\n\\'%s\\'\",\\nself.bucket.name,\\nobject_key,\\n)\\nraise\\nreturn response', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\nrequire \"net/http\"\\n# Creates a presigned URL that can be used to upload content to an object.\\n#\\n# @param bucket [Aws::S3::Bucket] An existing Amazon S3 bucket.\\n# @param object_key [String] The key to give the uploaded object.\\n# @return [URI, nil] The parsed URI if successful; otherwise nil.\\ndef get_presigned_url(bucket, object_key)\\nurl = bucket.object(object_key).presigned_url(:put)\\nputs \"Created presigned URL: #{url}\"\\nURI(url)\\nrescue Aws::Errors::ServiceError => e\\nputs \"Couldn\\'t create presigned URL for #{bucket.name}:#{object_key}. Here\\'s\\nwhy: #{e.message}\"', '']]]\n",
      "[[['', 'end\\n# Example usage:\\ndef run_demo\\nbucket_name = \"doc-example-bucket\"\\nobject_key = \"my-file.txt\"\\nobject_content = \"This is the content of my-file.txt.\"\\nbucket = Aws::S3::Bucket.new(bucket_name)\\npresigned_url = get_presigned_url(bucket, object_key)\\nreturn unless presigned_url\\nresponse = Net::HTTP.start(presigned_url.host) do |http|\\nhttp.send_request(\"PUT\", presigned_url.request_uri, object_content,\\n\"content_type\" => \"\")\\nend\\ncase response\\nwhen Net::HTTPSuccess\\nputs \"Content uploaded!\"\\nelse\\nputs response.value\\nend\\nend\\nrun_demo if $PROGRAM_NAME == __FILE__', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'async fn get_object(\\nclient: &Client,', '']]]\n",
      "[[['', 'bucket: &str,\\nobject: &str,\\nexpires_in: u64,\\n) -> Result<(), Box<dyn Error>> {\\nlet expires_in = Duration::from_secs(expires_in);\\nlet presigned_request = client\\n.get_object()\\n.bucket(bucket)\\n.key(object)\\n.presigned(PresigningConfig::expires_in(expires_in)?)\\n.await?;\\nprintln!(\"Object URI: {}\", presigned_request.uri());\\nOk(())\\n}\\nasync fn put_object(\\nclient: &Client,\\nbucket: &str,\\nobject: &str,\\nexpires_in: u64,\\n) -> Result<(), Box<dyn Error>> {\\nlet expires_in = Duration::from_secs(expires_in);\\nlet presigned_request = client\\n.put_object()\\n.bucket(bucket)\\n.key(object)\\n.presigned(PresigningConfig::expires_in(expires_in)?)\\n.await?;\\nprintln!(\"Object URI: {}\", presigned_request.uri());\\nOk(())\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import { useEffect, useState } from \"react\";\\nimport {\\nListObjectsCommand,\\nListObjectsCommandOutput,\\nS3Client,\\n} from \"@aws-sdk/client-s3\";\\nimport { fromCognitoIdentityPool } from \"@aws-sdk/credential-providers\";\\nimport \"./App.css\";\\nfunction App() {\\nconst [objects, setObjects] = useState<\\nRequired<ListObjectsCommandOutput>[\"Contents\"]\\n>([]);\\nuseEffect(() => {\\nconst client = new S3Client({\\nregion: \"us-east-1\",\\n// Unless you have a public bucket, you\\'ll need access to a private bucket.\\n// One way to do this is to create an Amazon Cognito identity pool, attach\\na role to the pool,\\n// and grant the role access to the \\'s3:GetObject\\' action.\\n//\\n// You\\'ll also need to configure the CORS settings on the bucket to allow\\ntraffic from', '']]]\n",
      "[[['', '// this example site. Here\\'s an example configuration that allows all\\norigins. Don\\'t\\n// do this in production.\\n//[\\n// {\\n// \"AllowedHeaders\": [\"*\"],\\n// \"AllowedMethods\": [\"GET\"],\\n// \"AllowedOrigins\": [\"*\"],\\n// \"ExposeHeaders\": [],\\n// },\\n//]\\n//\\ncredentials: fromCognitoIdentityPool({\\nclientConfig: { region: \"us-east-1\" },\\nidentityPoolId: \"<YOUR_IDENTITY_POOL_ID>\",\\n}),\\n});\\nconst command = new ListObjectsCommand({ Bucket: \"bucket-name\" });\\nclient.send(command).then(({ Contents }) => setObjects(Contents || []));\\n}, []);\\nreturn (\\n<div className=\"App\">\\n{objects.map((o) => (\\n<div key={o.ETag}>{o.Key}</div>\\n))}\\n</div>\\n);\\n}\\nexport default App;', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'public static void abortIncompleteMultipartUploadsFromList() {\\nListMultipartUploadsRequest listMultipartUploadsRequest =\\nListMultipartUploadsRequest.builder()\\n.bucket(bucketName)\\n.build();\\nListMultipartUploadsResponse response =\\ns3Client.listMultipartUploads(listMultipartUploadsRequest);\\nList<MultipartUpload> uploads = response.uploads();\\nAbortMultipartUploadRequest abortMultipartUploadRequest;\\nfor (MultipartUpload upload : uploads) {\\nabortMultipartUploadRequest = AbortMultipartUploadRequest.builder()\\n.bucket(bucketName)\\n.key(upload.key())\\n.expectedBucketOwner(accountId)\\n.uploadId(upload.uploadId())\\n.build();\\nAbortMultipartUploadResponse abortMultipartUploadResponse =\\ns3Client.abortMultipartUpload(abortMultipartUploadRequest);\\nif (abortMultipartUploadResponse.sdkHttpResponse().isSuccessful()) {\\nlogger.info(\"Upload ID [{}] to bucket [{}] successfully\\naborted.\", upload.uploadId(), bucketName);', '']]]\n",
      "[[['', '}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'static void abortIncompleteMultipartUploadsOlderThan(Instant pointInTime) {\\nListMultipartUploadsRequest listMultipartUploadsRequest =\\nListMultipartUploadsRequest.builder()\\n.bucket(bucketName)\\n.build();\\nListMultipartUploadsResponse response =\\ns3Client.listMultipartUploads(listMultipartUploadsRequest);\\nList<MultipartUpload> uploads = response.uploads();\\nAbortMultipartUploadRequest abortMultipartUploadRequest;\\nfor (MultipartUpload upload : uploads) {\\nlogger.info(\"Found multipartUpload with upload ID [{}], initiated\\n[{}]\", upload.uploadId(), upload.initiated());\\nif (upload.initiated().isBefore(pointInTime)) {\\nabortMultipartUploadRequest =\\nAbortMultipartUploadRequest.builder()\\n.bucket(bucketName)\\n.key(upload.key())\\n.expectedBucketOwner(accountId)\\n.uploadId(upload.uploadId())\\n.build();\\nAbortMultipartUploadResponse abortMultipartUploadResponse =\\ns3Client.abortMultipartUpload(abortMultipartUploadRequest);\\nif\\n(abortMultipartUploadResponse.sdkHttpResponse().isSuccessful()) {\\nlogger.info(\"Upload ID [{}] to bucket [{}] successfully\\naborted.\", upload.uploadId(), bucketName);\\n}\\n}\\n}\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', 'static void abortMultipartUploadUsingUploadId() {\\nString uploadId = startUploadReturningUploadId();\\nAbortMultipartUploadResponse response = s3Client.abortMultipartUpload(b -\\n> b\\n.uploadId(uploadId)\\n.bucket(bucketName)\\n.key(key));\\nif (response.sdkHttpResponse().isSuccessful()) {\\nlogger.info(\"Upload ID [{}] to bucket [{}] successfully aborted.\",\\nuploadId, bucketName);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'static void abortMultipartUploadsUsingLifecycleConfig() {\\nCollection<LifecycleRule> lifeCycleRules =\\nList.of(LifecycleRule.builder()\\n.abortIncompleteMultipartUpload(b -> b.\\ndaysAfterInitiation(7))\\n.status(\"Enabled\")\\n.filter(SdkBuilder::build) // Filter element is required.\\n.build());\\n// If the action is successful, the service sends back an HTTP 200\\nresponse with an empty HTTP body.\\nPutBucketLifecycleConfigurationResponse response =\\ns3Client.putBucketLifecycleConfiguration(b -> b\\n.bucket(bucketName)\\n.lifecycleConfiguration(b1 -> b1.rules(lifeCycleRules)));\\nif (response.sdkHttpResponse().isSuccessful()) {', '']]]\n",
      "[[['', 'logger.info(\"Rule to abort incomplete multipart uploads added to\\nbucket.\");\\n} else {\\nlogger.error(\"Unsuccessfully applied rule. HTTP status code is [{}]\",\\nresponse.sdkHttpResponse().statusCode());\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import org.slf4j.Logger;', '']]]\n",
      "[[['', 'import org.slf4j.LoggerFactory;\\nimport software.amazon.awssdk.core.sync.RequestBody;\\nimport software.amazon.awssdk.services.s3.model.ObjectIdentifier;\\nimport software.amazon.awssdk.transfer.s3.S3TransferManager;\\nimport software.amazon.awssdk.transfer.s3.model.CompletedDirectoryDownload;\\nimport software.amazon.awssdk.transfer.s3.model.DirectoryDownload;\\nimport software.amazon.awssdk.transfer.s3.model.DownloadDirectoryRequest;\\nimport java.io.IOException;\\nimport java.net.URI;\\nimport java.net.URISyntaxException;\\nimport java.nio.file.Files;\\nimport java.nio.file.Path;\\nimport java.nio.file.Paths;\\nimport java.util.HashSet;\\nimport java.util.Set;\\nimport java.util.UUID;\\nimport java.util.stream.Collectors;\\npublic Integer downloadObjectsToDirectory(S3TransferManager transferManager,\\nURI destinationPathURI, String bucketName) {\\nDirectoryDownload directoryDownload =\\ntransferManager.downloadDirectory(DownloadDirectoryRequest.builder()\\n.destination(Paths.get(destinationPathURI))\\n.bucket(bucketName)\\n.build());\\nCompletedDirectoryDownload completedDirectoryDownload =\\ndirectoryDownload.completionFuture().join();\\ncompletedDirectoryDownload.failedTransfers()\\n.forEach(fail -> logger.warn(\"Object [{}] failed to transfer\",\\nfail.toString()));\\nreturn completedDirectoryDownload.failedTransfers().size();\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun createS3Client(): S3Client {\\n// Configure your S3Client to use the Asymmetric Sigv4 (Sigv4a)\\nsigning algorithm.\\nval sigV4AScheme = SigV4AsymmetricAuthScheme(CrtAwsSigner)\\nval s3 = S3Client.fromEnvironment {\\nauthSchemes = listOf(sigV4AScheme)\\n}\\nreturn s3\\n}', ''], ['', '', '']], [['', '', ''], ['', 'suspend fun getObjectFromMrap(\\ns3: S3Client,\\nmrapArn: String,\\nkeyName: String,\\n): String? {\\nval request = GetObjectRequest {\\nbucket = mrapArn // Use the ARN instead of the bucket name for object\\noperations.\\nkey = keyName\\n}', '']]]\n",
      "[[['', 'var stringObj: String? = null\\ns3.getObject(request) { resp ->\\nstringObj = resp.body?.decodeToString()\\nif (stringObj != null) {\\nprintln(\"Successfully read $keyName from $mrapArn\")\\n}\\n}\\nreturn stringObj\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'use aws_sdk_s3::{\\nerror::SdkError,\\noperation::head_object::HeadObjectError,\\nprimitives::{ByteStream, DateTime, DateTimeFormat},\\nClient, Error,\\n};', '']]]\n",
      "[[['', 'use tracing::{error, warn};\\nconst KEY: &str = \"key\";\\nconst BODY: &str = \"Hello, world!\";\\n/// Demonstrate how `if-modified-since` reports that matching objects haven\\'t\\n/// changed.\\n///\\n/// # Steps\\n/// - Create a bucket.\\n/// - Put an object in the bucket.\\n/// - Get the bucket headers.\\n/// - Get the bucket headers again but only if modified.\\n/// - Delete the bucket.\\n#[tokio::main]\\nasync fn main() -> Result<(), Error> {\\ntracing_subscriber::fmt::init();\\n// Get a new UUID to use when creating a unique bucket name.\\nlet uuid = uuid::Uuid::new_v4();\\n// Load the AWS configuration from the environment.\\nlet client = Client::new(&aws_config::load_from_env().await);\\n// Generate a unique bucket name using the previously generated UUID.\\n// Then create a new bucket with that name.\\nlet bucket_name = format!(\"if-modified-since-{uuid}\");\\nclient\\n.create_bucket()\\n.bucket(bucket_name.clone())\\n.send()\\n.await?;\\n// Create a new object in the bucket whose name is `KEY` and whose\\n// contents are `BODY`.\\nlet put_object_output = client\\n.put_object()\\n.bucket(bucket_name.as_str())\\n.key(KEY)\\n.body(ByteStream::from_static(BODY.as_bytes()))\\n.send()\\n.await;\\n// If the `PutObject` succeeded, get the eTag string from it. Otherwise,', '']]]\n",
      "[[['', '// report an error and return an empty string.\\nlet e_tag_1 = match put_object_output {\\nOk(put_object) => put_object.e_tag.unwrap(),\\nErr(err) => {\\nerror!(\"{err:?}\");\\nString::new()\\n}\\n};\\n// Request the object\\'s headers.\\nlet head_object_output = client\\n.head_object()\\n.bucket(bucket_name.as_str())\\n.key(KEY)\\n.send()\\n.await;\\n// If the `HeadObject` request succeeded, create a tuple containing the\\n// values of the headers `last-modified` and `etag`. If the request\\n// failed, return the error in a tuple instead.\\nlet (last_modified, e_tag_2) = match head_object_output {\\nOk(head_object) => (\\nOk(head_object.last_modified().cloned().unwrap()),\\nhead_object.e_tag.unwrap(),\\n),\\nErr(err) => (Err(err), String::new()),\\n};\\nwarn!(\"last modified: {last_modified:?}\");\\nassert_eq!(\\ne_tag_1, e_tag_2,\\n\"PutObject and first GetObject had differing eTags\"\\n);\\nprintln!(\"First value of last_modified: {last_modified:?}\");\\nprintln!(\"First tag: {}\\\\n\", e_tag_1);\\n// Send a second `HeadObject` request. This time, the `if_modified_since`\\n// option is specified, giving the `last_modified` value returned by the\\n// first call to `HeadObject`.\\n//\\n// Since the object hasn\\'t been changed, and there are no other objects in\\n// the bucket, there should be no matching objects.', '']]]\n",
      "[[['', 'let head_object_output = client\\n.head_object()\\n.bucket(bucket_name.as_str())\\n.key(KEY)\\n.if_modified_since(last_modified.unwrap())\\n.send()\\n.await;\\n// If the `HeadObject` request succeeded, the result is a typle containing\\n// the `last_modified` and `e_tag_1` properties. This is _not_ the expected\\n// result.\\n//\\n// The _expected_ result of the second call to `HeadObject` is an\\n// `SdkError::ServiceError` containing the HTTP error response. If that\\'s\\n// the case and the HTTP status is 304 (not modified), the output is a\\n// tuple containing the values of the HTTP `last-modified` and `etag`\\n// headers.\\n//\\n// If any other HTTP error occurred, the error is returned as an\\n// `SdkError::ServiceError`.\\nlet (last_modified, e_tag_2): (Result<DateTime, SdkError<HeadObjectError>>,\\nString) =\\nmatch head_object_output {\\nOk(head_object) => (\\nOk(head_object.last_modified().cloned().unwrap()),\\nhead_object.e_tag.unwrap(),\\n),\\nErr(err) => match err {\\nSdkError::ServiceError(err) => {\\n// Get the raw HTTP response. If its status is 304, the\\n// object has not changed. This is the expected code path.\\nlet http = err.raw();\\nmatch http.status().as_u16() {\\n// If the HTTP status is 304: Not Modified, return a\\n// tuple containing the values of the HTTP\\n// `last-modified` and `etag` headers.\\n304 => (\\nOk(DateTime::from_str(\\nhttp.headers().get(\"last-modified\").unwrap(),\\nDateTimeFormat::HttpDate,\\n)\\n.unwrap()),', '']]]\n",
      "[[['', 'http.headers().get(\"etag\").map(|t|\\nt.into()).unwrap(),\\n),\\n// Any other HTTP status code is returned as an\\n// `SdkError::ServiceError`.\\n_ => (Err(SdkError::ServiceError(err)), String::new()),\\n}\\n}\\n// Any other kind of error is returned in a tuple containing the\\n// error and an empty string.\\n_ => (Err(err), String::new()),\\n},\\n};\\nwarn!(\"last modified: {last_modified:?}\");\\nassert_eq!(\\ne_tag_1, e_tag_2,\\n\"PutObject and second HeadObject had different eTags\"\\n);\\nprintln!(\"Second value of last modified: {last_modified:?}\");\\nprintln!(\"Second tag: {}\", e_tag_2);\\n// Clean up by deleting the object and the bucket.\\nclient\\n.delete_object()\\n.bucket(bucket_name.as_str())\\n.key(KEY)\\n.send()\\n.await?;\\nclient\\n.delete_bucket()\\n.bucket(bucket_name.as_str())\\n.send()\\n.await?;\\nOk(())\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', \"public class S3_Basics\\n{\\npublic static async Task Main()\\n{\\n// Create an Amazon S3 client object. The constructor uses the\\n// default user installed on the system. To work with Amazon S3\\n// features in a different AWS Region, pass the AWS Region as a\\n// parameter to the client constructor.\\nIAmazonS3 client = new AmazonS3Client();\\nstring bucketName = string.Empty;\\nstring filePath = string.Empty;\\nstring keyName = string.Empty;\\nvar sepBar = new string('-', Console.WindowWidth);\", '']]]\n",
      "[[['', 'Console.WriteLine(sepBar);\\nConsole.WriteLine(\"Amazon Simple Storage Service (Amazon S3) basic\");\\nConsole.WriteLine(\"procedures. This application will:\");\\nConsole.WriteLine(\"\\\\n\\\\t1. Create a bucket\");\\nConsole.WriteLine(\"\\\\n\\\\t2. Upload an object to the new bucket\");\\nConsole.WriteLine(\"\\\\n\\\\t3. Copy the uploaded object to a folder in the\\nbucket\");\\nConsole.WriteLine(\"\\\\n\\\\t4. List the items in the new bucket\");\\nConsole.WriteLine(\"\\\\n\\\\t5. Delete all the items in the bucket\");\\nConsole.WriteLine(\"\\\\n\\\\t6. Delete the bucket\");\\nConsole.WriteLine(sepBar);\\n// Create a bucket.\\nConsole.WriteLine($\"\\\\n{sepBar}\");\\nConsole.WriteLine(\"\\\\nCreate a new Amazon S3 bucket.\\\\n\");\\nConsole.WriteLine(sepBar);\\nConsole.Write(\"Please enter a name for the new bucket: \");\\nbucketName = Console.ReadLine();\\nvar success = await S3Bucket.CreateBucketAsync(client, bucketName);\\nif (success)\\n{\\nConsole.WriteLine($\"Successfully created bucket: {bucketName}.\\n\\\\n\");\\n}\\nelse\\n{\\nConsole.WriteLine($\"Could not create bucket: {bucketName}.\\\\n\");\\n}\\nConsole.WriteLine(sepBar);\\nConsole.WriteLine(\"Upload a file to the new bucket.\");\\nConsole.WriteLine(sepBar);\\n// Get the local path and filename for the file to upload.\\nwhile (string.IsNullOrEmpty(filePath))\\n{\\nConsole.Write(\"Please enter the path and filename of the file to\\nupload: \");\\nfilePath = Console.ReadLine();\\n// Confirm that the file exists on the local computer.\\nif (!File.Exists(filePath))', '']]]\n",
      "[[['', '{\\nConsole.WriteLine($\"Couldn\\'t find {filePath}. Try again.\\\\n\");\\nfilePath = string.Empty;\\n}\\n}\\n// Get the file name from the full path.\\nkeyName = Path.GetFileName(filePath);\\nsuccess = await S3Bucket.UploadFileAsync(client, bucketName, keyName,\\nfilePath);\\nif (success)\\n{\\nConsole.WriteLine($\"Successfully uploaded {keyName} from\\n{filePath} to {bucketName}.\\\\n\");\\n}\\nelse\\n{\\nConsole.WriteLine($\"Could not upload {keyName}.\\\\n\");\\n}\\n// Set the file path to an empty string to avoid overwriting the\\n// file we just uploaded to the bucket.\\nfilePath = string.Empty;\\n// Now get a new location where we can save the file.\\nwhile (string.IsNullOrEmpty(filePath))\\n{\\n// First get the path to which the file will be downloaded.\\nConsole.Write(\"Please enter the path where the file will be\\ndownloaded: \");\\nfilePath = Console.ReadLine();\\n// Confirm that the file exists on the local computer.\\nif (File.Exists($\"{filePath}\\\\\\\\{keyName}\"))\\n{\\nConsole.WriteLine($\"Sorry, the file already exists in that\\nlocation.\\\\n\");\\nfilePath = string.Empty;\\n}\\n}\\n// Download an object from a bucket.', '']]]\n",
      "[[['', 'success = await S3Bucket.DownloadObjectFromBucketAsync(client,\\nbucketName, keyName, filePath);\\nif (success)\\n{\\nConsole.WriteLine($\"Successfully downloaded {keyName}.\\\\n\");\\n}\\nelse\\n{\\nConsole.WriteLine($\"Sorry, could not download {keyName}.\\\\n\");\\n}\\n// Copy the object to a different folder in the bucket.\\nstring folderName = string.Empty;\\nwhile (string.IsNullOrEmpty(folderName))\\n{\\nConsole.Write(\"Please enter the name of the folder to copy your\\nobject to: \");\\nfolderName = Console.ReadLine();\\n}\\nwhile (string.IsNullOrEmpty(keyName))\\n{\\n// Get the name to give to the object once uploaded.\\nConsole.Write(\"Enter the name of the object to copy: \");\\nkeyName = Console.ReadLine();\\n}\\nawait S3Bucket.CopyObjectInBucketAsync(client, bucketName, keyName,\\nfolderName);\\n// List the objects in the bucket.\\nawait S3Bucket.ListBucketContentsAsync(client, bucketName);\\n// Delete the contents of the bucket.\\nawait S3Bucket.DeleteBucketContentsAsync(client, bucketName);\\n// Deleting the bucket too quickly after deleting its contents will\\n// cause an error that the bucket isn\\'t empty. So...\\nConsole.WriteLine(\"Press <Enter> when you are ready to delete the\\nbucket.\");\\n_ = Console.ReadLine();', '']]]\n",
      "[[['', '// Delete the bucket.\\nawait S3Bucket.DeleteBucketAsync(client, bucketName);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '###############################################################################\\n# function s3_getting_started\\n#\\n# This function creates, copies, and deletes S3 buckets and objects.\\n#\\n# Returns:\\n# 0 - If successful.\\n# 1 - If an error occurred.\\n###############################################################################\\nfunction s3_getting_started() {\\n{\\nif [ \"$BUCKET_OPERATIONS_SOURCED\" != \"True\" ]; then', '']]]\n",
      "[[['', 'cd bucket-lifecycle-operations || exit\\nsource ./bucket_operations.sh\\ncd ..\\nfi\\n}\\necho_repeat \"*\" 88\\necho \"Welcome to the Amazon S3 getting started demo.\"\\necho_repeat \"*\" 88\\nlocal bucket_name\\nbucket_name=$(generate_random_name \"doc-example-bucket\")\\nlocal region_code\\nregion_code=$(aws configure get region)\\nif create_bucket -b \"$bucket_name\" -r \"$region_code\"; then\\necho \"Created demo bucket named $bucket_name\"\\nelse\\nerrecho \"The bucket failed to create. This demo will exit.\"\\nreturn 1\\nfi\\nlocal file_name\\nwhile [ -z \"$file_name\" ]; do\\necho -n \"Enter a file you want to upload to your bucket: \"\\nget_input\\nfile_name=$get_input_result\\nif [ ! -f \"$file_name\" ]; then\\necho \"Could not find file $file_name. Are you sure it exists?\"\\nfile_name=\"\"\\nfi\\ndone\\nlocal key\\nkey=\"$(basename \"$file_name\")\"\\nlocal result=0\\nif copy_file_to_bucket \"$bucket_name\" \"$file_name\" \"$key\"; then\\necho \"Uploaded file $file_name into bucket $bucket_name with key $key.\"\\nelse\\nresult=1', '']]]\n",
      "[[['', 'fi\\nlocal destination_file\\ndestination_file=\"$file_name.download\"\\nif yes_no_input \"Would you like to download $key to the file $destination_file?\\n(y/n) \"; then\\nif download_object_from_bucket \"$bucket_name\" \"$destination_file\" \"$key\";\\nthen\\necho \"Downloaded $key in the bucket $bucket_name to the file\\n$destination_file.\"\\nelse\\nresult=1\\nfi\\nfi\\nif yes_no_input \"Would you like to copy $key a new object key in your bucket?\\n(y/n) \"; then\\nlocal to_key\\nto_key=\"demo/$key\"\\nif copy_item_in_bucket \"$bucket_name\" \"$key\" \"$to_key\"; then\\necho \"Copied $key in the bucket $bucket_name to the $to_key.\"\\nelse\\nresult=1\\nfi\\nfi\\nlocal bucket_items\\nbucket_items=$(list_items_in_bucket \"$bucket_name\")\\n# shellcheck disable=SC2181\\nif [[ $? -ne 0 ]]; then\\nresult=1\\nfi\\necho \"Your bucket contains the following items.\"\\necho -e \"Name\\\\t\\\\tSize\"\\necho \"$bucket_items\"\\nif yes_no_input \"Delete the bucket, $bucket_name, as well as the objects in it?\\n(y/n) \"; then\\nbucket_items=$(echo \"$bucket_items\" | cut -f 1)\\nif delete_items_in_bucket \"$bucket_name\" \"$bucket_items\"; then\\necho \"The following items were deleted from the bucket $bucket_name\"', '']]]\n",
      "[[['', 'echo \"$bucket_items\"\\nelse\\nresult=1\\nfi\\nif delete_bucket \"$bucket_name\"; then\\necho \"Deleted the bucket $bucket_name\"\\nelse\\nresult=1\\nfi\\nfi\\nreturn $result\\n}', ''], ['', '', '']], [['', '', ''], ['', '###############################################################################\\n# function create-bucket\\n#\\n# This function creates the specified bucket in the specified AWS Region, unless\\n# it already exists.\\n#\\n# Parameters:\\n# -b bucket_name -- The name of the bucket to create.\\n# -r region_code -- The code for an AWS Region in which to\\n# create the bucket.\\n#\\n# Returns:\\n# The URL of the bucket that was created.\\n# And:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction create_bucket() {\\nlocal bucket_name region_code response\\nlocal option OPTARG # Required to use getopts command in a function.\\n# bashsupport disable=BP5008\\nfunction usage() {\\necho \"function create_bucket\"\\necho \"Creates an Amazon S3 bucket. You must supply a bucket name:\"', '']]]\n",
      "[[['', 'echo \" -b bucket_name The name of the bucket. It must be globally\\nunique.\"\\necho \" [-r region_code] The code for an AWS Region in which the bucket is\\ncreated.\"\\necho \"\"\\n}\\n# Retrieve the calling parameters.\\nwhile getopts \"b:r:h\" option; do\\ncase \"${option}\" in\\nb) bucket_name=\"${OPTARG}\" ;;\\nr) region_code=\"${OPTARG}\" ;;\\nh)\\nusage\\nreturn 0\\n;;\\n\\\\?)\\necho \"Invalid parameter\"\\nusage\\nreturn 1\\n;;\\nesac\\ndone\\nif [[ -z \"$bucket_name\" ]]; then\\nerrecho \"ERROR: You must provide a bucket name with the -b parameter.\"\\nusage\\nreturn 1\\nfi\\nlocal bucket_config_arg\\n# A location constraint for \"us-east-1\" returns an error.\\nif [[ -n \"$region_code\" ]] && [[ \"$region_code\" != \"us-east-1\" ]]; then\\nbucket_config_arg=\"--create-bucket-configuration LocationConstraint=\\n$region_code\"\\nfi\\niecho \"Parameters:\\\\n\"\\niecho \" Bucket name: $bucket_name\"\\niecho \" Region code: $region_code\"\\niecho \"\"\\n# If the bucket already exists, we don\\'t want to try to create it.\\nif (bucket_exists \"$bucket_name\"); then', '']]]\n",
      "[[['', 'errecho \"ERROR: A bucket with that name already exists. Try again.\"\\nreturn 1\\nfi\\n# shellcheck disable=SC2086\\nresponse=$(aws s3api create-bucket \\\\\\n--bucket \"$bucket_name\" \\\\\\n$bucket_config_arg)\\n# shellcheck disable=SC2181\\nif [[ ${?} -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports create-bucket operation failed.\\\\n$response\"\\nreturn 1\\nfi\\n}\\n###############################################################################\\n# function copy_file_to_bucket\\n#\\n# This function creates a file in the specified bucket.\\n#\\n# Parameters:\\n# $1 - The name of the bucket to copy the file to.\\n# $2 - The path and file name of the local file to copy to the bucket.\\n# $3 - The key (name) to call the copy of the file in the bucket.\\n#\\n# Returns:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction copy_file_to_bucket() {\\nlocal response bucket_name source_file destination_file_name\\nbucket_name=$1\\nsource_file=$2\\ndestination_file_name=$3\\nresponse=$(aws s3api put-object \\\\\\n--bucket \"$bucket_name\" \\\\\\n--body \"$source_file\" \\\\\\n--key \"$destination_file_name\")\\n# shellcheck disable=SC2181\\nif [[ ${?} -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports put-object operation failed.\\\\n$response\"', '']]]\n",
      "[[['', 'return 1\\nfi\\n}\\n###############################################################################\\n# function download_object_from_bucket\\n#\\n# This function downloads an object in a bucket to a file.\\n#\\n# Parameters:\\n# $1 - The name of the bucket to download the object from.\\n# $2 - The path and file name to store the downloaded bucket.\\n# $3 - The key (name) of the object in the bucket.\\n#\\n# Returns:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction download_object_from_bucket() {\\nlocal bucket_name=$1\\nlocal destination_file_name=$2\\nlocal object_name=$3\\nlocal response\\nresponse=$(aws s3api get-object \\\\\\n--bucket \"$bucket_name\" \\\\\\n--key \"$object_name\" \\\\\\n\"$destination_file_name\")\\n# shellcheck disable=SC2181\\nif [[ ${?} -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports put-object operation failed.\\\\n$response\"\\nreturn 1\\nfi\\n}\\n###############################################################################\\n# function copy_item_in_bucket\\n#\\n# This function creates a copy of the specified file in the same bucket.\\n#\\n# Parameters:\\n# $1 - The name of the bucket to copy the file from and to.\\n# $2 - The key of the source file to copy.', '']]]\n",
      "[[['', '# $3 - The key of the destination file.\\n#\\n# Returns:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction copy_item_in_bucket() {\\nlocal bucket_name=$1\\nlocal source_key=$2\\nlocal destination_key=$3\\nlocal response\\nresponse=$(aws s3api copy-object \\\\\\n--bucket \"$bucket_name\" \\\\\\n--copy-source \"$bucket_name/$source_key\" \\\\\\n--key \"$destination_key\")\\n# shellcheck disable=SC2181\\nif [[ $? -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports s3api copy-object operation failed.\\\\n$response\"\\nreturn 1\\nfi\\n}\\n###############################################################################\\n# function list_items_in_bucket\\n#\\n# This function displays a list of the files in the bucket with each file\\'s\\n# size. The function uses the --query parameter to retrieve only the key and\\n# size fields from the Contents collection.\\n#\\n# Parameters:\\n# $1 - The name of the bucket.\\n#\\n# Returns:\\n# The list of files in text format.\\n# And:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction list_items_in_bucket() {\\nlocal bucket_name=$1\\nlocal response', '']]]\n",
      "[[['', 'response=$(aws s3api list-objects \\\\\\n--bucket \"$bucket_name\" \\\\\\n--output text \\\\\\n--query \\'Contents[].{Key: Key, Size: Size}\\')\\n# shellcheck disable=SC2181\\nif [[ ${?} -eq 0 ]]; then\\necho \"$response\"\\nelse\\nerrecho \"ERROR: AWS reports s3api list-objects operation failed.\\\\n$response\"\\nreturn 1\\nfi\\n}\\n###############################################################################\\n# function delete_items_in_bucket\\n#\\n# This function deletes the specified list of keys from the specified bucket.\\n#\\n# Parameters:\\n# $1 - The name of the bucket.\\n# $2 - A list of keys in the bucket to delete.\\n# Returns:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction delete_items_in_bucket() {\\nlocal bucket_name=$1\\nlocal keys=$2\\nlocal response\\n# Create the JSON for the items to delete.\\nlocal delete_items\\ndelete_items=\"{\\\\\"Objects\\\\\":[\"\\nfor key in $keys; do\\ndelete_items=\"$delete_items{\\\\\"Key\\\\\": \\\\\"$key\\\\\"},\"\\ndone\\ndelete_items=${delete_items%?} # Remove the final comma.\\ndelete_items=\"$delete_items]}\"\\nresponse=$(aws s3api delete-objects \\\\\\n--bucket \"$bucket_name\" \\\\\\n--delete \"$delete_items\")', '']]]\n",
      "[[['', '# shellcheck disable=SC2181\\nif [[ $? -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports s3api delete-object operation failed.\\\\n\\n$response\"\\nreturn 1\\nfi\\n}\\n###############################################################################\\n# function delete_bucket\\n#\\n# This function deletes the specified bucket.\\n#\\n# Parameters:\\n# $1 - The name of the bucket.\\n# Returns:\\n# 0 - If successful.\\n# 1 - If it fails.\\n###############################################################################\\nfunction delete_bucket() {\\nlocal bucket_name=$1\\nlocal response\\nresponse=$(aws s3api delete-bucket \\\\\\n--bucket \"$bucket_name\")\\n# shellcheck disable=SC2181\\nif [[ $? -ne 0 ]]; then\\nerrecho \"ERROR: AWS reports s3api delete-bucket failed.\\\\n$response\"\\nreturn 1\\nfi\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '#include <iostream>\\n#include <aws/core/Aws.h>\\n#include <aws/s3/S3Client.h>\\n#include <aws/s3/model/CopyObjectRequest.h>\\n#include <aws/s3/model/CreateBucketRequest.h>\\n#include <aws/s3/model/DeleteBucketRequest.h>\\n#include <aws/s3/model/DeleteObjectRequest.h>\\n#include <aws/s3/model/GetObjectRequest.h>\\n#include <aws/s3/model/ListObjectsV2Request.h>\\n#include <aws/s3/model/PutObjectRequest.h>\\n#include <aws/s3/model/BucketLocationConstraint.h>\\n#include <aws/s3/model/CreateBucketConfiguration.h>\\n#include <aws/core/utils/UUID.h>\\n#include <aws/core/utils/StringUtils.h>\\n#include <aws/core/utils/memory/stl/AWSAllocator.h>\\n#include <aws/core/utils/memory/stl/AWSStreamFwd.h>\\n#include <fstream>\\n#include \"awsdoc/s3/s3_examples.h\"\\nnamespace AwsDoc {\\nnamespace S3 {\\n//! Delete an S3 bucket.\\n/*!\\n\\\\sa DeleteBucket()\\n\\\\param bucketName The S3 bucket\\'s name.\\n\\\\param client An S3 client.', '']]]\n",
      "[[['', '*/\\nstatic bool\\nDeleteBucket(const Aws::String &bucketName, Aws::S3::S3Client &client);\\n//! Delete an object in an S3 bucket.\\n/*! \\\\sa DeleteObjectFromBucket()\\n\\\\param bucketName The S3 bucket\\'s name.\\n\\\\param key The key for the object in the S3 bucket.\\n\\\\param client An S3 client.\\n*/\\nstatic bool\\nDeleteObjectFromBucket(const Aws::String &bucketName, const Aws::String\\n&key,\\nAws::S3::S3Client &client);\\n}\\n}\\n//! Scenario to create, copy, and delete S3 buckets and objects.\\n/*!\\n\\\\sa S3_GettingStartedScenario()\\n\\\\param uploadFilePath Path to file to upload to an Amazon S3 bucket.\\n\\\\param saveFilePath Path for saving a downloaded S3 object.\\n\\\\param clientConfig Aws client configuration.\\n*/\\nbool AwsDoc::S3::S3_GettingStartedScenario(const Aws::String &uploadFilePath,\\nconst Aws::String &saveFilePath,\\nconst Aws::Client::ClientConfiguration\\n&clientConfig) {\\nAws::S3::S3Client client(clientConfig);\\n// Create a unique bucket name which is only temporary and will be deleted.\\n// Format: \"doc-example-bucket-\" + lowercase UUID.\\nAws::String uuid = Aws::Utils::UUID::RandomUUID();\\nAws::String bucketName = \"doc-example-bucket-\" +\\nAws::Utils::StringUtils::ToLower(uuid.c_str());\\n// 1. Create a bucket.\\n{\\nAws::S3::Model::CreateBucketRequest request;\\nrequest.SetBucket(bucketName);\\nif (clientConfig.region != Aws::Region::US_EAST_1) {', '']]]\n",
      "[[['', 'Aws::S3::Model::CreateBucketConfiguration createBucketConfiguration;\\ncreateBucketConfiguration.WithLocationConstraint(\\nAws::S3::Model::BucketLocationConstraintMapper::GetBucketLocationConstraintForNam\\nclientConfig.region));\\nrequest.WithCreateBucketConfiguration(createBucketConfiguration);\\n}\\nAws::S3::Model::CreateBucketOutcome outcome =\\nclient.CreateBucket(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &err = outcome.GetError();\\nstd::cerr << \"Error: CreateBucket: \" <<\\nerr.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\nreturn false;\\n}\\nelse {\\nstd::cout << \"Created the bucket, \\'\" << bucketName <<\\n\"\\', in the region, \\'\" << clientConfig.region << \"\\'.\" <<\\nstd::endl;\\n}\\n}\\n// 2. Upload a local file to the bucket.\\nAws::String key = \"key-for-test\";\\n{\\nAws::S3::Model::PutObjectRequest request;\\nrequest.SetBucket(bucketName);\\nrequest.SetKey(key);\\nstd::shared_ptr<Aws::FStream> input_data =\\nAws::MakeShared<Aws::FStream>(\"SampleAllocationTag\",\\nuploadFilePath,\\nstd::ios_base::in |\\nstd::ios_base::binary);\\nif (!input_data->is_open()) {\\nstd::cerr << \"Error: unable to open file, \\'\" << uploadFilePath <<\\n\"\\'.\"\\n<< std::endl;\\nAwsDoc::S3::DeleteBucket(bucketName, client);\\nreturn false;', 'e']]]\n",
      "[[['', '}\\nrequest.SetBody(input_data);\\nAws::S3::Model::PutObjectOutcome outcome =\\nclient.PutObject(request);\\nif (!outcome.IsSuccess()) {\\nstd::cerr << \"Error: PutObject: \" <<\\noutcome.GetError().GetMessage() << std::endl;\\nAwsDoc::S3::DeleteObjectFromBucket(bucketName, key, client);\\nAwsDoc::S3::DeleteBucket(bucketName, client);\\nreturn false;\\n}\\nelse {\\nstd::cout << \"Added the object with the key, \\'\" << key\\n<< \"\\', to the bucket, \\'\"\\n<< bucketName << \"\\'.\" << std::endl;\\n}\\n}\\n// 3. Download the object to a local file.\\n{\\nAws::S3::Model::GetObjectRequest request;\\nrequest.SetBucket(bucketName);\\nrequest.SetKey(key);\\nAws::S3::Model::GetObjectOutcome outcome =\\nclient.GetObject(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &err = outcome.GetError();\\nstd::cerr << \"Error: GetObject: \" <<\\nerr.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nstd::cout << \"Downloaded the object with the key, \\'\" << key\\n<< \"\\', in the bucket, \\'\"\\n<< bucketName << \"\\'.\" << std::endl;\\nAws::IOStream &ioStream = outcome.GetResultWithOwnership().\\nGetBody();\\nAws::OFStream outStream(saveFilePath,', '']]]\n",
      "[[['', 'std::ios_base::out | std::ios_base::binary);\\nif (!outStream.is_open()) {\\nstd::cout << \"Error: unable to open file, \\'\" << saveFilePath <<\\n\"\\'.\"\\n<< std::endl;\\n}\\nelse {\\noutStream << ioStream.rdbuf();\\nstd::cout << \"Wrote the downloaded object to the file \\'\"\\n<< saveFilePath << \"\\'.\" << std::endl;\\n}\\n}\\n}\\n// 4. Copy the object to a different \"folder\" in the bucket.\\nAws::String copiedToKey = \"test-folder/\" + key;\\n{\\nAws::S3::Model::CopyObjectRequest request;\\nrequest.WithBucket(bucketName)\\n.WithKey(copiedToKey)\\n.WithCopySource(bucketName + \"/\" + key);\\nAws::S3::Model::CopyObjectOutcome outcome =\\nclient.CopyObject(request);\\nif (!outcome.IsSuccess()) {\\nstd::cerr << \"Error: CopyObject: \" <<\\noutcome.GetError().GetMessage() << std::endl;\\n}\\nelse {\\nstd::cout << \"Copied the object with the key, \\'\" << key\\n<< \"\\', to the key, \\'\" << copiedToKey\\n<< \", in the bucket, \\'\" << bucketName << \"\\'.\" << std::endl;\\n}\\n}\\n// 5. List objects in the bucket.\\n{\\nAws::S3::Model::ListObjectsV2Request request;\\nrequest.WithBucket(bucketName);\\nAws::String continuationToken;\\nAws::Vector<Aws::S3::Model::Object> allObjects;\\ndo {', '']]]\n",
      "[[['', 'if (!continuationToken.empty()) {\\nrequest.SetContinuationToken(continuationToken);\\n}\\nAws::S3::Model::ListObjectsV2Outcome outcome = client.ListObjectsV2(\\nrequest);\\nif (!outcome.IsSuccess()) {\\nstd::cerr << \"Error: ListObjects: \" <<\\noutcome.GetError().GetMessage() << std::endl;\\nbreak;\\n}\\nelse {\\nAws::Vector<Aws::S3::Model::Object> objects =\\noutcome.GetResult().GetContents();\\nallObjects.insert(allObjects.end(), objects.begin(),\\nobjects.end());\\ncontinuationToken = outcome.GetResult().GetContinuationToken();\\n}\\n} while (!continuationToken.empty());\\nstd::cout << allObjects.size() << \" objects in the bucket, \\'\" <<\\nbucketName\\n<< \"\\':\" << std::endl;\\nfor (Aws::S3::Model::Object &object: allObjects) {\\nstd::cout << \" \\'\" << object.GetKey() << \"\\'\" << std::endl;\\n}\\n}\\n// 6. Delete all objects in the bucket.\\n// All objects in the bucket must be deleted before deleting the bucket.\\nAwsDoc::S3::DeleteObjectFromBucket(bucketName, copiedToKey, client);\\nAwsDoc::S3::DeleteObjectFromBucket(bucketName, key, client);\\n// 7. Delete the bucket.\\nreturn AwsDoc::S3::DeleteBucket(bucketName, client);\\n}\\nbool AwsDoc::S3::DeleteObjectFromBucket(const Aws::String &bucketName,\\nconst Aws::String &key,\\nAws::S3::S3Client &client) {\\nAws::S3::Model::DeleteObjectRequest request;\\nrequest.SetBucket(bucketName);\\nrequest.SetKey(key);', '']]]\n",
      "[[['', 'Aws::S3::Model::DeleteObjectOutcome outcome =\\nclient.DeleteObject(request);\\nif (!outcome.IsSuccess()) {\\nstd::cerr << \"Error: DeleteObject: \" <<\\noutcome.GetError().GetMessage() << std::endl;\\n}\\nelse {\\nstd::cout << \"Deleted the object with the key, \\'\" << key\\n<< \"\\', from the bucket, \\'\"\\n<< bucketName << \"\\'.\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}\\nbool\\nAwsDoc::S3::DeleteBucket(const Aws::String &bucketName, Aws::S3::S3Client\\n&client) {\\nAws::S3::Model::DeleteBucketRequest request;\\nrequest.SetBucket(bucketName);\\nAws::S3::Model::DeleteBucketOutcome outcome =\\nclient.DeleteBucket(request);\\nif (!outcome.IsSuccess()) {\\nconst Aws::S3::S3Error &err = outcome.GetError();\\nstd::cerr << \"Error: DeleteBucket: \" <<\\nerr.GetExceptionName() << \": \" << err.GetMessage() <<\\nstd::endl;\\n}\\nelse {\\nstd::cout << \"Deleted the bucket, \\'\" << bucketName << \"\\'.\" << std::endl;\\n}\\nreturn outcome.IsSuccess();\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3)\\nactions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform\\nbucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}\\n// ListBuckets lists the buckets in the current account.\\nfunc (basics BucketBasics) ListBuckets() ([]types.Bucket, error) {\\nresult, err := basics.S3Client.ListBuckets(context.TODO(),\\n&s3.ListBucketsInput{})\\nvar buckets []types.Bucket\\nif err != nil {\\nlog.Printf(\"Couldn\\'t list buckets for your account. Here\\'s why: %v\\\\n\", err)\\n} else {\\nbuckets = result.Buckets', '']]]\n",
      "[[['', '}\\nreturn buckets, err\\n}\\n// BucketExists checks whether a bucket exists in the current account.\\nfunc (basics BucketBasics) BucketExists(bucketName string) (bool, error) {\\n_, err := basics.S3Client.HeadBucket(context.TODO(), &s3.HeadBucketInput{\\nBucket: aws.String(bucketName),\\n})\\nexists := true\\nif err != nil {\\nvar apiError smithy.APIError\\nif errors.As(err, &apiError) {\\nswitch apiError.(type) {\\ncase *types.NotFound:\\nlog.Printf(\"Bucket %v is available.\\\\n\", bucketName)\\nexists = false\\nerr = nil\\ndefault:\\nlog.Printf(\"Either you don\\'t have access to bucket %v or another error\\noccurred. \"+\\n\"Here\\'s what happened: %v\\\\n\", bucketName, err)\\n}\\n}\\n} else {\\nlog.Printf(\"Bucket %v exists and you already own it.\", bucketName)\\n}\\nreturn exists, err\\n}\\n// CreateBucket creates a bucket with the specified name in the specified Region.\\nfunc (basics BucketBasics) CreateBucket(name string, region string) error {\\n_, err := basics.S3Client.CreateBucket(context.TODO(), &s3.CreateBucketInput{\\nBucket: aws.String(name),\\nCreateBucketConfiguration: &types.CreateBucketConfiguration{\\nLocationConstraint: types.BucketLocationConstraint(region),\\n},\\n})\\nif err != nil {', '']]]\n",
      "[[['', 'log.Printf(\"Couldn\\'t create bucket %v in Region %v. Here\\'s why: %v\\\\n\",\\nname, region, err)\\n}\\nreturn err\\n}\\n// UploadFile reads from a file and puts the data into an object in a bucket.\\nfunc (basics BucketBasics) UploadFile(bucketName string, objectKey string,\\nfileName string) error {\\nfile, err := os.Open(fileName)\\nif err != nil {\\nlog.Printf(\"Couldn\\'t open file %v to upload. Here\\'s why: %v\\\\n\", fileName, err)\\n} else {\\ndefer file.Close()\\n_, err = basics.S3Client.PutObject(context.TODO(), &s3.PutObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\nBody: file,\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t upload file %v to %v:%v. Here\\'s why: %v\\\\n\",\\nfileName, bucketName, objectKey, err)\\n}\\n}\\nreturn err\\n}\\n// UploadLargeObject uses an upload manager to upload data to an object in a\\nbucket.\\n// The upload manager breaks large data into parts and uploads the parts\\nconcurrently.\\nfunc (basics BucketBasics) UploadLargeObject(bucketName string, objectKey string,\\nlargeObject []byte) error {\\nlargeBuffer := bytes.NewReader(largeObject)\\nvar partMiBs int64 = 10\\nuploader := manager.NewUploader(basics.S3Client, func(u *manager.Uploader) {\\nu.PartSize = partMiBs * 1024 * 1024\\n})\\n_, err := uploader.Upload(context.TODO(), &s3.PutObjectInput{\\nBucket: aws.String(bucketName),', '']]]\n",
      "[[['', 'Key: aws.String(objectKey),\\nBody: largeBuffer,\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t upload large object to %v:%v. Here\\'s why: %v\\\\n\",\\nbucketName, objectKey, err)\\n}\\nreturn err\\n}\\n// DownloadFile gets an object from a bucket and stores it in a local file.\\nfunc (basics BucketBasics) DownloadFile(bucketName string, objectKey string,\\nfileName string) error {\\nresult, err := basics.S3Client.GetObject(context.TODO(), &s3.GetObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t get object %v:%v. Here\\'s why: %v\\\\n\", bucketName,\\nobjectKey, err)\\nreturn err\\n}\\ndefer result.Body.Close()\\nfile, err := os.Create(fileName)\\nif err != nil {\\nlog.Printf(\"Couldn\\'t create file %v. Here\\'s why: %v\\\\n\", fileName, err)\\nreturn err\\n}\\ndefer file.Close()\\nbody, err := io.ReadAll(result.Body)\\nif err != nil {\\nlog.Printf(\"Couldn\\'t read object body from %v. Here\\'s why: %v\\\\n\", objectKey,\\nerr)\\n}\\n_, err = file.Write(body)\\nreturn err\\n}', '']]]\n",
      "[[['', '// DownloadLargeObject uses a download manager to download an object from a\\nbucket.\\n// The download manager gets the data in parts and writes them to a buffer until\\nall of\\n// the data has been downloaded.\\nfunc (basics BucketBasics) DownloadLargeObject(bucketName string, objectKey\\nstring) ([]byte, error) {\\nvar partMiBs int64 = 10\\ndownloader := manager.NewDownloader(basics.S3Client, func(d *manager.Downloader)\\n{\\nd.PartSize = partMiBs * 1024 * 1024\\n})\\nbuffer := manager.NewWriteAtBuffer([]byte{})\\n_, err := downloader.Download(context.TODO(), buffer, &s3.GetObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t download large object from %v:%v. Here\\'s why: %v\\\\n\",\\nbucketName, objectKey, err)\\n}\\nreturn buffer.Bytes(), err\\n}\\n// CopyToFolder copies an object in a bucket to a subfolder in the same bucket.\\nfunc (basics BucketBasics) CopyToFolder(bucketName string, objectKey string,\\nfolderName string) error {\\n_, err := basics.S3Client.CopyObject(context.TODO(), &s3.CopyObjectInput{\\nBucket: aws.String(bucketName),\\nCopySource: aws.String(fmt.Sprintf(\"%v/%v\", bucketName, objectKey)),\\nKey: aws.String(fmt.Sprintf(\"%v/%v\", folderName, objectKey)),\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t copy object from %v:%v to %v:%v/%v. Here\\'s why: %v\\\\n\",\\nbucketName, objectKey, bucketName, folderName, objectKey, err)\\n}\\nreturn err\\n}\\n// CopyToBucket copies an object in a bucket to another bucket.', '']]]\n",
      "[[['', 'func (basics BucketBasics) CopyToBucket(sourceBucket string, destinationBucket\\nstring, objectKey string) error {\\n_, err := basics.S3Client.CopyObject(context.TODO(), &s3.CopyObjectInput{\\nBucket: aws.String(destinationBucket),\\nCopySource: aws.String(fmt.Sprintf(\"%v/%v\", sourceBucket, objectKey)),\\nKey: aws.String(objectKey),\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t copy object from %v:%v to %v:%v. Here\\'s why: %v\\\\n\",\\nsourceBucket, objectKey, destinationBucket, objectKey, err)\\n}\\nreturn err\\n}\\n// ListObjects lists the objects in a bucket.\\nfunc (basics BucketBasics) ListObjects(bucketName string) ([]types.Object, error)\\n{\\nresult, err := basics.S3Client.ListObjectsV2(context.TODO(),\\n&s3.ListObjectsV2Input{\\nBucket: aws.String(bucketName),\\n})\\nvar contents []types.Object\\nif err != nil {\\nlog.Printf(\"Couldn\\'t list objects in bucket %v. Here\\'s why: %v\\\\n\", bucketName,\\nerr)\\n} else {\\ncontents = result.Contents\\n}\\nreturn contents, err\\n}\\n// DeleteObjects deletes a list of objects from a bucket.\\nfunc (basics BucketBasics) DeleteObjects(bucketName string, objectKeys []string)\\nerror {\\nvar objectIds []types.ObjectIdentifier\\nfor _, key := range objectKeys {\\nobjectIds = append(objectIds, types.ObjectIdentifier{Key: aws.String(key)})\\n}\\noutput, err := basics.S3Client.DeleteObjects(context.TODO(),\\n&s3.DeleteObjectsInput{', '']]]\n",
      "[[['', 'Bucket: aws.String(bucketName),\\nDelete: &types.Delete{Objects: objectIds},\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t delete objects from bucket %v. Here\\'s why: %v\\\\n\",\\nbucketName, err)\\n} else {\\nlog.Printf(\"Deleted %v objects.\\\\n\", len(output.Deleted))\\n}\\nreturn err\\n}\\n// DeleteBucket deletes a bucket. The bucket must be empty or an error is\\nreturned.\\nfunc (basics BucketBasics) DeleteBucket(bucketName string) error {\\n_, err := basics.S3Client.DeleteBucket(context.TODO(), &s3.DeleteBucketInput{\\nBucket: aws.String(bucketName)})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t delete bucket %v. Here\\'s why: %v\\\\n\", bucketName, err)\\n}\\nreturn err\\n}', ''], ['', '', '']], [['', '', ''], ['', '// RunGetStartedScenario is an interactive example that shows you how to use\\nAmazon\\n// Simple Storage Service (Amazon S3) to create an S3 bucket and use it to store\\nobjects.\\n//\\n// 1. Create a bucket.\\n// 2. Upload a local file to the bucket.\\n// 3. Upload a large object to the bucket by using an upload manager.\\n// 4. Download an object to a local file.\\n// 5. Download a large object by using a download manager.\\n// 6. Copy an object to a different folder in the bucket.\\n// 7. List objects in the bucket.\\n// 8. Delete all objects in the bucket.', '']]]\n",
      "[[['', '// 9. Delete the bucket.\\n//\\n// This example creates an Amazon S3 service client from the specified sdkConfig\\nso that\\n// you can replace it with a mocked or stubbed config for unit testing.\\n//\\n// It uses a questioner from the `demotools` package to get input during the\\nexample.\\n// This package can be found in the ..\\\\..\\\\demotools folder of this repo.\\nfunc RunGetStartedScenario(sdkConfig aws.Config, questioner\\ndemotools.IQuestioner) {\\ndefer func() {\\nif r := recover(); r != nil {\\nfmt.Println(\"Something went wrong with the demo.\\\\n\", r)\\n}\\n}()\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Println(\"Welcome to the Amazon S3 getting started demo.\")\\nlog.Println(strings.Repeat(\"-\", 88))\\ns3Client := s3.NewFromConfig(sdkConfig)\\nbucketBasics := actions.BucketBasics{S3Client: s3Client}\\ncount := 10\\nlog.Printf(\"Let\\'s list up to %v buckets for your account:\", count)\\nbuckets, err := bucketBasics.ListBuckets()\\nif err != nil {\\npanic(err)\\n}\\nif len(buckets) == 0 {\\nlog.Println(\"You don\\'t have any buckets!\")\\n} else {\\nif count > len(buckets) {\\ncount = len(buckets)\\n}\\nfor _, bucket := range buckets[:count] {\\nlog.Printf(\"\\\\t%v\\\\n\", *bucket.Name)\\n}\\n}\\nbucketName := questioner.Ask(\"Let\\'s create a bucket. Enter a name for your\\nbucket:\",\\ndemotools.NotEmpty{})', '']]]\n",
      "[[['', 'bucketExists, err := bucketBasics.BucketExists(bucketName)\\nif err != nil {\\npanic(err)\\n}\\nif !bucketExists {\\nerr = bucketBasics.CreateBucket(bucketName, sdkConfig.Region)\\nif err != nil {\\npanic(err)\\n} else {\\nlog.Println(\"Bucket created.\")\\n}\\n}\\nlog.Println(strings.Repeat(\"-\", 88))\\nfmt.Println(\"Let\\'s upload a file to your bucket.\")\\nsmallFile := questioner.Ask(\"Enter the path to a file you want to upload:\",\\ndemotools.NotEmpty{})\\nconst smallKey = \"doc-example-key\"\\nerr = bucketBasics.UploadFile(bucketName, smallKey, smallFile)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"Uploaded %v as %v.\\\\n\", smallFile, smallKey)\\nlog.Println(strings.Repeat(\"-\", 88))\\nmibs := 30\\nlog.Printf(\"Let\\'s create a slice of %v MiB of random bytes and upload it to your\\nbucket. \", mibs)\\nquestioner.Ask(\"Press Enter when you\\'re ready.\")\\nlargeBytes := make([]byte, 1024*1024*mibs)\\nrand.Seed(time.Now().Unix())\\nrand.Read(largeBytes)\\nlargeKey := \"doc-example-large\"\\nlog.Println(\"Uploading...\")\\nerr = bucketBasics.UploadLargeObject(bucketName, largeKey, largeBytes)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"Uploaded %v MiB object as %v\", mibs, largeKey)\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Printf(\"Let\\'s download %v to a file.\", smallKey)\\ndownloadFileName := questioner.Ask(\"Enter a name for the downloaded file:\",\\ndemotools.NotEmpty{})', '']]]\n",
      "[[['', 'err = bucketBasics.DownloadFile(bucketName, smallKey, downloadFileName)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"File %v downloaded.\", downloadFileName)\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Printf(\"Let\\'s download the %v MiB object.\", mibs)\\nquestioner.Ask(\"Press Enter when you\\'re ready.\")\\nlog.Println(\"Downloading...\")\\nlargeDownload, err := bucketBasics.DownloadLargeObject(bucketName, largeKey)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"Downloaded %v bytes.\", len(largeDownload))\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Printf(\"Let\\'s copy %v to a folder in the same bucket.\", smallKey)\\nfolderName := questioner.Ask(\"Enter a folder name: \", demotools.NotEmpty{})\\nerr = bucketBasics.CopyToFolder(bucketName, smallKey, folderName)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"Copied %v to %v/%v.\\\\n\", smallKey, folderName, smallKey)\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Println(\"Let\\'s list the objects in your bucket.\")\\nquestioner.Ask(\"Press Enter when you\\'re ready.\")\\nobjects, err := bucketBasics.ListObjects(bucketName)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"Found %v objects.\\\\n\", len(objects))\\nvar objKeys []string\\nfor _, object := range objects {\\nobjKeys = append(objKeys, *object.Key)\\nlog.Printf(\"\\\\t%v\\\\n\", *object.Key)\\n}\\nlog.Println(strings.Repeat(\"-\", 88))\\nif questioner.AskBool(\"Do you want to delete your bucket and all of its \"+\\n\"contents? (y/n)\", \"y\") {\\nlog.Println(\"Deleting objects.\")\\nerr = bucketBasics.DeleteObjects(bucketName, objKeys)', '']]]\n",
      "[[['', 'if err != nil {\\npanic(err)\\n}\\nlog.Println(\"Deleting bucket.\")\\nerr = bucketBasics.DeleteBucket(bucketName)\\nif err != nil {\\npanic(err)\\n}\\nlog.Printf(\"Deleting downloaded file %v.\\\\n\", downloadFileName)\\nerr = os.Remove(downloadFileName)\\nif err != nil {\\npanic(err)\\n}\\n} else {\\nlog.Println(\"Okay. Don\\'t forget to delete objects from your bucket to avoid\\ncharges.\")\\n}\\nlog.Println(strings.Repeat(\"-\", 88))\\nlog.Println(\"Thanks for watching!\")\\nlog.Println(strings.Repeat(\"-\", 88))\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/**\\n* Before running this Java V2 code example, set up your development\\n* environment, including your credentials.\\n*\\n* For more information, see the following documentation topic:\\n*\\n* https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/get-\\nstarted.html\\n*\\n* This Java code example performs the following tasks:\\n*\\n* 1. Creates an Amazon S3 bucket.\\n* 2. Uploads an object to the bucket.\\n* 3. Downloads the object to another local file.\\n* 4. Uploads an object using multipart upload.\\n* 5. List all objects located in the Amazon S3 bucket.\\n* 6. Copies the object to another Amazon S3 bucket.\\n* 7. Deletes the object from the Amazon S3 bucket.\\n* 8. Deletes the Amazon S3 bucket.\\n*/\\npublic class S3Scenario {\\npublic static final String DASHES = new String(new char[80]).replace(\"\\\\0\",\\n\"-\");\\npublic static void main(String[] args) throws IOException {\\nfinal String usage = \"\"\"\\nUsage:\\n<bucketName> <key> <objectPath> <savePath> <toBucket>\\nWhere:', '']]]\n",
      "[[['', 'bucketName - The Amazon S3 bucket to create.\\nkey - The key to use.\\nobjectPath - The path where the file is located (for example,\\nC:/AWS/book2.pdf).\\nsavePath - The path where the file is saved after it\\'s\\ndownloaded (for example, C:/AWS/book2.pdf).\\ntoBucket - An Amazon S3 bucket to where an object is copied\\nto (for example, C:/AWS/book2.pdf).\\\\s\\n\"\"\";\\nif (args.length != 5) {\\nSystem.out.println(usage);\\nSystem.exit(1);\\n}\\nString bucketName = args[0];\\nString key = args[1];\\nString objectPath = args[2];\\nString savePath = args[3];\\nString toBucket = args[4];\\nRegion region = Region.US_EAST_1;\\nS3Client s3 = S3Client.builder()\\n.region(region)\\n.build();\\nSystem.out.println(DASHES);\\nSystem.out.println(\"Welcome to the Amazon S3 example scenario.\");\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"1. Create an Amazon S3 bucket.\");\\ncreateBucket(s3, bucketName);\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"2. Update a local file to the Amazon S3 bucket.\");\\nuploadLocalFile(s3, bucketName, key, objectPath);\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"3. Download the object to another local file.\");\\ngetObjectBytes(s3, bucketName, key, savePath);\\nSystem.out.println(DASHES);', '']]]\n",
      "[[['', 'System.out.println(DASHES);\\nSystem.out.println(\"4. Perform a multipart upload.\");\\nString multipartKey = \"multiPartKey\";\\nmultipartUpload(s3, toBucket, multipartKey);\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"5. List all objects located in the Amazon S3\\nbucket.\");\\nlistAllObjects(s3, bucketName);\\nanotherListExample(s3, bucketName);\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"6. Copy the object to another Amazon S3 bucket.\");\\ncopyBucketObject(s3, bucketName, key, toBucket);\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"7. Delete the object from the Amazon S3 bucket.\");\\ndeleteObjectFromBucket(s3, bucketName, key);\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"8. Delete the Amazon S3 bucket.\");\\ndeleteBucket(s3, bucketName);\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"All Amazon S3 operations were successfully\\nperformed\");\\nSystem.out.println(DASHES);\\ns3.close();\\n}\\n// Create a bucket by using a S3Waiter object.\\npublic static void createBucket(S3Client s3Client, String bucketName) {\\ntry {\\nS3Waiter s3Waiter = s3Client.waiter();\\nCreateBucketRequest bucketRequest = CreateBucketRequest.builder()\\n.bucket(bucketName)\\n.build();\\ns3Client.createBucket(bucketRequest);', '']]]\n",
      "[[['', 'HeadBucketRequest bucketRequestWait = HeadBucketRequest.builder()\\n.bucket(bucketName)\\n.build();\\n// Wait until the bucket is created and print out the response.\\nWaiterResponse<HeadBucketResponse> waiterResponse =\\ns3Waiter.waitUntilBucketExists(bucketRequestWait);\\nwaiterResponse.matched().response().ifPresent(System.out::println);\\nSystem.out.println(bucketName + \" is ready\");\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\npublic static void deleteBucket(S3Client client, String bucket) {\\nDeleteBucketRequest deleteBucketRequest = DeleteBucketRequest.builder()\\n.bucket(bucket)\\n.build();\\nclient.deleteBucket(deleteBucketRequest);\\nSystem.out.println(bucket + \" was deleted.\");\\n}\\n/**\\n* Upload an object in parts.\\n*/\\npublic static void multipartUpload(S3Client s3, String bucketName, String\\nkey) {\\nint mB = 1024 * 1024;\\n// First create a multipart upload and get the upload id.\\nCreateMultipartUploadRequest createMultipartUploadRequest =\\nCreateMultipartUploadRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.build();\\nCreateMultipartUploadResponse response =\\ns3.createMultipartUpload(createMultipartUploadRequest);\\nString uploadId = response.uploadId();\\nSystem.out.println(uploadId);\\n// Upload all the different parts of the object.', '']]]\n",
      "[[['', 'UploadPartRequest uploadPartRequest1 = UploadPartRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.uploadId(uploadId)\\n.partNumber(1).build();\\nString etag1 = s3.uploadPart(uploadPartRequest1,\\nRequestBody.fromByteBuffer(getRandomByteBuffer(5 * mB)))\\n.eTag();\\nCompletedPart part1 =\\nCompletedPart.builder().partNumber(1).eTag(etag1).build();\\nUploadPartRequest uploadPartRequest2 =\\nUploadPartRequest.builder().bucket(bucketName).key(key)\\n.uploadId(uploadId)\\n.partNumber(2).build();\\nString etag2 = s3.uploadPart(uploadPartRequest2,\\nRequestBody.fromByteBuffer(getRandomByteBuffer(3 * mB)))\\n.eTag();\\nCompletedPart part2 =\\nCompletedPart.builder().partNumber(2).eTag(etag2).build();\\n// Call completeMultipartUpload operation to tell S3 to merge all\\nuploaded\\n// parts and finish the multipart operation.\\nCompletedMultipartUpload completedMultipartUpload =\\nCompletedMultipartUpload.builder()\\n.parts(part1, part2)\\n.build();\\nCompleteMultipartUploadRequest completeMultipartUploadRequest =\\nCompleteMultipartUploadRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.uploadId(uploadId)\\n.multipartUpload(completedMultipartUpload)\\n.build();\\ns3.completeMultipartUpload(completeMultipartUploadRequest);\\n}\\nprivate static ByteBuffer getRandomByteBuffer(int size) {\\nbyte[] b = new byte[size];\\nnew Random().nextBytes(b);', '']]]\n",
      "[[['', 'return ByteBuffer.wrap(b);\\n}\\npublic static void getObjectBytes(S3Client s3, String bucketName, String\\nkeyName, String path) {\\ntry {\\nGetObjectRequest objectRequest = GetObjectRequest\\n.builder()\\n.key(keyName)\\n.bucket(bucketName)\\n.build();\\nResponseBytes<GetObjectResponse> objectBytes =\\ns3.getObjectAsBytes(objectRequest);\\nbyte[] data = objectBytes.asByteArray();\\n// Write the data to a local file.\\nFile myFile = new File(path);\\nOutputStream os = new FileOutputStream(myFile);\\nos.write(data);\\nSystem.out.println(\"Successfully obtained bytes from an S3 object\");\\nos.close();\\n} catch (IOException ex) {\\nex.printStackTrace();\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\n}\\npublic static void uploadLocalFile(S3Client s3, String bucketName, String\\nkey, String objectPath) {\\nPutObjectRequest objectRequest = PutObjectRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.build();\\ns3.putObject(objectRequest, RequestBody.fromFile(new File(objectPath)));\\n}\\npublic static void listAllObjects(S3Client s3, String bucketName) {\\nListObjectsV2Request listObjectsReqManual =\\nListObjectsV2Request.builder()', '']]]\n",
      "[[['', '.bucket(bucketName)\\n.maxKeys(1)\\n.build();\\nboolean done = false;\\nwhile (!done) {\\nListObjectsV2Response listObjResponse =\\ns3.listObjectsV2(listObjectsReqManual);\\nfor (S3Object content : listObjResponse.contents()) {\\nSystem.out.println(content.key());\\n}\\nif (listObjResponse.nextContinuationToken() == null) {\\ndone = true;\\n}\\nlistObjectsReqManual = listObjectsReqManual.toBuilder()\\n.continuationToken(listObjResponse.nextContinuationToken())\\n.build();\\n}\\n}\\npublic static void anotherListExample(S3Client s3, String bucketName) {\\nListObjectsV2Request listReq = ListObjectsV2Request.builder()\\n.bucket(bucketName)\\n.maxKeys(1)\\n.build();\\nListObjectsV2Iterable listRes = s3.listObjectsV2Paginator(listReq);\\n// Process response pages.\\nlistRes.stream()\\n.flatMap(r -> r.contents().stream())\\n.forEach(content -> System.out.println(\" Key: \" + content.key() +\\n\" size = \" + content.size()));\\n// Helper method to work with paginated collection of items directly.\\nlistRes.contents().stream()\\n.forEach(content -> System.out.println(\" Key: \" + content.key() +\\n\" size = \" + content.size()));\\nfor (S3Object content : listRes.contents()) {\\nSystem.out.println(\" Key: \" + content.key() + \" size = \" +\\ncontent.size());', '']]]\n",
      "[[['', '}\\n}\\npublic static void deleteObjectFromBucket(S3Client s3, String bucketName,\\nString key) {\\nDeleteObjectRequest deleteObjectRequest = DeleteObjectRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.build();\\ns3.deleteObject(deleteObjectRequest);\\nSystem.out.println(key + \" was deleted\");\\n}\\npublic static String copyBucketObject(S3Client s3, String fromBucket, String\\nobjectKey, String toBucket) {\\nString encodedUrl = null;\\ntry {\\nencodedUrl = URLEncoder.encode(fromBucket + \"/\" + objectKey,\\nStandardCharsets.UTF_8.toString());\\n} catch (UnsupportedEncodingException e) {\\nSystem.out.println(\"URL could not be encoded: \" + e.getMessage());\\n}\\nCopyObjectRequest copyReq = CopyObjectRequest.builder()\\n.copySource(encodedUrl)\\n.destinationBucket(toBucket)\\n.destinationKey(objectKey)\\n.build();\\ntry {\\nCopyObjectResponse copyRes = s3.copyObject(copyReq);\\nSystem.out.println(\"The \" + objectKey + \" was copied to \" +\\ntoBucket);\\nreturn copyRes.copyObjectResult().toString();\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nSystem.exit(1);\\n}\\nreturn \"\";\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Used to check if currently running file is this file.\\nimport { fileURLToPath } from \"url\";\\nimport { readdirSync, readFileSync, writeFileSync } from \"fs\";\\n// Local helper utils.\\nimport { dirnameFromMetaUrl } from \"@aws-doc-sdk-examples/lib/utils/util-fs.js\";\\nimport { Prompter } from \"@aws-doc-sdk-examples/lib/prompter.js\";\\nimport { wrapText } from \"@aws-doc-sdk-examples/lib/utils/util-string.js\";\\nimport {\\nS3Client,\\nCreateBucketCommand,\\nPutObjectCommand,\\nListObjectsCommand,\\nCopyObjectCommand,\\nGetObjectCommand,\\nDeleteObjectsCommand,\\nDeleteBucketCommand,', '']]]\n",
      "[[['', '} from \"@aws-sdk/client-s3\";', ''], ['', '', '']], [['', '', ''], ['', 'export const dirnameFromMetaUrl = (metaUrl) =>\\nfileURLToPath(new URL(\".\", metaUrl));\\nimport { select, input, confirm, checkbox } from \"@inquirer/prompts\";\\nexport class Prompter {\\n/**\\n* @param {{ message: string, choices: { name: string, value: string }[]}}\\noptions\\n*/\\nselect(options) {\\nreturn select(options);\\n}\\n/**\\n* @param {{ message: string }} options\\n*/\\ninput(options) {\\nreturn input(options);\\n}\\n/**\\n* @param {string} prompt\\n*/\\ncheckContinue = async (prompt = \"\") => {\\nconst prefix = prompt && prompt + \" \";\\nlet ok = await this.confirm({\\nmessage: `${prefix}Continue?`,\\n});\\nif (!ok) throw new Error(\"Exiting...\");\\n};\\n/**\\n* @param {{ message: string }} options\\n*/\\nconfirm(options) {', '']]]\n",
      "[[['', 'return confirm(options);\\n}\\n/**\\n* @param {{ message: string, choices: { name: string, value: string }[]}}\\noptions\\n*/\\ncheckbox(options) {\\nreturn checkbox(options);\\n}\\n}\\nexport const wrapText = (text, char = \"=\") => {\\nconst rule = char.repeat(80);\\nreturn `${rule}\\\\n ${text}\\\\n${rule}\\\\n`;\\n};', ''], ['', '', '']], [['', '', ''], ['', 'export const createBucket = async () => {\\nconst bucketName = await prompter.input({\\nmessage: \"Enter a bucket name. Bucket names must be globally unique:\",\\n});\\nconst command = new CreateBucketCommand({ Bucket: bucketName });\\nawait s3Client.send(command);\\nconsole.log(\"Bucket created successfully.\\\\n\");\\nreturn bucketName;\\n};', ''], ['', '', '']], [['', '', ''], ['', 'export const uploadFilesToBucket = async ({ bucketName, folderPath }) => {\\nconsole.log(`Uploading files from ${folderPath}\\\\n`);\\nconst keys = readdirSync(folderPath);\\nconst files = keys.map((key) => {\\nconst filePath = `${folderPath}/${key}`;\\nconst fileContent = readFileSync(filePath);\\nreturn {\\nKey: key,\\nBody: fileContent,', '']]]\n",
      "[[['', '};\\n});\\nfor (let file of files) {\\nawait s3Client.send(\\nnew PutObjectCommand({\\nBucket: bucketName,\\nBody: file.Body,\\nKey: file.Key,\\n}),\\n);\\nconsole.log(`${file.Key} uploaded successfully.`);\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', 'export const listFilesInBucket = async ({ bucketName }) => {\\nconst command = new ListObjectsCommand({ Bucket: bucketName });\\nconst { Contents } = await s3Client.send(command);\\nconst contentsList = Contents.map((c) => ` • ${c.Key}`).join(\"\\\\n\");\\nconsole.log(\"\\\\nHere\\'s a list of files in the bucket:\");\\nconsole.log(contentsList + \"\\\\n\");\\n};', ''], ['', '', '']], [['', '', ''], ['', 'export const copyFileFromBucket = async ({ destinationBucket }) => {\\nconst proceed = await prompter.confirm({\\nmessage: \"Would you like to copy an object from another bucket?\",\\n});\\nif (!proceed) {\\nreturn;\\n} else {\\nconst copy = async () => {\\ntry {', '']]]\n",
      "[[['', 'const sourceBucket = await prompter.input({\\nmessage: \"Enter source bucket name:\",\\n});\\nconst sourceKey = await prompter.input({\\nmessage: \"Enter source key:\",\\n});\\nconst destinationKey = await prompter.input({\\nmessage: \"Enter destination key:\",\\n});\\nconst command = new CopyObjectCommand({\\nBucket: destinationBucket,\\nCopySource: `${sourceBucket}/${sourceKey}`,\\nKey: destinationKey,\\n});\\nawait s3Client.send(command);\\nawait copyFileFromBucket({ destinationBucket });\\n} catch (err) {\\nconsole.error(`Copy error.`);\\nconsole.error(err);\\nconst retryAnswer = await prompter.confirm({ message: \"Try again?\" });\\nif (retryAnswer) {\\nawait copy();\\n}\\n}\\n};\\nawait copy();\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', 'export const downloadFilesFromBucket = async ({ bucketName }) => {\\nconst { Contents } = await s3Client.send(\\nnew ListObjectsCommand({ Bucket: bucketName }),\\n);\\nconst path = await prompter.input({\\nmessage: \"Enter destination path for files:\",\\n});\\nfor (let content of Contents) {', '']]]\n",
      "[[['', 'const obj = await s3Client.send(\\nnew GetObjectCommand({ Bucket: bucketName, Key: content.Key }),\\n);\\nwriteFileSync(\\n`${path}/${content.Key}`,\\nawait obj.Body.transformToByteArray(),\\n);\\n}\\nconsole.log(\"Files downloaded successfully.\\\\n\");\\n};', ''], ['', '', '']], [['', '', ''], ['', 'export const emptyBucket = async ({ bucketName }) => {\\nconst listObjectsCommand = new ListObjectsCommand({ Bucket: bucketName });\\nconst { Contents } = await s3Client.send(listObjectsCommand);\\nconst keys = Contents.map((c) => c.Key);\\nconst deleteObjectsCommand = new DeleteObjectsCommand({\\nBucket: bucketName,\\nDelete: { Objects: keys.map((key) => ({ Key: key })) },\\n});\\nawait s3Client.send(deleteObjectsCommand);\\nconsole.log(`${bucketName} emptied successfully.\\\\n`);\\n};\\nexport const deleteBucket = async ({ bucketName }) => {\\nconst command = new DeleteBucketCommand({ Bucket: bucketName });\\nawait s3Client.send(command);\\nconsole.log(`${bucketName} deleted successfully.\\\\n`);\\n};', ''], ['', '', '']], [['', '', ''], ['', 'const main = async () => {\\nconst OBJECT_DIRECTORY = `${dirnameFromMetaUrl(\\nimport.meta.url,\\n)}../../../../resources/sample_files/.sample_media`;', '']]]\n",
      "[[['', 'try {\\nconsole.log(wrapText(\"Welcome to the Amazon S3 getting started example.\"));\\nconsole.log(\"Let\\'s create a bucket.\");\\nconst bucketName = await createBucket();\\nawait prompter.confirm({ message: continueMessage });\\nconsole.log(wrapText(\"File upload.\"));\\nconsole.log(\\n\"I have some default files ready to go. You can edit the source code to\\nprovide your own.\",\\n);\\nawait uploadFilesToBucket({\\nbucketName,\\nfolderPath: OBJECT_DIRECTORY,\\n});\\nawait listFilesInBucket({ bucketName });\\nawait prompter.confirm({ message: continueMessage });\\nconsole.log(wrapText(\"Copy files.\"));\\nawait copyFileFromBucket({ destinationBucket: bucketName });\\nawait listFilesInBucket({ bucketName });\\nawait prompter.confirm({ message: continueMessage });\\nconsole.log(wrapText(\"Download files.\"));\\nawait downloadFilesFromBucket({ bucketName });\\nconsole.log(wrapText(\"Clean up.\"));\\nawait emptyBucket({ bucketName });\\nawait deleteBucket({ bucketName });\\n} catch (err) {\\nconsole.error(err);\\n}\\n};', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'suspend fun main(args: Array<String>) {\\nval usage = \"\"\"\\nUsage:\\n<bucketName> <key> <objectPath> <savePath> <toBucket>\\nWhere:\\nbucketName - The Amazon S3 bucket to create.\\nkey - The key to use.\\nobjectPath - The path where the file is located (for example, C:/AWS/\\nbook2.pdf).\\nsavePath - The path where the file is saved after it\\'s downloaded (for\\nexample, C:/AWS/book2.pdf).\\ntoBucket - An Amazon S3 bucket to where an object is copied to (for\\nexample, C:/AWS/book2.pdf).\\n\"\"\"\\nif (args.size != 4) {\\nprintln(usage)\\nexitProcess(1)\\n}\\nval bucketName = args[0]\\nval key = args[1]\\nval objectPath = args[2]\\nval savePath = args[3]\\nval toBucket = args[4]', '']]]\n",
      "[[['', '// Create an Amazon S3 bucket.\\ncreateBucket(bucketName)\\n// Update a local file to the Amazon S3 bucket.\\nputObject(bucketName, key, objectPath)\\n// Download the object to another local file.\\ngetObjectFromMrap(bucketName, key, savePath)\\n// List all objects located in the Amazon S3 bucket.\\nlistBucketObs(bucketName)\\n// Copy the object to another Amazon S3 bucket\\ncopyBucketOb(bucketName, key, toBucket)\\n// Delete the object from the Amazon S3 bucket.\\ndeleteBucketObs(bucketName, key)\\n// Delete the Amazon S3 bucket.\\ndeleteBucket(bucketName)\\nprintln(\"All Amazon S3 operations were successfully performed\")\\n}\\nsuspend fun createBucket(bucketName: String) {\\nval request =\\nCreateBucketRequest {\\nbucket = bucketName\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\ns3.createBucket(request)\\nprintln(\"$bucketName is ready\")\\n}\\n}\\nsuspend fun putObject(\\nbucketName: String,\\nobjectKey: String,\\nobjectPath: String,\\n) {\\nval metadataVal = mutableMapOf<String, String>()\\nmetadataVal[\"myVal\"] = \"test\"\\nval request =', '']]]\n",
      "[[['', 'PutObjectRequest {\\nbucket = bucketName\\nkey = objectKey\\nmetadata = metadataVal\\nthis.body = Paths.get(objectPath).asByteStream()\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\nval response = s3.putObject(request)\\nprintln(\"Tag information is ${response.eTag}\")\\n}\\n}\\nsuspend fun getObjectFromMrap(\\nbucketName: String,\\nkeyName: String,\\npath: String,\\n) {\\nval request =\\nGetObjectRequest {\\nkey = keyName\\nbucket = bucketName\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\ns3.getObject(request) { resp ->\\nval myFile = File(path)\\nresp.body?.writeToFile(myFile)\\nprintln(\"Successfully read $keyName from $bucketName\")\\n}\\n}\\n}\\nsuspend fun listBucketObs(bucketName: String) {\\nval request =\\nListObjectsRequest {\\nbucket = bucketName\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\nval response = s3.listObjects(request)\\nresponse.contents?.forEach { myObject ->\\nprintln(\"The name of the key is ${myObject.key}\")', '']]]\n",
      "[[['', 'println(\"The owner is ${myObject.owner}\")\\n}\\n}\\n}\\nsuspend fun copyBucketOb(\\nfromBucket: String,\\nobjectKey: String,\\ntoBucket: String,\\n) {\\nvar encodedUrl = \"\"\\ntry {\\nencodedUrl = URLEncoder.encode(\"$fromBucket/$objectKey\",\\nStandardCharsets.UTF_8.toString())\\n} catch (e: UnsupportedEncodingException) {\\nprintln(\"URL could not be encoded: \" + e.message)\\n}\\nval request =\\nCopyObjectRequest {\\ncopySource = encodedUrl\\nbucket = toBucket\\nkey = objectKey\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\ns3.copyObject(request)\\n}\\n}\\nsuspend fun deleteBucketObs(\\nbucketName: String,\\nobjectName: String,\\n) {\\nval objectId =\\nObjectIdentifier {\\nkey = objectName\\n}\\nval delOb =\\nDelete {\\nobjects = listOf(objectId)\\n}\\nval request =', '']]]\n",
      "[[['', 'DeleteObjectsRequest {\\nbucket = bucketName\\ndelete = delOb\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\ns3.deleteObjects(request)\\nprintln(\"$objectName was deleted from $bucketName\")\\n}\\n}\\nsuspend fun deleteBucket(bucketName: String?) {\\nval request =\\nDeleteBucketRequest {\\nbucket = bucketName\\n}\\nS3Client { region = \"us-east-1\" }.use { s3 ->\\ns3.deleteBucket(request)\\nprintln(\"The $bucketName was successfully deleted!\")\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'echo(\"\\\\n\");\\necho(\"--------------------------------------\\\\n\");\\nprint(\"Welcome to the Amazon S3 getting started demo using PHP!\\\\n\");\\necho(\"--------------------------------------\\\\n\");\\n$region = \\'us-west-2\\';\\n$this->s3client = new S3Client([\\n\\'region\\' => $region,\\n]);\\n/* Inline declaration example\\n$s3client = new Aws\\\\S3\\\\S3Client([\\'region\\' => \\'us-west-2\\']);\\n*/\\n$this->bucketName = \"doc-example-bucket-\" . uniqid();\\ntry {\\n$this->s3client->createBucket([\\n\\'Bucket\\' => $this->bucketName,\\n\\'CreateBucketConfiguration\\' => [\\'LocationConstraint\\' => $region],\\n]);\\necho \"Created bucket named: $this->bucketName \\\\n\";\\n} catch (Exception $exception) {\\necho \"Failed to create bucket $this->bucketName with error: \" .\\n$exception->getMessage();\\nexit(\"Please fix error with bucket creation before continuing.\");\\n}\\n$fileName = __DIR__ . \"/local-file-\" . uniqid();\\ntry {\\n$this->s3client->putObject([\\n\\'Bucket\\' => $this->bucketName,', '']]]\n",
      "[[['', '\\'Key\\' => $fileName,\\n\\'SourceFile\\' => __DIR__ . \\'/testfile.txt\\'\\n]);\\necho \"Uploaded $fileName to $this->bucketName.\\\\n\";\\n} catch (Exception $exception) {\\necho \"Failed to upload $fileName with error: \" . $exception-\\n>getMessage();\\nexit(\"Please fix error with file upload before continuing.\");\\n}\\ntry {\\n$file = $this->s3client->getObject([\\n\\'Bucket\\' => $this->bucketName,\\n\\'Key\\' => $fileName,\\n]);\\n$body = $file->get(\\'Body\\');\\n$body->rewind();\\necho \"Downloaded the file and it begins with: {$body->read(26)}.\\\\n\";\\n} catch (Exception $exception) {\\necho \"Failed to download $fileName from $this->bucketName with error:\\n\" . $exception->getMessage();\\nexit(\"Please fix error with file downloading before continuing.\");\\n}\\ntry {\\n$folder = \"copied-folder\";\\n$this->s3client->copyObject([\\n\\'Bucket\\' => $this->bucketName,\\n\\'CopySource\\' => \"$this->bucketName/$fileName\",\\n\\'Key\\' => \"$folder/$fileName-copy\",\\n]);\\necho \"Copied $fileName to $folder/$fileName-copy.\\\\n\";\\n} catch (Exception $exception) {\\necho \"Failed to copy $fileName with error: \" . $exception-\\n>getMessage();\\nexit(\"Please fix error with object copying before continuing.\");\\n}\\ntry {\\n$contents = $this->s3client->listObjectsV2([\\n\\'Bucket\\' => $this->bucketName,\\n]);\\necho \"The contents of your bucket are: \\\\n\";\\nforeach ($contents[\\'Contents\\'] as $content) {', '']]]\n",
      "[[['', 'echo $content[\\'Key\\'] . \"\\\\n\";\\n}\\n} catch (Exception $exception) {\\necho \"Failed to list objects in $this->bucketName with error: \" .\\n$exception->getMessage();\\nexit(\"Please fix error with listing objects before continuing.\");\\n}\\ntry {\\n$objects = [];\\nforeach ($contents[\\'Contents\\'] as $content) {\\n$objects[] = [\\n\\'Key\\' => $content[\\'Key\\'],\\n];\\n}\\n$this->s3client->deleteObjects([\\n\\'Bucket\\' => $this->bucketName,\\n\\'Delete\\' => [\\n\\'Objects\\' => $objects,\\n],\\n]);\\n$check = $this->s3client->listObjectsV2([\\n\\'Bucket\\' => $this->bucketName,\\n]);\\nif (count($check) <= 0) {\\nthrow new Exception(\"Bucket wasn\\'t empty.\");\\n}\\necho \"Deleted all objects and folders from $this->bucketName.\\\\n\";\\n} catch (Exception $exception) {\\necho \"Failed to delete $fileName from $this->bucketName with error:\\n\" . $exception->getMessage();\\nexit(\"Please fix error with object deletion before continuing.\");\\n}\\ntry {\\n$this->s3client->deleteBucket([\\n\\'Bucket\\' => $this->bucketName,\\n]);\\necho \"Deleted bucket $this->bucketName.\\\\n\";\\n} catch (Exception $exception) {\\necho \"Failed to delete $this->bucketName with error: \" . $exception-\\n>getMessage();\\nexit(\"Please fix error with bucket deletion before continuing.\");\\n}', '']]]\n",
      "[[['', 'echo \"Successfully ran the Amazon S3 with PHP demo.\\\\n\";', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import io\\nimport os\\nimport uuid\\nimport boto3\\nfrom boto3.s3.transfer import S3UploadFailedError\\nfrom botocore.exceptions import ClientError\\ndef do_scenario(s3_resource):\\nprint(\"-\" * 88)\\nprint(\"Welcome to the Amazon S3 getting started demo!\")\\nprint(\"-\" * 88)', '']]]\n",
      "[[['', 'bucket_name = f\"doc-example-bucket-{uuid.uuid4()}\"\\nbucket = s3_resource.Bucket(bucket_name)\\ntry:\\nbucket.create(\\nCreateBucketConfiguration={\\n\"LocationConstraint\": s3_resource.meta.client.meta.region_name\\n}\\n)\\nprint(f\"Created demo bucket named {bucket.name}.\")\\nexcept ClientError as err:\\nprint(f\"Tried and failed to create demo bucket {bucket_name}.\")\\nprint(f\"\\\\t{err.response[\\'Error\\'][\\'Code\\']}:{err.response[\\'Error\\']\\n[\\'Message\\']}\")\\nprint(f\"\\\\nCan\\'t continue the demo without a bucket!\")\\nreturn\\nfile_name = None\\nwhile file_name is None:\\nfile_name = input(\"\\\\nEnter a file you want to upload to your bucket: \")\\nif not os.path.exists(file_name):\\nprint(f\"Couldn\\'t find file {file_name}. Are you sure it exists?\")\\nfile_name = None\\nobj = bucket.Object(os.path.basename(file_name))\\ntry:\\nobj.upload_file(file_name)\\nprint(\\nf\"Uploaded file {file_name} into bucket {bucket.name} with key\\n{obj.key}.\"\\n)\\nexcept S3UploadFailedError as err:\\nprint(f\"Couldn\\'t upload file {file_name} to {bucket.name}.\")\\nprint(f\"\\\\t{err}\")\\nanswer = input(f\"\\\\nDo you want to download {obj.key} into memory (y/n)? \")\\nif answer.lower() == \"y\":\\ndata = io.BytesIO()\\ntry:\\nobj.download_fileobj(data)\\ndata.seek(0)\\nprint(f\"Got your object. Here are the first 20 bytes:\\\\n\")\\nprint(f\"\\\\t{data.read(20)}\")\\nexcept ClientError as err:\\nprint(f\"Couldn\\'t download {obj.key}.\")', '']]]\n",
      "[[['', 'print(\\nf\"\\\\t{err.response[\\'Error\\'][\\'Code\\']}:{err.response[\\'Error\\']\\n[\\'Message\\']}\"\\n)\\nanswer = input(\\nf\"\\\\nDo you want to copy {obj.key} to a subfolder in your bucket (y/n)? \"\\n)\\nif answer.lower() == \"y\":\\ndest_obj = bucket.Object(f\"demo-folder/{obj.key}\")\\ntry:\\ndest_obj.copy({\"Bucket\": bucket.name, \"Key\": obj.key})\\nprint(f\"Copied {obj.key} to {dest_obj.key}.\")\\nexcept ClientError as err:\\nprint(f\"Couldn\\'t copy {obj.key} to {dest_obj.key}.\")\\nprint(\\nf\"\\\\t{err.response[\\'Error\\'][\\'Code\\']}:{err.response[\\'Error\\']\\n[\\'Message\\']}\"\\n)\\nprint(\"\\\\nYour bucket contains the following objects:\")\\ntry:\\nfor o in bucket.objects.all():\\nprint(f\"\\\\t{o.key}\")\\nexcept ClientError as err:\\nprint(f\"Couldn\\'t list the objects in bucket {bucket.name}.\")\\nprint(f\"\\\\t{err.response[\\'Error\\'][\\'Code\\']}:{err.response[\\'Error\\']\\n[\\'Message\\']}\")\\nanswer = input(\\n\"\\\\nDo you want to delete all of the objects as well as the bucket (y/n)?\\n\"\\n)\\nif answer.lower() == \"y\":\\ntry:\\nbucket.objects.delete()\\nbucket.delete()\\nprint(f\"Emptied and deleted bucket {bucket.name}.\\\\n\")\\nexcept ClientError as err:\\nprint(f\"Couldn\\'t empty and delete bucket {bucket.name}.\")\\nprint(\\nf\"\\\\t{err.response[\\'Error\\'][\\'Code\\']}:{err.response[\\'Error\\']\\n[\\'Message\\']}\"\\n)', '']]]\n",
      "[[['', 'print(\"Thanks for watching!\")\\nprint(\"-\" * 88)\\nif __name__ == \"__main__\":\\ndo_scenario(boto3.resource(\"s3\"))', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \"aws-sdk-s3\"\\n# Wraps the getting started scenario actions.\\nclass ScenarioGettingStarted\\nattr_reader :s3_resource\\n# @param s3_resource [Aws::S3::Resource] An Amazon S3 resource.\\ndef initialize(s3_resource)\\n@s3_resource = s3_resource\\nend', '']]]\n",
      "[[['', '# Creates a bucket with a random name in the currently configured account and\\n# AWS Region.\\n#\\n# @return [Aws::S3::Bucket] The newly created bucket.\\ndef create_bucket\\nbucket = @s3_resource.create_bucket(\\nbucket: \"doc-example-bucket-#{Random.uuid}\",\\ncreate_bucket_configuration: {\\nlocation_constraint: \"us-east-1\" # Note: only certain regions permitted\\n}\\n)\\nputs(\"Created demo bucket named #{bucket.name}.\")\\nrescue Aws::Errors::ServiceError => e\\nputs(\"Tried and failed to create demo bucket.\")\\nputs(\"\\\\t#{e.code}: #{e.message}\")\\nputs(\"\\\\nCan\\'t continue the demo without a bucket!\")\\nraise\\nelse\\nbucket\\nend\\n# Requests a file name from the user.\\n#\\n# @return The name of the file.\\ndef create_file\\nFile.open(\"demo.txt\", w) { |f| f.write(\"This is a demo file.\") }\\nend\\n# Uploads a file to an Amazon S3 bucket.\\n#\\n# @param bucket [Aws::S3::Bucket] The bucket object representing the upload\\ndestination\\n# @return [Aws::S3::Object] The Amazon S3 object that contains the uploaded\\nfile.\\ndef upload_file(bucket)\\nFile.open(\"demo.txt\", \"w+\") { |f| f.write(\"This is a demo file.\") }\\ns3_object = bucket.object(File.basename(\"demo.txt\"))\\ns3_object.upload_file(\"demo.txt\")\\nputs(\"Uploaded file demo.txt into bucket #{bucket.name} with key\\n#{s3_object.key}.\")\\nrescue Aws::Errors::ServiceError => e\\nputs(\"Couldn\\'t upload file demo.txt to #{bucket.name}.\")\\nputs(\"\\\\t#{e.code}: #{e.message}\")', '']]]\n",
      "[[['', 'raise\\nelse\\ns3_object\\nend\\n# Downloads an Amazon S3 object to a file.\\n#\\n# @param s3_object [Aws::S3::Object] The object to download.\\ndef download_file(s3_object)\\nputs(\"\\\\nDo you want to download #{s3_object.key} to a local file (y/n)? \")\\nanswer = gets.chomp.downcase\\nif answer == \"y\"\\nputs(\"Enter a name for the downloaded file: \")\\nfile_name = gets.chomp\\ns3_object.download_file(file_name)\\nputs(\"Object #{s3_object.key} successfully downloaded to #{file_name}.\")\\nend\\nrescue Aws::Errors::ServiceError => e\\nputs(\"Couldn\\'t download #{s3_object.key}.\")\\nputs(\"\\\\t#{e.code}: #{e.message}\")\\nraise\\nend\\n# Copies an Amazon S3 object to a subfolder within the same bucket.\\n#\\n# @param source_object [Aws::S3::Object] The source object to copy.\\n# @return [Aws::S3::Object, nil] The destination object.\\ndef copy_object(source_object)\\ndest_object = nil\\nputs(\"\\\\nDo you want to copy #{source_object.key} to a subfolder in your\\nbucket (y/n)? \")\\nanswer = gets.chomp.downcase\\nif answer == \"y\"\\ndest_object = source_object.bucket.object(\"demo-folder/\\n#{source_object.key}\")\\ndest_object.copy_from(source_object)\\nputs(\"Copied #{source_object.key} to #{dest_object.key}.\")\\nend\\nrescue Aws::Errors::ServiceError => e\\nputs(\"Couldn\\'t copy #{source_object.key}.\")\\nputs(\"\\\\t#{e.code}: #{e.message}\")\\nraise\\nelse\\ndest_object', '']]]\n",
      "[[['', 'end\\n# Lists the objects in an Amazon S3 bucket.\\n#\\n# @param bucket [Aws::S3::Bucket] The bucket to query.\\ndef list_objects(bucket)\\nputs(\"\\\\nYour bucket contains the following objects:\")\\nbucket.objects.each do |obj|\\nputs(\"\\\\t#{obj.key}\")\\nend\\nrescue Aws::Errors::ServiceError => e\\nputs(\"Couldn\\'t list the objects in bucket #{bucket.name}.\")\\nputs(\"\\\\t#{e.code}: #{e.message}\")\\nraise\\nend\\n# Deletes the objects in an Amazon S3 bucket and deletes the bucket.\\n#\\n# @param bucket [Aws::S3::Bucket] The bucket to empty and delete.\\ndef delete_bucket(bucket)\\nputs(\"\\\\nDo you want to delete all of the objects as well as the bucket (y/n)?\\n\")\\nanswer = gets.chomp.downcase\\nif answer == \"y\"\\nbucket.objects.batch_delete!\\nbucket.delete\\nputs(\"Emptied and deleted bucket #{bucket.name}.\\\\n\")\\nend\\nrescue Aws::Errors::ServiceError => e\\nputs(\"Couldn\\'t empty and delete bucket #{bucket.name}.\")\\nputs(\"\\\\t#{e.code}: #{e.message}\")\\nraise\\nend\\nend\\n# Runs the Amazon S3 getting started scenario.\\ndef run_scenario(scenario)\\nputs(\"-\" * 88)\\nputs(\"Welcome to the Amazon S3 getting started demo!\")\\nputs(\"-\" * 88)\\nbucket = scenario.create_bucket\\ns3_object = scenario.upload_file(bucket)\\nscenario.download_file(s3_object)', '']]]\n",
      "[[['', 'scenario.copy_object(s3_object)\\nscenario.list_objects(bucket)\\nscenario.delete_bucket(bucket)\\nputs(\"Thanks for watching!\")\\nputs(\"-\" * 88)\\nrescue Aws::Errors::ServiceError\\nputs(\"Something went wrong with the demo!\")\\nend\\nrun_scenario(ScenarioGettingStarted.new(Aws::S3::Resource.new)) if $PROGRAM_NAME\\n== __FILE__', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'use aws_config::meta::region::RegionProviderChain;\\nuse aws_sdk_s3::{config::Region, Client};', '']]]\n",
      "[[['', 'use s3_service::error::Error;\\nuse uuid::Uuid;\\n#[tokio::main]\\nasync fn main() -> Result<(), Error> {\\nlet (region, client, bucket_name, file_name, key, target_key) =\\ninitialize_variables().await;\\nif let Err(e) = run_s3_operations(region, client, bucket_name, file_name,\\nkey, target_key).await\\n{\\nprintln!(\"{:?}\", e);\\n};\\nOk(())\\n}\\nasync fn initialize_variables() -> (Region, Client, String, String, String,\\nString) {\\nlet region_provider = RegionProviderChain::first_try(Region::new(\"us-\\nwest-2\"));\\nlet region = region_provider.region().await.unwrap();\\nlet shared_config =\\naws_config::from_env().region(region_provider).load().await;\\nlet client = Client::new(&shared_config);\\nlet bucket_name = format!(\"doc-example-bucket-{}\", Uuid::new_v4());\\nlet file_name = \"s3/testfile.txt\".to_string();\\nlet key = \"test file key name\".to_string();\\nlet target_key = \"target_key\".to_string();\\n(region, client, bucket_name, file_name, key, target_key)\\n}\\nasync fn run_s3_operations(\\nregion: Region,\\nclient: Client,\\nbucket_name: String,\\nfile_name: String,\\nkey: String,\\ntarget_key: String,', '']]]\n",
      "[[['', ') -> Result<(), Error> {\\ns3_service::create_bucket(&client, &bucket_name, region.as_ref()).await?;\\ns3_service::upload_object(&client, &bucket_name, &file_name, &key).await?;\\nlet _object = s3_service::download_object(&client, &bucket_name, &key).await;\\ns3_service::copy_object(&client, &bucket_name, &key, &target_key).await?;\\ns3_service::list_objects(&client, &bucket_name).await?;\\ns3_service::delete_objects(&client, &bucket_name).await?;\\ns3_service::delete_bucket(&client, &bucket_name).await?;\\nOk(())\\n}', ''], ['', '', '']], [['', '', ''], ['', 'use aws_sdk_s3::operation::{\\ncopy_object::{CopyObjectError, CopyObjectOutput},\\ncreate_bucket::{CreateBucketError, CreateBucketOutput},\\nget_object::{GetObjectError, GetObjectOutput},\\nlist_objects_v2::ListObjectsV2Output,\\nput_object::{PutObjectError, PutObjectOutput},\\n};\\nuse aws_sdk_s3::types::{\\nBucketLocationConstraint, CreateBucketConfiguration, Delete,\\nObjectIdentifier,\\n};\\nuse aws_sdk_s3::{error::SdkError, primitives::ByteStream, Client};\\nuse error::Error;\\nuse std::path::Path;\\nuse std::str;\\npub mod error;\\npub async fn delete_bucket(client: &Client, bucket_name: &str) -> Result<(),\\nError> {\\nclient.delete_bucket().bucket(bucket_name).send().await?;\\nprintln!(\"Bucket deleted\");\\nOk(())\\n}', '']]]\n",
      "[[['', 'pub async fn delete_objects(client: &Client, bucket_name: &str) ->\\nResult<Vec<String>, Error> {\\nlet objects = client.list_objects_v2().bucket(bucket_name).send().await?;\\nlet mut delete_objects: Vec<ObjectIdentifier> = vec![];\\nfor obj in objects.contents() {\\nlet obj_id = ObjectIdentifier::builder()\\n.set_key(Some(obj.key().unwrap().to_string()))\\n.build()\\n.map_err(Error::from)?;\\ndelete_objects.push(obj_id);\\n}\\nlet return_keys = delete_objects.iter().map(|o| o.key.clone()).collect();\\nif !delete_objects.is_empty() {\\nclient\\n.delete_objects()\\n.bucket(bucket_name)\\n.delete(\\nDelete::builder()\\n.set_objects(Some(delete_objects))\\n.build()\\n.map_err(Error::from)?,\\n)\\n.send()\\n.await?;\\n}\\nlet objects: ListObjectsV2Output =\\nclient.list_objects_v2().bucket(bucket_name).send().await?;\\neprintln!(\"{objects:?}\");\\nmatch objects.key_count {\\nSome(0) => Ok(return_keys),\\n_ => Err(Error::unhandled(\\n\"There were still objects left in the bucket.\",\\n)),\\n}\\n}\\npub async fn list_objects(client: &Client, bucket: &str) -> Result<(), Error> {\\nlet mut response = client', '']]]\n",
      "[[['', '.list_objects_v2()\\n.bucket(bucket.to_owned())\\n.max_keys(10) // In this example, go 10 at a time.\\n.into_paginator()\\n.send();\\nwhile let Some(result) = response.next().await {\\nmatch result {\\nOk(output) => {\\nfor object in output.contents() {\\nprintln!(\" - {}\", object.key().unwrap_or(\"Unknown\"));\\n}\\n}\\nErr(err) => {\\neprintln!(\"{err:?}\")\\n}\\n}\\n}\\nOk(())\\n}\\npub async fn copy_object(\\nclient: &Client,\\nbucket_name: &str,\\nobject_key: &str,\\ntarget_key: &str,\\n) -> Result<CopyObjectOutput, SdkError<CopyObjectError>> {\\nlet mut source_bucket_and_object: String = \"\".to_owned();\\nsource_bucket_and_object.push_str(bucket_name);\\nsource_bucket_and_object.push(\\'/\\');\\nsource_bucket_and_object.push_str(object_key);\\nclient\\n.copy_object()\\n.copy_source(source_bucket_and_object)\\n.bucket(bucket_name)\\n.key(target_key)\\n.send()\\n.await\\n}\\npub async fn download_object(\\nclient: &Client,', '']]]\n",
      "[[['', 'bucket_name: &str,\\nkey: &str,\\n) -> Result<GetObjectOutput, SdkError<GetObjectError>> {\\nclient\\n.get_object()\\n.bucket(bucket_name)\\n.key(key)\\n.send()\\n.await\\n}\\npub async fn upload_object(\\nclient: &Client,\\nbucket_name: &str,\\nfile_name: &str,\\nkey: &str,\\n) -> Result<PutObjectOutput, SdkError<PutObjectError>> {\\nlet body = ByteStream::from_path(Path::new(file_name)).await;\\nclient\\n.put_object()\\n.bucket(bucket_name)\\n.key(key)\\n.body(body.unwrap())\\n.send()\\n.await\\n}\\npub async fn create_bucket(\\nclient: &Client,\\nbucket_name: &str,\\nregion: &str,\\n) -> Result<CreateBucketOutput, SdkError<CreateBucketError>> {\\nlet constraint = BucketLocationConstraint::from(region);\\nlet cfg = CreateBucketConfiguration::builder()\\n.location_constraint(constraint)\\n.build();\\nclient\\n.create_bucket()\\n.create_bucket_configuration(cfg)\\n.bucket(bucket_name)\\n.send()\\n.await\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'DATA(lo_session) = /aws1/cl_rt_session_aws=>create( cv_pfl ).\\nDATA(lo_s3) = /aws1/cl_s3_factory=>create( lo_session ).\\n\" Create an Amazon Simple Storage Service (Amazon S3) bucket. \"\\nTRY.\\nlo_s3->createbucket(\\niv_bucket = iv_bucket_name\\n).\\nMESSAGE \\'S3 bucket created.\\' TYPE \\'I\\'.\\nCATCH /aws1/cx_s3_bucketalrdyexists.\\nMESSAGE \\'Bucket name already exists.\\' TYPE \\'E\\'.\\nCATCH /aws1/cx_s3_bktalrdyownedbyyou.\\nMESSAGE \\'Bucket already exists and is owned by you.\\' TYPE \\'E\\'.\\nENDTRY.\\n\"Upload an object to an S3 bucket.\"', '']]]\n",
      "[[['', 'TRY.\\n\"Get contents of file from application server.\"\\nDATA lv_file_content TYPE xstring.\\nOPEN DATASET iv_key FOR INPUT IN BINARY MODE.\\nREAD DATASET iv_key INTO lv_file_content.\\nCLOSE DATASET iv_key.\\nlo_s3->putobject(\\niv_bucket = iv_bucket_name\\niv_key = iv_key\\niv_body = lv_file_content\\n).\\nMESSAGE \\'Object uploaded to S3 bucket.\\' TYPE \\'I\\'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE \\'Bucket does not exist.\\' TYPE \\'E\\'.\\nENDTRY.\\n\" Get an object from a bucket. \"\\nTRY.\\nDATA(lo_result) = lo_s3->getobject(\\niv_bucket = iv_bucket_name\\niv_key = iv_key\\n).\\nDATA(lv_object_data) = lo_result->get_body( ).\\nMESSAGE \\'Object retrieved from S3 bucket.\\' TYPE \\'I\\'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE \\'Bucket does not exist.\\' TYPE \\'E\\'.\\nCATCH /aws1/cx_s3_nosuchkey.\\nMESSAGE \\'Object key does not exist.\\' TYPE \\'E\\'.\\nENDTRY.\\n\" Copy an object to a subfolder in a bucket. \"\\nTRY.\\nlo_s3->copyobject(\\niv_bucket = iv_bucket_name\\niv_key = |{ iv_copy_to_folder }/{ iv_key }|\\niv_copysource = |{ iv_bucket_name }/{ iv_key }|\\n).\\nMESSAGE \\'Object copied to a subfolder.\\' TYPE \\'I\\'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE \\'Bucket does not exist.\\' TYPE \\'E\\'.\\nCATCH /aws1/cx_s3_nosuchkey.\\nMESSAGE \\'Object key does not exist.\\' TYPE \\'E\\'.\\nENDTRY.', '']]]\n",
      "[[['', '\" List objects in the bucket. \"\\nTRY.\\nDATA(lo_list) = lo_s3->listobjects(\\niv_bucket = iv_bucket_name\\n).\\nMESSAGE \\'Retrieved list of objects in S3 bucket.\\' TYPE \\'I\\'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE \\'Bucket does not exist.\\' TYPE \\'E\\'.\\nENDTRY.\\nDATA text TYPE string VALUE \\'Object List - \\'.\\nDATA lv_object_key TYPE /aws1/s3_objectkey.\\nLOOP AT lo_list->get_contents( ) INTO DATA(lo_object).\\nlv_object_key = lo_object->get_key( ).\\nCONCATENATE lv_object_key \\', \\' INTO text.\\nENDLOOP.\\nMESSAGE text TYPE\\'I\\'.\\n\" Delete the objects in a bucket. \"\\nTRY.\\nlo_s3->deleteobject(\\niv_bucket = iv_bucket_name\\niv_key = iv_key\\n).\\nlo_s3->deleteobject(\\niv_bucket = iv_bucket_name\\niv_key = |{ iv_copy_to_folder }/{ iv_key }|\\n).\\nMESSAGE \\'Objects deleted from S3 bucket.\\' TYPE \\'I\\'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE \\'Bucket does not exist.\\' TYPE \\'E\\'.\\nENDTRY.\\n\" Delete the bucket. \"\\nTRY.\\nlo_s3->deletebucket(\\niv_bucket = iv_bucket_name\\n).\\nMESSAGE \\'Deleted S3 bucket.\\' TYPE \\'I\\'.\\nCATCH /aws1/cx_s3_nosuchbucket.\\nMESSAGE \\'Bucket does not exist.\\' TYPE \\'E\\'.\\nENDTRY.', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nThis is prerelease documentation for an SDK in preview release. It is subject to\\nchange.', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import Foundation\\nimport AWSS3\\nimport ClientRuntime\\nimport AWSClientRuntime\\n/// A class containing all the code that interacts with the AWS SDK for Swift.\\npublic class ServiceHandler {\\nlet client: S3Client', '']]]\n",
      "[[['', '/// Initialize and return a new ``ServiceHandler`` object, which is used to\\ndrive the AWS calls\\n/// used for the example.\\n///\\n/// - Returns: A new ``ServiceHandler`` object, ready to be called to\\n/// execute AWS operations.\\npublic init() async {\\ndo {\\nclient = try S3Client(region: \"us-east-2\")\\n} catch {\\nprint(\"ERROR: \", dump(error, name: \"Initializing S3 client\"))\\nexit(1)\\n}\\n}\\n/// Create a new user given the specified name.\\n///\\n/// - Parameters:\\n/// - name: Name of the bucket to create.\\n/// Throws an exception if an error occurs.\\npublic func createBucket(name: String) async throws {\\nlet config = S3ClientTypes.CreateBucketConfiguration(\\nlocationConstraint: .usEast2\\n)\\nlet input = CreateBucketInput(\\nbucket: name,\\ncreateBucketConfiguration: config\\n)\\n_ = try await client.createBucket(input: input)\\n}\\n/// Delete a bucket.\\n/// - Parameter name: Name of the bucket to delete.\\npublic func deleteBucket(name: String) async throws {\\nlet input = DeleteBucketInput(\\nbucket: name\\n)\\n_ = try await client.deleteBucket(input: input)\\n}\\n/// Upload a file from local storage to the bucket.\\n/// - Parameters:\\n/// - bucket: Name of the bucket to upload the file to.\\n/// - key: Name of the file to create.', '']]]\n",
      "[[['', \"/// - file: Path name of the file to upload.\\npublic func uploadFile(bucket: String, key: String, file: String) async\\nthrows {\\nlet fileUrl = URL(fileURLWithPath: file)\\nlet fileData = try Data(contentsOf: fileUrl)\\nlet dataStream = ByteStream.from(data: fileData)\\nlet input = PutObjectInput(\\nbody: dataStream,\\nbucket: bucket,\\nkey: key\\n)\\n_ = try await client.putObject(input: input)\\n}\\n/// Create a file in the specified bucket with the given name. The new\\n/// file's contents are uploaded from a `Data` object.\\n///\\n/// - Parameters:\\n/// - bucket: Name of the bucket to create a file in.\\n/// - key: Name of the file to create.\\n/// - data: A `Data` object to write into the new file.\\npublic func createFile(bucket: String, key: String, withData data: Data)\\nasync throws {\\nlet dataStream = ByteStream.from(data: data)\\nlet input = PutObjectInput(\\nbody: dataStream,\\nbucket: bucket,\\nkey: key\\n)\\n_ = try await client.putObject(input: input)\\n}\\n/// Download the named file to the given directory on the local device.\\n///\\n/// - Parameters:\\n/// - bucket: Name of the bucket that contains the file to be copied.\\n/// - key: The name of the file to copy from the bucket.\\n/// - to: The path of the directory on the local device where you want to\\n/// download the file.\\npublic func downloadFile(bucket: String, key: String, to: String) async\\nthrows {\\nlet fileUrl = URL(fileURLWithPath: to).appendingPathComponent(key)\", '']]]\n",
      "[[['', 'let input = GetObjectInput(\\nbucket: bucket,\\nkey: key\\n)\\nlet output = try await client.getObject(input: input)\\n// Get the data stream object. Return immediately if there isn\\'t one.\\nguard let body = output.body,\\nlet data = try await body.readData() else {\\nreturn\\n}\\ntry data.write(to: fileUrl)\\n}\\n/// Read the specified file from the given S3 bucket into a Swift\\n/// `Data` object.\\n///\\n/// - Parameters:\\n/// - bucket: Name of the bucket containing the file to read.\\n/// - key: Name of the file within the bucket to read.\\n///\\n/// - Returns: A `Data` object containing the complete file data.\\npublic func readFile(bucket: String, key: String) async throws -> Data {\\nlet input = GetObjectInput(\\nbucket: bucket,\\nkey: key\\n)\\nlet output = try await client.getObject(input: input)\\n// Get the stream and return its contents in a `Data` object. If\\n// there is no stream, return an empty `Data` object instead.\\nguard let body = output.body,\\nlet data = try await body.readData() else {\\nreturn \"\".data(using: .utf8)!\\n}\\nreturn data\\n}\\n/// Copy a file from one bucket to another.\\n///\\n/// - Parameters:\\n/// - sourceBucket: Name of the bucket containing the source file.', '']]]\n",
      "[[['', '/// - name: Name of the source file.\\n/// - destBucket: Name of the bucket to copy the file into.\\npublic func copyFile(from sourceBucket: String, name: String, to destBucket:\\nString) async throws {\\nlet srcUrl = (\"\\\\(sourceBucket)/\\n\\\\(name)\").addingPercentEncoding(withAllowedCharacters: .urlPathAllowed)\\nlet input = CopyObjectInput(\\nbucket: destBucket,\\ncopySource: srcUrl,\\nkey: name\\n)\\n_ = try await client.copyObject(input: input)\\n}\\n/// Deletes the specified file from Amazon S3.\\n///\\n/// - Parameters:\\n/// - bucket: Name of the bucket containing the file to delete.\\n/// - key: Name of the file to delete.\\n///\\npublic func deleteFile(bucket: String, key: String) async throws {\\nlet input = DeleteObjectInput(\\nbucket: bucket,\\nkey: key\\n)\\ndo {\\n_ = try await client.deleteObject(input: input)\\n} catch {\\nthrow error\\n}\\n}\\n/// Returns an array of strings, each naming one file in the\\n/// specified bucket.\\n///\\n/// - Parameter bucket: Name of the bucket to get a file listing for.\\n/// - Returns: An array of `String` objects, each giving the name of\\n/// one file contained in the bucket.\\npublic func listBucketFiles(bucket: String) async throws -> [String] {\\nlet input = ListObjectsV2Input(\\nbucket: bucket\\n)', '']]]\n",
      "[[['', 'let output = try await client.listObjectsV2(input: input)\\nvar names: [String] = []\\nguard let objList = output.contents else {\\nreturn []\\n}\\nfor obj in objList {\\nif let objName = obj.key {\\nnames.append(objName)\\n}\\n}\\nreturn names\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import Foundation\\nimport ServiceHandler\\nimport ArgumentParser\\n/// The command-line arguments and options available for this\\n/// example command.\\nstruct ExampleCommand: ParsableCommand {\\n@Argument(help: \"Name of the S3 bucket to create\")\\nvar bucketName: String\\n@Argument(help: \"Pathname of the file to upload to the S3 bucket\")\\nvar uploadSource: String\\n@Argument(help: \"The name (key) to give the file in the S3 bucket\")\\nvar objName: String\\n@Argument(help: \"S3 bucket to copy the object to\")\\nvar destBucket: String\\n@Argument(help: \"Directory where you want to download the file from the S3\\nbucket\")\\nvar downloadDir: String', '']]]\n",
      "[[['', 'static var configuration = CommandConfiguration(\\ncommandName: \"s3-basics\",\\nabstract: \"Demonstrates a series of basic AWS S3 functions.\",\\ndiscussion: \"\"\"\\nPerforms the following Amazon S3 commands:\\n* `CreateBucket`\\n* `PutObject`\\n* `GetObject`\\n* `CopyObject`\\n* `ListObjects`\\n* `DeleteObjects`\\n* `DeleteBucket`\\n\"\"\"\\n)\\n/// Called by ``main()`` to do the actual running of the AWS\\n/// example.\\nfunc runAsync() async throws {\\nlet serviceHandler = await ServiceHandler()\\n// 1. Create the bucket.\\nprint(\"Creating the bucket \\\\(bucketName)...\")\\ntry await serviceHandler.createBucket(name: bucketName)\\n// 2. Upload a file to the bucket.\\nprint(\"Uploading the file \\\\(uploadSource)...\")\\ntry await serviceHandler.uploadFile(bucket: bucketName, key: objName,\\nfile: uploadSource)\\n// 3. Download the file.\\nprint(\"Downloading the file \\\\(objName) to \\\\(downloadDir)...\")\\ntry await serviceHandler.downloadFile(bucket: bucketName, key: objName,\\nto: downloadDir)\\n// 4. Copy the file to another bucket.\\nprint(\"Copying the file to the bucket \\\\(destBucket)...\")\\ntry await serviceHandler.copyFile(from: bucketName, name: objName, to:\\ndestBucket)\\n// 5. List the contents of the bucket.\\nprint(\"Getting a list of the files in the bucket \\\\(bucketName)\")', '']]]\n",
      "[[['', 'let fileList = try await serviceHandler.listBucketFiles(bucket:\\nbucketName)\\nlet numFiles = fileList.count\\nif numFiles != 0 {\\nprint(\"\\\\(numFiles) file\\\\((numFiles > 1) ? \"s\" : \"\") in bucket\\n\\\\(bucketName):\")\\nfor name in fileList {\\nprint(\" \\\\(name)\")\\n}\\n} else {\\nprint(\"No files found in bucket \\\\(bucketName)\")\\n}\\n// 6. Delete the objects from the bucket.\\nprint(\"Deleting the file \\\\(objName) from the bucket \\\\(bucketName)...\")\\ntry await serviceHandler.deleteFile(bucket: bucketName, key: objName)\\nprint(\"Deleting the file \\\\(objName) from the bucket \\\\(destBucket)...\")\\ntry await serviceHandler.deleteFile(bucket: destBucket, key: objName)\\n// 7. Delete the bucket.\\nprint(\"Deleting the bucket \\\\(bucketName)...\")\\ntry await serviceHandler.deleteBucket(name: bucketName)\\nprint(\"Done.\")\\n}\\n}\\n//\\n// Main program entry point.\\n//\\n@main\\nstruct Main {\\nstatic func main() async {\\nlet args = Array(CommandLine.arguments.dropFirst())\\ndo {\\nlet command = try ExampleCommand.parse(args)\\ntry await command.runAsync()\\n} catch {\\nExampleCommand.exit(withError: error)\\n}\\n}\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.IO;\\nusing System.Security.Cryptography;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example shows how to apply client encryption to an object in an', '']]]\n",
      "[[['', '/// Amazon Simple Storage Service (Amazon S3) bucket.\\n/// </summary>\\npublic class SSEClientEncryption\\n{\\npublic static async Task Main()\\n{\\nstring bucketName = \"doc-example-bucket\";\\nstring keyName = \"exampleobject.txt\";\\nstring copyTargetKeyName = \"examplecopy.txt\";\\n// If the AWS Region defined for your default user is different\\n// from the Region where your Amazon S3 bucket is located,\\n// pass the Region name to the Amazon S3 client object\\'s constructor.\\n// For example: RegionEndpoint.USWest2.\\nIAmazonS3 client = new AmazonS3Client();\\ntry\\n{\\n// Create an encryption key.\\nAes aesEncryption = Aes.Create();\\naesEncryption.KeySize = 256;\\naesEncryption.GenerateKey();\\nstring base64Key = Convert.ToBase64String(aesEncryption.Key);\\n// Upload the object.\\nPutObjectRequest putObjectRequest = await\\nUploadObjectAsync(client, bucketName, keyName, base64Key);\\n// Download the object and verify that its contents match what\\nyou uploaded.\\nawait DownloadObjectAsync(client, bucketName, keyName, base64Key,\\nputObjectRequest);\\n// Get object metadata and verify that the object uses AES-256\\nencryption.\\nawait GetObjectMetadataAsync(client, bucketName, keyName,\\nbase64Key);\\n// Copy both the source and target objects using server-side\\nencryption with\\n// an encryption key.\\nawait CopyObjectAsync(client, bucketName, keyName,\\ncopyTargetKeyName, aesEncryption, base64Key);\\n}', '']]]\n",
      "[[['', 'catch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error: {ex.Message}\");\\n}\\n}\\n/// <summary>\\n/// Uploads an object to an Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// PutObjectAsync.</param>\\n/// <param name=\"bucketName\">The name of the Amazon S3 bucket to which\\nthe\\n/// object will be uploaded.</param>\\n/// <param name=\"keyName\">The name of the object to upload to the Amazon\\nS3\\n/// bucket.</param>\\n/// <param name=\"base64Key\">The encryption key.</param>\\n/// <returns>The PutObjectRequest object for use by\\nDownloadObjectAsync.</returns>\\npublic static async Task<PutObjectRequest> UploadObjectAsync(\\nIAmazonS3 client,\\nstring bucketName,\\nstring keyName,\\nstring base64Key)\\n{\\nPutObjectRequest putObjectRequest = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nContentBody = \"sample text\",\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key,\\n};\\nPutObjectResponse putObjectResponse = await\\nclient.PutObjectAsync(putObjectRequest);\\nreturn putObjectRequest;\\n}\\n/// <summary>\\n/// Downloads an encrypted object from an Amazon S3 bucket.\\n/// </summary>', '']]]\n",
      "[[['', '/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// GetObjectAsync.</param>\\n/// <param name=\"bucketName\">The name of the Amazon S3 bucket where the\\nobject\\n/// is located.</param>\\n/// <param name=\"keyName\">The name of the Amazon S3 object to download.</\\nparam>\\n/// <param name=\"base64Key\">The encryption key used to encrypt the\\n/// object.</param>\\n/// <param name=\"putObjectRequest\">The PutObjectRequest used to upload\\n/// the object.</param>\\npublic static async Task DownloadObjectAsync(\\nIAmazonS3 client,\\nstring bucketName,\\nstring keyName,\\nstring base64Key,\\nPutObjectRequest putObjectRequest)\\n{\\nGetObjectRequest getObjectRequest = new GetObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\n// Provide encryption information for the object stored in Amazon\\nS3.\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key,\\n};\\nusing (GetObjectResponse getResponse = await\\nclient.GetObjectAsync(getObjectRequest))\\nusing (StreamReader reader = new\\nStreamReader(getResponse.ResponseStream))\\n{\\nstring content = reader.ReadToEnd();\\nif (string.Compare(putObjectRequest.ContentBody, content) == 0)\\n{\\nConsole.WriteLine(\"Object content is same as we uploaded\");\\n}\\nelse\\n{\\nConsole.WriteLine(\"Error...Object content is not same.\");', '']]]\n",
      "[[['', '}\\nif (getResponse.ServerSideEncryptionCustomerMethod ==\\nServerSideEncryptionCustomerMethod.AES256)\\n{\\nConsole.WriteLine(\"Object encryption method is AES256, same\\nas we set\");\\n}\\nelse\\n{\\nConsole.WriteLine(\"Error...Object encryption method is not\\nthe same as AES256 we set\");\\n}\\n}\\n}\\n/// <summary>\\n/// Retrieves the metadata associated with an Amazon S3 object.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used\\n/// to call GetObjectMetadataAsync.</param>\\n/// <param name=\"bucketName\">The name of the Amazon S3 bucket containing\\nthe\\n/// object for which we want to retrieve metadata.</param>\\n/// <param name=\"keyName\">The name of the object for which we wish to\\n/// retrieve the metadata.</param>\\n/// <param name=\"base64Key\">The encryption key associated with the\\n/// object.</param>\\npublic static async Task GetObjectMetadataAsync(\\nIAmazonS3 client,\\nstring bucketName,\\nstring keyName,\\nstring base64Key)\\n{\\nGetObjectMetadataRequest getObjectMetadataRequest = new\\nGetObjectMetadataRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\n// The object stored in Amazon S3 is encrypted, so provide the\\nnecessary encryption information.\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,', '']]]\n",
      "[[['', 'ServerSideEncryptionCustomerProvidedKey = base64Key,\\n};\\nGetObjectMetadataResponse getObjectMetadataResponse = await\\nclient.GetObjectMetadataAsync(getObjectMetadataRequest);\\nConsole.WriteLine(\"The object metadata show encryption method used\\nis: {0}\", getObjectMetadataResponse.ServerSideEncryptionCustomerMethod);\\n}\\n/// <summary>\\n/// Copies an encrypted object from one Amazon S3 bucket to another.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// CopyObjectAsync.</param>\\n/// <param name=\"bucketName\">The Amazon S3 bucket containing the object\\n/// to copy.</param>\\n/// <param name=\"keyName\">The name of the object to copy.</param>\\n/// <param name=\"copyTargetKeyName\">The Amazon S3 bucket to which the\\nobject\\n/// will be copied.</param>\\n/// <param name=\"aesEncryption\">The encryption type to use.</param>\\n/// <param name=\"base64Key\">The encryption key to use.</param>\\npublic static async Task CopyObjectAsync(\\nIAmazonS3 client,\\nstring bucketName,\\nstring keyName,\\nstring copyTargetKeyName,\\nAes aesEncryption,\\nstring base64Key)\\n{\\naesEncryption.GenerateKey();\\nstring copyBase64Key = Convert.ToBase64String(aesEncryption.Key);\\nCopyObjectRequest copyRequest = new CopyObjectRequest\\n{\\nSourceBucket = bucketName,\\nSourceKey = keyName,\\nDestinationBucket = bucketName,\\nDestinationKey = copyTargetKeyName,\\n// Information about the source object\\'s encryption.\\nCopySourceServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,', '']]]\n",
      "[[['', \"CopySourceServerSideEncryptionCustomerProvidedKey = base64Key,\\n// Information about the target object's encryption.\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = copyBase64Key,\\n};\\nawait client.CopyObjectAsync(copyRequest);\\n}\\n}\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nusing Amazon;', '']]]\n",
      "[[['', 'using Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example shows how to work with tags in Amazon Simple Storage\\n/// Service (Amazon S3) objects.\\n/// </summary>\\npublic class ObjectTag\\n{\\npublic static async Task Main()\\n{\\nstring bucketName = \"doc-example-bucket\";\\nstring keyName = \"newobject.txt\";\\nstring filePath = @\"*** file path ***\";\\n// Specify your bucket region (an example region is shown).\\nRegionEndpoint bucketRegion = RegionEndpoint.USWest2;\\nvar client = new AmazonS3Client(bucketRegion);\\nawait PutObjectsWithTagsAsync(client, bucketName, keyName, filePath);\\n}\\n/// <summary>\\n/// This method uploads an object with tags. It then shows the tag\\n/// values, changes the tags, and shows the new tags.\\n/// </summary>\\n/// <param name=\"client\">The Initialized Amazon S3 client object used\\n/// to call the methods to create and change an objects tags.</param>\\n/// <param name=\"bucketName\">A string representing the name of the\\n/// bucket where the object will be stored.</param>\\n/// <param name=\"keyName\">A string representing the key name of the\\n/// object to be tagged.</param>\\n/// <param name=\"filePath\">The directory location and file name of the\\n/// object to be uploaded to the Amazon S3 bucket.</param>\\npublic static async Task PutObjectsWithTagsAsync(IAmazonS3 client, string\\nbucketName, string keyName, string filePath)\\n{\\ntry\\n{\\n// Create an object with tags.\\nvar putRequest = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,', '']]]\n",
      "[[['', 'FilePath = filePath,\\nTagSet = new List<Tag>\\n{\\nnew Tag { Key = \"Keyx1\", Value = \"Value1\" },\\nnew Tag { Key = \"Keyx2\", Value = \"Value2\" },\\n},\\n};\\nPutObjectResponse response = await\\nclient.PutObjectAsync(putRequest);\\n// Now retrieve the new object\\'s tags.\\nGetObjectTaggingRequest getTagsRequest = new\\nGetObjectTaggingRequest()\\n{\\nBucketName = bucketName,\\nKey = keyName,\\n};\\nGetObjectTaggingResponse objectTags = await\\nclient.GetObjectTaggingAsync(getTagsRequest);\\n// Display the tag values.\\nobjectTags.Tagging\\n.ForEach(t => Console.WriteLine($\"Key: {t.Key}, Value:\\n{t.Value}\"));\\nTagging newTagSet = new Tagging()\\n{\\nTagSet = new List<Tag>\\n{\\nnew Tag { Key = \"Key3\", Value = \"Value3\" },\\nnew Tag { Key = \"Key4\", Value = \"Value4\" },\\n},\\n};\\nPutObjectTaggingRequest putObjTagsRequest = new\\nPutObjectTaggingRequest()\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nTagging = newTagSet,\\n};', '']]]\n",
      "[[['', 'PutObjectTaggingResponse response2 = await\\nclient.PutObjectTaggingAsync(putObjTagsRequest);\\n// Retrieve the tags again and show the values.\\nGetObjectTaggingRequest getTagsRequest2 = new\\nGetObjectTaggingRequest()\\n{\\nBucketName = bucketName,\\nKey = keyName,\\n};\\nGetObjectTaggingResponse objectTags2 = await\\nclient.GetObjectTaggingAsync(getTagsRequest2);\\nobjectTags2.Tagging\\n.ForEach(t => Console.WriteLine($\"Key: {t.Key}, Value:\\n{t.Value}\"));\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine(\\n$\"Error: \\'{ex.Message}\\'\");\\n}\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Get the legal hold details for an S3 object.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket of the object.</param>\\n/// <param name=\"objectKey\">The object key.</param>\\n/// <returns>The object legal hold details.</returns>\\npublic async Task<ObjectLockLegalHold> GetObjectLegalHold(string bucketName,\\nstring objectKey)\\n{\\ntry\\n{\\nvar request = new GetObjectLegalHoldRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey\\n};\\nvar response = await _amazonS3.GetObjectLegalHoldAsync(request);\\nConsole.WriteLine($\"\\\\tObject legal hold for {objectKey} in\\n{bucketName}: \" +\\n$\"\\\\n\\\\tStatus: {response.LegalHold.Status}\");\\nreturn response.LegalHold;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tUnable to fetch legal hold: \\'{ex.Message}\\'\");\\nreturn new ObjectLockLegalHold();\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Get the legal hold details for an S3 object.\\npublic ObjectLockLegalHold getObjectLegalHold(String bucketName, String\\nobjectKey) {\\ntry {\\nGetObjectLegalHoldRequest legalHoldRequest =\\nGetObjectLegalHoldRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.build();\\nGetObjectLegalHoldResponse response =\\ngetClient().getObjectLegalHold(legalHoldRequest);\\nSystem.out.println(\"Object legal hold for \" + objectKey + \" in \" +\\nbucketName +\\n\":\\\\n\\\\tStatus: \" + response.legalHold().status());\\nreturn response.legalHold();\\n} catch (S3Exception ex) {\\nSystem.out.println(\"\\\\tUnable to fetch legal hold: \\'\" +\\nex.getMessage() + \"\\'\");\\n}\\nreturn null;\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { fileURLToPath } from \"url\";\\nimport { GetObjectLegalHoldCommand, S3Client } from \"@aws-sdk/client-s3\";\\n/**\\n* @param {S3Client} client\\n* @param {string} bucketName\\n* @param {string} objectKey\\n*/\\nexport const main = async (client, bucketName, objectKey) => {\\nconst command = new GetObjectLegalHoldCommand({\\nBucket: bucketName,\\nKey: objectKey,\\n// Optionally, you can provide additional parameters\\n// ExpectedBucketOwner: \"ACCOUNT_ID\",\\n// RequestPayer: \"requester\",\\n// VersionId: \"OBJECT_VERSION_ID\",\\n});\\ntry {\\nconst response = await client.send(command);\\nconsole.log(`Legal Hold Status: ${response.LegalHold.Status}`);\\n} catch (err) {\\nconsole.error(err);\\n}\\n};\\n// Invoke main function if this file was run directly.\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\nmain(new S3Client(), \"DOC-EXAMPLE-BUCKET\", \"OBJECT_KEY\");', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using Amazon.S3;\\nusing Amazon.S3.Model;\\nusing Microsoft.Extensions.Configuration;\\nusing Microsoft.Extensions.DependencyInjection;\\nusing Microsoft.Extensions.Hosting;\\nusing Microsoft.Extensions.Logging;\\nusing Microsoft.Extensions.Logging.Console;\\nusing Microsoft.Extensions.Logging.Debug;\\nnamespace S3ObjectLockScenario;\\npublic static class S3ObjectLockWorkflow\\n{\\n/*', '']]]\n",
      "[[['', 'Before running this .NET code example, set up your development environment,\\nincluding your credentials.\\nThis .NET example performs the following tasks:\\n1. Create test Amazon Simple Storage Service (S3) buckets with different\\nlock policies.\\n2. Upload sample objects to each bucket.\\n3. Set some Legal Hold and Retention Periods on objects and buckets.\\n4. Investigate lock policies by viewing settings or attempting to delete\\nor overwrite objects.\\n5. Clean up objects and buckets.\\n*/\\npublic static S3ActionsWrapper _s3ActionsWrapper = null!;\\npublic static IConfiguration _configuration = null!;\\nprivate static string _resourcePrefix = null!;\\nprivate static string noLockBucketName = null!;\\nprivate static string lockEnabledBucketName = null!;\\nprivate static string retentionAfterCreationBucketName = null!;\\nprivate static List<string> bucketNames = new List<string>();\\nprivate static List<string> fileNames = new List<string>();\\npublic static async Task Main(string[] args)\\n{\\n// Set up dependency injection for the Amazon service.\\nusing var host = Host.CreateDefaultBuilder(args)\\n.ConfigureLogging(logging =>\\nlogging.AddFilter(\"System\", LogLevel.Debug)\\n.AddFilter<DebugLoggerProvider>(\"Microsoft\",\\nLogLevel.Information)\\n.AddFilter<ConsoleLoggerProvider>(\"Microsoft\",\\nLogLevel.Trace))\\n.ConfigureServices((_, services) =>\\nservices.AddAWSService<IAmazonS3>()\\n.AddTransient<S3ActionsWrapper>()\\n)\\n.Build();\\n_configuration = new ConfigurationBuilder()\\n.SetBasePath(Directory.GetCurrentDirectory())\\n.AddJsonFile(\"settings.json\") // Load settings from .json file.\\n.AddJsonFile(\"settings.local.json\",\\ntrue) // Optionally, load local settings.\\n.Build();', '']]]\n",
      "[[['', 'ConfigurationSetup();\\nServicesSetup(host);\\ntry\\n{\\nConsole.WriteLine(new string(\\'-\\', 80));\\nConsole.WriteLine(\"Welcome to the Amazon Simple Storage Service (S3)\\nObject Locking Workflow Scenario.\");\\nConsole.WriteLine(new string(\\'-\\', 80));\\nawait Setup(true);\\nawait DemoActionChoices();\\nConsole.WriteLine(new string(\\'-\\', 80));\\nConsole.WriteLine(\"Cleaning up resources.\");\\nConsole.WriteLine(new string(\\'-\\', 80));\\nawait Cleanup(true);\\nConsole.WriteLine(new string(\\'-\\', 80));\\nConsole.WriteLine(\"Amazon S3 Object Locking Workflow is complete.\");\\nConsole.WriteLine(new string(\\'-\\', 80));\\n}\\ncatch (Exception ex)\\n{\\nConsole.WriteLine(new string(\\'-\\', 80));\\nConsole.WriteLine($\"There was a problem: {ex.Message}\");\\nawait Cleanup(true);\\nConsole.WriteLine(new string(\\'-\\', 80));\\n}\\n}\\n/// <summary>\\n/// Populate the services for use within the console application.\\n/// </summary>\\n/// <param name=\"host\">The services host.</param>\\nprivate static void ServicesSetup(IHost host)\\n{\\n_s3ActionsWrapper = host.Services.GetRequiredService<S3ActionsWrapper>();\\n}\\n/// <summary>\\n/// Any setup operations needed.', '']]]\n",
      "[[['', '/// </summary>\\npublic static void ConfigurationSetup()\\n{\\n_resourcePrefix = _configuration[\"resourcePrefix\"] ?? \"dotnet-example\";\\nnoLockBucketName = _resourcePrefix + \"-no-lock\";\\nlockEnabledBucketName = _resourcePrefix + \"-lock-enabled\";\\nretentionAfterCreationBucketName = _resourcePrefix + \"-retention-after-\\ncreation\";\\nbucketNames.Add(noLockBucketName);\\nbucketNames.Add(lockEnabledBucketName);\\nbucketNames.Add(retentionAfterCreationBucketName);\\n}\\n// <summary>\\n/// Deploy necessary resources for the scenario.\\n/// </summary>\\n/// <param name=\"interactive\">True to run as interactive.</param>\\n/// <returns>True if successful.</returns>\\npublic static async Task<bool> Setup(bool interactive)\\n{\\nConsole.WriteLine(\\n\"\\\\nFor this workflow, we will use the AWS SDK for .NET to create\\nseveral S3\\\\n\" +\\n\"buckets and files to demonstrate working with S3 locking features.\\n\\\\n\");\\nConsole.WriteLine(new string(\\'-\\', 80));\\nConsole.WriteLine(\"Press Enter when you are ready to start.\");\\nif (interactive)\\nConsole.ReadLine();\\nConsole.WriteLine(\"\\\\nS3 buckets can be created either with or without\\nobject lock enabled.\");\\nawait _s3ActionsWrapper.CreateBucketWithObjectLock(noLockBucketName,\\nfalse);\\nawait _s3ActionsWrapper.CreateBucketWithObjectLock(lockEnabledBucketName,\\ntrue);\\nawait\\n_s3ActionsWrapper.CreateBucketWithObjectLock(retentionAfterCreationBucketName,\\nfalse);\\nConsole.WriteLine(\"Press Enter to continue.\");', '']]]\n",
      "[[['', 'if (interactive)\\nConsole.ReadLine();\\nConsole.WriteLine(\"\\\\nA bucket can be configured to use object locking\\nwith a default retention period.\");\\nawait\\n_s3ActionsWrapper.ModifyBucketDefaultRetention(retentionAfterCreationBucketName,\\ntrue,\\nObjectLockRetentionMode.Governance, DateTime.UtcNow.AddDays(1));\\nConsole.WriteLine(\"Press Enter to continue.\");\\nif (interactive)\\nConsole.ReadLine();\\nConsole.WriteLine(\"\\\\nObject lock policies can also be added to existing\\nbuckets.\");\\nawait _s3ActionsWrapper.EnableObjectLockOnBucket(lockEnabledBucketName);\\nConsole.WriteLine(\"Press Enter to continue.\");\\nif (interactive)\\nConsole.ReadLine();\\n// Upload some files to the buckets.\\nConsole.WriteLine(\"\\\\nNow let\\'s add some test files:\");\\nvar fileName = _configuration[\"exampleFileName\"] ?? \"exampleFile.txt\";\\nint fileCount = 2;\\n// Create the file if it does not already exist.\\nif (!File.Exists(fileName))\\n{\\nawait using StreamWriter sw = File.CreateText(fileName);\\nawait sw.WriteLineAsync(\\n\"This is a sample file for uploading to a bucket.\");\\n}\\nforeach (var bucketName in bucketNames)\\n{\\nfor (int i = 0; i < fileCount; i++)\\n{\\nvar numberedFileName = Path.GetFileNameWithoutExtension(fileName)\\n+ i + Path.GetExtension(fileName);\\nfileNames.Add(numberedFileName);\\nawait _s3ActionsWrapper.UploadFileAsync(bucketName,\\nnumberedFileName, fileName);\\n}', '']]]\n",
      "[[['', '}\\nConsole.WriteLine(\"Press Enter to continue.\");\\nif (interactive)\\nConsole.ReadLine();\\nif (!interactive)\\nreturn true;\\nConsole.WriteLine(\"\\\\nNow we can set some object lock policies on\\nindividual files:\");\\nforeach (var bucketName in bucketNames)\\n{\\nfor (int i = 0; i < fileNames.Count; i++)\\n{\\n// No modifications to the objects in the first bucket.\\nif (bucketName != bucketNames[0])\\n{\\nvar exampleFileName = fileNames[i];\\nswitch (i)\\n{\\ncase 0:\\n{\\nvar question =\\n$\"\\\\nWould you like to add a legal hold to\\n{exampleFileName} in {bucketName}? (y/n)\";\\nif (GetYesNoResponse(question))\\n{\\n// Set a legal hold.\\nawait\\n_s3ActionsWrapper.ModifyObjectLegalHold(bucketName, exampleFileName,\\nObjectLockLegalHoldStatus.On);\\n}\\nbreak;\\n}\\ncase 1:\\n{\\nvar question =\\n$\"\\\\nWould you like to add a 1 day Governance\\nretention period to {exampleFileName} in {bucketName}? (y/n)\" +\\n\"\\\\nReminder: Only a user with the\\ns3:BypassGovernanceRetention permission will be able to delete this file or its\\nbucket until the retention period has expired.\";\\nif (GetYesNoResponse(question))\\n{', '']]]\n",
      "[[['', '// Set a Governance mode retention period for\\n1 day.\\nawait\\n_s3ActionsWrapper.ModifyObjectRetentionPeriod(\\nbucketName, exampleFileName,\\nObjectLockRetentionMode.Governance,\\nDateTime.UtcNow.AddDays(1));\\n}\\nbreak;\\n}\\n}\\n}\\n}\\n}\\nConsole.WriteLine(new string(\\'-\\', 80));\\nreturn true;\\n}\\n// <summary>\\n/// List all of the current buckets and objects.\\n/// </summary>\\n/// <param name=\"interactive\">True to run as interactive.</param>\\n/// <returns>The list of buckets and objects.</returns>\\npublic static async Task<List<S3ObjectVersion>> ListBucketsAndObjects(bool\\ninteractive)\\n{\\nvar allObjects = new List<S3ObjectVersion>();\\nforeach (var bucketName in bucketNames)\\n{\\nvar objectsInBucket = await\\n_s3ActionsWrapper.ListBucketObjectsAndVersions(bucketName);\\nforeach (var objectKey in objectsInBucket.Versions)\\n{\\nallObjects.Add(objectKey);\\n}\\n}\\nif (interactive)\\n{\\nConsole.WriteLine(\"\\\\nCurrent buckets and objects:\\\\n\");\\nint i = 0;\\nforeach (var bucketObject in allObjects)\\n{\\ni++;', '']]]\n",
      "[[['', 'Console.WriteLine(\\n$\"{i}: {bucketObject.Key} \\\\n\\\\tBucket:\\n{bucketObject.BucketName}\\\\n\\\\tVersion: {bucketObject.VersionId}\");\\n}\\n}\\nreturn allObjects;\\n}\\n/// <summary>\\n/// Present the user with the demo action choices.\\n/// </summary>\\n/// <returns>Async task.</returns>\\npublic static async Task<bool> DemoActionChoices()\\n{\\nvar choices = new string[]{\\n\"List all files in buckets.\",\\n\"Attempt to delete a file.\",\\n\"Attempt to delete a file with retention period bypass.\",\\n\"Attempt to overwrite a file.\",\\n\"View the object and bucket retention settings for a file.\",\\n\"View the legal hold settings for a file.\",\\n\"Finish the workflow.\"};\\nvar choice = 0;\\n// Keep asking the user until they choose to move on.\\nwhile (choice != 6)\\n{\\nConsole.WriteLine(new string(\\'-\\', 80));\\nchoice = GetChoiceResponse(\\n\"\\\\nExplore the S3 locking features by selecting one of the\\nfollowing choices:\"\\n, choices);\\nConsole.WriteLine(new string(\\'-\\', 80));\\nswitch (choice)\\n{\\ncase 0:\\n{\\nawait ListBucketsAndObjects(true);\\nbreak;\\n}\\ncase 1:\\n{', '']]]\n",
      "[[['', 'Console.WriteLine(\"\\\\nEnter the number of the object to\\ndelete:\");\\nvar allFiles = await ListBucketsAndObjects(true);\\nvar fileChoice = GetChoiceResponse(null,\\nallFiles.Select(f => f.Key).ToArray());\\nawait\\n_s3ActionsWrapper.DeleteObjectFromBucket(allFiles[fileChoice].BucketName,\\nallFiles[fileChoice].Key, false, allFiles[fileChoice].VersionId);\\nbreak;\\n}\\ncase 2:\\n{\\nConsole.WriteLine(\"\\\\nEnter the number of the object to\\ndelete:\");\\nvar allFiles = await ListBucketsAndObjects(true);\\nvar fileChoice = GetChoiceResponse(null,\\nallFiles.Select(f => f.Key).ToArray());\\nawait\\n_s3ActionsWrapper.DeleteObjectFromBucket(allFiles[fileChoice].BucketName,\\nallFiles[fileChoice].Key, true, allFiles[fileChoice].VersionId);\\nbreak;\\n}\\ncase 3:\\n{\\nvar allFiles = await ListBucketsAndObjects(true);\\nConsole.WriteLine(\"\\\\nEnter the number of the object to\\noverwrite:\");\\nvar fileChoice = GetChoiceResponse(null,\\nallFiles.Select(f => f.Key).ToArray());\\n// Create the file if it does not already exist.\\nif (!File.Exists(allFiles[fileChoice].Key))\\n{\\nawait using StreamWriter sw =\\nFile.CreateText(allFiles[fileChoice].Key);\\nawait sw.WriteLineAsync(\\n\"This is a sample file for uploading to a\\nbucket.\");\\n}\\nawait\\n_s3ActionsWrapper.UploadFileAsync(allFiles[fileChoice].BucketName,\\nallFiles[fileChoice].Key, allFiles[fileChoice].Key);\\nbreak;\\n}\\ncase 4:', '']]]\n",
      "[[['', '{\\nvar allFiles = await ListBucketsAndObjects(true);\\nConsole.WriteLine(\"\\\\nEnter the number of the object and\\nbucket to view:\");\\nvar fileChoice = GetChoiceResponse(null,\\nallFiles.Select(f => f.Key).ToArray());\\nawait\\n_s3ActionsWrapper.GetObjectRetention(allFiles[fileChoice].BucketName,\\nallFiles[fileChoice].Key);\\nawait\\n_s3ActionsWrapper.GetBucketObjectLockConfiguration(allFiles[fileChoice].BucketNam\\nbreak;\\n}\\ncase 5:\\n{\\nvar allFiles = await ListBucketsAndObjects(true);\\nConsole.WriteLine(\"\\\\nEnter the number of the object to\\nview:\");\\nvar fileChoice = GetChoiceResponse(null,\\nallFiles.Select(f => f.Key).ToArray());\\nawait\\n_s3ActionsWrapper.GetObjectLegalHold(allFiles[fileChoice].BucketName,\\nallFiles[fileChoice].Key);\\nbreak;\\n}\\n}\\n}\\nreturn true;\\n}\\n// <summary>\\n/// Clean up the resources from the scenario.\\n/// </summary>\\n/// <param name=\"interactive\">True to run as interactive.</param>\\n/// <returns>True if successful.</returns>\\npublic static async Task<bool> Cleanup(bool interactive)\\n{\\nConsole.WriteLine(new string(\\'-\\', 80));\\nif (!interactive || GetYesNoResponse(\"Do you want to clean up all files\\nand buckets? (y/n) \"))\\n{\\n// Remove all locks and delete all buckets and objects.\\nvar allFiles = await ListBucketsAndObjects(false);', 'e']]]\n",
      "[[['', 'foreach (var fileInfo in allFiles)\\n{\\n// Check for a legal hold.\\nvar legalHold = await\\n_s3ActionsWrapper.GetObjectLegalHold(fileInfo.BucketName, fileInfo.Key);\\nif (legalHold?.Status?.Value == ObjectLockLegalHoldStatus.On)\\n{\\nawait\\n_s3ActionsWrapper.ModifyObjectLegalHold(fileInfo.BucketName, fileInfo.Key,\\nObjectLockLegalHoldStatus.Off);\\n}\\n// Check for a retention period.\\nvar retention = await\\n_s3ActionsWrapper.GetObjectRetention(fileInfo.BucketName, fileInfo.Key);\\nvar hasRetentionPeriod = retention?.Mode ==\\nObjectLockRetentionMode.Governance && retention.RetainUntilDate >\\nDateTime.UtcNow.Date;\\nawait\\n_s3ActionsWrapper.DeleteObjectFromBucket(fileInfo.BucketName, fileInfo.Key,\\nhasRetentionPeriod, fileInfo.VersionId);\\n}\\nforeach (var bucketName in bucketNames)\\n{\\nawait _s3ActionsWrapper.DeleteBucketByName(bucketName);\\n}\\n}\\nelse\\n{\\nConsole.WriteLine(\\n\"Ok, we\\'ll leave the resources intact.\\\\n\" +\\n\"Don\\'t forget to delete them when you\\'re done with them or you\\nmight incur unexpected charges.\"\\n);\\n}\\nConsole.WriteLine(new string(\\'-\\', 80));\\nreturn true;\\n}\\n/// <summary>\\n/// Helper method to get a yes or no response from the user.', '']]]\n",
      "[[['', '/// </summary>\\n/// <param name=\"question\">The question string to print on the console.</\\nparam>\\n/// <returns>True if the user responds with a yes.</returns>\\nprivate static bool GetYesNoResponse(string question)\\n{\\nConsole.WriteLine(question);\\nvar ynResponse = Console.ReadLine();\\nvar response = ynResponse != null && ynResponse.Equals(\"y\",\\nStringComparison.InvariantCultureIgnoreCase);\\nreturn response;\\n}\\n/// <summary>\\n/// Helper method to get a choice response from the user.\\n/// </summary>\\n/// <param name=\"question\">The question string to print on the console.</\\nparam>\\n/// <param name=\"choices\">The choices to print on the console.</param>\\n/// <returns>The index of the selected choice</returns>\\nprivate static int GetChoiceResponse(string? question, string[] choices)\\n{\\nif (question != null)\\n{\\nConsole.WriteLine(question);\\nfor (int i = 0; i < choices.Length; i++)\\n{\\nConsole.WriteLine($\"\\\\t{i + 1}. {choices[i]}\");\\n}\\n}\\nvar choiceNumber = 0;\\nwhile (choiceNumber < 1 || choiceNumber > choices.Length)\\n{\\nvar choice = Console.ReadLine();\\nInt32.TryParse(choice, out choiceNumber);\\n}\\nreturn choiceNumber - 1;\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'using System.Net;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing Microsoft.Extensions.Configuration;\\nnamespace S3ObjectLockScenario;\\n/// <summary>\\n/// Encapsulate the Amazon S3 operations.\\n/// </summary>\\npublic class S3ActionsWrapper\\n{\\nprivate readonly IAmazonS3 _amazonS3;\\n/// <summary>\\n/// Constructor for the S3ActionsWrapper.\\n/// </summary>\\n/// <param name=\"amazonS3\">The injected S3 client.</param>\\npublic S3ActionsWrapper(IAmazonS3 amazonS3, IConfiguration configuration)\\n{\\n_amazonS3 = amazonS3;\\n}\\n/// <summary>\\n/// Create a new Amazon S3 bucket with object lock actions.\\n/// </summary>\\n/// <param name=\"bucketName\">The name of the bucket to create.</param>\\n/// <param name=\"enableObjectLock\">True to enable object lock on the\\nbucket.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> CreateBucketWithObjectLock(string bucketName, bool\\nenableObjectLock)\\n{\\nConsole.WriteLine($\"\\\\tCreating bucket {bucketName} with object lock\\n{enableObjectLock}.\");\\ntry\\n{\\nvar request = new PutBucketRequest\\n{\\nBucketName = bucketName,\\nUseClientRegion = true,', '']]]\n",
      "[[['', 'ObjectLockEnabledForBucket = enableObjectLock,\\n};\\nvar response = await _amazonS3.PutBucketAsync(request);\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error creating bucket: \\'{ex.Message}\\'\");\\nreturn false;\\n}\\n}\\n/// <summary>\\n/// Enable object lock on an existing bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The name of the bucket to modify.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> EnableObjectLockOnBucket(string bucketName)\\n{\\ntry\\n{\\n// First, enable Versioning on the bucket.\\nawait _amazonS3.PutBucketVersioningAsync(new\\nPutBucketVersioningRequest()\\n{\\nBucketName = bucketName,\\nVersioningConfig = new S3BucketVersioningConfig()\\n{\\nEnableMfaDelete = false,\\nStatus = VersionStatus.Enabled\\n}\\n});\\nvar request = new PutObjectLockConfigurationRequest()\\n{\\nBucketName = bucketName,\\nObjectLockConfiguration = new ObjectLockConfiguration()\\n{\\nObjectLockEnabled = new ObjectLockEnabled(\"Enabled\"),\\n},\\n};', '']]]\n",
      "[[['', 'var response = await\\n_amazonS3.PutObjectLockConfigurationAsync(request);\\nConsole.WriteLine($\"\\\\tAdded an object lock policy to bucket\\n{bucketName}.\");\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error modifying object lock: \\'{ex.Message}\\'\");\\nreturn false;\\n}\\n}\\n/// <summary>\\n/// Set or modify a retention period on an object in an S3 bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket of the object.</param>\\n/// <param name=\"objectKey\">The key of the object.</param>\\n/// <param name=\"retention\">The retention mode.</param>\\n/// <param name=\"retainUntilDate\">The date retention expires.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> ModifyObjectRetentionPeriod(string bucketName,\\nstring objectKey, ObjectLockRetentionMode retention, DateTime\\nretainUntilDate)\\n{\\ntry\\n{\\nvar request = new PutObjectRetentionRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey,\\nRetention = new ObjectLockRetention()\\n{\\nMode = retention,\\nRetainUntilDate = retainUntilDate\\n}\\n};\\nvar response = await _amazonS3.PutObjectRetentionAsync(request);\\nConsole.WriteLine($\"\\\\tSet retention for {objectKey} in {bucketName}\\nuntil {retainUntilDate:d}.\");\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)', '']]]\n",
      "[[['', '{\\nConsole.WriteLine($\"\\\\tError modifying retention period:\\n\\'{ex.Message}\\'\");\\nreturn false;\\n}\\n}\\n/// <summary>\\n/// Set or modify a retention period on an S3 bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket to modify.</param>\\n/// <param name=\"retention\">The retention mode.</param>\\n/// <param name=\"retainUntilDate\">The date for retention until.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> ModifyBucketDefaultRetention(string bucketName, bool\\nenableObjectLock, ObjectLockRetentionMode retention, DateTime retainUntilDate)\\n{\\nvar enabledString = enableObjectLock ? \"Enabled\" : \"Disabled\";\\nvar timeDifference = retainUntilDate.Subtract(DateTime.Now);\\ntry\\n{\\n// First, enable Versioning on the bucket.\\nawait _amazonS3.PutBucketVersioningAsync(new\\nPutBucketVersioningRequest()\\n{\\nBucketName = bucketName,\\nVersioningConfig = new S3BucketVersioningConfig()\\n{\\nEnableMfaDelete = false,\\nStatus = VersionStatus.Enabled\\n}\\n});\\nvar request = new PutObjectLockConfigurationRequest()\\n{\\nBucketName = bucketName,\\nObjectLockConfiguration = new ObjectLockConfiguration()\\n{\\nObjectLockEnabled = new ObjectLockEnabled(enabledString),\\nRule = new ObjectLockRule()\\n{\\nDefaultRetention = new DefaultRetention()\\n{\\nMode = retention,', '']]]\n",
      "[[['', 'Days = timeDifference.Days // Can be specified in\\ndays or years but not both.\\n}\\n}\\n}\\n};\\nvar response = await\\n_amazonS3.PutObjectLockConfigurationAsync(request);\\nConsole.WriteLine($\"\\\\tAdded a default retention to bucket\\n{bucketName}.\");\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tError modifying object lock: \\'{ex.Message}\\'\");\\nreturn false;\\n}\\n}\\n/// <summary>\\n/// Get the retention period for an S3 object.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket of the object.</param>\\n/// <param name=\"objectKey\">The object key.</param>\\n/// <returns>The object retention details.</returns>\\npublic async Task<ObjectLockRetention> GetObjectRetention(string bucketName,\\nstring objectKey)\\n{\\ntry\\n{\\nvar request = new GetObjectRetentionRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey\\n};\\nvar response = await _amazonS3.GetObjectRetentionAsync(request);\\nConsole.WriteLine($\"\\\\tObject retention for {objectKey} in\\n{bucketName}: \" +\\n$\"\\\\n\\\\t{response.Retention.Mode} until\\n{response.Retention.RetainUntilDate:d}.\");\\nreturn response.Retention;\\n}', '']]]\n",
      "[[['', 'catch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tUnable to fetch object lock retention:\\n\\'{ex.Message}\\'\");\\nreturn new ObjectLockRetention();\\n}\\n}\\n/// <summary>\\n/// Set or modify a legal hold on an object in an S3 bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket of the object.</param>\\n/// <param name=\"objectKey\">The key of the object.</param>\\n/// <param name=\"holdStatus\">The On or Off status for the legal hold.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> ModifyObjectLegalHold(string bucketName,\\nstring objectKey, ObjectLockLegalHoldStatus holdStatus)\\n{\\ntry\\n{\\nvar request = new PutObjectLegalHoldRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey,\\nLegalHold = new ObjectLockLegalHold()\\n{\\nStatus = holdStatus\\n}\\n};\\nvar response = await _amazonS3.PutObjectLegalHoldAsync(request);\\nConsole.WriteLine($\"\\\\tModified legal hold for {objectKey} in\\n{bucketName}.\");\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tError modifying legal hold: \\'{ex.Message}\\'\");\\nreturn false;\\n}\\n}\\n/// <summary>\\n/// Get the legal hold details for an S3 object.', '']]]\n",
      "[[['', '/// </summary>\\n/// <param name=\"bucketName\">The bucket of the object.</param>\\n/// <param name=\"objectKey\">The object key.</param>\\n/// <returns>The object legal hold details.</returns>\\npublic async Task<ObjectLockLegalHold> GetObjectLegalHold(string bucketName,\\nstring objectKey)\\n{\\ntry\\n{\\nvar request = new GetObjectLegalHoldRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey\\n};\\nvar response = await _amazonS3.GetObjectLegalHoldAsync(request);\\nConsole.WriteLine($\"\\\\tObject legal hold for {objectKey} in\\n{bucketName}: \" +\\n$\"\\\\n\\\\tStatus: {response.LegalHold.Status}\");\\nreturn response.LegalHold;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tUnable to fetch legal hold: \\'{ex.Message}\\'\");\\nreturn new ObjectLockLegalHold();\\n}\\n}\\n/// <summary>\\n/// Get the object lock configuration details for an S3 bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The bucket to get details.</param>\\n/// <returns>The bucket\\'s object lock configuration details.</returns>\\npublic async Task<ObjectLockConfiguration>\\nGetBucketObjectLockConfiguration(string bucketName)\\n{\\ntry\\n{\\nvar request = new GetObjectLockConfigurationRequest()\\n{\\nBucketName = bucketName\\n};', '']]]\n",
      "[[['', 'var response = await\\n_amazonS3.GetObjectLockConfigurationAsync(request);\\nConsole.WriteLine($\"\\\\tBucket object lock config for {bucketName} in\\n{bucketName}: \" +\\n$\"\\\\n\\\\tEnabled:\\n{response.ObjectLockConfiguration.ObjectLockEnabled}\" +\\n$\"\\\\n\\\\tRule:\\n{response.ObjectLockConfiguration.Rule?.DefaultRetention}\");\\nreturn response.ObjectLockConfiguration;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tUnable to fetch object lock config:\\n\\'{ex.Message}\\'\");\\nreturn new ObjectLockConfiguration();\\n}\\n}\\n/// <summary>\\n/// Upload a file from the local computer to an Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The Amazon S3 bucket to use.</param>\\n/// <param name=\"objectName\">The object to upload.</param>\\n/// <param name=\"filePath\">The path, including file name, of the object to\\nupload.</param>\\n/// <returns>True if success.<returns>\\npublic async Task<bool> UploadFileAsync(string bucketName, string objectName,\\nstring filePath)\\n{\\nvar request = new PutObjectRequest\\n{\\nBucketName = bucketName,\\nKey = objectName,\\nFilePath = filePath,\\nChecksumAlgorithm = ChecksumAlgorithm.SHA256\\n};\\nvar response = await _amazonS3.PutObjectAsync(request);\\nif (response.HttpStatusCode == System.Net.HttpStatusCode.OK)\\n{\\nConsole.WriteLine($\"\\\\tSuccessfully uploaded {objectName} to\\n{bucketName}.\");\\nreturn true;', '']]]\n",
      "[[['', '}\\nelse\\n{\\nConsole.WriteLine($\"\\\\tCould not upload {objectName} to\\n{bucketName}.\");\\nreturn false;\\n}\\n}\\n/// <summary>\\n/// List bucket objects and versions.\\n/// </summary>\\n/// <param name=\"bucketName\">The Amazon S3 bucket to use.</param>\\n/// <returns>The list of objects and versions.</returns>\\npublic async Task<ListVersionsResponse> ListBucketObjectsAndVersions(string\\nbucketName)\\n{\\nvar request = new ListVersionsRequest()\\n{\\nBucketName = bucketName\\n};\\nvar response = await _amazonS3.ListVersionsAsync(request);\\nreturn response;\\n}\\n/// <summary>\\n/// Delete an object from a specific bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The Amazon S3 bucket to use.</param>\\n/// <param name=\"objectKey\">The key of the object to delete.</param>\\n/// <param name=\"hasRetention\">True if the object has retention settings.</\\nparam>\\n/// <param name=\"versionId\">Optional versionId.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> DeleteObjectFromBucket(string bucketName, string\\nobjectKey, bool hasRetention, string? versionId = null)\\n{\\ntry\\n{\\nvar request = new DeleteObjectRequest()\\n{\\nBucketName = bucketName,\\nKey = objectKey,', '']]]\n",
      "[[['', 'VersionId = versionId,\\n};\\nif (hasRetention)\\n{\\n// Set the BypassGovernanceRetention header\\n// if the file has retention settings.\\nrequest.BypassGovernanceRetention = true;\\n}\\nawait _amazonS3.DeleteObjectAsync(request);\\nConsole.WriteLine(\\n$\"Deleted {objectKey} in {bucketName}.\");\\nreturn true;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tUnable to delete object {objectKey} in bucket\\n{bucketName}: \" + ex.Message);\\nreturn false;\\n}\\n}\\n/// <summary>\\n/// Delete a specific bucket.\\n/// </summary>\\n/// <param name=\"bucketName\">The Amazon S3 bucket to use.</param>\\n/// <param name=\"objectKey\">The key of the object to delete.</param>\\n/// <param name=\"versionId\">Optional versionId.</param>\\n/// <returns>True if successful.</returns>\\npublic async Task<bool> DeleteBucketByName(string bucketName)\\n{\\ntry\\n{\\nvar request = new DeleteBucketRequest() { BucketName = bucketName, };\\nvar response = await _amazonS3.DeleteBucketAsync(request);\\nConsole.WriteLine($\"\\\\tDelete for {bucketName} complete.\");\\nreturn response.HttpStatusCode == HttpStatusCode.OK;\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"\\\\tUnable to delete bucket {bucketName}: \" +\\nex.Message);\\nreturn false;\\n}', '']]]\n",
      "[[['', '}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.services.s3.model.ObjectLockLegalHold;\\nimport software.amazon.awssdk.services.s3.model.ObjectLockRetention;\\nimport java.io.BufferedWriter;\\nimport java.io.IOException;\\nimport java.time.LocalDateTime;\\nimport java.time.format.DateTimeFormatter;\\nimport java.util.ArrayList;\\nimport java.util.List;\\nimport java.util.Scanner;\\nimport java.util.stream.Collectors;\\n/*\\nBefore running this Java V2 code example, set up your development\\nenvironment, including your credentials.', '']]]\n",
      "[[['', 'For more information, see the following documentation topic:\\nhttps://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/setup.html\\nThis Java example performs the following tasks:\\n1. Create test Amazon Simple Storage Service (S3) buckets with different lock\\npolicies.\\n2. Upload sample objects to each bucket.\\n3. Set some Legal Hold and Retention Periods on objects and buckets.\\n4. Investigate lock policies by viewing settings or attempting to delete or\\noverwrite objects.\\n5. Clean up objects and buckets.\\n*/\\npublic class S3ObjectLockWorkflow {\\npublic static final String DASHES = new String(new char[80]).replace(\"\\\\0\",\\n\"-\");\\nstatic String bucketName;\\nstatic S3LockActions s3LockActions;\\nprivate static final List<String> bucketNames = new ArrayList<>();\\nprivate static final List<String> fileNames = new ArrayList<>();\\npublic static void main(String[] args) {\\n// Get the current date and time to ensure bucket name is unique.\\nLocalDateTime currentTime = LocalDateTime.now();\\n// Format the date and time as a string.\\nDateTimeFormatter formatter =\\nDateTimeFormatter.ofPattern(\"yyyyMMddHHmmss\");\\nString timeStamp = currentTime.format(formatter);\\ns3LockActions = new S3LockActions();\\nbucketName = \"bucket\"+timeStamp;\\nScanner scanner = new Scanner(System.in);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"Welcome to the Amazon Simple Storage Service (S3)\\nObject Locking Workflow Scenario.\");\\nSystem.out.println(\"Press Enter to continue...\");\\nscanner.nextLine();\\nconfigurationSetup();\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);', '']]]\n",
      "[[['', 'setup();\\nSystem.out.println(\"Setup is complete. Press Enter to continue...\");\\nscanner.nextLine();\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"Lets present the user with choices.\");\\nSystem.out.println(\"Press Enter to continue...\");\\nscanner.nextLine();\\ndemoActionChoices() ;\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"Would you like to clean up the resources? (y/n)\");\\nString delAns = scanner.nextLine().trim();\\nif (delAns.equalsIgnoreCase(\"y\")) {\\ncleanup();\\nSystem.out.println(\"Clean up is complete.\");\\n}\\nSystem.out.println(\"Press Enter to continue...\");\\nscanner.nextLine();\\nSystem.out.println(DASHES);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"Amazon S3 Object Locking Workflow is complete.\");\\nSystem.out.println(DASHES);\\n}\\n// Present the user with the demo action choices.\\npublic static void demoActionChoices() {\\nString[] choices = {\\n\"List all files in buckets.\",\\n\"Attempt to delete a file.\",\\n\"Attempt to delete a file with retention period bypass.\",\\n\"Attempt to overwrite a file.\",\\n\"View the object and bucket retention settings for a file.\",\\n\"View the legal hold settings for a file.\",\\n\"Finish the workflow.\"\\n};\\nint choice = 0;\\nwhile (true) {\\nSystem.out.println(DASHES);', '']]]\n",
      "[[['', 'choice = getChoiceResponse(\"Explore the S3 locking features by\\nselecting one of the following choices:\", choices);\\nSystem.out.println(DASHES);\\nSystem.out.println(\"You selected \"+choices[choice]);\\nswitch (choice) {\\ncase 0 -> {\\ns3LockActions.listBucketsAndObjects(bucketNames, true);\\n}\\ncase 1 -> {\\nSystem.out.println(\"Enter the number of the object to\\ndelete:\");\\nList<S3InfoObject> allFiles =\\ns3LockActions.listBucketsAndObjects(bucketNames, true);\\nList<String> fileKeys = allFiles.stream().map(f ->\\nf.getKeyName()).collect(Collectors.toList());\\nString[] fileKeysArray = fileKeys.toArray(new String[0]);\\nint fileChoice = getChoiceResponse(null, fileKeysArray);\\nString objectKey = fileKeys.get(fileChoice);\\nString bucketName = allFiles.get(fileChoice).getBucketName();\\nString version = allFiles.get(fileChoice).getVersion();\\ns3LockActions.deleteObjectFromBucket(bucketName, objectKey,\\nfalse, version);\\n}\\ncase 2 -> {\\nSystem.out.println(\"Enter the number of the object to\\ndelete:\");\\nList<S3InfoObject> allFiles =\\ns3LockActions.listBucketsAndObjects(bucketNames, true);\\nList<String> fileKeys = allFiles.stream().map(f ->\\nf.getKeyName()).collect(Collectors.toList());\\nString[] fileKeysArray = fileKeys.toArray(new String[0]);\\nint fileChoice = getChoiceResponse(null, fileKeysArray);\\nString objectKey = fileKeys.get(fileChoice);\\nString bucketName = allFiles.get(fileChoice).getBucketName();\\nString version = allFiles.get(fileChoice).getVersion();\\ns3LockActions.deleteObjectFromBucket(bucketName, objectKey,\\ntrue, version);\\n}\\ncase 3 -> {\\nSystem.out.println(\"Enter the number of the object to\\noverwrite:\");', '']]]\n",
      "[[['', 'List<S3InfoObject> allFiles =\\ns3LockActions.listBucketsAndObjects(bucketNames, true);\\nList<String> fileKeys = allFiles.stream().map(f ->\\nf.getKeyName()).collect(Collectors.toList());\\nString[] fileKeysArray = fileKeys.toArray(new String[0]);\\nint fileChoice = getChoiceResponse(null, fileKeysArray);\\nString objectKey = fileKeys.get(fileChoice);\\nString bucketName = allFiles.get(fileChoice).getBucketName();\\n// Attempt to overwrite the file.\\ntry (BufferedWriter writer = new BufferedWriter(new\\njava.io.FileWriter(objectKey))) {\\nwriter.write(\"This is a modified text.\");\\n} catch (IOException e) {\\ne.printStackTrace();\\n}\\ns3LockActions.uploadFile(bucketName, objectKey, objectKey);\\n}\\ncase 4 -> {\\nSystem.out.println(\"Enter the number of the object to\\noverwrite:\");\\nList<S3InfoObject> allFiles =\\ns3LockActions.listBucketsAndObjects(bucketNames, true);\\nList<String> fileKeys = allFiles.stream().map(f ->\\nf.getKeyName()).collect(Collectors.toList());\\nString[] fileKeysArray = fileKeys.toArray(new String[0]);\\nint fileChoice = getChoiceResponse(null, fileKeysArray);\\nString objectKey = fileKeys.get(fileChoice);\\nString bucketName = allFiles.get(fileChoice).getBucketName();\\ns3LockActions.getObjectRetention(bucketName, objectKey);\\n}\\ncase 5 -> {\\nSystem.out.println(\"Enter the number of the object to\\nview:\");\\nList<S3InfoObject> allFiles =\\ns3LockActions.listBucketsAndObjects(bucketNames, true);\\nList<String> fileKeys = allFiles.stream().map(f ->\\nf.getKeyName()).collect(Collectors.toList());\\nString[] fileKeysArray = fileKeys.toArray(new String[0]);\\nint fileChoice = getChoiceResponse(null, fileKeysArray);\\nString objectKey = fileKeys.get(fileChoice);', '']]]\n",
      "[[['', 'String bucketName = allFiles.get(fileChoice).getBucketName();\\ns3LockActions.getObjectLegalHold(bucketName, objectKey);\\ns3LockActions.getBucketObjectLockConfiguration(bucketName);\\n}\\ncase 6 -> {\\nSystem.out.println(\"Exiting the workflow...\");\\nreturn;\\n}\\ndefault -> {\\nSystem.out.println(\"Invalid choice. Please select again.\");\\n}\\n}\\n}\\n}\\n// Clean up the resources from the scenario.\\nprivate static void cleanup() {\\nList<S3InfoObject> allFiles =\\ns3LockActions.listBucketsAndObjects(bucketNames, false);\\nfor (S3InfoObject fileInfo : allFiles) {\\nString bucketName = fileInfo.getBucketName();\\nString key = fileInfo.getKeyName();\\nString version = fileInfo.getVersion();\\nif (bucketName.contains(\"lock-enabled\") ||\\n(bucketName.contains(\"retention-after-creation\"))) {\\nObjectLockLegalHold legalHold =\\ns3LockActions.getObjectLegalHold(bucketName, key);\\nif (legalHold != null) {\\nString holdStatus = legalHold.status().name();\\nSystem.out.println(holdStatus);\\nif (holdStatus.compareTo(\"ON\") == 0) {\\ns3LockActions.modifyObjectLegalHold(bucketName, key,\\nfalse);\\n}\\n}\\n// Check for a retention period.\\nObjectLockRetention retention =\\ns3LockActions.getObjectRetention(bucketName, key);\\nboolean hasRetentionPeriod ;\\nhasRetentionPeriod = retention != null;\\ns3LockActions.deleteObjectFromBucket(bucketName,\\nkey,hasRetentionPeriod, version);', '']]]\n",
      "[[['', '} else {\\nSystem.out.println(bucketName +\" objects do not have a legal\\nlock\");\\ns3LockActions.deleteObjectFromBucket(bucketName, key,false,\\nversion);\\n}\\n}\\n// Delete the buckets.\\nSystem.out.println(\"Delete \"+bucketName);\\nfor (String bucket : bucketNames){\\ns3LockActions.deleteBucketByName(bucket);\\n}\\n}\\nprivate static void setup() {\\nScanner scanner = new Scanner(System.in);\\nSystem.out.println(\"\"\"\\nFor this workflow, we will use the AWS SDK for Java to create\\nseveral S3\\nbuckets and files to demonstrate working with S3 locking\\nfeatures.\\n\"\"\");\\nSystem.out.println(\"S3 buckets can be created either with or without\\nobject lock enabled.\");\\nSystem.out.println(\"Press Enter to continue...\");\\nscanner.nextLine();\\n// Create three S3 buckets.\\ns3LockActions.createBucketWithLockOptions(false, bucketNames.get(0));\\ns3LockActions.createBucketWithLockOptions(true, bucketNames.get(1));\\ns3LockActions.createBucketWithLockOptions(false, bucketNames.get(2));\\nSystem.out.println(\"Press Enter to continue.\");\\nscanner.nextLine();\\nSystem.out.println(\"Bucket \"+bucketNames.get(2) +\" will be configured to\\nuse object locking with a default retention period.\");\\ns3LockActions.modifyBucketDefaultRetention(bucketNames.get(2));\\nSystem.out.println(\"Press Enter to continue.\");\\nscanner.nextLine();', '']]]\n",
      "[[['', 'System.out.println(\"Object lock policies can also be added to existing\\nbuckets. For this example, we will use \"+bucketNames.get(1));\\ns3LockActions.enableObjectLockOnBucket(bucketNames.get(1));\\nSystem.out.println(\"Press Enter to continue.\");\\nscanner.nextLine();\\n// Upload some files to the buckets.\\nSystem.out.println(\"Now let\\'s add some test files:\");\\nString fileName = \"exampleFile.txt\";\\nint fileCount = 2;\\ntry (BufferedWriter writer = new BufferedWriter(new\\njava.io.FileWriter(fileName))) {\\nwriter.write(\"This is a sample file for uploading to a bucket.\");\\n} catch (IOException e) {\\ne.printStackTrace();\\n}\\nfor (String bucketName : bucketNames){\\nfor (int i = 0; i < fileCount; i++) {\\n// Get the file name without extension.\\nString fileNameWithoutExtension =\\njava.nio.file.Paths.get(fileName).getFileName().toString();\\nint extensionIndex = fileNameWithoutExtension.lastIndexOf(\\'.\\');\\nif (extensionIndex > 0) {\\nfileNameWithoutExtension =\\nfileNameWithoutExtension.substring(0, extensionIndex);\\n}\\n// Create the numbered file names.\\nString numberedFileName = fileNameWithoutExtension + i +\\ngetFileExtension(fileName);\\nfileNames.add(numberedFileName);\\ns3LockActions.uploadFile(bucketName, numberedFileName, fileName);\\n}\\n}\\nString question = null;\\nSystem.out.print(\"Press Enter to continue...\");\\nscanner.nextLine();\\nSystem.out.println(\"Now we can set some object lock policies on\\nindividual files:\");\\nfor (String bucketName : bucketNames) {\\nfor (int i = 0; i < fileNames.size(); i++){', '']]]\n",
      "[[['', '// No modifications to the objects in the first bucket.\\nif (!bucketName.equals(bucketNames.get(0))) {\\nString exampleFileName = fileNames.get(i);\\nswitch (i) {\\ncase 0 -> {\\nquestion = \"Would you like to add a legal hold to \" +\\nexampleFileName + \" in \" + bucketName + \" (y/n)?\";\\nSystem.out.println(question);\\nString ans = scanner.nextLine().trim();\\nif (ans.equalsIgnoreCase(\"y\")) {\\nSystem.out.println(\"**** You have selected to put\\na legal hold \" + exampleFileName);\\n// Set a legal hold.\\ns3LockActions.modifyObjectLegalHold(bucketName,\\nexampleFileName, true);\\n}\\n}\\ncase 1 -> {\\n\"\"\"\\nWould you like to add a 1 day Governance\\nretention period to %s in %s (y/n)?\\nReminder: Only a user with the\\ns3:BypassGovernanceRetention permission will be able to delete this file or its\\nbucket until the retention period has expired.\\n\"\"\".formatted(exampleFileName, bucketName);\\nSystem.out.println(question);\\nString ans2 = scanner.nextLine().trim();\\nif (ans2.equalsIgnoreCase(\"y\")) {\\ns3LockActions.modifyObjectRetentionPeriod(bucketName, exampleFileName);\\n}\\n}\\n}\\n}\\n}\\n}\\n}\\n// Get file extension.\\nprivate static String getFileExtension(String fileName) {\\nint dotIndex = fileName.lastIndexOf(\\'.\\');\\nif (dotIndex > 0) {', '']]]\n",
      "[[['', 'return fileName.substring(dotIndex);\\n}\\nreturn \"\";\\n}\\npublic static void configurationSetup() {\\nString noLockBucketName = bucketName + \"-no-lock\";\\nString lockEnabledBucketName = bucketName + \"-lock-enabled\";\\nString retentionAfterCreationBucketName = bucketName + \"-retention-after-\\ncreation\";\\nbucketNames.add(noLockBucketName);\\nbucketNames.add(lockEnabledBucketName);\\nbucketNames.add(retentionAfterCreationBucketName);\\n}\\npublic static int getChoiceResponse(String question, String[] choices) {\\nScanner scanner = new Scanner(System.in);\\nif (question != null) {\\nSystem.out.println(question);\\nfor (int i = 0; i < choices.length; i++) {\\nSystem.out.println(\"\\\\t\" + (i + 1) + \". \" + choices[i]);\\n}\\n}\\nint choiceNumber = 0;\\nwhile (choiceNumber < 1 || choiceNumber > choices.length) {\\nString choice = scanner.nextLine();\\ntry {\\nchoiceNumber = Integer.parseInt(choice);\\n} catch (NumberFormatException e) {\\nSystem.out.println(\"Invalid choice. Please enter a valid\\nnumber.\");\\n}\\n}\\nreturn choiceNumber - 1;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import software.amazon.awssdk.regions.Region;', '']]]\n",
      "[[['', 'import software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.BucketVersioningStatus;\\nimport software.amazon.awssdk.services.s3.model.ChecksumAlgorithm;\\nimport software.amazon.awssdk.services.s3.model.CreateBucketRequest;\\nimport software.amazon.awssdk.services.s3.model.DefaultRetention;\\nimport software.amazon.awssdk.services.s3.model.DeleteBucketRequest;\\nimport software.amazon.awssdk.services.s3.model.DeleteObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.GetObjectLegalHoldRequest;\\nimport software.amazon.awssdk.services.s3.model.GetObjectLegalHoldResponse;\\nimport\\nsoftware.amazon.awssdk.services.s3.model.GetObjectLockConfigurationRequest;\\nimport\\nsoftware.amazon.awssdk.services.s3.model.GetObjectLockConfigurationResponse;\\nimport software.amazon.awssdk.services.s3.model.GetObjectRetentionRequest;\\nimport software.amazon.awssdk.services.s3.model.GetObjectRetentionResponse;\\nimport software.amazon.awssdk.services.s3.model.HeadBucketRequest;\\nimport software.amazon.awssdk.services.s3.model.ListObjectVersionsRequest;\\nimport software.amazon.awssdk.services.s3.model.ListObjectVersionsResponse;\\nimport software.amazon.awssdk.services.s3.model.MFADelete;\\nimport software.amazon.awssdk.services.s3.model.ObjectLockConfiguration;\\nimport software.amazon.awssdk.services.s3.model.ObjectLockEnabled;\\nimport software.amazon.awssdk.services.s3.model.ObjectLockLegalHold;\\nimport software.amazon.awssdk.services.s3.model.ObjectLockLegalHoldStatus;\\nimport software.amazon.awssdk.services.s3.model.ObjectLockRetention;\\nimport software.amazon.awssdk.services.s3.model.ObjectLockRetentionMode;\\nimport software.amazon.awssdk.services.s3.model.ObjectLockRule;\\nimport software.amazon.awssdk.services.s3.model.PutBucketVersioningRequest;\\nimport software.amazon.awssdk.services.s3.model.PutObjectLegalHoldRequest;\\nimport\\nsoftware.amazon.awssdk.services.s3.model.PutObjectLockConfigurationRequest;\\nimport software.amazon.awssdk.services.s3.model.PutObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.PutObjectResponse;\\nimport software.amazon.awssdk.services.s3.model.PutObjectRetentionRequest;\\nimport software.amazon.awssdk.services.s3.model.S3Exception;\\nimport software.amazon.awssdk.services.s3.model.VersioningConfiguration;\\nimport software.amazon.awssdk.services.s3.waiters.S3Waiter;\\nimport java.nio.file.Path;\\nimport java.nio.file.Paths;\\nimport java.time.Instant;\\nimport java.time.ZoneId;\\nimport java.time.ZonedDateTime;\\nimport java.time.format.DateTimeFormatter;\\nimport java.time.temporal.ChronoUnit;\\nimport java.util.List;', '']]]\n",
      "[[['', 'import java.util.concurrent.atomic.AtomicInteger;\\nimport java.util.stream.Collectors;\\n// Contains application logic for the Amazon S3 operations used in this workflow.\\npublic class S3LockActions {\\nprivate static S3Client getClient() {\\nreturn S3Client.builder()\\n.region(Region.US_EAST_1)\\n.build();\\n}\\n// Set or modify a retention period on an object in an S3 bucket.\\npublic void modifyObjectRetentionPeriod(String bucketName, String objectKey)\\n{\\n// Calculate the instant one day from now.\\nInstant futureInstant = Instant.now().plus(1, ChronoUnit.DAYS);\\n// Convert the Instant to a ZonedDateTime object with a specific time\\nzone.\\nZonedDateTime zonedDateTime =\\nfutureInstant.atZone(ZoneId.systemDefault());\\n// Define a formatter for human-readable output.\\nDateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd\\nHH:mm:ss\");\\n// Format the ZonedDateTime object to a human-readable date string.\\nString humanReadableDate = formatter.format(zonedDateTime);\\n// Print the formatted date string.\\nSystem.out.println(\"Formatted Date: \" + humanReadableDate);\\nObjectLockRetention retention = ObjectLockRetention.builder()\\n.mode(ObjectLockRetentionMode.GOVERNANCE)\\n.retainUntilDate(futureInstant)\\n.build();\\nPutObjectRetentionRequest retentionRequest =\\nPutObjectRetentionRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.retention(retention)\\n.build();', '']]]\n",
      "[[['', 'getClient().putObjectRetention(retentionRequest);\\nSystem.out.println(\"Set retention for \"+objectKey +\" in \" +bucketName +\"\\nuntil \"+ humanReadableDate +\".\");\\n}\\n// Get the legal hold details for an S3 object.\\npublic ObjectLockLegalHold getObjectLegalHold(String bucketName, String\\nobjectKey) {\\ntry {\\nGetObjectLegalHoldRequest legalHoldRequest =\\nGetObjectLegalHoldRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.build();\\nGetObjectLegalHoldResponse response =\\ngetClient().getObjectLegalHold(legalHoldRequest);\\nSystem.out.println(\"Object legal hold for \" + objectKey + \" in \" +\\nbucketName +\\n\":\\\\n\\\\tStatus: \" + response.legalHold().status());\\nreturn response.legalHold();\\n} catch (S3Exception ex) {\\nSystem.out.println(\"\\\\tUnable to fetch legal hold: \\'\" +\\nex.getMessage() + \"\\'\");\\n}\\nreturn null;\\n}\\n// Create a new Amazon S3 bucket with object lock options.\\npublic void createBucketWithLockOptions(boolean enableObjectLock, String\\nbucketName) {\\nS3Waiter s3Waiter = getClient().waiter();\\nCreateBucketRequest bucketRequest = CreateBucketRequest.builder()\\n.bucket(bucketName)\\n.objectLockEnabledForBucket(enableObjectLock)\\n.build();\\ngetClient().createBucket(bucketRequest);\\nHeadBucketRequest bucketRequestWait = HeadBucketRequest.builder()\\n.bucket(bucketName)\\n.build();', '']]]\n",
      "[[['', '// Wait until the bucket is created and print out the response.\\ns3Waiter.waitUntilBucketExists(bucketRequestWait);\\nSystem.out.println(bucketName + \" is ready\");\\n}\\npublic List<S3InfoObject> listBucketsAndObjects(List<String> bucketNames,\\nBoolean interactive) {\\nAtomicInteger counter = new AtomicInteger(0); // Initialize counter.\\nreturn bucketNames.stream()\\n.flatMap(bucketName ->\\nlistBucketObjectsAndVersions(bucketName).versions().stream()\\n.map(version -> {\\nS3InfoObject s3InfoObject = new S3InfoObject();\\ns3InfoObject.setBucketName(bucketName);\\ns3InfoObject.setVersion(version.versionId());\\ns3InfoObject.setKeyName(version.key());\\nreturn s3InfoObject;\\n}))\\n.peek(s3InfoObject -> {\\nint i = counter.incrementAndGet(); // Increment and get the\\nupdated value.\\nif (interactive) {\\nSystem.out.println(i + \": \"+ s3InfoObject.getKeyName());\\nSystem.out.printf(\"%5s Bucket name: %s\\\\n\", \"\",\\ns3InfoObject.getBucketName());\\nSystem.out.printf(\"%5s Version: %s\\\\n\", \"\",\\ns3InfoObject.getVersion());\\n}\\n})\\n.collect(Collectors.toList());\\n}\\npublic ListObjectVersionsResponse listBucketObjectsAndVersions(String\\nbucketName) {\\nListObjectVersionsRequest versionsRequest =\\nListObjectVersionsRequest.builder()\\n.bucket(bucketName)\\n.build();\\nreturn getClient().listObjectVersions(versionsRequest);\\n}\\n// Set or modify a retention period on an S3 bucket.\\npublic void modifyBucketDefaultRetention(String bucketName) {', '']]]\n",
      "[[['', 'VersioningConfiguration versioningConfiguration =\\nVersioningConfiguration.builder()\\n.mfaDelete(MFADelete.DISABLED)\\n.status(BucketVersioningStatus.ENABLED)\\n.build();\\nPutBucketVersioningRequest versioningRequest =\\nPutBucketVersioningRequest.builder()\\n.bucket(bucketName)\\n.versioningConfiguration(versioningConfiguration)\\n.build();\\ngetClient().putBucketVersioning(versioningRequest);\\nDefaultRetention rention = DefaultRetention.builder()\\n.days(1)\\n.mode(ObjectLockRetentionMode.GOVERNANCE)\\n.build();\\nObjectLockRule lockRule = ObjectLockRule.builder()\\n.defaultRetention(rention)\\n.build();\\nObjectLockConfiguration objectLockConfiguration =\\nObjectLockConfiguration.builder()\\n.objectLockEnabled(ObjectLockEnabled.ENABLED)\\n.rule(lockRule)\\n.build();\\nPutObjectLockConfigurationRequest putObjectLockConfigurationRequest =\\nPutObjectLockConfigurationRequest.builder()\\n.bucket(bucketName)\\n.objectLockConfiguration(objectLockConfiguration)\\n.build();\\ngetClient().putObjectLockConfiguration(putObjectLockConfigurationRequest) ;\\nSystem.out.println(\"Added a default retention to bucket \"+bucketName\\n+\".\");\\n}\\n// Enable object lock on an existing bucket.\\npublic void enableObjectLockOnBucket(String bucketName) {\\ntry {', '']]]\n",
      "[[['', 'VersioningConfiguration versioningConfiguration =\\nVersioningConfiguration.builder()\\n.status(BucketVersioningStatus.ENABLED)\\n.build();\\nPutBucketVersioningRequest putBucketVersioningRequest =\\nPutBucketVersioningRequest.builder()\\n.bucket(bucketName)\\n.versioningConfiguration(versioningConfiguration)\\n.build();\\n// Enable versioning on the bucket.\\ngetClient().putBucketVersioning(putBucketVersioningRequest);\\nPutObjectLockConfigurationRequest request =\\nPutObjectLockConfigurationRequest.builder()\\n.bucket(bucketName)\\n.objectLockConfiguration(ObjectLockConfiguration.builder()\\n.objectLockEnabled(ObjectLockEnabled.ENABLED)\\n.build())\\n.build();\\ngetClient().putObjectLockConfiguration(request);\\nSystem.out.println(\"Successfully enabled object lock on\\n\"+bucketName);\\n} catch (S3Exception ex) {\\nSystem.out.println(\"Error modifying object lock: \\'\" + ex.getMessage()\\n+ \"\\'\");\\n}\\n}\\npublic void uploadFile(String bucketName, String objectName, String filePath)\\n{\\nPath file = Paths.get(filePath);\\nPutObjectRequest request = PutObjectRequest.builder()\\n.bucket(bucketName)\\n.key(objectName)\\n.checksumAlgorithm(ChecksumAlgorithm.SHA256)\\n.build();\\nPutObjectResponse response = getClient().putObject(request, file);\\nif (response != null) {\\nSystem.out.println(\"\\\\tSuccessfully uploaded \" + objectName + \" to \" +\\nbucketName + \".\");', '']]]\n",
      "[[['', '} else {\\nSystem.out.println(\"\\\\tCould not upload \" + objectName + \" to \" +\\nbucketName + \".\");\\n}\\n}\\n// Set or modify a legal hold on an object in an S3 bucket.\\npublic void modifyObjectLegalHold(String bucketName, String objectKey,\\nboolean legalHoldOn) {\\nObjectLockLegalHold legalHold ;\\nif (legalHoldOn) {\\nlegalHold = ObjectLockLegalHold.builder()\\n.status(ObjectLockLegalHoldStatus.ON)\\n.build();\\n} else {\\nlegalHold = ObjectLockLegalHold.builder()\\n.status(ObjectLockLegalHoldStatus.OFF)\\n.build();\\n}\\nPutObjectLegalHoldRequest legalHoldRequest =\\nPutObjectLegalHoldRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.legalHold(legalHold)\\n.build();\\ngetClient().putObjectLegalHold(legalHoldRequest) ;\\nSystem.out.println(\"Modified legal hold for \"+ objectKey +\" in\\n\"+bucketName +\".\");\\n}\\n// Delete an object from a specific bucket.\\npublic void deleteObjectFromBucket(String bucketName, String objectKey,\\nboolean hasRetention, String versionId) {\\ntry {\\nDeleteObjectRequest objectRequest;\\nif (hasRetention) {\\nobjectRequest = DeleteObjectRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.versionId(versionId)\\n.bypassGovernanceRetention(true)\\n.build();', '']]]\n",
      "[[['', '} else {\\nobjectRequest = DeleteObjectRequest.builder()\\n.bucket(bucketName)\\n.key(objectKey)\\n.versionId(versionId)\\n.build();\\n}\\ngetClient().deleteObject(objectRequest) ;\\nSystem.out.println(\"The object was successfully deleted\");\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\n}\\n}\\n// Get the retention period for an S3 object.\\npublic ObjectLockRetention getObjectRetention(String bucketName, String key){\\ntry {\\nGetObjectRetentionRequest retentionRequest =\\nGetObjectRetentionRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.build();\\nGetObjectRetentionResponse response =\\ngetClient().getObjectRetention(retentionRequest);\\nSystem.out.println(\"tObject retention for \"+key +\"\\nin \"+ bucketName +\": \" + response.retention().mode() +\" until \"+\\nresponse.retention().retainUntilDate() +\".\");\\nreturn response.retention();\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\nreturn null;\\n}\\n}\\npublic void deleteBucketByName(String bucketName) {\\ntry {\\nDeleteBucketRequest request = DeleteBucketRequest.builder()\\n.bucket(bucketName)\\n.build();', '']]]\n",
      "[[['', 'getClient().deleteBucket(request);\\nSystem.out.println(bucketName +\" was deleted.\");\\n} catch (S3Exception e) {\\nSystem.err.println(e.awsErrorDetails().errorMessage());\\n}\\n}\\n// Get the object lock configuration details for an S3 bucket.\\npublic void getBucketObjectLockConfiguration(String bucketName) {\\nGetObjectLockConfigurationRequest objectLockConfigurationRequest =\\nGetObjectLockConfigurationRequest.builder()\\n.bucket(bucketName)\\n.build();\\nGetObjectLockConfigurationResponse response =\\ngetClient().getObjectLockConfiguration(objectLockConfigurationRequest);\\nSystem.out.println(\"Bucket object lock config for \"+bucketName +\": \");\\nSystem.out.println(\"\\\\tEnabled:\\n\"+response.objectLockConfiguration().objectLockEnabled());\\nSystem.out.println(\"\\\\tRule: \"+\\nresponse.objectLockConfiguration().rule().defaultRetention());\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport * as Scenarios from \"@aws-doc-sdk-examples/lib/scenario/index.js\";\\nimport {\\nexitOnFalse,\\nloadState,\\nsaveState,\\n} from \"@aws-doc-sdk-examples/lib/scenario/steps-common.js\";\\nimport { welcome, welcomeContinue } from \"./welcome.steps.js\";\\nimport {\\nconfirmCreateBuckets,\\nconfirmPopulateBuckets,\\nconfirmSetLegalHoldFileEnabled,\\nconfirmSetLegalHoldFileRetention,\\nconfirmSetRetentionPeriodFileEnabled,\\nconfirmSetRetentionPeriodFileRetention,\\nconfirmUpdateLockPolicy,\\nconfirmUpdateRetention,\\ncreateBuckets,\\ncreateBucketsAction,\\npopulateBuckets,\\npopulateBucketsAction,\\nsetLegalHoldFileEnabledAction,\\nsetLegalHoldFileRetentionAction,\\nsetRetentionPeriodFileEnabledAction,\\nsetRetentionPeriodFileRetentionAction,\\nupdateLockPolicy,\\nupdateLockPolicyAction,', '']]]\n",
      "[[['', 'updateRetention,\\nupdateRetentionAction,\\n} from \"./setup.steps.js\";\\n/**\\n* @param {Scenarios} scenarios\\n* @param {Record<string, any>} initialState\\n*/\\nexport const getWorkflowStages = (scenarios, initialState = {}) => {\\nconst client = new S3Client({});\\nreturn {\\ndeploy: new scenarios.Scenario(\\n\"S3 Object Locking - Deploy\",\\n[\\nwelcome(scenarios),\\nwelcomeContinue(scenarios),\\nexitOnFalse(scenarios, \"welcomeContinue\"),\\ncreateBuckets(scenarios),\\nconfirmCreateBuckets(scenarios),\\nexitOnFalse(scenarios, \"confirmCreateBuckets\"),\\ncreateBucketsAction(scenarios, client),\\nupdateRetention(scenarios),\\nconfirmUpdateRetention(scenarios),\\nexitOnFalse(scenarios, \"confirmUpdateRetention\"),\\nupdateRetentionAction(scenarios, client),\\npopulateBuckets(scenarios),\\nconfirmPopulateBuckets(scenarios),\\nexitOnFalse(scenarios, \"confirmPopulateBuckets\"),\\npopulateBucketsAction(scenarios, client),\\nupdateLockPolicy(scenarios),\\nconfirmUpdateLockPolicy(scenarios),\\nexitOnFalse(scenarios, \"confirmUpdateLockPolicy\"),\\nupdateLockPolicyAction(scenarios, client),\\nconfirmSetLegalHoldFileEnabled(scenarios),\\nsetLegalHoldFileEnabledAction(scenarios, client),\\nconfirmSetRetentionPeriodFileEnabled(scenarios),\\nsetRetentionPeriodFileEnabledAction(scenarios, client),\\nconfirmSetLegalHoldFileRetention(scenarios),\\nsetLegalHoldFileRetentionAction(scenarios, client),\\nconfirmSetRetentionPeriodFileRetention(scenarios),\\nsetRetentionPeriodFileRetentionAction(scenarios, client),\\nsaveState,\\n],', '']]]\n",
      "[[['', 'initialState,\\n),\\ndemo: new scenarios.Scenario(\\n\"S3 Object Locking - Demo\",\\n[loadState, replAction(scenarios, client)],\\ninitialState,\\n),\\nclean: new scenarios.Scenario(\\n\"S3 Object Locking - Destroy\",\\n[\\nloadState,\\nconfirmCleanup(scenarios),\\nexitOnFalse(scenarios, \"confirmCleanup\"),\\ncleanupAction(scenarios, client),\\n],\\ninitialState,\\n),\\n};\\n};\\n// Call function if run directly\\nimport { fileURLToPath } from \"url\";\\nimport { S3Client } from \"@aws-sdk/client-s3\";\\nimport { cleanupAction, confirmCleanup } from \"./clean.steps.js\";\\nimport { replAction } from \"./repl.steps.js\";\\nif (process.argv[1] === fileURLToPath(import.meta.url)) {\\nconst objectLockingScenarios = getWorkflowStages(Scenarios);\\nScenarios.parseScenarioArgs(objectLockingScenarios);\\n}', ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\n/**\\n* @typedef {import(\"@aws-doc-sdk-examples/lib/scenario/index.js\")} Scenarios\\n*/\\n/**\\n* @param {Scenarios} scenarios\\n*/', '']]]\n",
      "[[['', 'const welcome = (scenarios) =>\\nnew scenarios.ScenarioOutput(\\n\"welcome\",\\n`Welcome to the Amazon Simple Storage Service (S3) Object Locking Workflow\\nScenario. For this workflow, we will use the AWS SDK for JavaScript to create\\nseveral S3 buckets and files to demonstrate working with S3 locking features.`,\\n{ header: true },\\n);\\n/**\\n* @param {Scenarios} scenarios\\n*/\\nconst welcomeContinue = (scenarios) =>\\nnew scenarios.ScenarioInput(\\n\"welcomeContinue\",\\n\"Press Enter when you are ready to start.\",\\n{ type: \"confirm\" },\\n);\\nexport { welcome, welcomeContinue };', ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport {\\nBucketVersioningStatus,\\nChecksumAlgorithm,\\nCreateBucketCommand,\\nMFADeleteStatus,\\nPutBucketVersioningCommand,\\nPutObjectCommand,\\nPutObjectLockConfigurationCommand,\\nPutObjectLegalHoldCommand,\\nPutObjectRetentionCommand,\\nObjectLockLegalHoldStatus,\\nObjectLockRetentionMode,\\n} from \"@aws-sdk/client-s3\";\\n/**\\n* @typedef {import(\"@aws-doc-sdk-examples/lib/scenario/index.js\")} Scenarios\\n*/', '']]]\n",
      "[[['', '/**\\n* @typedef {import(\"@aws-sdk/client-s3\").S3Client} S3Client\\n*/\\nconst bucketPrefix = \"js-object-locking\";\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst createBuckets = (scenarios) =>\\nnew scenarios.ScenarioOutput(\\n\"createBuckets\",\\n`The following buckets will be created:\\n${bucketPrefix}-no-lock with object lock False.\\n${bucketPrefix}-lock-enabled with object lock True.\\n${bucketPrefix}-retention-after-creation with object lock False.`,\\n{ preformatted: true },\\n);\\n/**\\n* @param {Scenarios} scenarios\\n*/\\nconst confirmCreateBuckets = (scenarios) =>\\nnew scenarios.ScenarioInput(\"confirmCreateBuckets\", \"Create the buckets?\", {\\ntype: \"confirm\",\\n});\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst createBucketsAction = (scenarios, client) =>\\nnew scenarios.ScenarioAction(\"createBucketsAction\", async (state) => {\\nconst noLockBucketName = `${bucketPrefix}-no-lock`;\\nconst lockEnabledBucketName = `${bucketPrefix}-lock-enabled`;\\nconst retentionBucketName = `${bucketPrefix}-retention-after-creation`;\\nawait client.send(new CreateBucketCommand({ Bucket: noLockBucketName }));\\nawait client.send(\\nnew CreateBucketCommand({\\nBucket: lockEnabledBucketName,\\nObjectLockEnabledForBucket: true,', '']]]\n",
      "[[['', '}),\\n);\\nawait client.send(new CreateBucketCommand({ Bucket: retentionBucketName }));\\nstate.noLockBucketName = noLockBucketName;\\nstate.lockEnabledBucketName = lockEnabledBucketName;\\nstate.retentionBucketName = retentionBucketName;\\n});\\n/**\\n* @param {Scenarios} scenarios\\n*/\\nconst populateBuckets = (scenarios) =>\\nnew scenarios.ScenarioOutput(\\n\"populateBuckets\",\\n`The following test files will be created:\\nfile0.txt in ${bucketPrefix}-no-lock.\\nfile1.txt in ${bucketPrefix}-no-lock.\\nfile0.txt in ${bucketPrefix}-lock-enabled.\\nfile1.txt in ${bucketPrefix}-lock-enabled.\\nfile0.txt in ${bucketPrefix}-retention-after-creation.\\nfile1.txt in ${bucketPrefix}-retention-after-creation.`,\\n{ preformatted: true },\\n);\\n/**\\n* @param {Scenarios} scenarios\\n*/\\nconst confirmPopulateBuckets = (scenarios) =>\\nnew scenarios.ScenarioInput(\\n\"confirmPopulateBuckets\",\\n\"Populate the buckets?\",\\n{ type: \"confirm\" },\\n);\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst populateBucketsAction = (scenarios, client) =>\\nnew scenarios.ScenarioAction(\"populateBucketsAction\", async (state) => {\\nawait client.send(\\nnew PutObjectCommand({\\nBucket: state.noLockBucketName,', '']]]\n",
      "[[['', 'Key: \"file0.txt\",\\nBody: \"Content\",\\nChecksumAlgorithm: ChecksumAlgorithm.SHA256,\\n}),\\n);\\nawait client.send(\\nnew PutObjectCommand({\\nBucket: state.noLockBucketName,\\nKey: \"file1.txt\",\\nBody: \"Content\",\\nChecksumAlgorithm: ChecksumAlgorithm.SHA256,\\n}),\\n);\\nawait client.send(\\nnew PutObjectCommand({\\nBucket: state.lockEnabledBucketName,\\nKey: \"file0.txt\",\\nBody: \"Content\",\\nChecksumAlgorithm: ChecksumAlgorithm.SHA256,\\n}),\\n);\\nawait client.send(\\nnew PutObjectCommand({\\nBucket: state.lockEnabledBucketName,\\nKey: \"file1.txt\",\\nBody: \"Content\",\\nChecksumAlgorithm: ChecksumAlgorithm.SHA256,\\n}),\\n);\\nawait client.send(\\nnew PutObjectCommand({\\nBucket: state.retentionBucketName,\\nKey: \"file0.txt\",\\nBody: \"Content\",\\nChecksumAlgorithm: ChecksumAlgorithm.SHA256,\\n}),\\n);\\nawait client.send(\\nnew PutObjectCommand({\\nBucket: state.retentionBucketName,\\nKey: \"file1.txt\",\\nBody: \"Content\",\\nChecksumAlgorithm: ChecksumAlgorithm.SHA256,\\n}),', '']]]\n",
      "[[['', ');\\n});\\n/**\\n* @param {Scenarios} scenarios\\n*/\\nconst updateRetention = (scenarios) =>\\nnew scenarios.ScenarioOutput(\\n\"updateRetention\",\\n`A bucket can be configured to use object locking with a default retention\\nperiod.\\nA default retention period will be configured for ${bucketPrefix}-retention-\\nafter-creation.`,\\n{ preformatted: true },\\n);\\n/**\\n* @param {Scenarios} scenarios\\n*/\\nconst confirmUpdateRetention = (scenarios) =>\\nnew scenarios.ScenarioInput(\\n\"confirmUpdateRetention\",\\n\"Configure default retention period?\",\\n{ type: \"confirm\" },\\n);\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst updateRetentionAction = (scenarios, client) =>\\nnew scenarios.ScenarioAction(\"updateRetentionAction\", async (state) => {\\nawait client.send(\\nnew PutBucketVersioningCommand({\\nBucket: state.retentionBucketName,\\nVersioningConfiguration: {\\nMFADelete: MFADeleteStatus.Disabled,\\nStatus: BucketVersioningStatus.Enabled,\\n},\\n}),\\n);\\nawait client.send(\\nnew PutObjectLockConfigurationCommand({', '']]]\n",
      "[[['', 'Bucket: state.retentionBucketName,\\nObjectLockConfiguration: {\\nObjectLockEnabled: \"Enabled\",\\nRule: {\\nDefaultRetention: {\\nMode: \"GOVERNANCE\",\\nYears: 1,\\n},\\n},\\n},\\n}),\\n);\\n});\\n/**\\n* @param {Scenarios} scenarios\\n*/\\nconst updateLockPolicy = (scenarios) =>\\nnew scenarios.ScenarioOutput(\\n\"updateLockPolicy\",\\n`Object lock policies can also be added to existing buckets.\\nAn object lock policy will be added to ${bucketPrefix}-lock-enabled.`,\\n{ preformatted: true },\\n);\\n/**\\n* @param {Scenarios} scenarios\\n*/\\nconst confirmUpdateLockPolicy = (scenarios) =>\\nnew scenarios.ScenarioInput(\\n\"confirmUpdateLockPolicy\",\\n\"Add object lock policy?\",\\n{ type: \"confirm\" },\\n);\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst updateLockPolicyAction = (scenarios, client) =>\\nnew scenarios.ScenarioAction(\"updateLockPolicyAction\", async (state) => {\\nawait client.send(\\nnew PutObjectLockConfigurationCommand({\\nBucket: state.lockEnabledBucketName,', '']]]\n",
      "[[['', 'ObjectLockConfiguration: {\\nObjectLockEnabled: \"Enabled\",\\n},\\n}),\\n);\\n});\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst confirmSetLegalHoldFileEnabled = (scenarios) =>\\nnew scenarios.ScenarioInput(\\n\"confirmSetLegalHoldFileEnabled\",\\n(state) =>\\n`Would you like to add a legal hold to file0.txt in\\n${state.lockEnabledBucketName}?`,\\n{\\ntype: \"confirm\",\\n},\\n);\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst setLegalHoldFileEnabledAction = (scenarios, client) =>\\nnew scenarios.ScenarioAction(\\n\"setLegalHoldFileEnabledAction\",\\nasync (state) => {\\nawait client.send(\\nnew PutObjectLegalHoldCommand({\\nBucket: state.lockEnabledBucketName,\\nKey: \"file0.txt\",\\nLegalHold: {\\nStatus: ObjectLockLegalHoldStatus.ON,\\n},\\n}),\\n);\\nconsole.log(\\n`Modified legal hold for file0.txt in ${state.lockEnabledBucketName}.`,\\n);\\n},\\n{ skipWhen: (state) => !state.confirmSetLegalHoldFileEnabled },', '']]]\n",
      "[[['', ');\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst confirmSetRetentionPeriodFileEnabled = (scenarios) =>\\nnew scenarios.ScenarioInput(\\n\"confirmSetRetentionPeriodFileEnabled\",\\n(state) =>\\n`Would you like to add a 1 day Governance retention period to file1.txt in\\n${state.lockEnabledBucketName}?\\nReminder: Only a user with the s3:BypassGovernanceRetention permission will be\\nable to delete this file or its bucket until the retention period has expired.`,\\n{\\ntype: \"confirm\",\\n},\\n);\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst setRetentionPeriodFileEnabledAction = (scenarios, client) =>\\nnew scenarios.ScenarioAction(\\n\"setRetentionPeriodFileEnabledAction\",\\nasync (state) => {\\nconst retentionDate = new Date();\\nretentionDate.setDate(retentionDate.getDate() + 1);\\nawait client.send(\\nnew PutObjectRetentionCommand({\\nBucket: state.lockEnabledBucketName,\\nKey: \"file1.txt\",\\nRetention: {\\nMode: ObjectLockRetentionMode.GOVERNANCE,\\nRetainUntilDate: retentionDate,\\n},\\n}),\\n);\\nconsole.log(\\n`Set retention for file1.txt in ${state.lockEnabledBucketName} until\\n${retentionDate.toISOString().split(\"T\")[0]}.`,\\n);\\n},', '']]]\n",
      "[[['', '{ skipWhen: (state) => !state.confirmSetRetentionPeriodFileEnabled },\\n);\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst confirmSetLegalHoldFileRetention = (scenarios) =>\\nnew scenarios.ScenarioInput(\\n\"confirmSetLegalHoldFileRetention\",\\n(state) =>\\n`Would you like to add a legal hold to file0.txt in\\n${state.retentionBucketName}?`,\\n{\\ntype: \"confirm\",\\n},\\n);\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst setLegalHoldFileRetentionAction = (scenarios, client) =>\\nnew scenarios.ScenarioAction(\\n\"setLegalHoldFileRetentionAction\",\\nasync (state) => {\\nawait client.send(\\nnew PutObjectLegalHoldCommand({\\nBucket: state.retentionBucketName,\\nKey: \"file0.txt\",\\nLegalHold: {\\nStatus: ObjectLockLegalHoldStatus.ON,\\n},\\n}),\\n);\\nconsole.log(\\n`Modified legal hold for file0.txt in ${state.retentionBucketName}.`,\\n);\\n},\\n{ skipWhen: (state) => !state.confirmSetLegalHoldFileRetention },\\n);\\n/**\\n* @param {Scenarios} scenarios', '']]]\n",
      "[[['', '*/\\nconst confirmSetRetentionPeriodFileRetention = (scenarios) =>\\nnew scenarios.ScenarioInput(\\n\"confirmSetRetentionPeriodFileRetention\",\\n(state) =>\\n`Would you like to add a 1 day Governance retention period to file1.txt in\\n${state.retentionBucketName}?\\nReminder: Only a user with the s3:BypassGovernanceRetention permission will be\\nable to delete this file or its bucket until the retention period has expired.`,\\n{\\ntype: \"confirm\",\\n},\\n);\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst setRetentionPeriodFileRetentionAction = (scenarios, client) =>\\nnew scenarios.ScenarioAction(\\n\"setRetentionPeriodFileRetentionAction\",\\nasync (state) => {\\nconst retentionDate = new Date();\\nretentionDate.setDate(retentionDate.getDate() + 1);\\nawait client.send(\\nnew PutObjectRetentionCommand({\\nBucket: state.retentionBucketName,\\nKey: \"file1.txt\",\\nRetention: {\\nMode: ObjectLockRetentionMode.GOVERNANCE,\\nRetainUntilDate: retentionDate,\\n},\\nBypassGovernanceRetention: true,\\n}),\\n);\\nconsole.log(\\n`Set retention for file1.txt in ${state.retentionBucketName} until\\n${retentionDate.toISOString().split(\"T\")[0]}.`,\\n);\\n},\\n{ skipWhen: (state) => !state.confirmSetRetentionPeriodFileRetention },\\n);\\nexport {', '']]]\n",
      "[[['', 'createBuckets,\\nconfirmCreateBuckets,\\ncreateBucketsAction,\\npopulateBuckets,\\nconfirmPopulateBuckets,\\npopulateBucketsAction,\\nupdateRetention,\\nconfirmUpdateRetention,\\nupdateRetentionAction,\\nupdateLockPolicy,\\nconfirmUpdateLockPolicy,\\nupdateLockPolicyAction,\\nconfirmSetLegalHoldFileEnabled,\\nsetLegalHoldFileEnabledAction,\\nconfirmSetRetentionPeriodFileEnabled,\\nsetRetentionPeriodFileEnabledAction,\\nconfirmSetLegalHoldFileRetention,\\nsetLegalHoldFileRetentionAction,\\nconfirmSetRetentionPeriodFileRetention,\\nsetRetentionPeriodFileRetentionAction,\\n};', ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport {\\nChecksumAlgorithm,\\nDeleteObjectCommand,\\nGetObjectLegalHoldCommand,\\nGetObjectLockConfigurationCommand,\\nGetObjectRetentionCommand,\\nListObjectVersionsCommand,\\nPutObjectCommand,\\n} from \"@aws-sdk/client-s3\";\\n/**\\n* @typedef {import(\"@aws-doc-sdk-examples/lib/scenario/index.js\")} Scenarios\\n*/\\n/**\\n* @typedef {import(\"@aws-sdk/client-s3\").S3Client} S3Client', '']]]\n",
      "[[['', '*/\\nconst choices = {\\nEXIT: 0,\\nLIST_ALL_FILES: 1,\\nDELETE_FILE: 2,\\nDELETE_FILE_WITH_RETENTION: 3,\\nOVERWRITE_FILE: 4,\\nVIEW_RETENTION_SETTINGS: 5,\\nVIEW_LEGAL_HOLD_SETTINGS: 6,\\n};\\n/**\\n* @param {Scenarios} scenarios\\n*/\\nconst replInput = (scenarios) =>\\nnew scenarios.ScenarioInput(\\n\"replChoice\",\\n`Explore the S3 locking features by selecting one of the following choices`,\\n{\\ntype: \"select\",\\nchoices: [\\n{ name: \"List all files in buckets\", value: choices.LIST_ALL_FILES },\\n{ name: \"Attempt to delete a file.\", value: choices.DELETE_FILE },\\n{\\nname: \"Attempt to delete a file with retention period bypass.\",\\nvalue: choices.DELETE_FILE_WITH_RETENTION,\\n},\\n{ name: \"Attempt to overwrite a file.\", value: choices.OVERWRITE_FILE },\\n{\\nname: \"View the object and bucket retention settings for a file.\",\\nvalue: choices.VIEW_RETENTION_SETTINGS,\\n},\\n{\\nname: \"View the legal hold settings for a file.\",\\nvalue: choices.VIEW_LEGAL_HOLD_SETTINGS,\\n},\\n{ name: \"Finish the workflow.\", value: choices.EXIT },\\n],\\n},\\n);\\n/**\\n* @param {S3Client} client', '']]]\n",
      "[[['', '* @param {string[]} buckets\\n*/\\nconst getAllFiles = async (client, buckets) => {\\n/** @type {{bucket: string, key: string, version: string}[]} */\\nconst files = [];\\nfor (const bucket of buckets) {\\nconst objectsResponse = await client.send(\\nnew ListObjectVersionsCommand({ Bucket: bucket }),\\n);\\nfor (const version of objectsResponse.Versions || []) {\\nconst { Key, VersionId } = version;\\nfiles.push({ bucket, key: Key, version: VersionId });\\n}\\n}\\nreturn files;\\n};\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst replAction = (scenarios, client) =>\\nnew scenarios.ScenarioAction(\\n\"replAction\",\\nasync (state) => {\\nconst files = await getAllFiles(client, [\\nstate.noLockBucketName,\\nstate.lockEnabledBucketName,\\nstate.retentionBucketName,\\n]);\\nconst fileInput = new scenarios.ScenarioInput(\\n\"selectedFile\",\\n\"Select a file:\",\\n{\\ntype: \"select\",\\nchoices: files.map((file, index) => ({\\nname: `${index + 1}: ${file.bucket}: ${file.key} (version: ${\\nfile.version\\n})`,\\nvalue: index,\\n})),\\n},', '']]]\n",
      "[[['', ');\\nconst { replChoice } = state;\\nswitch (replChoice) {\\ncase choices.LIST_ALL_FILES: {\\nconst files = await getAllFiles(client, [\\nstate.noLockBucketName,\\nstate.lockEnabledBucketName,\\nstate.retentionBucketName,\\n]);\\nstate.replOutput = files\\n.map(\\n(file) =>\\n`${file.bucket}: ${file.key} (version: ${file.version})`,\\n)\\n.join(\"\\\\n\");\\nbreak;\\n}\\ncase choices.DELETE_FILE: {\\n/** @type {number} */\\nconst fileToDelete = await fileInput.handle(state);\\nconst selectedFile = files[fileToDelete];\\ntry {\\nawait client.send(\\nnew DeleteObjectCommand({\\nBucket: selectedFile.bucket,\\nKey: selectedFile.key,\\nVersionId: selectedFile.version,\\n}),\\n);\\nstate.replOutput = `Deleted ${selectedFile.key} in\\n${selectedFile.bucket}.`;\\n} catch (err) {\\nstate.replOutput = `Unable to delete object ${selectedFile.key} in\\nbucket ${selectedFile.bucket}: ${err.message}`;\\n}\\nbreak;\\n}\\ncase choices.DELETE_FILE_WITH_RETENTION: {\\n/** @type {number} */\\nconst fileToDelete = await fileInput.handle(state);\\nconst selectedFile = files[fileToDelete];\\ntry {', '']]]\n",
      "[[['', 'await client.send(\\nnew DeleteObjectCommand({\\nBucket: selectedFile.bucket,\\nKey: selectedFile.key,\\nVersionId: selectedFile.version,\\nBypassGovernanceRetention: true,\\n}),\\n);\\nstate.replOutput = `Deleted ${selectedFile.key} in\\n${selectedFile.bucket}.`;\\n} catch (err) {\\nstate.replOutput = `Unable to delete object ${selectedFile.key} in\\nbucket ${selectedFile.bucket}: ${err.message}`;\\n}\\nbreak;\\n}\\ncase choices.OVERWRITE_FILE: {\\n/** @type {number} */\\nconst fileToOverwrite = await fileInput.handle(state);\\nconst selectedFile = files[fileToOverwrite];\\ntry {\\nawait client.send(\\nnew PutObjectCommand({\\nBucket: selectedFile.bucket,\\nKey: selectedFile.key,\\nBody: \"New content\",\\nChecksumAlgorithm: ChecksumAlgorithm.SHA256,\\n}),\\n);\\nstate.replOutput = `Overwrote ${selectedFile.key} in\\n${selectedFile.bucket}.`;\\n} catch (err) {\\nstate.replOutput = `Unable to overwrite object ${selectedFile.key} in\\nbucket ${selectedFile.bucket}: ${err.message}`;\\n}\\nbreak;\\n}\\ncase choices.VIEW_RETENTION_SETTINGS: {\\n/** @type {number} */\\nconst fileToView = await fileInput.handle(state);\\nconst selectedFile = files[fileToView];\\ntry {\\nconst retention = await client.send(\\nnew GetObjectRetentionCommand({', '']]]\n",
      "[[['', \"Bucket: selectedFile.bucket,\\nKey: selectedFile.key,\\nVersionId: selectedFile.version,\\n}),\\n);\\nconst bucketConfig = await client.send(\\nnew GetObjectLockConfigurationCommand({\\nBucket: selectedFile.bucket,\\n}),\\n);\\nstate.replOutput = `Object retention for ${selectedFile.key}\\nin ${selectedFile.bucket}: ${retention.Retention?.Mode} until\\n${retention.Retention?.RetainUntilDate?.toISOString()}.\\nBucket object lock config for ${selectedFile.bucket} in ${selectedFile.bucket}:\\nEnabled: ${bucketConfig.ObjectLockConfiguration?.ObjectLockEnabled}\\nRule:\\n${JSON.stringify(bucketConfig.ObjectLockConfiguration?.Rule?.DefaultRetention)}`;\\n} catch (err) {\\nstate.replOutput = `Unable to fetch object lock retention:\\n'${err.message}'`;\\n}\\nbreak;\\n}\\ncase choices.VIEW_LEGAL_HOLD_SETTINGS: {\\n/** @type {number} */\\nconst fileToView = await fileInput.handle(state);\\nconst selectedFile = files[fileToView];\\ntry {\\nconst legalHold = await client.send(\\nnew GetObjectLegalHoldCommand({\\nBucket: selectedFile.bucket,\\nKey: selectedFile.key,\\nVersionId: selectedFile.version,\\n}),\\n);\\nstate.replOutput = `Object legal hold for ${selectedFile.key} in\\n${selectedFile.bucket}: Status: ${legalHold.LegalHold?.Status}`;\\n} catch (err) {\\nstate.replOutput = `Unable to fetch legal hold: '${err.message}'`;\\n}\\nbreak;\\n}\\ndefault:\\nthrow new Error(`Invalid replChoice: ${replChoice}`);\", '']]]\n",
      "[[['', '}\\n},\\n{\\nwhileConfig: {\\nwhileFn: ({ replChoice }) => replChoice !== choices.EXIT,\\ninput: replInput(scenarios),\\noutput: new scenarios.ScenarioOutput(\\n\"REPL output\",\\n(state) => state.replOutput,\\n{ preformatted: true },\\n),\\n},\\n},\\n);\\nexport { replInput, replAction, choices };', ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport {\\nDeleteObjectCommand,\\nDeleteBucketCommand,\\nListObjectVersionsCommand,\\nGetObjectLegalHoldCommand,\\nGetObjectRetentionCommand,\\nPutObjectLegalHoldCommand,\\n} from \"@aws-sdk/client-s3\";\\n/**\\n* @typedef {import(\"@aws-doc-sdk-examples/lib/scenario/index.js\")} Scenarios\\n*/\\n/**\\n* @typedef {import(\"@aws-sdk/client-s3\").S3Client} S3Client\\n*/\\n/**\\n* @param {Scenarios} scenarios\\n*/\\nconst confirmCleanup = (scenarios) =>', '']]]\n",
      "[[['', 'new scenarios.ScenarioInput(\"confirmCleanup\", \"Clean up resources?\", {\\ntype: \"confirm\",\\n});\\n/**\\n* @param {Scenarios} scenarios\\n* @param {S3Client} client\\n*/\\nconst cleanupAction = (scenarios, client) =>\\nnew scenarios.ScenarioAction(\"cleanupAction\", async (state) => {\\nconst { noLockBucketName, lockEnabledBucketName, retentionBucketName } =\\nstate;\\nconst buckets = [\\nnoLockBucketName,\\nlockEnabledBucketName,\\nretentionBucketName,\\n];\\nfor (const bucket of buckets) {\\n/** @type {import(\"@aws-sdk/client-s3\").ListObjectVersionsCommandOutput} */\\nlet objectsResponse;\\ntry {\\nobjectsResponse = await client.send(\\nnew ListObjectVersionsCommand({\\nBucket: bucket,\\n}),\\n);\\n} catch (e) {\\nif (e instanceof Error && e.name === \"NoSuchBucket\") {\\nconsole.log(\"Object\\'s bucket has already been deleted.\");\\ncontinue;\\n} else {\\nthrow e;\\n}\\n}\\nfor (const version of objectsResponse.Versions || []) {\\nconst { Key, VersionId } = version;\\ntry {\\nconst legalHold = await client.send(\\nnew GetObjectLegalHoldCommand({', '']]]\n",
      "[[['', 'Bucket: bucket,\\nKey,\\nVersionId,\\n}),\\n);\\nif (legalHold.LegalHold?.Status === \"ON\") {\\nawait client.send(\\nnew PutObjectLegalHoldCommand({\\nBucket: bucket,\\nKey,\\nVersionId,\\nLegalHold: {\\nStatus: \"OFF\",\\n},\\n}),\\n);\\n}\\n} catch (err) {\\nconsole.log(\\n`Unable to fetch legal hold for ${Key} in ${bucket}:\\n\\'${err.message}\\'`,\\n);\\n}\\ntry {\\nconst retention = await client.send(\\nnew GetObjectRetentionCommand({\\nBucket: bucket,\\nKey,\\nVersionId,\\n}),\\n);\\nif (retention.Retention?.Mode === \"GOVERNANCE\") {\\nawait client.send(\\nnew DeleteObjectCommand({\\nBucket: bucket,\\nKey,\\nVersionId,\\nBypassGovernanceRetention: true,\\n}),\\n);\\n}', '']]]\n",
      "[[['', \"} catch (err) {\\nconsole.log(\\n`Unable to fetch object lock retention for ${Key} in ${bucket}:\\n'${err.message}'`,\\n);\\n}\\nawait client.send(\\nnew DeleteObjectCommand({\\nBucket: bucket,\\nKey,\\nVersionId,\\n}),\\n);\\n}\\nawait client.send(new DeleteBucketCommand({ Bucket: bucket }));\\nconsole.log(`Delete for ${bucket} complete.`);\\n}\\n});\\nexport { confirmCleanup, cleanupAction };\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example shows how to manage Amazon Simple Storage Service\\n/// (Amazon S3) access control lists (ACLs) to control Amazon S3 bucket\\n/// access.\\n/// </summary>\\npublic class ManageACLs\\n{\\npublic static async Task Main()\\n{\\nstring bucketName = \"doc-example-bucket1\";\\nstring newBucketName = \"doc-example-bucket2\";\\nstring keyName = \"sample-object.txt\";\\nstring emailAddress = \"someone@example.com\";\\n// If the AWS Region where your bucket is located is different from\\n// the Region defined for the default user, pass the Amazon S3\\nbucket\\'s\\n// name to the client constructor. It should look like this:\\n// RegionEndpoint bucketRegion = RegionEndpoint.USEast1;', '']]]\n",
      "[[['', 'IAmazonS3 client = new AmazonS3Client();\\nawait TestBucketObjectACLsAsync(client, bucketName, newBucketName,\\nkeyName, emailAddress);\\n}\\n/// <summary>\\n/// Creates a new Amazon S3 bucket with a canned ACL, then retrieves the\\nACL\\n/// information and then adds a new ACL to one of the objects in the\\n/// Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\ncall\\n/// methods to create a bucket, get an ACL, and add a different ACL to\\n/// one of the objects.</param>\\n/// <param name=\"bucketName\">A string representing the original Amazon S3\\n/// bucket name.</param>\\n/// <param name=\"newBucketName\">A string representing the name of the\\n/// new bucket that will be created.</param>\\n/// <param name=\"keyName\">A string representing the key name of an Amazon\\nS3\\n/// object for which we will change the ACL.</param>\\n/// <param name=\"emailAddress\">A string representing the email address\\n/// belonging to the person to whom access to the Amazon S3 bucket will\\nbe\\n/// granted.</param>\\npublic static async Task TestBucketObjectACLsAsync(\\nIAmazonS3 client,\\nstring bucketName,\\nstring newBucketName,\\nstring keyName,\\nstring emailAddress)\\n{\\ntry\\n{\\n// Create a new Amazon S3 bucket and specify canned ACL.\\nvar success = await CreateBucketWithCannedACLAsync(client,\\nnewBucketName);\\n// Get the ACL on a bucket.\\nawait GetBucketACLAsync(client, bucketName);\\n// Add (replace) the ACL on an object in a bucket.', '']]]\n",
      "[[['', 'await AddACLToExistingObjectAsync(client, bucketName, keyName,\\nemailAddress);\\n}\\ncatch (AmazonS3Exception amazonS3Exception)\\n{\\nConsole.WriteLine($\"Exception: {amazonS3Exception.Message}\");\\n}\\n}\\n/// <summary>\\n/// Creates a new Amazon S3 bucket with a canned ACL attached.\\n/// </summary>\\n/// <param name=\"client\">The initialized client object used to call\\n/// PutBucketAsync.</param>\\n/// <param name=\"newBucketName\">A string representing the name of the\\n/// new Amazon S3 bucket.</param>\\n/// <returns>Returns a boolean value indicating success or failure.</\\nreturns>\\npublic static async Task<bool> CreateBucketWithCannedACLAsync(IAmazonS3\\nclient, string newBucketName)\\n{\\nvar request = new PutBucketRequest()\\n{\\nBucketName = newBucketName,\\nBucketRegion = S3Region.EUWest1,\\n// Add a canned ACL.\\nCannedACL = S3CannedACL.LogDeliveryWrite,\\n};\\nvar response = await client.PutBucketAsync(request);\\nreturn response.HttpStatusCode == System.Net.HttpStatusCode.OK;\\n}\\n/// <summary>\\n/// Retrieves the ACL associated with the Amazon S3 bucket name in the\\n/// bucketName parameter.\\n/// </summary>\\n/// <param name=\"client\">The initialized client object used to call\\n/// PutBucketAsync.</param>\\n/// <param name=\"bucketName\">The Amazon S3 bucket for which we want to\\nget the\\n/// ACL list.</param>', '']]]\n",
      "[[['', '/// <returns>Returns an S3AccessControlList returned from the call to\\n/// GetACLAsync.</returns>\\npublic static async Task<S3AccessControlList> GetBucketACLAsync(IAmazonS3\\nclient, string bucketName)\\n{\\nGetACLResponse response = await client.GetACLAsync(new GetACLRequest\\n{\\nBucketName = bucketName,\\n});\\nreturn response.AccessControlList;\\n}\\n/// <summary>\\n/// Adds a new ACL to an existing object in the Amazon S3 bucket.\\n/// </summary>\\n/// <param name=\"client\">The initialized client object used to call\\n/// PutBucketAsync.</param>\\n/// <param name=\"bucketName\">A string representing the name of the Amazon\\nS3\\n/// bucket containing the object to which we want to apply a new ACL.</\\nparam>\\n/// <param name=\"keyName\">A string representing the name of the object\\n/// to which we want to apply the new ACL.</param>\\n/// <param name=\"emailAddress\">The email address of the person to whom\\n/// we will be applying to whom access will be granted.</param>\\npublic static async Task AddACLToExistingObjectAsync(IAmazonS3 client,\\nstring bucketName, string keyName, string emailAddress)\\n{\\n// Retrieve the ACL for an object.\\nGetACLResponse aclResponse = await client.GetACLAsync(new\\nGetACLRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\n});\\nS3AccessControlList acl = aclResponse.AccessControlList;\\n// Retrieve the owner.\\nOwner owner = acl.Owner;', '']]]\n",
      "[[['', '// Clear existing grants.\\nacl.Grants.Clear();\\n// Add a grant to reset the owner\\'s full permission\\n// (the previous clear statement removed all permissions).\\nvar fullControlGrant = new S3Grant\\n{\\nGrantee = new S3Grantee { CanonicalUser = acl.Owner.Id },\\n};\\nacl.AddGrant(fullControlGrant.Grantee, S3Permission.FULL_CONTROL);\\n// Specify email to identify grantee for granting permissions.\\nvar grantUsingEmail = new S3Grant\\n{\\nGrantee = new S3Grantee { EmailAddress = emailAddress },\\nPermission = S3Permission.WRITE_ACP,\\n};\\n// Specify log delivery group as grantee.\\nvar grantLogDeliveryGroup = new S3Grant\\n{\\nGrantee = new S3Grantee { URI = \"http://acs.amazonaws.com/groups/\\ns3/LogDelivery\" },\\nPermission = S3Permission.WRITE,\\n};\\n// Create a new ACL.\\nvar newAcl = new S3AccessControlList\\n{\\nGrants = new List<S3Grant> { grantUsingEmail,\\ngrantLogDeliveryGroup },\\nOwner = owner,\\n};\\n// Set the new ACL. We\\'re throwing away the response here.\\n_ = await client.PutACLAsync(new PutACLRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nAccessControlList = newAcl,\\n});\\n}\\n}', '']]]\n",
      "[[['', '', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport software.amazon.awssdk.regions.Region;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.S3Uri;\\nimport software.amazon.awssdk.services.s3.S3Utilities;\\nimport java.net.URI;\\nimport java.util.List;\\nimport java.util.Map;\\n/**\\n*', '']]]\n",
      "[[['', '* @param s3Client - An S3Client through which you acquire an S3Uri\\ninstance.\\n* @param s3ObjectUrl - A complex URL (String) that is used to demonstrate\\nS3Uri\\n* capabilities.\\n*/\\npublic static void parseS3UriExample(S3Client s3Client, String s3ObjectUrl) {\\nlogger.info(s3ObjectUrl);\\n// Console output:\\n// \\'https://s3.us-west-1.amazonaws.com/myBucket/resources/doc.txt?\\nversionId=abc123&partNumber=77&partNumber=88\\'.\\n// Create an S3Utilities object using the configuration of the s3Client.\\nS3Utilities s3Utilities = s3Client.utilities();\\n// From a String URL create a URI object to pass to the parseUri()\\nmethod.\\nURI uri = URI.create(s3ObjectUrl);\\nS3Uri s3Uri = s3Utilities.parseUri(uri);\\n// If the URI contains no value for the Region, bucket or key, the SDK\\nreturns\\n// an empty Optional.\\n// The SDK returns decoded URI values.\\nRegion region = s3Uri.region().orElse(null);\\nlog(\"region\", region);\\n// Console output: \\'region: us-west-1\\'.\\nString bucket = s3Uri.bucket().orElse(null);\\nlog(\"bucket\", bucket);\\n// Console output: \\'bucket: myBucket\\'.\\nString key = s3Uri.key().orElse(null);\\nlog(\"key\", key);\\n// Console output: \\'key: resources/doc.txt\\'.\\nBoolean isPathStyle = s3Uri.isPathStyle();\\nlog(\"isPathStyle\", isPathStyle);\\n// Console output: \\'isPathStyle: true\\'.\\n// If the URI contains no query parameters, the SDK returns an empty map.\\nMap<String, List<String>> queryParams = s3Uri.rawQueryParameters();\\nlog(\"rawQueryParameters\", queryParams);', '']]]\n",
      "[[['', '// Console output: \\'rawQueryParameters: {versionId=[abc123],\\npartNumber=[77,\\n// 88]}\\'.\\n// Retrieve the first or all values for a query parameter as shown in the\\n// following code.\\nString versionId =\\ns3Uri.firstMatchingRawQueryParameter(\"versionId\").orElse(null);\\nlog(\"firstMatchingRawQueryParameter-versionId\", versionId);\\n// Console output: \\'firstMatchingRawQueryParameter-versionId: abc123\\'.\\nString partNumber =\\ns3Uri.firstMatchingRawQueryParameter(\"partNumber\").orElse(null);\\nlog(\"firstMatchingRawQueryParameter-partNumber\", partNumber);\\n// Console output: \\'firstMatchingRawQueryParameter-partNumber: 77\\'.\\nList<String> partNumbers =\\ns3Uri.firstMatchingRawQueryParameters(\"partNumber\");\\nlog(\"firstMatchingRawQueryParameter\", partNumbers);\\n// Console output: \\'firstMatchingRawQueryParameter: [77, 88]\\'.\\n/*\\n* Object keys and query parameters with reserved or unsafe characters,\\nmust be\\n* URL-encoded.\\n* For example replace whitespace \" \" with \"%20\".\\n* Valid:\\n* \"https://s3.us-west-1.amazonaws.com/myBucket/object%20key?query=\\n%5Bbrackets%5D\"\\n* Invalid:\\n* \"https://s3.us-west-1.amazonaws.com/myBucket/object key?\\nquery=[brackets]\"\\n*\\n* Virtual-hosted-style URIs with bucket names that contain a dot, \".\",\\nthe dot\\n* must not be URL-encoded.\\n* Valid: \"https://my.Bucket.s3.us-west-1.amazonaws.com/key\"\\n* Invalid: \"https://my%2EBucket.s3.us-west-1.amazonaws.com/key\"\\n*/\\n}\\nprivate static void log(String s3UriElement, Object element) {\\nif (element == null) {\\nlogger.info(\"{}: {}\", s3UriElement, \"null\");', '']]]\n",
      "[[['', '} else {\\nlogger.info(\"{}: {}\", s3UriElement, element);\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// This example shows how to perform a multi-part copy from one Amazon\\n/// Simple Storage Service (Amazon S3) bucket to another.\\n/// </summary>\\npublic class MPUapiCopyObj\\n{\\nprivate const string SourceBucket = \"doc-example-bucket1\";\\nprivate const string TargetBucket = \"doc-example-bucket2\";\\nprivate const string SourceObjectKey = \"example.mov\";\\nprivate const string TargetObjectKey = \"copied_video_file.mov\";', '']]]\n",
      "[[['', '/// <summary>\\n/// This method starts the multi-part upload.\\n/// </summary>\\npublic static async Task Main()\\n{\\nvar s3Client = new AmazonS3Client();\\nConsole.WriteLine(\"Copying object...\");\\nawait MPUCopyObjectAsync(s3Client);\\n}\\n/// <summary>\\n/// This method uses the passed client object to perform a multipart\\n/// copy operation.\\n/// </summary>\\n/// <param name=\"client\">An Amazon S3 client object that will be used\\n/// to perform the copy.</param>\\npublic static async Task MPUCopyObjectAsync(AmazonS3Client client)\\n{\\n// Create a list to store the copy part responses.\\nvar copyResponses = new List<CopyPartResponse>();\\n// Setup information required to initiate the multipart upload.\\nvar initiateRequest = new InitiateMultipartUploadRequest\\n{\\nBucketName = TargetBucket,\\nKey = TargetObjectKey,\\n};\\n// Initiate the upload.\\nInitiateMultipartUploadResponse initResponse =\\nawait client.InitiateMultipartUploadAsync(initiateRequest);\\n// Save the upload ID.\\nstring uploadId = initResponse.UploadId;\\ntry\\n{\\n// Get the size of the object.\\nvar metadataRequest = new GetObjectMetadataRequest\\n{\\nBucketName = SourceBucket,\\nKey = SourceObjectKey,\\n};', '']]]\n",
      "[[['', 'GetObjectMetadataResponse metadataResponse =\\nawait client.GetObjectMetadataAsync(metadataRequest);\\nvar objectSize = metadataResponse.ContentLength; // Length in\\nbytes.\\n// Copy the parts.\\nvar partSize = 5 * (long)Math.Pow(2, 20); // Part size is 5 MB.\\nlong bytePosition = 0;\\nfor (int i = 1; bytePosition < objectSize; i++)\\n{\\nvar copyRequest = new CopyPartRequest\\n{\\nDestinationBucket = TargetBucket,\\nDestinationKey = TargetObjectKey,\\nSourceBucket = SourceBucket,\\nSourceKey = SourceObjectKey,\\nUploadId = uploadId,\\nFirstByte = bytePosition,\\nLastByte = bytePosition + partSize - 1 >= objectSize ?\\nobjectSize - 1 : bytePosition + partSize - 1,\\nPartNumber = i,\\n};\\ncopyResponses.Add(await client.CopyPartAsync(copyRequest));\\nbytePosition += partSize;\\n}\\n// Set up to complete the copy.\\nvar completeRequest = new CompleteMultipartUploadRequest\\n{\\nBucketName = TargetBucket,\\nKey = TargetObjectKey,\\nUploadId = initResponse.UploadId,\\n};\\ncompleteRequest.AddPartETags(copyResponses);\\n// Complete the copy.\\nCompleteMultipartUploadResponse completeUploadResponse =\\nawait client.CompleteMultipartUploadAsync(completeRequest);\\n}\\ncatch (AmazonS3Exception e)\\n{', '']]]\n",
      "[[['', 'Console.WriteLine($\"Error encountered on server.\\nMessage:\\'{e.Message}\\' when writing an object\");\\n}\\ncatch (Exception e)\\n{\\nConsole.WriteLine($\"Unknown encountered on server.\\nMessage:\\'{e.Message}\\' when writing an object\");\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import org.slf4j.Logger;', '']]]\n",
      "[[['', 'import org.slf4j.LoggerFactory;\\nimport software.amazon.awssdk.core.exception.SdkException;\\nimport software.amazon.awssdk.core.sync.RequestBody;\\nimport software.amazon.awssdk.services.s3.S3AsyncClient;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.CompletedMultipartUpload;\\nimport software.amazon.awssdk.services.s3.model.CompletedPart;\\nimport software.amazon.awssdk.services.s3.model.CreateMultipartUploadResponse;\\nimport software.amazon.awssdk.services.s3.model.PutObjectResponse;\\nimport software.amazon.awssdk.services.s3.model.UploadPartRequest;\\nimport software.amazon.awssdk.services.s3.model.UploadPartResponse;\\nimport software.amazon.awssdk.services.s3.waiters.S3Waiter;\\nimport software.amazon.awssdk.transfer.s3.S3TransferManager;\\nimport software.amazon.awssdk.transfer.s3.model.FileUpload;\\nimport software.amazon.awssdk.transfer.s3.model.UploadFileRequest;\\nimport java.io.IOException;\\nimport java.io.RandomAccessFile;\\nimport java.net.URISyntaxException;\\nimport java.net.URL;\\nimport java.nio.ByteBuffer;\\nimport java.nio.file.Paths;\\nimport java.util.ArrayList;\\nimport java.util.List;\\nimport java.util.Objects;\\nimport java.util.UUID;\\nimport java.util.concurrent.CompletableFuture;', ''], ['', '', '']], [['', '', ''], ['', 'public void multipartUploadWithTransferManager(String filePath) {\\nS3TransferManager transferManager = S3TransferManager.create();\\nUploadFileRequest uploadFileRequest = UploadFileRequest.builder()\\n.putObjectRequest(b -> b\\n.bucket(bucketName)\\n.key(key))\\n.source(Paths.get(filePath))\\n.build();\\nFileUpload fileUpload = transferManager.uploadFile(uploadFileRequest);\\nfileUpload.completionFuture().join();', '']]]\n",
      "[[['', 'transferManager.close();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public void multipartUploadWithS3Client(String filePath) {\\n// Initiate the multipart upload.\\nCreateMultipartUploadResponse createMultipartUploadResponse =\\ns3Client.createMultipartUpload(b -> b\\n.bucket(bucketName)\\n.key(key));\\nString uploadId = createMultipartUploadResponse.uploadId();\\n// Upload the parts of the file.\\nint partNumber = 1;\\nList<CompletedPart> completedParts = new ArrayList<>();\\nByteBuffer bb = ByteBuffer.allocate(1024 * 1024 * 5); // 5 MB byte buffer\\ntry (RandomAccessFile file = new RandomAccessFile(filePath, \"r\")) {\\nlong fileSize = file.length();\\nlong position = 0;\\nwhile (position < fileSize) {\\nfile.seek(position);\\nlong read = file.getChannel().read(bb);\\nbb.flip(); // Swap position and limit before reading from the\\nbuffer.\\nUploadPartRequest uploadPartRequest = UploadPartRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.uploadId(uploadId)\\n.partNumber(partNumber)\\n.build();\\nUploadPartResponse partResponse = s3Client.uploadPart(\\nuploadPartRequest,\\nRequestBody.fromByteBuffer(bb));\\nCompletedPart part = CompletedPart.builder()\\n.partNumber(partNumber)\\n.eTag(partResponse.eTag())', '']]]\n",
      "[[['', '.build();\\ncompletedParts.add(part);\\nbb.clear();\\nposition += read;\\npartNumber++;\\n}\\n} catch (IOException e) {\\nlogger.error(e.getMessage());\\n}\\n// Complete the multipart upload.\\ns3Client.completeMultipartUpload(b -> b\\n.bucket(bucketName)\\n.key(key)\\n.uploadId(uploadId)\\n.multipartUpload(CompletedMultipartUpload.builder().parts(completedParts).build()\\n}', ')'], ['', '', '']], [['', '', ''], ['', 'public void multipartUploadWithS3AsyncClient(String filePath) {\\n// Enable multipart support.\\nS3AsyncClient s3AsyncClient = S3AsyncClient.builder()\\n.multipartEnabled(true)\\n.build();\\nCompletableFuture<PutObjectResponse> response = s3AsyncClient.putObject(b\\n-> b\\n.bucket(bucketName)\\n.key(key),\\nPaths.get(filePath));\\nresponse.join();\\nlogger.info(\"File uploaded in multiple 8 MiB parts using\\nS3AsyncClient.\");\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'public void trackUploadFile(S3TransferManager transferManager, String\\nbucketName,\\nString key, URI filePathURI) {\\nUploadFileRequest uploadFileRequest = UploadFileRequest.builder()\\n.putObjectRequest(b -> b.bucket(bucketName).key(key))\\n.addTransferListener(LoggingTransferListener.create()) // Add\\nlistener.\\n.source(Paths.get(filePathURI))\\n.build();\\nFileUpload fileUpload = transferManager.uploadFile(uploadFileRequest);\\nfileUpload.completionFuture().join();\\n/*\\nThe SDK provides a LoggingTransferListener implementation of the\\nTransferListener interface.', '']]]\n",
      "[[['', 'You can also implement the interface to provide your own logic.\\nConfigure log4J2 with settings such as the following.\\n<Configuration status=\"WARN\">\\n<Appenders>\\n<Console name=\"AlignedConsoleAppender\"\\ntarget=\"SYSTEM_OUT\">\\n<PatternLayout pattern=\"%m%n\"/>\\n</Console>\\n</Appenders>\\n<Loggers>\\n<logger\\nname=\"software.amazon.awssdk.transfer.s3.progress.LoggingTransferListener\"\\nlevel=\"INFO\" additivity=\"false\">\\n<AppenderRef ref=\"AlignedConsoleAppender\"/>\\n</logger>\\n</Loggers>\\n</Configuration>\\nLog4J2 logs the progress. The following is example output for a 21.3\\nMB file upload.\\nTransfer initiated...\\n| | 0.0%\\n|==== | 21.1%\\n|============ | 60.5%\\n|====================| 100.0%\\nTransfer complete!\\n*/\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public void trackDownloadFile(S3TransferManager transferManager, String\\nbucketName,\\nString key, String downloadedFileWithPath) {\\nDownloadFileRequest downloadFileRequest = DownloadFileRequest.builder()\\n.getObjectRequest(b -> b.bucket(bucketName).key(key))\\n.addTransferListener(LoggingTransferListener.create()) // Add\\nlistener.\\n.destination(Paths.get(downloadedFileWithPath))\\n.build();', '']]]\n",
      "[[['', 'FileDownload downloadFile =\\ntransferManager.downloadFile(downloadFileRequest);\\nCompletedFileDownload downloadResult =\\ndownloadFile.completionFuture().join();\\n/*\\nThe SDK provides a LoggingTransferListener implementation of the\\nTransferListener interface.\\nYou can also implement the interface to provide your own logic.\\nConfigure log4J2 with settings such as the following.\\n<Configuration status=\"WARN\">\\n<Appenders>\\n<Console name=\"AlignedConsoleAppender\"\\ntarget=\"SYSTEM_OUT\">\\n<PatternLayout pattern=\"%m%n\"/>\\n</Console>\\n</Appenders>\\n<Loggers>\\n<logger\\nname=\"software.amazon.awssdk.transfer.s3.progress.LoggingTransferListener\"\\nlevel=\"INFO\" additivity=\"false\">\\n<AppenderRef ref=\"AlignedConsoleAppender\"/>\\n</logger>\\n</Loggers>\\n</Configuration>\\nLog4J2 logs the progress. The following is example output for a 21.3\\nMB file download.\\nTransfer initiated...\\n|======= | 39.4%\\n|=============== | 78.8%\\n|====================| 100.0%\\nTransfer complete!\\n*/\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', '[package]\\nname = \"testing-examples\"\\nversion = \"0.1.0\"\\nauthors = [\\n\"John Disanti <jdisanti@amazon.com>\",\\n\"Doug Schwartz <dougsch@amazon.com>\",\\n]\\nedition = \"2021\"\\n# snippet-start:[testing.rust.Cargo.toml]\\n[dependencies]\\nasync-trait = \"0.1.51\"\\naws-config = { version = \"1.0.1\", features = [\"behavior-version-latest\"] }\\naws-credential-types = { version = \"1.0.1\", features = [ \"hardcoded-\\ncredentials\", ] }\\naws-sdk-s3 = { version = \"1.4.0\" }\\naws-smithy-types = { version = \"1.0.1\" }\\naws-smithy-runtime = { version = \"1.0.1\", features = [\"test-util\"] }\\naws-smithy-runtime-api = { version = \"1.0.1\", features = [\"test-util\"] }\\naws-types = { version = \"1.0.1\" }\\nclap = { version = \"~4.4\", features = [\"derive\"] }', '']]]\n",
      "[[['', 'http = \"0.2.9\"\\nmockall = \"0.11.4\"\\nserde_json = \"1\"\\ntokio = { version = \"1.20.1\", features = [\"full\"] }\\ntracing-subscriber = { version = \"0.3.15\", features = [\"env-filter\"] }\\n# snippet-end:[testing.rust.Cargo.toml]\\n[[bin]]\\nname = \"main\"\\npath = \"src/main.rs\"', ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\n// snippet-start:[testing.rust.wrapper]\\n// snippet-start:[testing.rust.wrapper-uses]\\nuse aws_sdk_s3 as s3;\\n#[allow(unused_imports)]\\nuse mockall::automock;\\nuse s3::operation::list_objects_v2::{ListObjectsV2Error, ListObjectsV2Output};\\n// snippet-end:[testing.rust.wrapper-uses]\\n// snippet-start:[testing.rust.wrapper-which-impl]\\n#[cfg(test)]\\npub use MockS3Impl as S3;\\n#[cfg(not(test))]\\npub use S3Impl as S3;\\n// snippet-end:[testing.rust.wrapper-which-impl]\\n// snippet-start:[testing.rust.wrapper-impl]\\n#[allow(dead_code)]\\npub struct S3Impl {\\ninner: s3::Client,\\n}\\n#[cfg_attr(test, automock)]\\nimpl S3Impl {\\n#[allow(dead_code)]\\npub fn new(inner: s3::Client) -> Self {', '']]]\n",
      "[[['', 'Self { inner }\\n}\\n#[allow(dead_code)]\\npub async fn list_objects(\\n&self,\\nbucket: &str,\\nprefix: &str,\\ncontinuation_token: Option<String>,\\n) -> Result<ListObjectsV2Output, s3::error::SdkError<ListObjectsV2Error>> {\\nself.inner\\n.list_objects_v2()\\n.bucket(bucket)\\n.prefix(prefix)\\n.set_continuation_token(continuation_token)\\n.send()\\n.await\\n}\\n}\\n// snippet-end:[testing.rust.wrapper-impl]\\n// snippet-start:[testing.rust.wrapper-func]\\n#[allow(dead_code)]\\npub async fn determine_prefix_file_size(\\n// Now we take a reference to our trait object instead of the S3 client\\n// s3_list: ListObjectsService,\\ns3_list: S3,\\nbucket: &str,\\nprefix: &str,\\n) -> Result<usize, s3::Error> {\\nlet mut next_token: Option<String> = None;\\nlet mut total_size_bytes = 0;\\nloop {\\nlet result = s3_list\\n.list_objects(bucket, prefix, next_token.take())\\n.await?;\\n// Add up the file sizes we got back\\nfor object in result.contents() {\\ntotal_size_bytes += object.size().unwrap_or(0) as usize;\\n}\\n// Handle pagination, and break the loop if there are no more pages\\nnext_token = result.next_continuation_token.clone();', '']]]\n",
      "[[['', 'if next_token.is_none() {\\nbreak;\\n}\\n}\\nOk(total_size_bytes)\\n}\\n// snippet-end:[testing.rust.wrapper-func]\\n// snippet-end:[testing.rust.wrapper]\\n// snippet-start:[testing.rust.wrapper-test-mod]\\n#[cfg(test)]\\nmod test {\\n// snippet-start:[testing.rust.wrapper-tests]\\nuse super::*;\\nuse mockall::predicate::eq;\\n// snippet-start:[testing.rust.wrapper-test-single]\\n#[tokio::test]\\nasync fn test_single_page() {\\nlet mut mock = MockS3Impl::default();\\nmock.expect_list_objects()\\n.with(eq(\"test-bucket\"), eq(\"test-prefix\"), eq(None))\\n.return_once(|_, _, _| {\\nOk(ListObjectsV2Output::builder()\\n.set_contents(Some(vec![\\n// Mock content for ListObjectsV2 response\\ns3::types::Object::builder().size(5).build(),\\ns3::types::Object::builder().size(2).build(),\\n]))\\n.build())\\n});\\n// Run the code we want to test with it\\nlet size = determine_prefix_file_size(mock, \"test-bucket\", \"test-prefix\")\\n.await\\n.unwrap();\\n// Verify we got the correct total size back\\nassert_eq!(7, size);\\n}\\n// snippet-end:[testing.rust.wrapper-test-single]\\n// snippet-start:[testing.rust.wrapper-test-multiple]\\n#[tokio::test]', '']]]\n",
      "[[['', 'async fn test_multiple_pages() {\\n// Create the Mock instance with two pages of objects now\\nlet mut mock = MockS3Impl::default();\\nmock.expect_list_objects()\\n.with(eq(\"test-bucket\"), eq(\"test-prefix\"), eq(None))\\n.return_once(|_, _, _| {\\nOk(ListObjectsV2Output::builder()\\n.set_contents(Some(vec![\\n// Mock content for ListObjectsV2 response\\ns3::types::Object::builder().size(5).build(),\\ns3::types::Object::builder().size(2).build(),\\n]))\\n.set_next_continuation_token(Some(\"next\".to_string()))\\n.build())\\n});\\nmock.expect_list_objects()\\n.with(\\neq(\"test-bucket\"),\\neq(\"test-prefix\"),\\neq(Some(\"next\".to_string())),\\n)\\n.return_once(|_, _, _| {\\nOk(ListObjectsV2Output::builder()\\n.set_contents(Some(vec![\\n// Mock content for ListObjectsV2 response\\ns3::types::Object::builder().size(3).build(),\\ns3::types::Object::builder().size(9).build(),\\n]))\\n.build())\\n});\\n// Run the code we want to test with it\\nlet size = determine_prefix_file_size(mock, \"test-bucket\", \"test-prefix\")\\n.await\\n.unwrap();\\nassert_eq!(19, size);\\n}\\n// snippet-end:[testing.rust.wrapper-test-multiple]\\n// snippet-end:[testing.rust.wrapper-tests]\\n}\\n// snippet-end:[testing.rust.wrapper-test-mod]', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\n// snippet-start:[testing.rust.replay-uses]\\nuse aws_sdk_s3 as s3;\\n// snippet-end:[testing.rust.replay-uses]\\n#[allow(dead_code)]\\n// snippet-start:[testing.rust.replay]\\npub async fn determine_prefix_file_size(\\n// Now we take a reference to our trait object instead of the S3 client\\n// s3_list: ListObjectsService,\\ns3: s3::Client,\\nbucket: &str,\\nprefix: &str,\\n) -> Result<usize, s3::Error> {\\nlet mut next_token: Option<String> = None;\\nlet mut total_size_bytes = 0;\\nloop {\\nlet result = s3\\n.list_objects_v2()\\n.prefix(prefix)\\n.bucket(bucket)\\n.set_continuation_token(next_token.take())\\n.send()\\n.await?;\\n// Add up the file sizes we got back\\nfor object in result.contents() {\\ntotal_size_bytes += object.size().unwrap_or(0) as usize;\\n}\\n// Handle pagination, and break the loop if there are no more pages\\nnext_token = result.next_continuation_token.clone();\\nif next_token.is_none() {\\nbreak;\\n}\\n}\\nOk(total_size_bytes)\\n}\\n// snippet-end:[testing.rust.replay]', '']]]\n",
      "[[['', '#[allow(dead_code)]\\n// snippet-start:[testing.rust.replay-tests]\\n// snippet-start:[testing.rust.replay-make-credentials]\\nfn make_s3_test_credentials() -> s3::config::Credentials {\\ns3::config::Credentials::new(\\n\"ATESTCLIENT\",\\n\"astestsecretkey\",\\nSome(\"atestsessiontoken\".to_string()),\\nNone,\\n\"\",\\n)\\n}\\n// snippet-end:[testing.rust.replay-make-credentials]\\n// snippet-start:[testing.rust.replay-test-module]\\n#[cfg(test)]\\nmod test {\\n// snippet-start:[testing.rust.replay-test-single]\\nuse super::*;\\nuse aws_config::BehaviorVersion;\\nuse aws_sdk_s3 as s3;\\nuse aws_smithy_runtime::client::http::test_util::{ReplayEvent,\\nStaticReplayClient};\\nuse aws_smithy_types::body::SdkBody;\\n#[tokio::test]\\nasync fn test_single_page() {\\nlet page_1 = ReplayEvent::new(\\nhttp::Request::builder()\\n.method(\"GET\")\\n.uri(\"https://test-bucket.s3.us-east-1.amazonaws.com/?list-\\ntype=2&prefix=test-prefix\")\\n.body(SdkBody::empty())\\n.unwrap(),\\nhttp::Response::builder()\\n.status(200)\\n.body(SdkBody::from(include_str!(\"./testing/\\nresponse_1.xml\")))\\n.unwrap(),\\n);\\nlet replay_client = StaticReplayClient::new(vec![page_1]);\\nlet client: s3::Client = s3::Client::from_conf(\\ns3::Config::builder()\\n.behavior_version(BehaviorVersion::latest())', '']]]\n",
      "[[['', '.credentials_provider(make_s3_test_credentials())\\n.region(s3::config::Region::new(\"us-east-1\"))\\n.http_client(replay_client.clone())\\n.build(),\\n);\\n// Run the code we want to test with it\\nlet size = determine_prefix_file_size(client, \"test-bucket\", \"test-\\nprefix\")\\n.await\\n.unwrap();\\n// Verify we got the correct total size back\\nassert_eq!(7, size);\\nreplay_client.assert_requests_match(&[]);\\n}\\n// snippet-end:[testing.rust.replay-test-single]\\n// snippet-start:[testing.rust.replay-test-multiple]\\n#[tokio::test]\\nasync fn test_multiple_pages() {\\n// snippet-start:[testing.rust.replay-create-replay]\\nlet page_1 = ReplayEvent::new(\\nhttp::Request::builder()\\n.method(\"GET\")\\n.uri(\"https://test-bucket.s3.us-east-1.amazonaws.com/?list-\\ntype=2&prefix=test-prefix\")\\n.body(SdkBody::empty())\\n.unwrap(),\\nhttp::Response::builder()\\n.status(200)\\n.body(SdkBody::from(include_str!(\"./testing/\\nresponse_multi_1.xml\")))\\n.unwrap(),\\n);\\nlet page_2 = ReplayEvent::new(\\nhttp::Request::builder()\\n.method(\"GET\")\\n.uri(\"https://test-bucket.s3.us-east-1.amazonaws.com/?list-\\ntype=2&prefix=test-prefix&continuation-token=next\")\\n.body(SdkBody::empty())\\n.unwrap(),\\nhttp::Response::builder()\\n.status(200)', '']]]\n",
      "[[['', '.body(SdkBody::from(include_str!(\"./testing/\\nresponse_multi_2.xml\")))\\n.unwrap(),\\n);\\nlet replay_client = StaticReplayClient::new(vec![page_1, page_2]);\\n// snippet-end:[testing.rust.replay-create-replay]\\n// snippet-start:[testing.rust.replay-create-client]\\nlet client: s3::Client = s3::Client::from_conf(\\ns3::Config::builder()\\n.behavior_version(BehaviorVersion::latest())\\n.credentials_provider(make_s3_test_credentials())\\n.region(s3::config::Region::new(\"us-east-1\"))\\n.http_client(replay_client.clone())\\n.build(),\\n);\\n// snippet-end:[testing.rust.replay-create-client]\\n// Run the code we want to test with it\\n// snippet-start:[testing.rust.replay-test-and-verify]\\nlet size = determine_prefix_file_size(client, \"test-bucket\", \"test-\\nprefix\")\\n.await\\n.unwrap();\\nassert_eq!(19, size);\\nreplay_client.assert_requests_match(&[]);\\n// snippet-end:[testing.rust.replay-test-and-verify]\\n}\\n// snippet-end:[testing.rust.replay-test-multiple]\\n}\\n// snippet-end:[testing.rust.replay-tests]\\n// snippet-end:[testing.rust.replay-test-module]', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport software.amazon.awssdk.services.s3.model.ObjectIdentifier;\\nimport software.amazon.awssdk.transfer.s3.S3TransferManager;\\nimport software.amazon.awssdk.transfer.s3.model.CompletedDirectoryUpload;\\nimport software.amazon.awssdk.transfer.s3.model.DirectoryUpload;\\nimport software.amazon.awssdk.transfer.s3.model.UploadDirectoryRequest;\\nimport java.net.URI;\\nimport java.net.URISyntaxException;\\nimport java.net.URL;\\nimport java.nio.file.Paths;\\nimport java.util.UUID;\\npublic Integer uploadDirectory(S3TransferManager transferManager,\\nURI sourceDirectory, String bucketName) {\\nDirectoryUpload directoryUpload =\\ntransferManager.uploadDirectory(UploadDirectoryRequest.builder()\\n.source(Paths.get(sourceDirectory))\\n.bucket(bucketName)\\n.build());\\nCompletedDirectoryUpload completedDirectoryUpload =\\ndirectoryUpload.completionFuture().join();', '']]]\n",
      "[[['', 'completedDirectoryUpload.failedTransfers()\\n.forEach(fail -> logger.warn(\"Object [{}] failed to transfer\",\\nfail.toString()));\\nreturn completedDirectoryUpload.failedTransfers().size();\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'global using System.Text;\\nglobal using Amazon.S3;\\nglobal using Amazon.S3.Model;\\nglobal using Amazon.S3.Transfer;\\nglobal using TransferUtilityBasics;', '']]]\n",
      "[[['', '// This Amazon S3 client uses the default user credentials\\n// defined for this computer.\\nusing Microsoft.Extensions.Configuration;\\nIAmazonS3 client = new AmazonS3Client();\\nvar transferUtil = new TransferUtility(client);\\nIConfiguration _configuration;\\n_configuration = new ConfigurationBuilder()\\n.SetBasePath(Directory.GetCurrentDirectory())\\n.AddJsonFile(\"settings.json\") // Load test settings from JSON file.\\n.AddJsonFile(\"settings.local.json\",\\ntrue) // Optionally load local settings.\\n.Build();\\n// Edit the values in settings.json to use an S3 bucket and files that\\n// exist on your AWS account and on the local computer where you\\n// run this scenario.\\nvar bucketName = _configuration[\"BucketName\"];\\nvar localPath =\\n$\"{Environment.GetFolderPath(Environment.SpecialFolder.ApplicationData)}\\\\\\n\\\\TransferFolder\";\\nDisplayInstructions();\\nPressEnter();\\nConsole.WriteLine();\\n// Upload a single file to an S3 bucket.\\nDisplayTitle(\"Upload a single file\");\\nvar fileToUpload = _configuration[\"FileToUpload\"];\\nConsole.WriteLine($\"Uploading {fileToUpload} to the S3 bucket, {bucketName}.\");\\nvar success = await TransferMethods.UploadSingleFileAsync(transferUtil,\\nbucketName, fileToUpload, localPath);\\nif (success)\\n{\\nConsole.WriteLine($\"Successfully uploaded the file, {fileToUpload} to\\n{bucketName}.\");\\n}\\nPressEnter();', '']]]\n",
      "[[['', '// Upload a local directory to an S3 bucket.\\nDisplayTitle(\"Upload all files from a local directory\");\\nConsole.WriteLine(\"Upload all the files in a local folder to an S3 bucket.\");\\nconst string keyPrefix = \"UploadFolder\";\\nvar uploadPath = $\"{localPath}\\\\\\\\UploadFolder\";\\nConsole.WriteLine($\"Uploading the files in {uploadPath} to {bucketName}\");\\nDisplayTitle($\"{uploadPath} files\");\\nDisplayLocalFiles(uploadPath);\\nConsole.WriteLine();\\nPressEnter();\\nsuccess = await TransferMethods.UploadFullDirectoryAsync(transferUtil,\\nbucketName, keyPrefix, uploadPath);\\nif (success)\\n{\\nConsole.WriteLine($\"Successfully uploaded the files in {uploadPath} to\\n{bucketName}.\");\\nConsole.WriteLine($\"{bucketName} currently contains the following files:\");\\nawait DisplayBucketFiles(client, bucketName, keyPrefix);\\nConsole.WriteLine();\\n}\\nPressEnter();\\n// Download a single file from an S3 bucket.\\nDisplayTitle(\"Download a single file\");\\nConsole.WriteLine(\"Now we will download a single file from an S3 bucket.\");\\nvar keyName = _configuration[\"FileToDownload\"];\\nConsole.WriteLine($\"Downloading {keyName} from {bucketName}.\");\\nsuccess = await TransferMethods.DownloadSingleFileAsync(transferUtil, bucketName,\\nkeyName, localPath);\\nif (success)\\n{\\nConsole.WriteLine(\"$Successfully downloaded the file, {keyName} from\\n{bucketName}.\");\\n}\\nPressEnter();', '']]]\n",
      "[[['', '// Download the contents of a directory from an S3 bucket.\\nDisplayTitle(\"Download the contents of an S3 bucket\");\\nvar s3Path = _configuration[\"S3Path\"];\\nvar downloadPath = $\"{localPath}\\\\\\\\{s3Path}\";\\nConsole.WriteLine($\"Downloading the contents of {bucketName}\\\\\\\\{s3Path}\");\\nConsole.WriteLine($\"{bucketName}\\\\\\\\{s3Path} contains the following files:\");\\nawait DisplayBucketFiles(client, bucketName, s3Path);\\nConsole.WriteLine();\\nsuccess = await TransferMethods.DownloadS3DirectoryAsync(transferUtil,\\nbucketName, s3Path, downloadPath);\\nif (success)\\n{\\nConsole.WriteLine($\"Downloaded the files in {bucketName} to\\n{downloadPath}.\");\\nConsole.WriteLine($\"{downloadPath} now contains the following files:\");\\nDisplayLocalFiles(downloadPath);\\n}\\nConsole.WriteLine(\"\\\\nThe TransferUtility Basics application has completed.\");\\nPressEnter();\\n// Displays the title for a section of the scenario.\\nstatic void DisplayTitle(string titleText)\\n{\\nvar sepBar = new string(\\'-\\', Console.WindowWidth);\\nConsole.WriteLine(sepBar);\\nConsole.WriteLine(CenterText(titleText));\\nConsole.WriteLine(sepBar);\\n}\\n// Displays a description of the actions to be performed by the scenario.\\nstatic void DisplayInstructions()\\n{\\nvar sepBar = new string(\\'-\\', Console.WindowWidth);\\nDisplayTitle(\"Amazon S3 Transfer Utility Basics\");\\nConsole.WriteLine(\"This program shows how to use the Amazon S3 Transfer\\nUtility.\");\\nConsole.WriteLine(\"It performs the following actions:\");\\nConsole.WriteLine(\"\\\\t1. Upload a single object to an S3 bucket.\");', '']]]\n",
      "[[['', 'Console.WriteLine(\"\\\\t2. Upload an entire directory from the local computer to\\nan\\\\n\\\\t S3 bucket.\");\\nConsole.WriteLine(\"\\\\t3. Download a single object from an S3 bucket.\");\\nConsole.WriteLine(\"\\\\t4. Download the objects in an S3 bucket to a local\\ndirectory.\");\\nConsole.WriteLine($\"\\\\n{sepBar}\");\\n}\\n// Pauses the scenario.\\nstatic void PressEnter()\\n{\\nConsole.WriteLine(\"Press <Enter> to continue.\");\\n_ = Console.ReadLine();\\nConsole.WriteLine(\"\\\\n\");\\n}\\n// Returns the string textToCenter, padded on the left with spaces\\n// that center the text on the console display.\\nstatic string CenterText(string textToCenter)\\n{\\nvar centeredText = new StringBuilder();\\nvar screenWidth = Console.WindowWidth;\\ncenteredText.Append(new string(\\' \\', (int)(screenWidth -\\ntextToCenter.Length) / 2));\\ncenteredText.Append(textToCenter);\\nreturn centeredText.ToString();\\n}\\n// Displays a list of file names included in the specified path.\\nstatic void DisplayLocalFiles(string localPath)\\n{\\nvar fileList = Directory.GetFiles(localPath);\\nif (fileList.Length > 0)\\n{\\nforeach (var fileName in fileList)\\n{\\nConsole.WriteLine(fileName);\\n}\\n}\\n}\\n// Displays a list of the files in the specified S3 bucket and prefix.\\nstatic async Task DisplayBucketFiles(IAmazonS3 client, string bucketName, string\\ns3Path)', '']]]\n",
      "[[['', '{\\nListObjectsV2Request request = new()\\n{\\nBucketName = bucketName,\\nPrefix = s3Path,\\nMaxKeys = 5,\\n};\\nvar response = new ListObjectsV2Response();\\ndo\\n{\\nresponse = await client.ListObjectsV2Async(request);\\nresponse.S3Objects\\n.ForEach(obj => Console.WriteLine($\"{obj.Key}\"));\\n// If the response is truncated, set the request ContinuationToken\\n// from the NextContinuationToken property of the response.\\nrequest.ContinuationToken = response.NextContinuationToken;\\n} while (response.IsTruncated);\\n}', ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Uploads a single file from the local computer to an S3 bucket.\\n/// </summary>\\n/// <param name=\"transferUtil\">The transfer initialized TransferUtility\\n/// object.</param>\\n/// <param name=\"bucketName\">The name of the S3 bucket where the file\\n/// will be stored.</param>\\n/// <param name=\"fileName\">The name of the file to upload.</param>\\n/// <param name=\"localPath\">The local path where the file is stored.</\\nparam>\\n/// <returns>A boolean value indicating the success of the action.</\\nreturns>\\npublic static async Task<bool> UploadSingleFileAsync(\\nTransferUtility transferUtil,\\nstring bucketName,', '']]]\n",
      "[[['', 'string fileName,\\nstring localPath)\\n{\\nif (File.Exists($\"{localPath}\\\\\\\\{fileName}\"))\\n{\\ntry\\n{\\nawait transferUtil.UploadAsync(new\\nTransferUtilityUploadRequest\\n{\\nBucketName = bucketName,\\nKey = fileName,\\nFilePath = $\"{localPath}\\\\\\\\{fileName}\",\\n});\\nreturn true;\\n}\\ncatch (AmazonS3Exception s3Ex)\\n{\\nConsole.WriteLine($\"Could not upload {fileName} from\\n{localPath} because:\");\\nConsole.WriteLine(s3Ex.Message);\\nreturn false;\\n}\\n}\\nelse\\n{\\nConsole.WriteLine($\"{fileName} does not exist in {localPath}\");\\nreturn false;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Uploads all the files in a local directory to a directory in an S3\\n/// bucket.\\n/// </summary>\\n/// <param name=\"transferUtil\">The transfer initialized TransferUtility\\n/// object.</param>\\n/// <param name=\"bucketName\">The name of the S3 bucket where the files', '']]]\n",
      "[[['', '/// will be stored.</param>\\n/// <param name=\"keyPrefix\">The key prefix is the S3 directory where\\n/// the files will be stored.</param>\\n/// <param name=\"localPath\">The local directory that contains the files\\n/// to be uploaded.</param>\\n/// <returns>A Boolean value representing the success of the action.</\\nreturns>\\npublic static async Task<bool> UploadFullDirectoryAsync(\\nTransferUtility transferUtil,\\nstring bucketName,\\nstring keyPrefix,\\nstring localPath)\\n{\\nif (Directory.Exists(localPath))\\n{\\ntry\\n{\\nawait transferUtil.UploadDirectoryAsync(new\\nTransferUtilityUploadDirectoryRequest\\n{\\nBucketName = bucketName,\\nKeyPrefix = keyPrefix,\\nDirectory = localPath,\\n});\\nreturn true;\\n}\\ncatch (AmazonS3Exception s3Ex)\\n{\\nConsole.WriteLine($\"Can\\'t upload the contents of {localPath}\\nbecause:\");\\nConsole.WriteLine(s3Ex?.Message);\\nreturn false;\\n}\\n}\\nelse\\n{\\nConsole.WriteLine($\"The directory {localPath} does not exist.\");\\nreturn false;\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '/// <summary>\\n/// Download a single file from an S3 bucket to the local computer.\\n/// </summary>\\n/// <param name=\"transferUtil\">The transfer initialized TransferUtility\\n/// object.</param>\\n/// <param name=\"bucketName\">The name of the S3 bucket containing the\\n/// file to download.</param>\\n/// <param name=\"keyName\">The name of the file to download.</param>\\n/// <param name=\"localPath\">The path on the local computer where the\\n/// downloaded file will be saved.</param>\\n/// <returns>A Boolean value indicating the results of the action.</\\nreturns>\\npublic static async Task<bool> DownloadSingleFileAsync(\\nTransferUtility transferUtil,\\nstring bucketName,\\nstring keyName,\\nstring localPath)\\n{\\nawait transferUtil.DownloadAsync(new TransferUtilityDownloadRequest\\n{\\nBucketName = bucketName,\\nKey = keyName,\\nFilePath = $\"{localPath}\\\\\\\\{keyName}\",\\n});\\nreturn (File.Exists($\"{localPath}\\\\\\\\{keyName}\"));\\n}', ''], ['', '', '']], [['', '', ''], ['', '/// <summary>\\n/// Downloads the contents of a directory in an S3 bucket to a\\n/// directory on the local computer.\\n/// </summary>\\n/// <param name=\"transferUtil\">The transfer initialized TransferUtility\\n/// object.</param>', '']]]\n",
      "[[['', '/// <param name=\"bucketName\">The bucket containing the files to\\ndownload.</param>\\n/// <param name=\"s3Path\">The S3 directory where the files are located.</\\nparam>\\n/// <param name=\"localPath\">The local path to which the files will be\\n/// saved.</param>\\n/// <returns>A Boolean value representing the success of the action.</\\nreturns>\\npublic static async Task<bool> DownloadS3DirectoryAsync(\\nTransferUtility transferUtil,\\nstring bucketName,\\nstring s3Path,\\nstring localPath)\\n{\\nint fileCount = 0;\\n// If the directory doesn\\'t exist, it will be created.\\nif (Directory.Exists(s3Path))\\n{\\nvar files = Directory.GetFiles(localPath);\\nfileCount = files.Length;\\n}\\nawait transferUtil.DownloadDirectoryAsync(new\\nTransferUtilityDownloadDirectoryRequest\\n{\\nBucketName = bucketName,\\nLocalDirectory = localPath,\\nS3Directory = s3Path,\\n});\\nif (Directory.Exists(localPath))\\n{\\nvar files = Directory.GetFiles(localPath);\\nif (files.Length > fileCount)\\n{\\nreturn true;\\n}\\n// No change in the number of files. Assume\\n// the download failed.\\nreturn false;\\n}', '']]]\n",
      "[[['', \"// The local directory doesn't exist. No files\\n// were downloaded.\\nreturn false;\\n}\", ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Transfer;\\n/// <summary>\\n/// This example shows how to track the progress of a multipart upload\\n/// using the Amazon Simple Storage Service (Amazon S3) TransferUtility to\\n/// upload to an Amazon S3 bucket.\\n/// </summary>\\npublic class TrackMPUUsingHighLevelAPI\\n{\\npublic static async Task Main()\\n{\\nstring bucketName = \"doc-example-bucket\";\\nstring keyName = \"sample_pic.png\";\\nstring path = \"filepath/directory/\";\\nstring filePath = $\"{path}{keyName}\";\\n// If the AWS Region defined for your default user is different\\n// from the Region where your Amazon S3 bucket is located,\\n// pass the Region name to the Amazon S3 client object\\'s constructor.\\n// For example: RegionEndpoint.USWest2 or RegionEndpoint.USEast2.\\nIAmazonS3 client = new AmazonS3Client();\\nawait TrackMPUAsync(client, bucketName, filePath, keyName);\\n}\\n/// <summary>\\n/// Starts an Amazon S3 multipart upload and assigns an event handler to\\n/// track the progress of the upload.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 client object used to\\n/// perform the multipart upload.</param>', '']]]\n",
      "[[['', '/// <param name=\"bucketName\">The name of the bucket to which to upload\\n/// the file.</param>\\n/// <param name=\"filePath\">The path, including the file name of the\\n/// file to be uploaded to the Amazon S3 bucket.</param>\\n/// <param name=\"keyName\">The file name to be used in the\\n/// destination Amazon S3 bucket.</param>\\npublic static async Task TrackMPUAsync(\\nIAmazonS3 client,\\nstring bucketName,\\nstring filePath,\\nstring keyName)\\n{\\ntry\\n{\\nvar fileTransferUtility = new TransferUtility(client);\\n// Use TransferUtilityUploadRequest to configure options.\\n// In this example we subscribe to an event.\\nvar uploadRequest =\\nnew TransferUtilityUploadRequest\\n{\\nBucketName = bucketName,\\nFilePath = filePath,\\nKey = keyName,\\n};\\nuploadRequest.UploadProgressEvent +=\\nnew EventHandler<UploadProgressArgs>(\\nUploadRequest_UploadPartProgressEvent);\\nawait fileTransferUtility.UploadAsync(uploadRequest);\\nConsole.WriteLine(\"Upload completed\");\\n}\\ncatch (AmazonS3Exception ex)\\n{\\nConsole.WriteLine($\"Error:: {ex.Message}\");\\n}\\n}\\n/// <summary>\\n/// Event handler to check the progress of the multipart upload.\\n/// </summary>\\n/// <param name=\"sender\">The object that raised the event.</param>\\n/// <param name=\"e\">The object that contains multipart upload', '']]]\n",
      "[[['', '/// information.</param>\\npublic static void UploadRequest_UploadPartProgressEvent(object sender,\\nUploadProgressArgs e)\\n{\\n// Process event.\\nConsole.WriteLine($\"{e.TransferredBytes}/{e.TotalBytes}\");\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'using System;\\nusing System.Collections.Generic;\\nusing System.IO;\\nusing System.Security.Cryptography;\\nusing System.Threading.Tasks;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\n/// <summary>\\n/// Uses the Amazon Simple Storage Service (Amazon S3) low level API to\\n/// perform a multipart upload to an Amazon S3 bucket.\\n/// </summary>\\npublic class SSECLowLevelMPUcopyObject\\n{\\npublic static async Task Main()\\n{\\nstring existingBucketName = \"doc-example-bucket\";\\nstring sourceKeyName = \"sample_file.txt\";\\nstring targetKeyName = \"sample_file_copy.txt\";\\nstring filePath = $\"sample\\\\\\\\{targetKeyName}\";\\n// If the AWS Region defined for your default user is different\\n// from the Region where your Amazon S3 bucket is located,\\n// pass the Region name to the Amazon S3 client object\\'s constructor.\\n// For example: RegionEndpoint.USEast1.\\nIAmazonS3 client = new AmazonS3Client();\\n// Create the encryption key.\\nvar base64Key = CreateEncryptionKey();', '']]]\n",
      "[[['', 'await CreateSampleObjUsingClientEncryptionKeyAsync(\\nclient,\\nexistingBucketName,\\nsourceKeyName,\\nfilePath,\\nbase64Key);\\n}\\n/// <summary>\\n/// Creates the encryption key to use with the multipart upload.\\n/// </summary>\\n/// <returns>A string containing the base64-encoded key for encrypting\\n/// the multipart upload.</returns>\\npublic static string CreateEncryptionKey()\\n{\\nAes aesEncryption = Aes.Create();\\naesEncryption.KeySize = 256;\\naesEncryption.GenerateKey();\\nstring base64Key = Convert.ToBase64String(aesEncryption.Key);\\nreturn base64Key;\\n}\\n/// <summary>\\n/// Creates and uploads an object using a multipart upload.\\n/// </summary>\\n/// <param name=\"client\">The initialized Amazon S3 object used to\\n/// initialize and perform the multipart upload.</param>\\n/// <param name=\"existingBucketName\">The name of the bucket to which\\n/// the object will be uploaded.</param>\\n/// <param name=\"sourceKeyName\">The source object name.</param>\\n/// <param name=\"filePath\">The location of the source object.</param>\\n/// <param name=\"base64Key\">The encryption key to use with the upload.</\\nparam>\\npublic static async Task CreateSampleObjUsingClientEncryptionKeyAsync(\\nIAmazonS3 client,\\nstring existingBucketName,\\nstring sourceKeyName,\\nstring filePath,\\nstring base64Key)\\n{\\nList<UploadPartResponse> uploadResponses = new\\nList<UploadPartResponse>();', '']]]\n",
      "[[['', 'InitiateMultipartUploadRequest initiateRequest = new\\nInitiateMultipartUploadRequest\\n{\\nBucketName = existingBucketName,\\nKey = sourceKeyName,\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key,\\n};\\nInitiateMultipartUploadResponse initResponse =\\nawait client.InitiateMultipartUploadAsync(initiateRequest);\\nlong contentLength = new FileInfo(filePath).Length;\\nlong partSize = 5 * (long)Math.Pow(2, 20); // 5 MB\\ntry\\n{\\nlong filePosition = 0;\\nfor (int i = 1; filePosition < contentLength; i++)\\n{\\nUploadPartRequest uploadRequest = new UploadPartRequest\\n{\\nBucketName = existingBucketName,\\nKey = sourceKeyName,\\nUploadId = initResponse.UploadId,\\nPartNumber = i,\\nPartSize = partSize,\\nFilePosition = filePosition,\\nFilePath = filePath,\\nServerSideEncryptionCustomerMethod =\\nServerSideEncryptionCustomerMethod.AES256,\\nServerSideEncryptionCustomerProvidedKey = base64Key,\\n};\\n// Upload part and add response to our list.\\nuploadResponses.Add(await\\nclient.UploadPartAsync(uploadRequest));\\nfilePosition += partSize;\\n}\\nCompleteMultipartUploadRequest completeRequest = new\\nCompleteMultipartUploadRequest', '']]]\n",
      "[[['', '{\\nBucketName = existingBucketName,\\nKey = sourceKeyName,\\nUploadId = initResponse.UploadId,\\n};\\ncompleteRequest.AddPartETags(uploadResponses);\\nCompleteMultipartUploadResponse completeUploadResponse =\\nawait client.CompleteMultipartUploadAsync(completeRequest);\\n}\\ncatch (Exception exception)\\n{\\nConsole.WriteLine($\"Exception occurred: {exception.Message}\");\\n// If there was an error, abort the multipart upload.\\nAbortMultipartUploadRequest abortMPURequest = new\\nAbortMultipartUploadRequest\\n{\\nBucketName = existingBucketName,\\nKey = sourceKeyName,\\nUploadId = initResponse.UploadId,\\n};\\nawait client.AbortMultipartUploadAsync(abortMPURequest);\\n}\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '// BucketBasics encapsulates the Amazon Simple Storage Service (Amazon S3)\\nactions\\n// used in the examples.\\n// It contains S3Client, an Amazon S3 service client that is used to perform\\nbucket\\n// and object actions.\\ntype BucketBasics struct {\\nS3Client *s3.Client\\n}\\n// UploadLargeObject uses an upload manager to upload data to an object in a\\nbucket.\\n// The upload manager breaks large data into parts and uploads the parts\\nconcurrently.\\nfunc (basics BucketBasics) UploadLargeObject(bucketName string, objectKey string,\\nlargeObject []byte) error {\\nlargeBuffer := bytes.NewReader(largeObject)\\nvar partMiBs int64 = 10\\nuploader := manager.NewUploader(basics.S3Client, func(u *manager.Uploader) {\\nu.PartSize = partMiBs * 1024 * 1024\\n})\\n_, err := uploader.Upload(context.TODO(), &s3.PutObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\nBody: largeBuffer,\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t upload large object to %v:%v. Here\\'s why: %v\\\\n\",\\nbucketName, objectKey, err)\\n}\\nreturn err\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '// DownloadLargeObject uses a download manager to download an object from a\\nbucket.\\n// The download manager gets the data in parts and writes them to a buffer until\\nall of\\n// the data has been downloaded.\\nfunc (basics BucketBasics) DownloadLargeObject(bucketName string, objectKey\\nstring) ([]byte, error) {\\nvar partMiBs int64 = 10\\ndownloader := manager.NewDownloader(basics.S3Client, func(d *manager.Downloader)\\n{\\nd.PartSize = partMiBs * 1024 * 1024\\n})\\nbuffer := manager.NewWriteAtBuffer([]byte{})\\n_, err := downloader.Download(context.TODO(), buffer, &s3.GetObjectInput{\\nBucket: aws.String(bucketName),\\nKey: aws.String(objectKey),\\n})\\nif err != nil {\\nlog.Printf(\"Couldn\\'t download large object from %v:%v. Here\\'s why: %v\\\\n\",\\nbucketName, objectKey, err)\\n}\\nreturn buffer.Bytes(), err\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public Integer downloadObjectsToDirectory(S3TransferManager transferManager,\\nURI destinationPathURI, String bucketName) {\\nDirectoryDownload directoryDownload =\\ntransferManager.downloadDirectory(DownloadDirectoryRequest.builder()\\n.destination(Paths.get(destinationPathURI))\\n.bucket(bucketName)\\n.build());\\nCompletedDirectoryDownload completedDirectoryDownload =\\ndirectoryDownload.completionFuture().join();\\ncompletedDirectoryDownload.failedTransfers()\\n.forEach(fail -> logger.warn(\"Object [{}] failed to transfer\",\\nfail.toString()));\\nreturn completedDirectoryDownload.failedTransfers().size();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public Integer uploadDirectory(S3TransferManager transferManager,\\nURI sourceDirectory, String bucketName) {\\nDirectoryUpload directoryUpload =\\ntransferManager.uploadDirectory(UploadDirectoryRequest.builder()\\n.source(Paths.get(sourceDirectory))\\n.bucket(bucketName)\\n.build());\\nCompletedDirectoryUpload completedDirectoryUpload =\\ndirectoryUpload.completionFuture().join();\\ncompletedDirectoryUpload.failedTransfers()\\n.forEach(fail -> logger.warn(\"Object [{}] failed to transfer\",\\nfail.toString()));\\nreturn completedDirectoryUpload.failedTransfers().size();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public String uploadFile(S3TransferManager transferManager, String\\nbucketName,\\nString key, URI filePathURI) {\\nUploadFileRequest uploadFileRequest = UploadFileRequest.builder()', '']]]\n",
      "[[['', '.putObjectRequest(b -> b.bucket(bucketName).key(key))\\n.source(Paths.get(filePathURI))\\n.build();\\nFileUpload fileUpload = transferManager.uploadFile(uploadFileRequest);\\nCompletedFileUpload uploadResult = fileUpload.completionFuture().join();\\nreturn uploadResult.response().eTag();\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import {\\nCreateMultipartUploadCommand,\\nUploadPartCommand,\\nCompleteMultipartUploadCommand,\\nAbortMultipartUploadCommand,\\nS3Client,\\n} from \"@aws-sdk/client-s3\";\\nconst twentyFiveMB = 25 * 1024 * 1024;\\nexport const createString = (size = twentyFiveMB) => {\\nreturn \"x\".repeat(size);\\n};\\nexport const main = async () => {\\nconst s3Client = new S3Client({});\\nconst bucketName = \"test-bucket\";\\nconst key = \"multipart.txt\";\\nconst str = createString();', '']]]\n",
      "[[['', 'const buffer = Buffer.from(str, \"utf8\");\\nlet uploadId;\\ntry {\\nconst multipartUpload = await s3Client.send(\\nnew CreateMultipartUploadCommand({\\nBucket: bucketName,\\nKey: key,\\n}),\\n);\\nuploadId = multipartUpload.UploadId;\\nconst uploadPromises = [];\\n// Multipart uploads require a minimum size of 5 MB per part.\\nconst partSize = Math.ceil(buffer.length / 5);\\n// Upload each part.\\nfor (let i = 0; i < 5; i++) {\\nconst start = i * partSize;\\nconst end = start + partSize;\\nuploadPromises.push(\\ns3Client\\n.send(\\nnew UploadPartCommand({\\nBucket: bucketName,\\nKey: key,\\nUploadId: uploadId,\\nBody: buffer.subarray(start, end),\\nPartNumber: i + 1,\\n}),\\n)\\n.then((d) => {\\nconsole.log(\"Part\", i + 1, \"uploaded\");\\nreturn d;\\n}),\\n);\\n}\\nconst uploadResults = await Promise.all(uploadPromises);\\nreturn await s3Client.send(\\nnew CompleteMultipartUploadCommand({', '']]]\n",
      "[[['', 'Bucket: bucketName,\\nKey: key,\\nUploadId: uploadId,\\nMultipartUpload: {\\nParts: uploadResults.map(({ ETag }, i) => ({\\nETag,\\nPartNumber: i + 1,\\n})),\\n},\\n}),\\n);\\n// Verify the output by downloading the file from the Amazon Simple Storage\\nService (Amazon S3) console.\\n// Because the output is a 25 MB string, text editors might struggle to open\\nthe file.\\n} catch (err) {\\nconsole.error(err);\\nif (uploadId) {\\nconst abortCommand = new AbortMultipartUploadCommand({\\nBucket: bucketName,\\nKey: key,\\nUploadId: uploadId,\\n});\\nawait s3Client.send(abortCommand);\\n}\\n}\\n};', ''], ['', '', '']], [['', '', ''], ['', 'import { GetObjectCommand, S3Client } from \"@aws-sdk/client-s3\";\\nimport { createWriteStream } from \"fs\";\\nconst s3Client = new S3Client({});\\nconst oneMB = 1024 * 1024;\\nexport const getObjectRange = ({ bucket, key, start, end }) => {\\nconst command = new GetObjectCommand({\\nBucket: bucket,', '']]]\n",
      "[[['', 'Key: key,\\nRange: `bytes=${start}-${end}`,\\n});\\nreturn s3Client.send(command);\\n};\\n/**\\n* @param {string | undefined} contentRange\\n*/\\nexport const getRangeAndLength = (contentRange) => {\\nconst [range, length] = contentRange.split(\"/\");\\nconst [start, end] = range.split(\"-\");\\nreturn {\\nstart: parseInt(start),\\nend: parseInt(end),\\nlength: parseInt(length),\\n};\\n};\\nexport const isComplete = ({ end, length }) => end === length - 1;\\n// When downloading a large file, you might want to break it down into\\n// smaller pieces. Amazon S3 accepts a Range header to specify the start\\n// and end of the byte range to be downloaded.\\nconst downloadInChunks = async ({ bucket, key }) => {\\nconst writeStream = createWriteStream(\\nfileURLToPath(new URL(`./${key}`, import.meta.url)),\\n).on(\"error\", (err) => console.error(err));\\nlet rangeAndLength = { start: -1, end: -1, length: -1 };\\nwhile (!isComplete(rangeAndLength)) {\\nconst { end } = rangeAndLength;\\nconst nextRange = { start: end + 1, end: end + oneMB };\\nconsole.log(`Downloading bytes ${nextRange.start} to ${nextRange.end}`);\\nconst { ContentRange, Body } = await getObjectRange({\\nbucket,\\nkey,\\n...nextRange,\\n});', '']]]\n",
      "[[['', 'writeStream.write(await Body.transformToByteArray());\\nrangeAndLength = getRangeAndLength(ContentRange);\\n}\\n};\\nexport const main = async () => {\\nawait downloadInChunks({\\nbucket: \"my-cool-bucket\",\\nkey: \"my-cool-object.txt\",\\n});\\n};', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import sys\\nimport threading\\nimport boto3\\nfrom boto3.s3.transfer import TransferConfig\\nMB = 1024 * 1024\\ns3 = boto3.resource(\"s3\")\\nclass TransferCallback:\\n\"\"\"\\nHandle callbacks from the transfer manager.', '']]]\n",
      "[[['', 'The transfer manager periodically calls the __call__ method throughout\\nthe upload and download process so that it can take action, such as\\ndisplaying progress to the user and collecting data about the transfer.\\n\"\"\"\\ndef __init__(self, target_size):\\nself._target_size = target_size\\nself._total_transferred = 0\\nself._lock = threading.Lock()\\nself.thread_info = {}\\ndef __call__(self, bytes_transferred):\\n\"\"\"\\nThe callback method that is called by the transfer manager.\\nDisplay progress during file transfer and collect per-thread transfer\\ndata. This method can be called by multiple threads, so shared instance\\ndata is protected by a thread lock.\\n\"\"\"\\nthread = threading.current_thread()\\nwith self._lock:\\nself._total_transferred += bytes_transferred\\nif thread.ident not in self.thread_info.keys():\\nself.thread_info[thread.ident] = bytes_transferred\\nelse:\\nself.thread_info[thread.ident] += bytes_transferred\\ntarget = self._target_size * MB\\nsys.stdout.write(\\nf\"\\\\r{self._total_transferred} of {target} transferred \"\\nf\"({(self._total_transferred / target) * 100:.2f}%).\"\\n)\\nsys.stdout.flush()\\ndef upload_with_default_configuration(\\nlocal_file_path, bucket_name, object_key, file_size_mb\\n):\\n\"\"\"\\nUpload a file from a local folder to an Amazon S3 bucket, using the default\\nconfiguration.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\ns3.Bucket(bucket_name).upload_file(', '']]]\n",
      "[[['', 'local_file_path, object_key, Callback=transfer_callback\\n)\\nreturn transfer_callback.thread_info\\ndef upload_with_chunksize_and_meta(\\nlocal_file_path, bucket_name, object_key, file_size_mb, metadata=None\\n):\\n\"\"\"\\nUpload a file from a local folder to an Amazon S3 bucket, setting a\\nmultipart chunk size and adding metadata to the Amazon S3 object.\\nThe multipart chunk size controls the size of the chunks of data that are\\nsent in the request. A smaller chunk size typically results in the transfer\\nmanager using more threads for the upload.\\nThe metadata is a set of key-value pairs that are stored with the object\\nin Amazon S3.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\nconfig = TransferConfig(multipart_chunksize=1 * MB)\\nextra_args = {\"Metadata\": metadata} if metadata else None\\ns3.Bucket(bucket_name).upload_file(\\nlocal_file_path,\\nobject_key,\\nConfig=config,\\nExtraArgs=extra_args,\\nCallback=transfer_callback,\\n)\\nreturn transfer_callback.thread_info\\ndef upload_with_high_threshold(local_file_path, bucket_name, object_key,\\nfile_size_mb):\\n\"\"\"\\nUpload a file from a local folder to an Amazon S3 bucket, setting a\\nmultipart threshold larger than the size of the file.\\nSetting a multipart threshold larger than the size of the file results\\nin the transfer manager sending the file as a standard upload instead of\\na multipart upload.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)', '']]]\n",
      "[[['', 'config = TransferConfig(multipart_threshold=file_size_mb * 2 * MB)\\ns3.Bucket(bucket_name).upload_file(\\nlocal_file_path, object_key, Config=config, Callback=transfer_callback\\n)\\nreturn transfer_callback.thread_info\\ndef upload_with_sse(\\nlocal_file_path, bucket_name, object_key, file_size_mb, sse_key=None\\n):\\n\"\"\"\\nUpload a file from a local folder to an Amazon S3 bucket, adding server-side\\nencryption with customer-provided encryption keys to the object.\\nWhen this kind of encryption is specified, Amazon S3 encrypts the object\\nat rest and allows downloads only when the expected encryption key is\\nprovided in the download request.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\nif sse_key:\\nextra_args = {\"SSECustomerAlgorithm\": \"AES256\", \"SSECustomerKey\":\\nsse_key}\\nelse:\\nextra_args = None\\ns3.Bucket(bucket_name).upload_file(\\nlocal_file_path, object_key, ExtraArgs=extra_args,\\nCallback=transfer_callback\\n)\\nreturn transfer_callback.thread_info\\ndef download_with_default_configuration(\\nbucket_name, object_key, download_file_path, file_size_mb\\n):\\n\"\"\"\\nDownload a file from an Amazon S3 bucket to a local folder, using the\\ndefault configuration.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\ns3.Bucket(bucket_name).Object(object_key).download_file(\\ndownload_file_path, Callback=transfer_callback\\n)\\nreturn transfer_callback.thread_info', '']]]\n",
      "[[['', 'def download_with_single_thread(\\nbucket_name, object_key, download_file_path, file_size_mb\\n):\\n\"\"\"\\nDownload a file from an Amazon S3 bucket to a local folder, using a\\nsingle thread.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\nconfig = TransferConfig(use_threads=False)\\ns3.Bucket(bucket_name).Object(object_key).download_file(\\ndownload_file_path, Config=config, Callback=transfer_callback\\n)\\nreturn transfer_callback.thread_info\\ndef download_with_high_threshold(\\nbucket_name, object_key, download_file_path, file_size_mb\\n):\\n\"\"\"\\nDownload a file from an Amazon S3 bucket to a local folder, setting a\\nmultipart threshold larger than the size of the file.\\nSetting a multipart threshold larger than the size of the file results\\nin the transfer manager sending the file as a standard download instead\\nof a multipart download.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\nconfig = TransferConfig(multipart_threshold=file_size_mb * 2 * MB)\\ns3.Bucket(bucket_name).Object(object_key).download_file(\\ndownload_file_path, Config=config, Callback=transfer_callback\\n)\\nreturn transfer_callback.thread_info\\ndef download_with_sse(\\nbucket_name, object_key, download_file_path, file_size_mb, sse_key\\n):\\n\"\"\"\\nDownload a file from an Amazon S3 bucket to a local folder, adding a\\ncustomer-provided encryption key to the request.\\nWhen this kind of encryption is specified, Amazon S3 encrypts the object\\nat rest and allows downloads only when the expected encryption key is', '']]]\n",
      "[[['', 'provided in the download request.\\n\"\"\"\\ntransfer_callback = TransferCallback(file_size_mb)\\nif sse_key:\\nextra_args = {\"SSECustomerAlgorithm\": \"AES256\", \"SSECustomerKey\":\\nsse_key}\\nelse:\\nextra_args = None\\ns3.Bucket(bucket_name).Object(object_key).download_file(\\ndownload_file_path, ExtraArgs=extra_args, Callback=transfer_callback\\n)\\nreturn transfer_callback.thread_info', ''], ['', '', '']], [['', '', ''], ['', 'import hashlib\\nimport os\\nimport platform\\nimport shutil\\nimport time\\nimport boto3\\nfrom boto3.s3.transfer import TransferConfig\\nfrom botocore.exceptions import ClientError\\nfrom botocore.exceptions import ParamValidationError\\nfrom botocore.exceptions import NoCredentialsError\\nimport file_transfer\\nMB = 1024 * 1024\\n# These configuration attributes affect both uploads and downloads.\\nCONFIG_ATTRS = (\\n\"multipart_threshold\",\\n\"multipart_chunksize\",\\n\"max_concurrency\",\\n\"use_threads\",\\n)\\n# These configuration attributes affect only downloads.\\nDOWNLOAD_CONFIG_ATTRS = (\"max_io_queue\", \"io_chunksize\", \"num_download_attempts\")', '']]]\n",
      "[[['', 'class TransferDemoManager:\\n\"\"\"\\nManages the demonstration. Collects user input from a command line, reports\\ntransfer results, maintains a list of artifacts created during the\\ndemonstration, and cleans them up after the demonstration is completed.\\n\"\"\"\\ndef __init__(self):\\nself._s3 = boto3.resource(\"s3\")\\nself._chore_list = []\\nself._create_file_cmd = None\\nself._size_multiplier = 0\\nself.file_size_mb = 30\\nself.demo_folder = None\\nself.demo_bucket = None\\nself._setup_platform_specific()\\nself._terminal_width = shutil.get_terminal_size(fallback=(80, 80))[0]\\ndef collect_user_info(self):\\n\"\"\"\\nCollect local folder and Amazon S3 bucket name from the user. These\\nlocations are used to store files during the demonstration.\\n\"\"\"\\nwhile not self.demo_folder:\\nself.demo_folder = input(\\n\"Which file folder do you want to use to store \" \"demonstration\\nfiles? \"\\n)\\nif not os.path.isdir(self.demo_folder):\\nprint(f\"{self.demo_folder} isn\\'t a folder!\")\\nself.demo_folder = None\\nwhile not self.demo_bucket:\\nself.demo_bucket = input(\\n\"Which Amazon S3 bucket do you want to use to store \"\\n\"demonstration files? \"\\n)\\ntry:\\nself._s3.meta.client.head_bucket(Bucket=self.demo_bucket)\\nexcept ParamValidationError as err:\\nprint(err)\\nself.demo_bucket = None', '']]]\n",
      "[[['', 'except ClientError as err:\\nprint(err)\\nprint(\\nf\"Either {self.demo_bucket} doesn\\'t exist or you don\\'t \"\\nf\"have access to it.\"\\n)\\nself.demo_bucket = None\\ndef demo(\\nself, question, upload_func, download_func, upload_args=None,\\ndownload_args=None\\n):\\n\"\"\"Run a demonstration.\\nAsk the user if they want to run this specific demonstration.\\nIf they say yes, create a file on the local path, upload it\\nusing the specified upload function, then download it using the\\nspecified download function.\\n\"\"\"\\nif download_args is None:\\ndownload_args = {}\\nif upload_args is None:\\nupload_args = {}\\nquestion = question.format(self.file_size_mb)\\nanswer = input(f\"{question} (y/n)\")\\nif answer.lower() == \"y\":\\nlocal_file_path, object_key, download_file_path =\\nself._create_demo_file()\\nfile_transfer.TransferConfig = self._config_wrapper(\\nTransferConfig, CONFIG_ATTRS\\n)\\nself._report_transfer_params(\\n\"Uploading\", local_file_path, object_key, **upload_args\\n)\\nstart_time = time.perf_counter()\\nthread_info = upload_func(\\nlocal_file_path,\\nself.demo_bucket,\\nobject_key,\\nself.file_size_mb,\\n**upload_args,\\n)\\nend_time = time.perf_counter()', '']]]\n",
      "[[['', 'self._report_transfer_result(thread_info, end_time - start_time)\\nfile_transfer.TransferConfig = self._config_wrapper(\\nTransferConfig, CONFIG_ATTRS + DOWNLOAD_CONFIG_ATTRS\\n)\\nself._report_transfer_params(\\n\"Downloading\", object_key, download_file_path, **download_args\\n)\\nstart_time = time.perf_counter()\\nthread_info = download_func(\\nself.demo_bucket,\\nobject_key,\\ndownload_file_path,\\nself.file_size_mb,\\n**download_args,\\n)\\nend_time = time.perf_counter()\\nself._report_transfer_result(thread_info, end_time - start_time)\\ndef last_name_set(self):\\n\"\"\"Get the name set used for the last demo.\"\"\"\\nreturn self._chore_list[-1]\\ndef cleanup(self):\\n\"\"\"\\nRemove files from the demo folder, and uploaded objects from the\\nAmazon S3 bucket.\\n\"\"\"\\nprint(\"-\" * self._terminal_width)\\nfor local_file_path, s3_object_key, downloaded_file_path in\\nself._chore_list:\\nprint(f\"Removing {local_file_path}\")\\ntry:\\nos.remove(local_file_path)\\nexcept FileNotFoundError as err:\\nprint(err)\\nprint(f\"Removing {downloaded_file_path}\")\\ntry:\\nos.remove(downloaded_file_path)\\nexcept FileNotFoundError as err:\\nprint(err)\\nif self.demo_bucket:', '']]]\n",
      "[[['', 'print(f\"Removing {self.demo_bucket}:{s3_object_key}\")\\ntry:\\nself._s3.Bucket(self.demo_bucket).Object(s3_object_key).delete()\\nexcept ClientError as err:\\nprint(err)\\ndef _setup_platform_specific(self):\\n\"\"\"Set up platform-specific command used to create a large file.\"\"\"\\nif platform.system() == \"Windows\":\\nself._create_file_cmd = \"fsutil file createnew {} {}\"\\nself._size_multiplier = MB\\nelif platform.system() == \"Linux\" or platform.system() == \"Darwin\":\\nself._create_file_cmd = f\"dd if=/dev/urandom of={{}} \" f\"bs={MB}\\ncount={{}}\"\\nself._size_multiplier = 1\\nelse:\\nraise EnvironmentError(\\nf\"Demo of platform {platform.system()} isn\\'t supported.\"\\n)\\ndef _create_demo_file(self):\\n\"\"\"\\nCreate a file in the demo folder specified by the user. Store the local\\npath, object name, and download path for later cleanup.\\nOnly the local file is created by this method. The Amazon S3 object and\\ndownload file are created later during the demonstration.\\nReturns:\\nA tuple that contains the local file path, object name, and download\\nfile path.\\n\"\"\"\\nfile_name_template = \"TestFile{}-{}.demo\"\\nlocal_suffix = \"local\"\\nobject_suffix = \"s3object\"\\ndownload_suffix = \"downloaded\"\\nfile_tag = len(self._chore_list) + 1\\nlocal_file_path = os.path.join(\\nself.demo_folder, file_name_template.format(file_tag, local_suffix)\\n)\\ns3_object_key = file_name_template.format(file_tag, object_suffix)', '']]]\n",
      "[[['', 'downloaded_file_path = os.path.join(\\nself.demo_folder, file_name_template.format(file_tag,\\ndownload_suffix)\\n)\\nfilled_cmd = self._create_file_cmd.format(\\nlocal_file_path, self.file_size_mb * self._size_multiplier\\n)\\nprint(\\nf\"Creating file of size {self.file_size_mb} MB \"\\nf\"in {self.demo_folder} by running:\"\\n)\\nprint(f\"{\\'\\':4}{filled_cmd}\")\\nos.system(filled_cmd)\\nchore = (local_file_path, s3_object_key, downloaded_file_path)\\nself._chore_list.append(chore)\\nreturn chore\\ndef _report_transfer_params(self, verb, source_name, dest_name, **kwargs):\\n\"\"\"Report configuration and extra arguments used for a file transfer.\"\"\"\\nprint(\"-\" * self._terminal_width)\\nprint(f\"{verb} {source_name} ({self.file_size_mb} MB) to {dest_name}\")\\nif kwargs:\\nprint(\"With extra args:\")\\nfor arg, value in kwargs.items():\\nprint(f\\'{\"\":4}{arg:<20}: {value}\\')\\n@staticmethod\\ndef ask_user(question):\\n\"\"\"\\nAsk the user a yes or no question.\\nReturns:\\nTrue when the user answers \\'y\\' or \\'Y\\'; otherwise, False.\\n\"\"\"\\nanswer = input(f\"{question} (y/n) \")\\nreturn answer.lower() == \"y\"\\n@staticmethod\\ndef _config_wrapper(func, config_attrs):\\ndef wrapper(*args, **kwargs):', '']]]\n",
      "[[['', 'config = func(*args, **kwargs)\\nprint(\"With configuration:\")\\nfor attr in config_attrs:\\nprint(f\\'{\"\":4}{attr:<20}: {getattr(config, attr)}\\')\\nreturn config\\nreturn wrapper\\n@staticmethod\\ndef _report_transfer_result(thread_info, elapsed):\\n\"\"\"Report the result of a transfer, including per-thread data.\"\"\"\\nprint(f\"\\\\nUsed {len(thread_info)} threads.\")\\nfor ident, byte_count in thread_info.items():\\nprint(f\"{\\'\\':4}Thread {ident} copied {byte_count} bytes.\")\\nprint(f\"Your transfer took {elapsed:.2f} seconds.\")\\ndef main():\\n\"\"\"\\nRun the demonstration script for s3_file_transfer.\\n\"\"\"\\ndemo_manager = TransferDemoManager()\\ndemo_manager.collect_user_info()\\n# Upload and download with default configuration. Because the file is 30 MB\\n# and the default multipart_threshold is 8 MB, both upload and download are\\n# multipart transfers.\\ndemo_manager.demo(\\n\"Do you want to upload and download a {} MB file \"\\n\"using the default configuration?\",\\nfile_transfer.upload_with_default_configuration,\\nfile_transfer.download_with_default_configuration,\\n)\\n# Upload and download with multipart_threshold set higher than the size of\\n# the file. This causes the transfer manager to use standard transfers\\n# instead of multipart transfers.\\ndemo_manager.demo(\\n\"Do you want to upload and download a {} MB file \"\\n\"as a standard (not multipart) transfer?\",\\nfile_transfer.upload_with_high_threshold,\\nfile_transfer.download_with_high_threshold,\\n)', '']]]\n",
      "[[['', '# Upload with specific chunk size and additional metadata.\\n# Download with a single thread.\\ndemo_manager.demo(\\n\"Do you want to upload a {} MB file with a smaller chunk size and \"\\n\"then download the same file using a single thread?\",\\nfile_transfer.upload_with_chunksize_and_meta,\\nfile_transfer.download_with_single_thread,\\nupload_args={\\n\"metadata\": {\\n\"upload_type\": \"chunky\",\\n\"favorite_color\": \"aqua\",\\n\"size\": \"medium\",\\n}\\n},\\n)\\n# Upload using server-side encryption with customer-provided\\n# encryption keys.\\n# Generate a 256-bit key from a passphrase.\\nsse_key = hashlib.sha256(\"demo_passphrase\".encode(\"utf-8\")).digest()\\ndemo_manager.demo(\\n\"Do you want to upload and download a {} MB file using \"\\n\"server-side encryption?\",\\nfile_transfer.upload_with_sse,\\nfile_transfer.download_with_sse,\\nupload_args={\"sse_key\": sse_key},\\ndownload_args={\"sse_key\": sse_key},\\n)\\n# Download without specifying an encryption key to show that the\\n# encryption key must be included to download an encrypted object.\\nif demo_manager.ask_user(\\n\"Do you want to try to download the encrypted \"\\n\"object without sending the required key?\"\\n):\\ntry:\\n_, object_key, download_file_path = demo_manager.last_name_set()\\nfile_transfer.download_with_default_configuration(\\ndemo_manager.demo_bucket,\\nobject_key,\\ndownload_file_path,\\ndemo_manager.file_size_mb,\\n)\\nexcept ClientError as err:', '']]]\n",
      "[[['', 'print(\\n\"Got expected error when trying to download an encrypted \"\\n\"object without specifying encryption info:\"\\n)\\nprint(f\"{\\'\\':4}{err}\")\\n# Remove all created and downloaded files, remove all objects from\\n# S3 storage.\\nif demo_manager.ask_user(\\n\"Demonstration complete. Do you want to remove local files \" \"and S3\\nobjects?\"\\n):\\ndemo_manager.cleanup()\\nif __name__ == \"__main__\":\\ntry:\\nmain()\\nexcept NoCredentialsError as error:\\nprint(error)\\nprint(\\n\"To run this example, you must have valid credentials in \"\\n\"a shared credential file or set in environment variables.\"\\n)', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'use std::fs::File;\\nuse std::io::prelude::*;\\nuse std::path::Path;', '']]]\n",
      "[[['', 'use aws_config::meta::region::RegionProviderChain;\\nuse aws_sdk_s3::error::DisplayErrorContext;\\nuse aws_sdk_s3::operation::{\\ncreate_multipart_upload::CreateMultipartUploadOutput,\\nget_object::GetObjectOutput,\\n};\\nuse aws_sdk_s3::types::{CompletedMultipartUpload, CompletedPart};\\nuse aws_sdk_s3::{config::Region, Client as S3Client};\\nuse aws_smithy_types::byte_stream::{ByteStream, Length};\\nuse rand::distributions::Alphanumeric;\\nuse rand::{thread_rng, Rng};\\nuse s3_service::error::Error;\\nuse std::process;\\nuse uuid::Uuid;\\n//In bytes, minimum chunk size of 5MB. Increase CHUNK_SIZE to send larger chunks.\\nconst CHUNK_SIZE: u64 = 1024 * 1024 * 5;\\nconst MAX_CHUNKS: u64 = 10000;\\n#[tokio::main]\\npub async fn main() {\\nif let Err(err) = run_example().await {\\neprintln!(\"Error: {}\", DisplayErrorContext(err));\\nprocess::exit(1);\\n}\\n}\\nasync fn run_example() -> Result<(), Error> {\\nlet shared_config = aws_config::load_from_env().await;\\nlet client = S3Client::new(&shared_config);\\nlet bucket_name = format!(\"doc-example-bucket-{}\", Uuid::new_v4());\\nlet region_provider = RegionProviderChain::first_try(Region::new(\"us-\\nwest-2\"));\\nlet region = region_provider.region().await.unwrap();\\ns3_service::create_bucket(&client, &bucket_name, region.as_ref()).await?;\\nlet key = \"sample.txt\".to_string();\\nlet multipart_upload_res: CreateMultipartUploadOutput = client\\n.create_multipart_upload()\\n.bucket(&bucket_name)\\n.key(&key)\\n.send()\\n.await', '']]]\n",
      "[[['', '.unwrap();\\nlet upload_id = multipart_upload_res.upload_id().unwrap();\\n//Create a file of random characters for the upload.\\nlet mut file = File::create(&key).expect(\"Could not create sample file.\");\\n// Loop until the file is 5 chunks.\\nwhile file.metadata().unwrap().len() <= CHUNK_SIZE * 4 {\\nlet rand_string: String = thread_rng()\\n.sample_iter(&Alphanumeric)\\n.take(256)\\n.map(char::from)\\n.collect();\\nlet return_string: String = \"\\\\n\".to_string();\\nfile.write_all(rand_string.as_ref())\\n.expect(\"Error writing to file.\");\\nfile.write_all(return_string.as_ref())\\n.expect(\"Error writing to file.\");\\n}\\nlet path = Path::new(&key);\\nlet file_size = tokio::fs::metadata(path)\\n.await\\n.expect(\"it exists I swear\")\\n.len();\\nlet mut chunk_count = (file_size / CHUNK_SIZE) + 1;\\nlet mut size_of_last_chunk = file_size % CHUNK_SIZE;\\nif size_of_last_chunk == 0 {\\nsize_of_last_chunk = CHUNK_SIZE;\\nchunk_count -= 1;\\n}\\nif file_size == 0 {\\npanic!(\"Bad file size.\");\\n}\\nif chunk_count > MAX_CHUNKS {\\npanic!(\"Too many chunks! Try increasing your chunk size.\")\\n}\\nlet mut upload_parts: Vec<CompletedPart> = Vec::new();\\nfor chunk_index in 0..chunk_count {\\nlet this_chunk = if chunk_count - 1 == chunk_index {\\nsize_of_last_chunk', '']]]\n",
      "[[['', '} else {\\nCHUNK_SIZE\\n};\\nlet stream = ByteStream::read_from()\\n.path(path)\\n.offset(chunk_index * CHUNK_SIZE)\\n.length(Length::Exact(this_chunk))\\n.build()\\n.await\\n.unwrap();\\n//Chunk index needs to start at 0, but part numbers start at 1.\\nlet part_number = (chunk_index as i32) + 1;\\nlet upload_part_res = client\\n.upload_part()\\n.key(&key)\\n.bucket(&bucket_name)\\n.upload_id(upload_id)\\n.body(stream)\\n.part_number(part_number)\\n.send()\\n.await?;\\nupload_parts.push(\\nCompletedPart::builder()\\n.e_tag(upload_part_res.e_tag.unwrap_or_default())\\n.part_number(part_number)\\n.build(),\\n);\\n}\\nlet completed_multipart_upload: CompletedMultipartUpload =\\nCompletedMultipartUpload::builder()\\n.set_parts(Some(upload_parts))\\n.build();\\nlet _complete_multipart_upload_res = client\\n.complete_multipart_upload()\\n.bucket(&bucket_name)\\n.key(&key)\\n.multipart_upload(completed_multipart_upload)\\n.upload_id(upload_id)\\n.send()\\n.await\\n.unwrap();', '']]]\n",
      "[[['', 'let data: GetObjectOutput = s3_service::download_object(&client,\\n&bucket_name, &key).await?;\\nlet data_length: u64 = data\\n.content_length()\\n.unwrap_or_default()\\n.try_into()\\n.unwrap();\\nif file.metadata().unwrap().len() == data_length {\\nprintln!(\"Data lengths match.\");\\n} else {\\nprintln!(\"The data was not the same size!\");\\n}\\ns3_service::delete_objects(&client, &bucket_name)\\n.await\\n.expect(\"Error emptying bucket.\");\\ns3_service::delete_bucket(&client, &bucket_name)\\n.await\\n.expect(\"Error deleting bucket.\");\\nOk(())\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', \"import com.example.s3.util.AsyncExampleUtils;\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport software.amazon.awssdk.core.async.AsyncRequestBody;\\nimport software.amazon.awssdk.core.async.BlockingInputStreamAsyncRequestBody;\\nimport software.amazon.awssdk.core.exception.SdkException;\\nimport software.amazon.awssdk.services.s3.S3AsyncClient;\\nimport software.amazon.awssdk.services.s3.model.PutObjectResponse;\\nimport java.io.ByteArrayInputStream;\\nimport java.util.UUID;\\nimport java.util.concurrent.CompletableFuture;\\n/**\\n* @param s33CrtAsyncClient - To upload content from a stream of unknown\\nsize, use the AWS CRT-based S3 client. For more information, see\\n* https://docs.aws.amazon.com/sdk-for-java/latest/\\ndeveloper-guide/crt-based-s3-client.html.\\n* @param bucketName - The name of the bucket.\\n* @param key - The name of the object.\\n* @return software.amazon.awssdk.services.s3.model.PutObjectResponse -\\nReturns metadata pertaining to the put object operation.\\n*/\\npublic PutObjectResponse putObjectFromStream(S3AsyncClient s33CrtAsyncClient,\\nString bucketName, String key) {\\nBlockingInputStreamAsyncRequestBody body =\\nAsyncRequestBody.forBlockingInputStream(null); // 'null'\\nindicates a stream will be provided later.\\nCompletableFuture<PutObjectResponse> responseFuture =\", '']]]\n",
      "[[['', 's33CrtAsyncClient.putObject(r -> r.bucket(bucketName).key(key),\\nbody);\\n// AsyncExampleUtils.randomString() returns a random string up to 100\\ncharacters.\\nString randomString = AsyncExampleUtils.randomString();\\nlogger.info(\"random string to upload: {}: length={}\", randomString,\\nrandomString.length());\\n// Provide the stream of data to be uploaded.\\nbody.writeInputStream(new ByteArrayInputStream(randomString.getBytes()));\\nPutObjectResponse response = responseFuture.join(); // Wait for the\\nresponse.\\nlogger.info(\"Object {} uploaded to bucket {}.\", key, bucketName);\\nreturn response;\\n}\\n}', ''], ['', '', '']], [['', '', ''], ['', 'import com.example.s3.util.AsyncExampleUtils;\\nimport org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport software.amazon.awssdk.core.async.AsyncRequestBody;\\nimport software.amazon.awssdk.core.async.BlockingInputStreamAsyncRequestBody;\\nimport software.amazon.awssdk.core.exception.SdkException;\\nimport software.amazon.awssdk.transfer.s3.S3TransferManager;\\nimport software.amazon.awssdk.transfer.s3.model.CompletedUpload;\\nimport software.amazon.awssdk.transfer.s3.model.Upload;\\nimport java.io.ByteArrayInputStream;\\nimport java.util.UUID;\\n/**\\n* @param transferManager - To upload content from a stream of unknown size,\\nuse the S3TransferManager based on the AWS CRT-based S3 client.\\n* For more information, see https://\\ndocs.aws.amazon.com/sdk-for-java/latest/developer-guide/transfer-manager.html.\\n* @param bucketName - The name of the bucket.\\n* @param key - The name of the object.', '']]]\n",
      "[[['', '* @return - software.amazon.awssdk.transfer.s3.model.CompletedUpload - The\\nresult of the completed upload.\\n*/\\npublic CompletedUpload uploadStream(S3TransferManager transferManager, String\\nbucketName, String key) {\\nBlockingInputStreamAsyncRequestBody body =\\nAsyncRequestBody.forBlockingInputStream(null); // \\'null\\'\\nindicates a stream will be provided later.\\nUpload upload = transferManager.upload(builder -> builder\\n.requestBody(body)\\n.putObjectRequest(req -> req.bucket(bucketName).key(key))\\n.build());\\n// AsyncExampleUtils.randomString() returns a random string up to 100\\ncharacters.\\nString randomString = AsyncExampleUtils.randomString();\\nlogger.info(\"random string to upload: {}: length={}\", randomString,\\nrandomString.length());\\n// Provide the stream of data to be uploaded.\\nbody.writeInputStream(new ByteArrayInputStream(randomString.getBytes()));\\nreturn upload.completionFuture().join();\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']], [['', '', ''], ['', 'import org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\nimport software.amazon.awssdk.core.exception.SdkException;\\nimport software.amazon.awssdk.core.sync.RequestBody;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport software.amazon.awssdk.services.s3.model.ChecksumAlgorithm;\\nimport software.amazon.awssdk.services.s3.model.ChecksumMode;\\nimport software.amazon.awssdk.services.s3.model.CompletedMultipartUpload;\\nimport software.amazon.awssdk.services.s3.model.CompletedPart;\\nimport software.amazon.awssdk.services.s3.model.CreateMultipartUploadResponse;\\nimport software.amazon.awssdk.services.s3.model.GetObjectResponse;\\nimport software.amazon.awssdk.services.s3.model.UploadPartRequest;\\nimport software.amazon.awssdk.services.s3.model.UploadPartResponse;\\nimport software.amazon.awssdk.services.s3.waiters.S3Waiter;\\nimport software.amazon.awssdk.transfer.s3.S3TransferManager;\\nimport software.amazon.awssdk.transfer.s3.model.FileUpload;\\nimport software.amazon.awssdk.transfer.s3.model.UploadFileRequest;\\nimport java.io.FileInputStream;\\nimport java.io.IOException;\\nimport java.io.RandomAccessFile;\\nimport java.net.URISyntaxException;\\nimport java.net.URL;\\nimport java.nio.ByteBuffer;\\nimport java.nio.file.Paths;\\nimport java.security.DigestInputStream;\\nimport java.security.MessageDigest;\\nimport java.security.NoSuchAlgorithmException;\\nimport java.util.ArrayList;\\nimport java.util.Base64;', '']]]\n",
      "[[['', 'import java.util.List;\\nimport java.util.Objects;\\nimport java.util.UUID;', ''], ['', '', '']], [['', '', ''], ['', 'public void putObjectWithChecksum() {\\ns3Client.putObject(b -> b\\n.bucket(bucketName)\\n.key(key)\\n.checksumAlgorithm(ChecksumAlgorithm.CRC32),\\nRequestBody.fromString(\"This is a test\"));\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public GetObjectResponse getObjectWithChecksum() {\\nreturn s3Client.getObject(b -> b\\n.bucket(bucketName)\\n.key(key)\\n.checksumMode(ChecksumMode.ENABLED))\\n.response();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public void putObjectWithPrecalculatedChecksum(String filePath) {\\nString checksum = calculateChecksum(filePath, \"SHA-256\");\\ns3Client.putObject((b -> b\\n.bucket(bucketName)\\n.key(key)\\n.checksumSHA256(checksum)),\\nRequestBody.fromFile(Paths.get(filePath)));\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'public void multipartUploadWithChecksumTm(String filePath) {\\nS3TransferManager transferManager = S3TransferManager.create();\\nUploadFileRequest uploadFileRequest = UploadFileRequest.builder()\\n.putObjectRequest(b -> b\\n.bucket(bucketName)\\n.key(key)\\n.checksumAlgorithm(ChecksumAlgorithm.SHA1))\\n.source(Paths.get(filePath))\\n.build();\\nFileUpload fileUpload = transferManager.uploadFile(uploadFileRequest);\\nfileUpload.completionFuture().join();\\ntransferManager.close();\\n}', ''], ['', '', '']], [['', '', ''], ['', 'public void multipartUploadWithChecksumS3Client(String filePath) {\\nChecksumAlgorithm algorithm = ChecksumAlgorithm.CRC32;\\n// Initiate the multipart upload.\\nCreateMultipartUploadResponse createMultipartUploadResponse =\\ns3Client.createMultipartUpload(b -> b\\n.bucket(bucketName)\\n.key(key)\\n.checksumAlgorithm(algorithm)); // Checksum specified on\\ninitiation.\\nString uploadId = createMultipartUploadResponse.uploadId();\\n// Upload the parts of the file.\\nint partNumber = 1;\\nList<CompletedPart> completedParts = new ArrayList<>();', '']]]\n",
      "[[['', 'ByteBuffer bb = ByteBuffer.allocate(1024 * 1024 * 5); // 5 MB byte buffer\\ntry (RandomAccessFile file = new RandomAccessFile(filePath, \"r\")) {\\nlong fileSize = file.length();\\nlong position = 0;\\nwhile (position < fileSize) {\\nfile.seek(position);\\nlong read = file.getChannel().read(bb);\\nbb.flip(); // Swap position and limit before reading from the\\nbuffer.\\nUploadPartRequest uploadPartRequest = UploadPartRequest.builder()\\n.bucket(bucketName)\\n.key(key)\\n.uploadId(uploadId)\\n.checksumAlgorithm(algorithm) // Checksum specified on\\neach part.\\n.partNumber(partNumber)\\n.build();\\nUploadPartResponse partResponse = s3Client.uploadPart(\\nuploadPartRequest,\\nRequestBody.fromByteBuffer(bb));\\nCompletedPart part = CompletedPart.builder()\\n.partNumber(partNumber)\\n.checksumCRC32(partResponse.checksumCRC32()) // Provide\\nthe calculated checksum.\\n.eTag(partResponse.eTag())\\n.build();\\ncompletedParts.add(part);\\nbb.clear();\\nposition += read;\\npartNumber++;\\n}\\n} catch (IOException e) {\\nSystem.err.println(e.getMessage());\\n}\\n// Complete the multipart upload.\\ns3Client.completeMultipartUpload(b -> b\\n.bucket(bucketName)\\n.key(key)', '']]]\n",
      "[[['', '.uploadId(uploadId)\\n.multipartUpload(CompletedMultipartUpload.builder().parts(completedParts).build()\\n}', ')'], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the AWS Code Examples Repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'def create_versioned_bucket(bucket_name, prefix):\\n\"\"\"\\nCreates an Amazon S3 bucket, enables it for versioning, and configures a\\nlifecycle\\nthat expires noncurrent object versions after 7 days.\\nAdding a lifecycle configuration to a versioned bucket is a best practice.\\nIt helps prevent objects in the bucket from accumulating a large number of\\nnoncurrent versions, which can slow down request performance.\\nUsage is shown in the usage_demo_single_object function at the end of this\\nmodule.\\n:param bucket_name: The name of the bucket to create.\\n:param prefix: Identifies which objects are automatically expired under the\\nconfigured lifecycle rules.\\n:return: The newly created bucket.\\n\"\"\"\\ntry:\\nbucket = s3.create_bucket(\\nBucket=bucket_name,\\nCreateBucketConfiguration={\\n\"LocationConstraint\": s3.meta.client.meta.region_name\\n},\\n)\\nlogger.info(\"Created bucket %s.\", bucket.name)\\nexcept ClientError as error:\\nif error.response[\"Error\"][\"Code\"] == \"BucketAlreadyOwnedByYou\":\\nlogger.warning(\"Bucket %s already exists! Using it.\", bucket_name)\\nbucket = s3.Bucket(bucket_name)\\nelse:\\nlogger.exception(\"Couldn\\'t create bucket %s.\", bucket_name)\\nraise\\ntry:\\nbucket.Versioning().enable()\\nlogger.info(\"Enabled versioning on bucket %s.\", bucket.name)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t enable versioning on bucket %s.\", bucket.name)\\nraise\\ntry:\\nexpiration = 7', '']]]\n",
      "[[['', 'bucket.LifecycleConfiguration().put(\\nLifecycleConfiguration={\\n\"Rules\": [\\n{\\n\"Status\": \"Enabled\",\\n\"Prefix\": prefix,\\n\"NoncurrentVersionExpiration\": {\"NoncurrentDays\":\\nexpiration},\\n}\\n]\\n}\\n)\\nlogger.info(\\n\"Configured lifecycle to expire noncurrent versions after %s days \"\\n\"on bucket %s.\",\\nexpiration,\\nbucket.name,\\n)\\nexcept ClientError as error:\\nlogger.warning(\\n\"Couldn\\'t configure lifecycle on bucket %s because %s. \"\\n\"Continuing anyway.\",\\nbucket.name,\\nerror,\\n)\\nreturn bucket\\ndef rollback_object(bucket, object_key, version_id):\\n\"\"\"\\nRolls back an object to an earlier version by deleting all versions that\\noccurred after the specified rollback version.\\nUsage is shown in the usage_demo_single_object function at the end of this\\nmodule.\\n:param bucket: The bucket that holds the object to roll back.\\n:param object_key: The object to roll back.\\n:param version_id: The version ID to roll back to.\\n\"\"\"\\n# Versions must be sorted by last_modified date because delete markers are\\n# at the end of the list even when they are interspersed in time.', '']]]\n",
      "[[['', 'versions = sorted(\\nbucket.object_versions.filter(Prefix=object_key),\\nkey=attrgetter(\"last_modified\"),\\nreverse=True,\\n)\\nlogger.debug(\\n\"Got versions:\\\\n%s\",\\n\"\\\\n\".join(\\n[\\nf\"\\\\t{version.version_id}, last modified {version.last_modified}\"\\nfor version in versions\\n]\\n),\\n)\\nif version_id in [ver.version_id for ver in versions]:\\nprint(f\"Rolling back to version {version_id}\")\\nfor version in versions:\\nif version.version_id != version_id:\\nversion.delete()\\nprint(f\"Deleted version {version.version_id}\")\\nelse:\\nbreak\\nprint(f\"Active version is now {bucket.Object(object_key).version_id}\")\\nelse:\\nraise KeyError(\\nf\"{version_id} was not found in the list of versions for \"\\nf\"{object_key}.\"\\n)\\ndef revive_object(bucket, object_key):\\n\"\"\"\\nRevives a versioned object that was deleted by removing the object\\'s active\\ndelete marker.\\nA versioned object presents as deleted when its latest version is a delete\\nmarker.\\nBy removing the delete marker, we make the previous version the latest\\nversion\\nand the object then presents as *not* deleted.', '']]]\n",
      "[[['', 'Usage is shown in the usage_demo_single_object function at the end of this\\nmodule.\\n:param bucket: The bucket that contains the object.\\n:param object_key: The object to revive.\\n\"\"\"\\n# Get the latest version for the object.\\nresponse = s3.meta.client.list_object_versions(\\nBucket=bucket.name, Prefix=object_key, MaxKeys=1\\n)\\nif \"DeleteMarkers\" in response:\\nlatest_version = response[\"DeleteMarkers\"][0]\\nif latest_version[\"IsLatest\"]:\\nlogger.info(\\n\"Object %s was indeed deleted on %s. Let\\'s revive it.\",\\nobject_key,\\nlatest_version[\"LastModified\"],\\n)\\nobj = bucket.Object(object_key)\\nobj.Version(latest_version[\"VersionId\"]).delete()\\nlogger.info(\\n\"Revived %s, active version is now %s with body \\'%s\\'\",\\nobject_key,\\nobj.version_id,\\nobj.get()[\"Body\"].read(),\\n)\\nelse:\\nlogger.warning(\\n\"Delete marker is not the latest version for %s!\", object_key\\n)\\nelif \"Versions\" in response:\\nlogger.warning(\"Got an active version for %s, nothing to do.\",\\nobject_key)\\nelse:\\nlogger.error(\"Couldn\\'t get any version info for %s.\", object_key)\\ndef permanently_delete_object(bucket, object_key):\\n\"\"\"\\nPermanently deletes a versioned object by deleting all of its versions.', '']]]\n",
      "[[['', 'Usage is shown in the usage_demo_single_object function at the end of this\\nmodule.\\n:param bucket: The bucket that contains the object.\\n:param object_key: The object to delete.\\n\"\"\"\\ntry:\\nbucket.object_versions.filter(Prefix=object_key).delete()\\nlogger.info(\"Permanently deleted all versions of object %s.\", object_key)\\nexcept ClientError:\\nlogger.exception(\"Couldn\\'t delete all versions of %s.\", object_key)\\nraise', ''], ['', '', '']], [['', '', ''], ['', 'def usage_demo_single_object(obj_prefix=\"demo-versioning/\"):\\n\"\"\"\\nDemonstrates usage of versioned object functions. This demo uploads a stanza\\nof a poem and performs a series of revisions, deletions, and revivals on it.\\n:param obj_prefix: The prefix to assign to objects created by this demo.\\n\"\"\"\\nwith open(\"father_william.txt\") as file:\\nstanzas = file.read().split(\"\\\\n\\\\n\")\\nwidth = get_terminal_size((80, 20))[0]\\nprint(\"-\" * width)\\nprint(\"Welcome to the usage demonstration of Amazon S3 versioning.\")\\nprint(\\n\"This demonstration uploads a single stanza of a poem to an Amazon \"\\n\"S3 bucket and then applies various revisions to it.\"\\n)\\nprint(\"-\" * width)\\nprint(\"Creating a version-enabled bucket for the demo...\")\\nbucket = create_versioned_bucket(\"bucket-\" + str(uuid.uuid1()), obj_prefix)\\nprint(\"\\\\nThe initial version of our stanza:\")\\nprint(stanzas[0])\\n# Add the first stanza and revise it a few times.', '']]]\n",
      "[[['', 'print(\"\\\\nApplying some revisions to the stanza...\")\\nobj_stanza_1 = bucket.Object(f\"{obj_prefix}stanza-1\")\\nobj_stanza_1.put(Body=bytes(stanzas[0], \"utf-8\"))\\nobj_stanza_1.put(Body=bytes(stanzas[0].upper(), \"utf-8\"))\\nobj_stanza_1.put(Body=bytes(stanzas[0].lower(), \"utf-8\"))\\nobj_stanza_1.put(Body=bytes(stanzas[0][::-1], \"utf-8\"))\\nprint(\\n\"The latest version of the stanza is now:\",\\nobj_stanza_1.get()[\"Body\"].read().decode(\"utf-8\"),\\nsep=\"\\\\n\",\\n)\\n# Versions are returned in order, most recent first.\\nobj_stanza_1_versions =\\nbucket.object_versions.filter(Prefix=obj_stanza_1.key)\\nprint(\\n\"The version data of the stanza revisions:\",\\n*[\\nf\" {version.version_id}, last modified {version.last_modified}\"\\nfor version in obj_stanza_1_versions\\n],\\nsep=\"\\\\n\",\\n)\\n# Rollback two versions.\\nprint(\"\\\\nRolling back two versions...\")\\nrollback_object(bucket, obj_stanza_1.key, list(obj_stanza_1_versions)\\n[2].version_id)\\nprint(\\n\"The latest version of the stanza:\",\\nobj_stanza_1.get()[\"Body\"].read().decode(\"utf-8\"),\\nsep=\"\\\\n\",\\n)\\n# Delete the stanza\\nprint(\"\\\\nDeleting the stanza...\")\\nobj_stanza_1.delete()\\ntry:\\nobj_stanza_1.get()\\nexcept ClientError as error:\\nif error.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\\nprint(\"The stanza is now deleted (as expected).\")\\nelse:\\nraise', '']]]\n",
      "[[['', '# Revive the stanza\\nprint(\"\\\\nRestoring the stanza...\")\\nrevive_object(bucket, obj_stanza_1.key)\\nprint(\\n\"The stanza is restored! The latest version is again:\",\\nobj_stanza_1.get()[\"Body\"].read().decode(\"utf-8\"),\\nsep=\"\\\\n\",\\n)\\n# Permanently delete all versions of the object. This cannot be undone!\\nprint(\"\\\\nPermanently deleting all versions of the stanza...\")\\npermanently_delete_object(bucket, obj_stanza_1.key)\\nobj_stanza_1_versions =\\nbucket.object_versions.filter(Prefix=obj_stanza_1.key)\\nif len(list(obj_stanza_1_versions)) == 0:\\nprint(\"The stanza has been permanently deleted and now has no versions.\")\\nelse:\\nprint(\"Something went wrong. The stanza still exists!\")\\nprint(f\"\\\\nRemoving {bucket.name}...\")\\nbucket.delete()\\nprint(f\"{bucket.name} deleted.\")\\nprint(\"Demo done!\")', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the Serverless examples repository.\", ''], ['', '', '']], [['', '', ''], ['', \"// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nusing System.Threading.Tasks;\\nusing Amazon.Lambda.Core;\\nusing Amazon.S3;\\nusing System;\\nusing Amazon.Lambda.S3Events;\\nusing System.Web;\\n// Assembly attribute to enable the Lambda function's JSON input to be converted\\ninto a .NET class.\\n[assembly:\\nLambdaSerializer(typeof(Amazon.Lambda.Serialization.SystemTextJson.DefaultLambdaJ\", 's']]]\n",
      "[[['', 'namespace S3Integration\\n{\\npublic class Function\\n{\\nprivate static AmazonS3Client _s3Client;\\npublic Function() : this(null)\\n{\\n}\\ninternal Function(AmazonS3Client s3Client)\\n{\\n_s3Client = s3Client ?? new AmazonS3Client();\\n}\\npublic async Task<string> Handler(S3Event evt, ILambdaContext context)\\n{\\ntry\\n{\\nif (evt.Records.Count <= 0)\\n{\\ncontext.Logger.LogLine(\"Empty S3 Event received\");\\nreturn string.Empty;\\n}\\nvar bucket = evt.Records[0].S3.Bucket.Name;\\nvar key = HttpUtility.UrlDecode(evt.Records[0].S3.Object.Key);\\ncontext.Logger.LogLine($\"Request is for {bucket} and {key}\");\\nvar objectResult = await _s3Client.GetObjectAsync(bucket, key);\\ncontext.Logger.LogLine($\"Returning {objectResult.Key}\");\\nreturn objectResult.Key;\\n}\\ncatch (Exception e)\\n{\\ncontext.Logger.LogLine($\"Error processing request -\\n{e.Message}\");\\nreturn string.Empty;\\n}\\n}\\n}', '']]]\n",
      "[[['', '}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the Serverless examples repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\npackage main\\nimport (\\n\"context\"\\n\"log\"\\n\"github.com/aws/aws-lambda-go/events\"\\n\"github.com/aws/aws-lambda-go/lambda\"\\n\"github.com/aws/aws-sdk-go-v2/config\"\\n\"github.com/aws/aws-sdk-go-v2/service/s3\"\\n)\\nfunc handler(ctx context.Context, s3Event events.S3Event) error {\\nsdkConfig, err := config.LoadDefaultConfig(ctx)\\nif err != nil {\\nlog.Printf(\"failed to load default config: %s\", err)\\nreturn err\\n}\\ns3Client := s3.NewFromConfig(sdkConfig)\\nfor _, record := range s3Event.Records {\\nbucket := record.S3.Bucket.Name\\nkey := record.S3.Object.URLDecodedKey\\nheadOutput, err := s3Client.HeadObject(ctx, &s3.HeadObjectInput{\\nBucket: &bucket,\\nKey: &key,', '']]]\n",
      "[[['', '})\\nif err != nil {\\nlog.Printf(\"error getting head of object %s/%s: %s\", bucket, key, err)\\nreturn err\\n}\\nlog.Printf(\"successfully retrieved %s/%s of type %s\", bucket, key,\\n*headOutput.ContentType)\\n}\\nreturn nil\\n}\\nfunc main() {\\nlambda.Start(handler)\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the Serverless examples repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\npackage example;\\nimport software.amazon.awssdk.services.s3.model.HeadObjectRequest;\\nimport software.amazon.awssdk.services.s3.model.HeadObjectResponse;\\nimport software.amazon.awssdk.services.s3.S3Client;\\nimport com.amazonaws.services.lambda.runtime.Context;\\nimport com.amazonaws.services.lambda.runtime.RequestHandler;\\nimport com.amazonaws.services.lambda.runtime.events.S3Event;\\nimport\\ncom.amazonaws.services.lambda.runtime.events.models.s3.S3EventNotification.S3Even', 't']]]\n",
      "[[['', 'import org.slf4j.Logger;\\nimport org.slf4j.LoggerFactory;\\npublic class Handler implements RequestHandler<S3Event, String> {\\nprivate static final Logger logger = LoggerFactory.getLogger(Handler.class);\\n@Override\\npublic String handleRequest(S3Event s3event, Context context) {\\ntry {\\nS3EventNotificationRecord record = s3event.getRecords().get(0);\\nString srcBucket = record.getS3().getBucket().getName();\\nString srcKey = record.getS3().getObject().getUrlDecodedKey();\\nS3Client s3Client = S3Client.builder().build();\\nHeadObjectResponse headObject = getHeadObject(s3Client, srcBucket,\\nsrcKey);\\nlogger.info(\"Successfully retrieved \" + srcBucket + \"/\" + srcKey + \" of\\ntype \" + headObject.contentType());\\nreturn \"Ok\";\\n} catch (Exception e) {\\nthrow new RuntimeException(e);\\n}\\n}\\nprivate HeadObjectResponse getHeadObject(S3Client s3Client, String bucket,\\nString key) {\\nHeadObjectRequest headObjectRequest = HeadObjectRequest.builder()\\n.bucket(bucket)\\n.key(key)\\n.build();\\nreturn s3Client.headObject(headObjectRequest);\\n}\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the Serverless examples repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { S3Client, HeadObjectCommand } from \"@aws-sdk/client-s3\";\\nconst client = new S3Client();\\nexports.handler = async (event, context) => {\\n// Get the object from the event and show its content type\\nconst bucket = event.Records[0].s3.bucket.name;\\nconst key = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\\\+/g,\\n\\' \\'));\\ntry {\\nconst { ContentType } = await client.send(new HeadObjectCommand({\\nBucket: bucket,\\nKey: key,\\n}));\\nconsole.log(\\'CONTENT TYPE:\\', ContentType);\\nreturn ContentType;\\n} catch (err) {\\nconsole.log(err);\\nconst message = `Error getting object ${key} from bucket ${bucket}. Make\\nsure they exist and your bucket is in the same region as this function.`;\\nconsole.log(message);\\nthrow new Error(message);\\n}\\n};', '']]]\n",
      "[[['', '', ''], ['', '', '']], [['', '', ''], ['', \"// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nimport { S3Event } from 'aws-lambda';\\nimport { S3Client, HeadObjectCommand } from '@aws-sdk/client-s3';\\nconst s3 = new S3Client({ region: process.env.AWS_REGION });\\nexport const handler = async (event: S3Event): Promise<string | undefined> => {\\n// Get the object from the event and show its content type\\nconst bucket = event.Records[0].s3.bucket.name;\\nconst key = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\\\+/g, '\\n'));\\nconst params = {\\nBucket: bucket,\\nKey: key,\\n};\\ntry {\\nconst { ContentType } = await s3.send(new HeadObjectCommand(params));\\nconsole.log('CONTENT TYPE:', ContentType);\\nreturn ContentType;\\n} catch (err) {\\nconsole.log(err);\\nconst message = `Error getting object ${key} from bucket ${bucket}. Make sure\\nthey exist and your bucket is in the same region as this function.`;\\nconsole.log(message);\\nthrow new Error(message);\\n}\\n};\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the Serverless examples repository.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\n<?php\\nuse Bref\\\\Context\\\\Context;\\nuse Bref\\\\Event\\\\S3\\\\S3Event;\\nuse Bref\\\\Event\\\\S3\\\\S3Handler;\\nuse Bref\\\\Logger\\\\StderrLogger;\\nrequire __DIR__ . \\'/vendor/autoload.php\\';\\nclass Handler extends S3Handler\\n{\\nprivate StderrLogger $logger;\\npublic function __construct(StderrLogger $logger)\\n{\\n$this->logger = $logger;\\n}\\npublic function handleS3(S3Event $event, Context $context) : void\\n{\\n$this->logger->info(\"Processing S3 records\");\\n// Get the object from the event and show its content type\\n$records = $event->getRecords();\\nforeach ($records as $record)\\n{\\n$bucket = $record->getBucket()->getName();\\n$key = urldecode($record->getObject()->getKey());\\ntry {\\n$fileSize = urldecode($record->getObject()->getSize());\\necho \"File Size: \" . $fileSize . \"\\\\n\";\\n// TODO: Implement your custom processing logic here\\n} catch (Exception $e) {\\necho $e->getMessage() . \"\\\\n\";\\necho \\'Error getting object \\' . $key . \\' from bucket \\' .\\n$bucket . \\'. Make sure they exist and your bucket is in the same region as this\\nfunction.\\' . \"\\\\n\";\\nthrow $e;', '']]]\n",
      "[[['', '}\\n}\\n}\\n}\\n$logger = new StderrLogger();\\nreturn new Handler($logger);', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the Serverless examples repository.\", ''], ['', '', '']], [['', '', ''], ['', '# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n# SPDX-License-Identifier: Apache-2.0\\nimport json\\nimport urllib.parse\\nimport boto3\\nprint(\\'Loading function\\')\\ns3 = boto3.client(\\'s3\\')\\ndef lambda_handler(event, context):\\n#print(\"Received event: \" + json.dumps(event, indent=2))\\n# Get the object from the event and show its content type\\nbucket = event[\\'Records\\'][0][\\'s3\\'][\\'bucket\\'][\\'name\\']\\nkey = urllib.parse.unquote_plus(event[\\'Records\\'][0][\\'s3\\'][\\'object\\'][\\'key\\'],\\nencoding=\\'utf-8\\')\\ntry:\\nresponse = s3.get_object(Bucket=bucket, Key=key)\\nprint(\"CONTENT TYPE: \" + response[\\'ContentType\\'])', '']]]\n",
      "[[['', \"return response['ContentType']\\nexcept Exception as e:\\nprint(e)\\nprint('Error getting object {} from bucket {}. Make sure they exist and\\nyour bucket is in the same region as this function.'.format(key, bucket))\\nraise e\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the Serverless examples repository.\", ''], ['', '', '']], [['', '', ''], ['', 'require \\'json\\'\\nrequire \\'uri\\'\\nrequire \\'aws-sdk\\'\\nputs \\'Loading function\\'\\ndef lambda_handler(event:, context:)\\ns3 = Aws::S3::Client.new(region: \\'region\\') # Your AWS region\\n# puts \"Received event: #{JSON.dump(event)}\"\\n# Get the object from the event and show its content type\\nbucket = event[\\'Records\\'][0][\\'s3\\'][\\'bucket\\'][\\'name\\']\\nkey = URI.decode_www_form_component(event[\\'Records\\'][0][\\'s3\\'][\\'object\\'][\\'key\\'],\\nEncoding::UTF_8)\\nbegin\\nresponse = s3.get_object(bucket: bucket, key: key)\\nputs \"CONTENT TYPE: #{response.content_type}\"\\nreturn response.content_type\\nrescue StandardError => e\\nputs e.message\\nputs \"Error getting object #{key} from bucket #{bucket}. Make sure they exist\\nand your bucket is in the same region as this function.\"', '']]]\n",
      "[[['', 'raise e\\nend\\nend', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nThere's more on GitHub. Find the complete example and learn how to set up and run\\nin the Serverless examples repository.\", ''], ['', '', '']], [['', '', ''], ['', '// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\\n// SPDX-License-Identifier: Apache-2.0\\nuse aws_lambda_events::event::s3::S3Event;\\nuse aws_sdk_s3::{Client};\\nuse lambda_runtime::{run, service_fn, Error, LambdaEvent};\\n/// Main function\\n#[tokio::main]\\nasync fn main() -> Result<(), Error> {\\ntracing_subscriber::fmt()\\n.with_max_level(tracing::Level::INFO)\\n.with_target(false)\\n.without_time()\\n.init();\\n// Initialize the AWS SDK for Rust\\nlet config = aws_config::load_from_env().await;\\nlet s3_client = Client::new(&config);\\nlet res = run(service_fn(|request: LambdaEvent<S3Event>| {\\nfunction_handler(&s3_client, request)\\n})).await;\\nres', '']]]\n",
      "[[['', '}\\nasync fn function_handler(\\ns3_client: &Client,\\nevt: LambdaEvent<S3Event>\\n) -> Result<(), Error> {\\ntracing::info!(records = ?evt.payload.records.len(), \"Received request from\\nSQS\");\\nif evt.payload.records.len() == 0 {\\ntracing::info!(\"Empty S3 event received\");\\n}\\nlet bucket = evt.payload.records[0].s3.bucket.name.as_ref().expect(\"Bucket\\nname to exist\");\\nlet key = evt.payload.records[0].s3.object.key.as_ref().expect(\"Object key to\\nexist\");\\ntracing::info!(\"Request is for {} and object {}\", bucket, key);\\nlet s3_get_object_result = s3_client\\n.get_object()\\n.bucket(bucket)\\n.key(key)\\n.send()\\n.await;\\nmatch s3_get_object_result {\\nOk(_) => tracing::info!(\"S3 Get Object success, the s3GetObjectResult\\ncontains a \\'body\\' property of type ByteStream\"),\\nErr(_) => tracing::info!(\"Failure with S3 Get Object request\")\\n}\\nOk(())\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Important\\nOn May 13, 2024, we started deploying a change to eliminate charges for unauthorized\\nrequests that aren't initiated by the bucket owner. After the deployment of this change is\\ncompleted, bucket owners will never incur request or bandwidth charges for requests that\\nreturn AccessDenied (HTTP 403 Forbidden) errors when these requests are initiated\\nfrom outside of their individual AWS account or AWS organization. For more information\\non a full list of HTTP 3XX and 4XX status codes that won't be billed, see Billing for Amazon\\nS3 error responses. This billing change requires no updates to your applications and applies\\nto all S3 buckets. When deployment of this change is completed in all AWS Regions, we’ll\\nupdate our documentation.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nFor Access Denied (HTTP 403 Forbidden), S3 doesn't charge the bucket owner when\\nthe request is initiated outside of the bucket owner's individual AWS account or the bucket\\nowner's AWS organization.\", ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf you're trying to troubleshoot a permissions issue, start with the Bucket policies and IAM\\npolicies section, and be sure to follow the guidance in Tips for checking permissions.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nAllow statements in a bucket policy apply only to objects that are owned by the same\\nbucket-owning account. However, Deny statements in a bucket policy apply to all objects\\nregardless of object ownership.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nTo view or edit a bucket policy, you must have the s3:GetBucketPolicy permission.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nIf you get locked out of a bucket because of an incorrect bucket policy, sign in to the AWS\\nManagement Console by using your root user credentials. To regain access to your bucket,\\nmake sure to delete the bucket policy by using your root user credentials.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nYou cannot grant GET object permissions with bucket ACL settings.', ''], ['', '', '']], [['', '', ''], ['', \"Important\\nIf the account that owns the object is different from the account that owns the bucket,\\nthen access to the object isn't controlled by the bucket policy.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Important\\nTo prevent an Access Denied (403 Forbidden) error, be sure to migrate the ACL\\npermissions to a bucket policy before you disable ACLs. For more information, see Bucket\\npolicy examples for migrating from ACL permissions.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nWhen you update your Object Ownership setting to bucket owner preferred, the\\nsetting is only applied to new objects that are uploaded to the bucket.', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nFor cross-account uploads, you can also require the bucket-owner-full-control\\ncanned object ACL in your bucket policy. For an example bucket policy, see Grant cross-\\naccount permissions to upload objects while ensuring that the bucket owner has full\\ncontrol.', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nAmazon S3 doesn’t transition objects that are smaller than 128 KB from the S3 Standard or\\nS3 Standard-IA storage class to the S3 Intelligent-Tiering, S3 Standard-IA, or S3 One Zone-\\nIA storage class.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\n• Amazon S3 rounds the transition or expiration date of an object to midnight UTC the\\nnext day. For more information, see Lifecycle rules: Based on an object's age.\\n• For S3 objects that are protected by Object Lock, current versions are not permanently\\ndeleted. Instead, a delete marker is added to the objects, making them noncurrent.\\nNoncurrent versions are then preserved and are not permanently expired.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nFor S3 objects that are protected by Object Lock, current versions are not permanently\\ndeleted. Instead, a delete marker is added to the objects, making them noncurrent.\\nNoncurrent versions are then preserved and are not permanently expired.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nIf the S3 bucket is protected by AWS Backup or S3 Replication, you might also be able to\\nuse these features to recover your expired objects.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nTo run HeadObject, you must have read access to the object that you're requesting.\\nA HEAD request has the same options as a GET request, without performing a GET\\noperation. For example, to run a HeadObject request by using the AWS Command Line\\nInterface (AWS CLI), you can run the following command. Replace the user input\\nplaceholders with your own information.\\naws s3api head-object --bucket my-bucket --key index.html\", ''], ['', '', '']], [['', '', ''], ['', 'aws s3api head-object --bucket my-bucket --key index.html', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Effect\":\"Allow\",\\n\"Action\":[\\n\"s3:ObjectOwnerOverrideToBucketOwner\"\\n],\\n\"Resource\":\"arn:aws:s3:::DestinationBucket/*\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '{\\n\"Version\": \"2012-10-17\",\\n\"Id\": \"Policy1644945280205\",\\n\"Statement\": [\\n{\\n\"Sid\": \"Stmt1644945277847\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789101:role/s3-replication-role\"\\n},\\n\"Action\": [\\n\"s3:ReplicateObject\",\\n\"s3:ReplicateTags\",\\n\"s3:ObjectOwnerOverrideToBucketOwner\"\\n],\\n\"Resource\": \"arn:aws:s3:::DestinationBucket/*\"\\n}\\n]\\n}', ''], ['', '', '']], [['', '', ''], ['', \"Note\\nIf the destination bucket's object ownership settings include Bucket owner enforced,\\nthen you don't need to update the setting to Change object ownership to the\\ndestination bucket owner in the replication rule. The object ownership change will occur\\nby default. For more information about changing replica ownership, see Changing the\\nreplica owner.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf the destination bucket is in a different account, specify an AWS KMS customer\\nmanaged key that is owned by the destination account. Don't use the default Amazon\\nS3 managed key (aws/s3). Using the default key encrypts the objects with the Amazon\\nS3 managed key that is owned by the source account, preventing the object from being\\nshared with another account. As a result, the destination account won't be able to access\\nthe objects in the destination bucket.\", ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Sid\": \"AllowS3ReplicationSourceRoleToUseTheKey\",\\n\"Effect\": \"Allow\",\\n\"Principal\": {\\n\"AWS\": \"arn:aws:iam::123456789101:role/s3-replication-role\"\\n},\\n\"Action\": [\"kms:GenerateDataKey\", \"kms:Encrypt\"],\\n\"Resource\": \"*\"\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'Note\\nKMS key policies that restrict access to specific CIDR ranges, VPC endpoints, or S3 access\\npoints can cause replication to fail.', ''], ['', '', '']], [['', '', ''], ['', '\"kms:EncryptionContext:arn:aws:arn\": [\\n\"arn:aws:s3:::SOURCE_BUCKET_NAME\"\\n]\\n\"kms:EncryptionContext:arn:aws:arn\": [\\n\"arn:aws:s3:::DESTINATION_BUCKET_NAME\"\\n]', ''], ['', '', '']], [['', '', ''], ['', '{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"kms:Decrypt\",\\n\"kms:GenerateDataKey\"\\n],\\n\"Resource\": [\\n\"SourceKmsKeyArn\"\\n]\\n},\\n{\\n\"Effect\": \"Allow\",\\n\"Action\": [\\n\"kms:GenerateDataKey\",\\n\"kms:Encrypt\"\\n],\\n\"Resource\": [\\n\"DestinationKmsKeyArn\"\\n]\\n}', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '\"s3:GetReplicationConfiguration\",\\n\"s3:ListBucket\",\\n\"s3:GetObjectVersionForReplication\",\\n\"s3:GetObjectVersionAcl\",\\n\"s3:GetObjectVersionTagging\"', ''], ['', '', '']], [['', '', ''], ['', '\"s3:ReplicateObject\",\\n\"s3:ReplicateDelete\",\\n\"s3:ReplicateTags\"', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[[['', '', ''], ['', \"Note\\nWe recommend that you choose a destination bucket that's different from the source\\nbucket. When the source bucket and destination bucket are the same, additional logs are\\ncreated for the logs that are written to the bucket, which can increase your storage bill.\\nThese extra logs about logs can also make it difficult to find the particular logs that you're\\nlooking for. For simpler log management, we recommend saving access logs in a different\\nbucket. For more information, see the section called “How do I enable log delivery?”.\", ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"Note\\nIf the destination bucket uses the Bucket owner enforced setting for Object Ownership,\\nbe aware of the following:\\n• ACLs are disabled and no longer affect permissions. This means that you can't update\\nyour bucket ACL to grant access to the S3 log delivery group. Instead, to grant access\\nto the logging service principal, you must update the bucket policy for the destination\\nbucket.\\n• You can't include destination grants in your PutBucketLogging configuration.\", ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', '{\\n\"AcceptRanges\": \"bytes\",\\n\"ContentType\": \"text/html\",\\n\"LastModified\": \"Thu, 16 Apr 2015 18:19:14 GMT\",\\n\"ContentLength\": 77,\\n\"VersionId\": \"Zg5HyL7m.eZU9iM7AVlJkrqAiE.0UG4q\",\\n\"ETag\": \"\\\\\"30a6ec7e1a9ad79c203d05a589c8b400\\\\\"\",\\n\"Metadata\": {}\\n}', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'Note\\nFor S3 Object Lock-enabled buckets, a DELETE object request with a protected object\\nversion ID causes a 403 Access Denied error. A DELETE object request without a version ID\\nadds a delete marker as the newest version of the object with a 200 OK response. Objects\\nprotected by Object Lock cannot be permanently deleted until their retention periods and\\nlegal holds are removed. For more information, see the section called “How S3 Object Lock\\nworks”.', ''], ['', '', '']]]\n",
      "[]\n",
      "[[['', '', ''], ['', 'x-amz-request-id: 79104EXAMPLEB723\\nx-amz-id-2: IOWQ4fDEXAMPLEQM+ey7N9WgVhSnQ6JEXAMPLEZb7hSQDASK+Jd1vEXAMPLEa3Km', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nHTTPS requests are encrypted and hidden in most packet captures.', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', '<Error><Code>AccessDenied</Code><Message>Access Denied</Message>\\n<RequestId>79104EXAMPLEB723</RequestId><HostId>IOWQ4fDEXAMPLEQM\\n+ey7N9WgVhSnQ6JEXAMPLEZb7hSQDASK+Jd1vEXAMPLEa3Km</HostId></Error>', ''], ['', '', '']], [['', '', ''], ['', 'Status Code: 403, AWS Service: Amazon S3, AWS Request ID: 79104EXAMPLEB723\\nAWS Error Code: AccessDenied AWS Error Message: Access Denied\\nS3 Extended Request ID: IOWQ4fDEXAMPLEQM+ey7N9WgVhSnQ6JEXAMPLEZb7hSQDASK\\n+Jd1vEXAMPLEa3Km', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', 'PutObjectRequest req = new PutObjectRequest(bucketName, key, createSampleFile());\\ns3.putObject(req);\\nS3ResponseMetadata md = s3.getCachedResponseMetadata(req);\\nSystem.out.println(\"Host ID: \" + md.getHostId() + \" RequestID: \" + md.getRequestId());', ''], ['', '', '']], [['', '', ''], ['', 'Note\\nBy default, the returned log contains only error information. To get the request IDs, the\\nconfig file must have AWSLogMetrics (and optionally, AWSResponseLogging) added.', ''], ['', '', '']], [['', '', ''], ['', 'import logging\\nimport boto3\\nlogging.basicConfig(filename=\\'logfile.txt\\', level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\ns3 = boto3.resource(\\'s3\\')\\nresponse = s3.Bucket(bucket_name).Object(object_key).put()\\nlogger.info(\"HTTPStatusCode: %s\", response[\\'ResponseMetadata\\'][\\'HTTPStatusCode\\'])\\nlogger.info(\"RequestId: %s\", response[\\'ResponseMetadata\\'][\\'RequestId\\'])\\nlogger.info(\"HostId: %s\", response[\\'ResponseMetadata\\'][\\'HostId\\'])\\nlogger.info(\"Date: %s\", response[\\'ResponseMetadata\\'][\\'HTTPHeaders\\'][\\'date\\'])', ''], ['', '', '']]]\n",
      "[[['', '', ''], ['', \"import boto3\\nboto3.set_stream_logger('', logging.DEBUG)\", ''], ['', '', '']], [['', '', ''], ['', 's3 = AWS::S3.new(:logger => Logger.new($stdout), :http_wire_trace => true)', ''], ['', '', '']], [['', '', ''], ['', 's3 = Aws::S3::Client.new(:logger => Logger.new($stdout), :http_wire_trace => true)', ''], ['', '', '']]]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[['Change', 'Description', 'Date'], ['Code examples\\nupdate', 'Code examples updated:\\n•\\nC#—Updated all of the examples to use the task-\\nbased asynchronous pattern. For more information,\\nsee A mazon Web Services Asynchronous APIs for .NET\\nin the AWS SDK for .NET Developer Guide. Code exa\\nmples are now compliant with version 3 of the AWS\\nSDK for .NET.\\n•\\nJava—Updated all of the examples to use the client\\nbuilder model. For more information about the client\\nbuilder model, see Creating Service Clients.\\n•', 'April 30,\\n2018']]]\n",
      "[[['Change', 'Description', 'Date'], ['', 'PHP—Updated all of the examples to use the AWS SDK\\nfor PHP 3.0. For more information about the AWS SDK\\nfor PHP 3.0, see AWS SDK for PHP.\\n•\\nRuby—Updated example code so that the examples\\nwork with the AWS SDK for Ruby version 3.', ''], ['Amazon S3 now\\nreports S3 Glacier\\nFlexible Retrieval\\nand O NEZONE_IA\\nstorage classes to\\nAmazon CloudWatch\\nLogs storage metrics', 'In addition to reporting actual bytes, these storage\\nmetrics include per-object overhead bytes for applicable\\nstorage classes (ONEZONE_IA , STANDARD_IA , and S3\\nGlacier Flexible Retrieval ):\\n•\\nFor ONEZONE_IA and STANDARD_IA storage class\\nobjects, Amazon S3 reports objects smaller than 128\\nKB as 128 KB. For more information, see Using Amazon\\nS3 storage classes.\\n•\\nFor S3 Glacier Flexible Retrieval storage class objects,\\nthe storage metrics report the following overheads:\\n•\\nA 32 KB per-object overhead, charged at S3 Glacier\\nFlexible Retrieval storage class pricing\\n•\\nAn 8 KB per-object overhead, charged at STANDARD\\nstorage class pricing\\nFor more information, see Transitioning objects using\\nAmazon S3 Lifecycle.\\nFor more information about storage metrics, see\\nMonitoring metrics with Amazon CloudWatch.', 'April 30,\\n2018']]]\n",
      "[[['Change', 'Description', 'Date'], ['New storage class', 'Amazon S3 now offers a new storage class, STANDARD_\\nIA (IA, for infrequent access) for storing objects.\\nThis storage class is optimized for long-lived and less\\nfrequently accessed data. For more information, see\\nUsing Amazon S3 storage classes.', 'April 4,\\n2018'], ['Amazon S3 Select', 'Amazon S3 now supports retrieving object content based\\non an SQL expression. For more information, see Filtering\\nand retrieving data using Amazon S3 Select.', 'April 4,\\n2018'], ['Asia Pacific (Osaka-Lo\\ncal) Region', 'Amazon S3 is now available in the Asia Pacific (Osaka-\\nLocal) Region. For more information about Amazon S3\\nRegions and endpoints, see Regions and Endpoints in the\\nAWS General Reference.\\nImportant\\nYou can use the Asia Pacific (Osaka-Local) Region\\nonly in conjunction with the Asia Pacific (Tokyo)\\nRegion. To request access to Asia Pacific (Osaka-\\nLocal) Region, contact your sales representative.', 'February\\n12, 2018'], ['Amazon S3 Inventory\\ncreation timestamp', 'Amazon S3 Inventory now includes a timestamp of\\nthe date and start time of the creation of the Amazon\\nS3 Inventory report. You can use the timestamp to\\ndetermine changes in your Amazon S3 storage from the\\nstart time of when the inventory report was generated.', 'January\\n16, 2018'], ['Europe (Paris) Region', 'Amazon S3 is now available in the Europe (Paris) Region.\\nFor more information about Amazon S3 Regions and\\nendpoints, see Regions and Endpoints in the AWS General\\nReference.', 'December\\n18, 2017']], [['', '', ''], ['', 'Important\\nYou can use the Asia Pacific (Osaka-Local) Region\\nonly in conjunction with the Asia Pacific (Tokyo)\\nRegion. To request access to Asia Pacific (Osaka-\\nLocal) Region, contact your sales representative.', ''], ['', '', '']]]\n",
      "[[['Change', 'Description', 'Date'], ['China (Ningxia)\\nRegion', 'Amazon S3 is now available in the China (Ningxia) Region.\\nFor more information about Amazon S3 Regions and\\nendpoints, see Regions and Endpoints in the AWS General\\nReference.', 'November\\n29, 2017'], ['Support for ORC-\\nformatted Amazon S3\\nInventory files', 'Amazon S3 now supports the Apache optimized row\\ncolumnar (ORC) format in addition to comma-sep\\narated values (CSV) file format for inventory output fil\\nes. Also, you can now query Amazon S3 inventory using\\nstandard SQL by using Amazon Athena, Amazon Redshift\\nSpectrum, and other tools such as Presto, Apache Hive,\\nand Apache Spark. For more information, see Amazon S3\\nInventory.', 'November\\n17, 2017'], ['Default encryption for\\nS3 buckets', 'Amazon S3 default encryption provides a way to set the\\ndefault encryption behavior for an S3 bucket. You can\\nset default encryption on a bucket so that all objects\\nare encrypted when they are stored in the bucket.\\nThe objects are encrypted using server-side encrypt\\nion with either Amazon S3 managed keys (SSE-S3) or\\nAWS managed keys (SSE-KMS). For more information,\\nsee Setting default server-side encryption behavior for\\nAmazon S3 buckets.', 'November\\n06, 2017'], ['Encryption status in\\nAmazon S3 Inventory', 'Amazon S3 now supports including encryption status in\\nAmazon S3 Inventory so you can see how your objects\\nare encrypted at rest for compliance auditing or other\\npurposes. You can also configure to encrypt Amazon S3\\nInventory with server-side encryption (SSE) or SSE-KMS\\nso that all inventory files are encrypted accordingly. For\\nmore information, see Amazon S3 Inventory.', 'November\\n06, 2017']]]\n",
      "[[['Change', 'Description', 'Date'], ['Cross-Region Replicati\\non (CRR) enhanceme\\nnts', 'Cross-Region Replication now supports the following:\\n•\\nIn a cross-account scenario, you can add a CRR conf\\niguration to change replica ownership to the AWS\\naccount that owns the destination bucket. For more inf\\normation, see Changing the replica owner.\\n•\\nBy default, Amazon S3 does not replicate objects in\\nyour source bucket that are created using server-si\\nde encryption using keys stored in AWS KMS In your\\nCRR configuration, you can now direct Amazon S3\\nto replicate these objects. For more information, see\\nReplicating encrypted objects (SSE-C, SSE-S3, SSE-KMS,\\nDSSE-KMS).', 'November\\n06, 2017'], ['Europe (London)\\nRegion', 'Amazon S3 is now available in the Europe (London)\\nRegion. For more information about Amazon S3 Regions\\nand endpoints, see Regions and Endpoints in the AWS\\nGeneral Reference.', 'December\\n13, 2016'], ['Canada (Central)\\nRegion', 'Amazon S3 is now available in the Canada (Central)\\nRegion. For more information about Amazon S3 Regions\\nand endpoints, see Regions and Endpoints in the AWS\\nGeneral Reference.', 'December\\n8, 2016']]]\n",
      "[[['Change', 'Description', 'Date'], ['Object tagging', 'Amazon S3 now supports object tagging. Object tagging\\nenables you to categorize storage. Object key name\\nprefixes also enable you to categorize storage, object\\ntagging adds another dimension to it.\\nThere are added benefits tagging offers. These include:\\n•\\nObject tags enable fine-grained access control of permi\\nssions (for example, you could grant an IAM user per\\nmissions to read-only objects with specific tags).\\n•\\nFine-grained control in specifying lifecycle configu\\nration. You can specify tags to select a subset of objec\\nts to which lifecycle rule applies.\\n•\\nIf you have Cross-Region Replication (CRR) configure\\nd, Amazon S3 can replicate the tags. You must grant\\nnecessary permission to the IAM role created for\\nAmazon S3 to assume to replicate objects on your\\nbehalf.\\n•\\nYou can also customize CloudWatch metrics and\\nCloudTrail events to display information by specific tag\\nfilters.\\nFor more information, see Categorizing your storage\\nusing tags.', 'November\\n29, 2016'], ['Amazon S3 Lifecycle\\nnow supports tag-\\nbased filters', 'Amazon S3 now supports tag-based filtering in lifecycle\\nconfiguration. You can now specify lifecycle rules in\\nwhich you can specify a key prefix, one or more object\\ntags, or a combination of both to select a subset of\\nobjects to which the lifecycle rule applies. For more infor\\nmation, see Managing your storage lifecycle.', 'November\\n29, 2016']]]\n",
      "[[['Change', 'Description', 'Date'], ['CloudWatch request\\nmetrics for buckets', 'Amazon S3 now supports CloudWatch metrics for\\nrequests made on buckets. When you enable these\\nmetrics for a bucket, the metrics report at 1-minut\\ne intervals. You can also configure which objects in\\na bucket will report these request metrics. For more\\ninformation, see Monitoring metrics with Amazon\\nCloudWatch.', 'November\\n29, 2016'], ['Amazon S3 Inventory', 'Amazon S3 now supports storage inventory. Amazon S3\\nInventory provides a flat-file output of your objects and\\ntheir corresponding metadata on a daily or weekly basis\\nfor an S3 bucket or a shared prefix (that is, objects that\\nhave names that begin with a common string).\\nFor more information, see Amazon S3 Inventory.', 'November\\n29, 2016'], ['Amazon S3 Analytics\\n– Storage Class\\nAnalysis', 'The new Amazon S3 analytics – storage class analysis\\nfeature observes data access patterns to help you\\ndetermine when to transition less frequently accessed\\nSTANDARD storage to the STANDARD_IA (IA, for\\ninfrequent access) storage class. After storage class\\nanalysis observes the infrequent access patterns of a\\nfiltered set of data over a period of time, you can use\\nthe analysis results to help you improve your lifecycle\\nconfigurations. This feature also includes a detailed daily\\nanalysis of your storage usage at the specified bucket, pr\\nefix, or tag level that you can export to an S3 bucket.', 'November\\n29, 2016'], ['New Expedited and\\nBulk data retrieval\\ns when restoring\\narchived objects from\\nS3 Glacier', 'Amazon S3 now supports Expedited and Bulk data\\nretrievals in addition to Standard retrievals when\\nrestoring objects archived to S3 Glacier. For more\\ninformation, see Restoring an archived object.', 'November\\n21, 2016']]]\n",
      "[[['Change', 'Description', 'Date'], ['CloudTrail object\\nlogging', 'CloudTrail supports logging Amazon S3 object level API\\noperations such as GetObject , PutObject , and\\nDeleteObject . You can configure your event selectors\\nto log object level API operations. For more information,\\nsee Logging Amazon S3 API calls using AWS CloudTrail.', 'November\\n21, 2016'], ['US East (Ohio) Region', 'Amazon S3 is now available in the US East (Ohio) Region.\\nFor more information about Amazon S3 Regions and\\nendpoints, see Regions and Endpoints in the AWS General\\nReference.', 'October\\n17, 2016'], ['IPv6 support for\\nAmazon S3 Transfer\\nAcceleration', 'Amazon S3 now supports Internet Protocol version 6\\n(IPv6) for Amazon S3 Transfer Acceleration. You can\\nconnect to Amazon S3 over IPv6 by using the new dual-\\nstack for Transfer Acceleration endpoint. For more\\ninformation, see Getting started with Amazon S3 Transfer\\nAcceleration.', 'October\\n6, 2016'], ['IPv6 support', 'Amazon S3 now supports Internet Protocol version 6\\n(IPv6). You can access Amazon S3 over IPv6 by using\\ndual-stack endpoints. For more information, see Making\\nrequests to Amazon S3 over IPv6.', 'August\\n11, 2016'], ['Asia Pacific (Mumbai)\\nRegion', 'Amazon S3 is now available in the Asia Pacific (Mumbai)\\nRegion. For more information about Amazon S3 Regions\\nand endpoints, see Regions and Endpoints in the AWS\\nGeneral Reference.', 'June 27,\\n2016'], ['Amazon S3 Transfer\\nAcceleration', 'Amazon S3 Transfer Acceleration enables fast, easy, and\\nsecure transfers of files over long distances between\\nyour client and an S3 bucket. Transfer Acceleration takes\\nadvantage of Amazon CloudFront globally distributed\\nedge locations.\\nFor more information, see Configuring fast, secure file\\ntransfers using Amazon S3 Transfer Acceleration.', 'April 19,\\n2016']]]\n",
      "[[['Change', 'Description', 'Date'], ['Lifecycle support to\\nremove expired object\\ndelete markers', 'Lifecycle configuration Expiration action now allows\\nyou to direct Amazon S3 to remove expired object delete\\nmarkers in a/ versioned bucket. For more information,\\nsee Elements to describe lifecycle actions.', 'March 16,\\n2016']]]\n",
      "[[['Change', 'Description', 'Date'], ['Bucket lifecycle\\nconfiguration now\\nsupports action to\\nstop incomplete\\nmultipart uploads', \"Bucket lifecycle configuration now supports the\\nAbortIncompleteMultipartUpload action\\nthat you can use to direct Amazon S3 to stop multipart\\nuploads that don't complete within a specified number\\nof days after being initiated. When a multipart upload\\nbecomes eligible for a stop operation, Amazon S3 deletes\\nany uploaded parts and stops the multipart upload.\\nFor conceptual information, see the following topics in\\nthe Amazon S3 User Guide:\\n•\\nAborting a multipart upload\\n•\\nElements to describe lifecycle actions\\nThe following API operations have been updated to\\nsupport the new action:\\n•\\nPUT Bucket lifecycle – The XML configuration now\\nallows you to specify the AbortIncompleteMul\\ntipartUpload action in a lifecycle configuration\\nrule.\\n•\\nList Parts and Initiate Multipart Upload – Both of\\nthese API operations now return two additional\\nresponse headers (x-amz-abort-date , and x -\\namz-abort-rule-id ) if the bucket has a lifecy\\ncle rule that specifies the A bortIncompleteMul\\ntipartUpload action. These headers in the\\nresponse indicate when the initiated multipart upload\\nbecomes eligible for a stop operation and which\\nlifecycle rule is applicable.\", 'March 16,\\n2016']]]\n",
      "[[['Change', 'Description', 'Date'], ['', '', ''], ['Asia Pacific (Seoul)\\nRegion', 'Amazon S3 is now available in the Asia Pacific (Seoul)\\nRegion. For more information about Amazon S3 Regions\\nand endpoints, see Regions and Endpoints in the AWS\\nGeneral Reference.', 'January\\n6, 2016'], ['New condition key\\nand a multipart\\nupload change', 'IAM policies now support an Amazon S3 s3:x-amz-\\nstorage-class condition key. For more information,\\nsee Bucket policy examples using condition keys.\\nYou no longer need to be the initiator of a multipart\\nupload to upload parts and complete the upload.\\nFor more information, see Multipart upload API and\\npermissions.', 'December\\n14, 2015'], ['Renamed the US\\nStandard Region', 'Changed the Region name string from \"US Standard\" to\\n\"US East (N. Virginia).\" This is only a Region name update,\\nthere is no change in the functionality.', 'December\\n11, 2015'], ['New storage class', 'Amazon S3 now offers a new storage class, STANDARD_IA\\n(IA, for infrequent access) for storing objects. This storage\\nclass is optimized for long-lived and less frequently\\naccessed data. For more information, see Using Amazon\\nS3 storage classes.\\nLifecycle configuration feature updates now allow you to\\ntransition objects to the STANDARD_IA storage class. For\\nmore information, see Managing your storage lifecycle.\\nPreviously, the Cross-Region Replication feature used\\nthe storage class of the source object for object replicas.\\nNow, when you configure Cross-Region Replication, you\\ncan specify a storage class for the object replica created\\nin the destination bucket. For more information, see\\nReplicating objects overview.', 'September\\n16, 2015']]]\n",
      "[[['Change', 'Description', 'Date'], ['AWS CloudTrail\\nintegration', 'New AWS CloudTrail integration allows you to record\\nAmazon S3 API activity in your S3 bucket. You can use\\nCloudTrail to track S3 bucket creations or deletions,\\naccess control modifications, or lifecycle configuration\\nchanges. For more information, see Logging Amazon S3\\nAPI calls using AWS CloudTrail.', 'September\\n1, 2015'], ['Bucket limit increase', 'Amazon S3 now supports bucket limit increases. By\\ndefault, customers can create up to 100 buckets in their\\nAWS account. Customers who need additional buckets\\ncan increase that limit by submitting a service limit\\nincrease. For information about how to increase your\\nbucket limit, go to AWS service quotas in the AWS\\nGeneral Reference. For more information, see Using the\\nAWS SDKs and Bucket restrictions and limitations.', 'August 4,\\n2015'], ['Consistency model\\nupdate', 'Amazon S3 now supports read-after-write consistency\\nfor new objects added to Amazon S3 in the US East (N.\\nVirginia) Region. Prior to this update, all Regions except\\nUS East (N. Virginia) Region supported read-after-write\\nconsistency for new objects uploaded to Amazon S3.\\nWith this enhancement, Amazon S3 now supports read-\\nafter-write consistency in all Regions for new objects\\nadded to Amazon S3. Read-after-write consistency allows\\nyou to retrieve objects immediately after creation in\\nAmazon S3. For more information, see Regions.', 'August 4,\\n2015'], ['Event notifications', 'Amazon S3 Event Notifications have been updated to\\nadd notifications when objects are deleted and to add\\nfiltering on object names with prefix and suffix matching.\\nFor more information, see Amazon S3 Event Notifications.', 'July 28,\\n2015']]]\n",
      "[[['Change', 'Description', 'Date'], ['Amazon CloudWatch\\nintegration', 'New Amazon CloudWatch integration allows you to\\nmonitor and set alarms on your Amazon S3 usage\\nthrough CloudWatch metrics for Amazon S3. Supported\\nmetrics include total bytes for Standard storage, total\\nbytes for Reduced-Redundancy Storage, and total\\nnumber of objects for a given S3 bucket. For more\\ninformation, see Monitoring metrics with Amazon\\nCloudWatch.', 'July 28,\\n2015'], ['Support for deleting\\nand emptying non-\\nempty buckets', 'Amazon S3 now supports deleting and emptying non-\\nempty buckets. For more information, see Emptying a\\nbucket.', 'July 16,\\n2015'], ['Bucket policies\\nfor Amazon VPC\\nendpoints', 'Amazon S3 has added support for bucket policies for\\nvirtual private cloud (VPC) (VPC) endpoints. You can\\nuse S3 bucket policies to control access to buckets from\\nspecific VPC endpoints, or specific VPCs. VPC endpoints\\nare easy to configure, are highly reliable, and provide\\na secure connection to Amazon S3 without requiring\\na gateway or a NAT instance. For more information,\\nsee Controlling access from VPC endpoints with bucket\\npolicies.', 'April 29,\\n2015'], ['Event notifications', 'Amazon S3 Event Notifications have been updated to\\nsupport the switch to resource-based permissions for\\nAWS Lambda functions. For more information, see\\nAmazon S3 Event Notifications.', 'April 9,\\n2015'], ['Cross-Region Replicati\\non', 'Amazon S3 now supports Cross-Region Replication.\\nCross-Region Replication is the automatic, asynchron\\nous copying of objects across buckets in different AWS\\nRegions. For more information, see Replicating objects\\noverview.', 'March 24,\\n2015']]]\n",
      "[[['Change', 'Description', 'Date'], ['Event notifications', 'Amazon S3 now supports new event types and destinati\\nons in a bucket notification configuration. Prior to\\nthis release, Amazon S3 supported only the s3:Reduce\\ndRedundancyLostObject event type and an Amazon SNS\\ntopic as the destination. For more information about the\\nnew event types, see Amazon S3 Event Notifications.', 'November\\n13, 2014'], ['Server-side encryptio\\nn with customer-\\nprovided encryption\\nkeys', 'Server-side encryption with AWS Key Management\\nService (AWS KMS) keys (SSE-KMS)\\nAmazon S3 now supports server-side encryption using\\nAWS KMS. This feature allows you to manage the\\nenvelope key through AWS KMS, and Amazon S3 calls\\nAWS KMS to access the envelope key within the permissio\\nns you set.\\nFor more information about server-side encryption\\nwith AWS KMS, see P rotecting Data Using Server-Side\\nEncryption with AWS Key Management Service.', 'November\\n12, 2014'], ['Europe (Frankfurt)\\nRegion', 'Amazon S3 is now available in the Europe (Frankfurt)\\nRegion.', 'October\\n23, 2014'], ['Server-side encryptio\\nn with customer-\\nprovided encryption\\nkeys', 'Amazon S3 now supports server-side encryption using\\ncustomer-provided encryption keys (SSE-C). Server-side\\nencryption enables you to request Amazon S3 to encrypt\\nyour data at rest. When using SSE-C, Amazon S3 encrypt\\ns your objects with the custom encryption keys that you\\nprovide. Since Amazon S3 performs the encryption for\\nyou, you get the benefits of using your own encryption\\nkeys without the cost of writing or executing your own\\nencryption code.\\nFor more information about SSE-C, see Server-Side\\nEncryption (Using Customer-Provided Encryption Keys).', 'June 12,\\n2014']]]\n",
      "[[['Change', 'Description', 'Date'], ['Lifecycle support for\\nversioning', 'Prior to this release, lifecycle configuration was supported\\nonly on nonversioned buckets. Now you can configure\\nlifecycle on both nonversioned and versioning-enabled\\nbuckets. For more information, see Managing your\\nstorage lifecycle.', 'May 20,\\n2014'], ['Access control topics\\nrevised', 'Revised Amazon S3 access control documentation. For\\nmore information, see Identity and Access Management\\nfor Amazon S3.', 'April 15,\\n2014'], ['Server access logging\\ntopic revised', 'Revised server access logging documentation. For more\\ninformation, see Logging requests with server access\\nlogging.', 'November\\n26, 2013'], ['.NET SDK samples\\nupdated to version\\n2.0', '.NET SDK samples in this guide are now compliant to\\nversion 2.0.', 'November\\n26, 2013'], ['SOAP Support Over\\nHTTP deprecated', 'SOAP support over HTTP is deprecated, but it is still\\navailable over HTTPS. New Amazon S3 features will not\\nbe supported for SOAP. We recommend that you use\\neither the REST API or the AWS SDKs.', 'September\\n20, 2013'], ['IAM policy variable\\nsupport', 'IAM policy language now supports variables. When a\\npolicy is evaluated, any policy variables are replaced with\\nvalues that are supplied by context-based information\\nfrom the authenticated user’s session. You can use policy\\nvariables to define general purpose policies without\\nexplicitly listing all the components of the policy. For\\nmore information about policy variables, see IAM Policy\\nVariables Overview in the IAM User Guide.\\nFor examples of policy variables in Amazon S3, see\\nIdentity-based policy examples for Amazon S3.', 'April 3,\\n2013']]]\n",
      "[[['Change', 'Description', 'Date'], ['Console support for\\nRequester Pays', 'You can now configure your bucket for Requester Pays by\\nusing the Amazon S3 console. For more information, see\\nUsing Requester Pays buckets for storage transfers and\\nusage.', 'December\\n31, 2012'], ['Root domain support\\nfor website hosting', 'Amazon S3 now supports hosting static websites at the\\nroot domain. Visitors to your website can access your site\\nfrom their browser without specifying www in the web\\naddress (for example, they can use example.com instead\\nof www.example.com). Many customers already host\\nstatic websites on Amazon S3 that are accessible from\\na www subdomain (for example, www.example.com).\\nPreviously, to support root domain access, you needed to\\nrun your own web server to proxy root domain requests\\nfrom browsers to your website on Amazon S3. Running\\na web server to proxy requests introduces additional\\ncosts, operational burden, and another potential point of\\nfailure. Now, you can take advantage of the high availabil\\nity and durability of Amazon S3 for both w ww and root\\ndomain addresses. For more information, see Hosting a\\nstatic website using Amazon S3.', 'December\\n27, 2012'], ['Console revision', 'Amazon S3 console has been updated. The documenta\\ntion topics that refer to the console have been revised\\naccordingly.', 'December\\n14, 2012']]]\n",
      "[[['Change', 'Description', 'Date'], ['Support for Archiving\\nData to S3 Glacier', \"Amazon S3 now supports a storage option that enables\\nyou to utilize S3 Glacier's low-cost storage service for\\ndata archival. To archive objects, you define archival rules\\nidentifying objects and a time frame when you want\\nAmazon S3 to archive these objects to S3 Glacier. You\\ncan easily set the rules on a bucket using the Amazon S3\\nconsole or programmatically using the Amazon S3 API or\\nAWS SDKs.\\nFor more information, see Managing your storage\\nlifecycle.\", 'November\\n13, 2012'], ['Support for Website\\nPage Redirects', 'For a bucket that is configured as a website, Amazon\\nS3 now supports redirecting a request for an object\\nto another object in the same bucket or to an external\\nURL. For more information, see (Optional) Configuring a\\nwebpage redirect.\\nFor information about hosting websites, see Hosting a\\nstatic website using Amazon S3.', 'October\\n4, 2012'], ['Support for Cross-Ori\\ngin Resource Sharing\\n(CORS)', 'Amazon S3 now supports Cross-Origin Resource Sharing\\n(CORS). CORS defines a way in which client web applicati\\nons that are loaded in one domain can interact with\\nor access resources in a different domain. With CORS\\nsupport in Amazon S3, you can build rich client-side web\\napplications on top of Amazon S3 and selectively allow\\ncross-domain access to your Amazon S3 resources. For\\nmore information, see Using cross-origin resource sharing\\n(CORS).', 'August\\n31, 2012'], ['Support for Cost\\nAllocation Tags', 'Amazon S3 now supports cost allocation tagging, which\\nallows you to label S3 buckets so you can more easily\\ntrack their cost against projects or other criteria. For\\nmore information about using tagging for buckets, see\\nUsing cost allocation S3 bucket tags.', 'August\\n21, 2012']]]\n",
      "[[['Change', 'Description', 'Date'], ['Support for MFA-\\nprotected API access\\nin bucket policies', 'Amazon S3 now supports MFA-protected API access, a\\nfeature that can enforce AWS Multi-Factor Authentic\\nation for an extra level of security when accessing your\\nAmazon S3 resources. It is a security feature that requires\\nusers to prove physical possession of an MFA device\\nby providing a valid MFA code. For more information,\\ngo to AWS Multi-Factor Authentication. You can now\\nrequire MFA authentication for any requests to access\\nyour Amazon S3 resources.\\nTo enforce MFA authentication, Amazon S3 now supports\\nthe aws:MultiFactorAuthAge key in a bucket\\npolicy. For an example bucket policy, see Requiring MFA.', 'July 10,\\n2012'], ['Object Expiration\\nsupport', 'You can use Object Expiration to schedule automatic\\nremoval of data after a configured time period. You set\\nobject expiration by adding lifecycle configuration to a\\nbucket.', '27\\nDecember\\n2011'], ['New Region\\nsupported', 'Amazon S3 now supports the South America (São Paulo)\\nRegion. For more information, see Accessing and listing\\nan Amazon S3 bucket.', 'December\\n14, 2011'], ['Multi-Object Delete', 'Amazon S3 now supports Multi-Object Delete API\\nthat enables you to delete multiple objects in a single\\nrequest. With this feature, you can remove large numbers\\nof objects from Amazon S3 more quickly than using\\nmultiple individual DELETE requests. For more informati\\non, see Deleting Amazon S3 objects.', 'December\\n7, 2011'], ['New Region\\nsupported', 'Amazon S3 now supports the US West (Oregon) Region.\\nFor more information, see Buckets and Regions.', 'November\\n8, 2011'], ['Documentation\\nUpdate', 'Documentation bug fixes.', 'November\\n8, 2011']]]\n",
      "[[['Change', 'Description', 'Date'], ['Documentation\\nUpdate', 'In addition to documentation bug fixes, this release\\nincludes the following enhancements:\\n•\\nNew server-side encryption sections using the AWS SDK\\nfor PHP and the AWS SDK for Ruby (see Specifying\\nserver-side encryption with Amazon S3 managed keys\\n(SSE-S3)).', 'October\\n17, 2011'], ['Server-side encryptio\\nn support', 'Amazon S3 now supports server-side encryption. It\\nenables you to request Amazon S3 to encrypt your data\\nat rest, that is, encrypt your object data when Amazon S3\\nwrites your data to disks in its data centers. In addition\\nto REST API updates, the AWS SDK for Java and .NET\\nprovide necessary functionality to request server-side\\nencryption. You can also request server-side encryptio\\nn when uploading objects using the AWS Managemen\\nt Console. To learn more about data encryption, go to\\nUsing Data Encryption.', 'October\\n4, 2011'], ['Documentation\\nUpdate', 'In addition to documentation bug fixes, this release\\nincludes the following enhancements:\\n•\\nAdded Ruby and PHP samples to the Making requests\\nsection.\\n•\\nAdded sections describing how to generate and use\\npresigned URLs. For more information, see Sharing\\nobjects with presigned URLs and Sharing objects with\\npresigned URLs.\\n•\\nUpdated an existing section to introduce AWS Explorers\\nfor Eclipse and Visual Studio. For more information,\\nsee D eveloping with Amazon S3 using the AWS SDKs.', 'September\\n22, 2011']]]\n",
      "[[['Change', 'Description', 'Date'], ['Support for sending\\nrequests using\\ntemporary security\\ncredentials', 'In addition to using your AWS account and IAM user\\nsecurity credentials to send authenticated requests to\\nAmazon S3, you can now send requests using temporary\\nsecurity credentials you obtain from AWS Identity and\\nAccess Management (IAM). You can use the AWS Security\\nToken Service API or the AWS SDK wrapper libraries\\nto request these temporary credentials from IAM. You\\ncan request these temporary security credentials for\\nyour own use or hand them out to federated users and\\napplications. This feature enables you to manage your\\nusers outside AWS and provide them with temporary\\nsecurity credentials to access your AWS resources.\\nFor more information, see Making requests.\\nFor more information about IAM support for temporary\\nsecurity credentials, see Temporary Security Credentials\\nin the IAM User Guide.', 'August 3,\\n2011'], ['Multipart Upload API\\nextended to enable\\ncopying objects up to\\n5 TB', 'Prior to this release, Amazon S3 API supported copying\\nobjects of up to 5 GB in size. To enable copying objects\\nlarger than 5 GB, Amazon S3 now extends the multipart\\nupload API with a new operation, Upload Part\\n(Copy). You can use this multipart upload operation to\\ncopy objects up to 5 TB in size. For more information, see\\nCopying, moving, and renaming objects.\\nFor conceptual information about multipart upload\\nAPI, see Uploading and copying objects using multipart\\nupload.', 'June 21,\\n2011'], ['SOAP API calls over\\nHTTP disabled', 'To increase security, SOAP API calls over HTTP are\\ndisabled. Authenticated and anonymous SOAP requests\\nmust be sent to Amazon S3 using SSL.', 'June 6,\\n2011']]]\n",
      "[[['Change', 'Description', 'Date'], ['IAM enables cross-acc\\nount delegation', 'Previously, to access an Amazon S3 resource, an IAM user\\nneeded permissions from both the parent AWS account\\nand the Amazon S3 resource owner. With cross-account\\naccess, the IAM user now only needs permission from the\\nowner account. That is, If a resource owner grants access\\nto an AWS account, the AWS account can now grant its\\nIAM users access to these resources.\\nFor more information, see Creating a role to delegate\\npermissions to an IAM user in the IAM User Guide.\\nFor more information on specifying principals in a bucket\\npolicy, see Principals for bucket policies.', 'June 6,\\n2011'], ['New link', \"This service's endpoint information is now located in the\\nAWS General Reference. For more information, go to\\nRegions and Endpoints in the AWS General Reference.\", 'March 1,\\n2011'], ['Support for hosting\\nstatic websites in\\nAmazon S3', 'Amazon S3 introduces enhanced support for hosting\\nstatic websites. This includes support for index\\ndocuments and custom error documents. When using\\nthese features, requests to the root of your bucket or a\\nsubfolder (for example, http://mywebsite.com/\\nsubfolder ) returns your index document instead of\\nthe list of objects in your bucket. If an error is encounter\\ned, Amazon S3 returns your custom error message\\ninstead of an Amazon S3 error message. For more\\ninformation, see H osting a static website using Amazon\\nS3.', 'June 6,\\n2011']]]\n",
      "[[['Change', 'Description', 'Date'], [\"This service's\\nendpoint informati\\non is now located\\nin the AWS General\\nReference. For\\nmore information,\\ngo to Regions and\\nEndpoints in the AWS\\nGeneral Reference.\", 'March 1, 2011', ''], ['Support for hosting\\nstatic websites in\\nAmazon S3', 'Amazon S3 introduces enhanced support for hosting\\nstatic websites. This includes support for index\\ndocuments and custom error documents. When using\\nthese features, requests to the root of your bucket or a\\nsubfolder (for example, http://mywebsite.com/\\nsubfolder ) returns your index document instead of\\nthe list of objects in your bucket. If an error is encounter\\ned, Amazon S3 returns your custom error message\\ninstead of an Amazon S3 error message. For more\\ninformation, see H osting a static website using Amazon\\nS3.', 'February\\n17, 2011'], ['Response Header API\\nSupport', 'The GET Object REST API now allows you to change the\\nresponse headers of the REST GET Object request for\\neach request. That is, you can alter object metadata in\\nthe response, without altering the object itself. For more\\ninformation, see Downloading objects.', 'January\\n14, 2011'], ['Large object support', 'Amazon S3 has increased the maximum size of an object\\nyou can store in an S3 bucket from 5 GB to 5 TB. If you\\nare using the REST API, you can upload objects of up to 5\\nGB in a single PUT operation. For larger objects, you must\\nuse the Multipart Upload REST API to upload objects in\\nparts. For more information, see Uploading and copying\\nobjects using multipart upload.', 'December\\n9, 2010']]]\n",
      "[[['Change', 'Description', 'Date'], ['Multipart upload', 'Multipart upload enables faster, more flexible uploads\\ninto Amazon S3. It allows you to upload a single object as\\na set of parts. For more information, see Uploading and\\ncopying objects using multipart upload.', 'November\\n10, 2010'], ['Canonical ID support\\nin bucket policies', 'You can now specify canonical IDs in bucket policies. For\\nmore information, see Principals for bucket policies', 'September\\n17, 2010'], ['Amazon S3 works\\nwith IAM', 'This service now integrates with AWS Identity and Access\\nManagement (IAM). For more information, go to AWS\\nservices that work with IAM in the IAM User Guide.', 'September\\n2, 2010'], ['Notifications', 'The Amazon S3 notifications feature enables you to\\nconfigure a bucket so that Amazon S3 publishes a\\nmessage to an Amazon Simple Notification Service\\n(Amazon SNS) topic when Amazon S3 detects a key\\nevent on a bucket. For more information, see Setting Up\\nNotification of Bucket Events.', 'July 14,\\n2010'], ['Bucket policies', 'Bucket policies are an access management system that\\nyou use to set access permissions across buckets, objects,\\nand sets of objects. This functionality supplements and\\nin many cases replaces access control lists. For more\\ninformation, see Bucket policies for Amazon S3.', 'July 6,\\n2010'], ['Path-style syntax\\navailable in all\\nRegions', 'Amazon S3 now supports the path-style syntax for any\\nbucket in the US Classic Region, or if the bucket is in the\\nsame Region as the endpoint of the request. For more\\ninformation, see Virtual Hosting.', 'June 9,\\n2010'], ['New endpoint for\\nEurope (Ireland)', 'Amazon S3 now provides an endpoint for Europe (Ireland)\\n: h ttp://s3-eu-west-1.amazonaws.com .', 'June 9,\\n2010']]]\n",
      "[[['Change', 'Description', 'Date'], ['Console', 'You can now use Amazon S3 through the AWS\\nManagement Console. You can read about all of the\\nAmazon S3 functionality in the console in the Amazon\\nSimple Storage Service User Guide.', 'June 9,\\n2010'], ['Reduced Redundancy', 'Amazon S3 now enables you to reduce your storage costs\\nby storing objects in Amazon S3 with reduced redundanc\\ny. For more information, see Reduced Redundancy\\nStorage.', 'May 12,\\n2010'], ['New Region\\nsupported', 'Amazon S3 now supports the Asia Pacific (Singapore)\\nRegion. For more information, see Buckets and Regions.', 'April 28,\\n2010'], ['Object Versioning', 'This release introduces object versioning. All objects now\\ncan have a key and a version. If you enable versioning for\\na bucket, Amazon S3 gives all objects added to a bucket\\na unique version ID. This feature enables you to recover\\nfrom unintended overwrites and deletions. For more\\ninformation, see Versioning and Using Versioning.', 'February\\n8, 2010'], ['New Region\\nsupported', 'Amazon S3 now supports the US West (N. California)\\nRegion. The new endpoint for requests to this Region is\\ns3-us-west-1.amazonaws.com . For more informati\\non, see Buckets and Regions.', 'December\\n2, 2009'], ['AWS SDK for .NET', \"AWS now provides libraries, sample code, tutorials, and\\nother resources for software developers who prefer\\nto build applications using .NET language-specific API\\noperations instead of REST or SOAP. These libraries\\nprovide basic functions (not included in the REST or\\nSOAP APIs), such as request authentication, request\\nretries, and error handling so that it's easier to get\\nstarted. For more information about language-specific\\nlibraries and resources, see Developing with Amazon S3\\nusing the AWS SDKs.\", 'November\\n11, 2009']]]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "with pdfplumber.open(\"s3-userguide.pdf\") as pdf:\n",
    "    # init raw data array\n",
    "    lines_data = []\n",
    "    # loop each page with\n",
    "    for page_num, page in enumerate(pdf.pages, start=1):\n",
    "\n",
    "        lines = filter_tables(page).extract_text(layout=True).split(\"\\n\")\n",
    "        for line_num, line in enumerate(lines, start=1):\n",
    "            lines_data.append(\n",
    "                {\n",
    "                    \"content\": line,\n",
    "                    \"title\": lines[2],\n",
    "                    \"section_title\": lines[-4],\n",
    "                }\n",
    "            )\n",
    "        page_tables = page.extract_tables()\n",
    "        for table in page_tables:\n",
    "            table_rows = [row for row in table]\n",
    "            for row in table_rows:\n",
    "                for col in row:\n",
    "                    if col != \"\":\n",
    "                        lines_data.append(\n",
    "                            {\n",
    "                                \"content\": col,\n",
    "                                \"title\": lines[2],\n",
    "                                \"section_title\": lines[-4],\n",
    "                            }\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "    with open(\"raw.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(lines_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean, add rule and group raw data to data which can import to ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"raw.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines_data = json.load(f)\n",
    "\n",
    "filtered_lines_data = []\n",
    "table_of_contents = {}\n",
    "current_content = \"\"\n",
    "for line_data in lines_data:\n",
    "    if line_data[\"content\"]:\n",
    "        content = line_data[\"content\"].strip()\n",
    "    \n",
    "        if content != \"\":\n",
    "            content = re.sub(r\"\\s+\", \" \", content)\n",
    "            if \".........\" in content:\n",
    "                content = re.sub(r\"\\.+\", \"@@@\", content)\n",
    "                title = content.split(\"@@@\")[0].strip()\n",
    "                page_num = content.split(\"@@@\")[1].strip()\n",
    "                table_of_contents[title] = page_num\n",
    "                continue\n",
    "            if (\n",
    "                \"API Version 2006-03-01\" not in content\n",
    "                and \"Amazon Simple Storage Service\" not in content\n",
    "                and content != \"\"\n",
    "            ):\n",
    "                if current_content:\n",
    "                    if content[0] == \"-\" or content[0] == \"•\" or content[0].isnumeric():\n",
    "                        current_content += \"\\n\" + content\n",
    "                    elif content[0].islower():\n",
    "                        current_content += \" \" + content\n",
    "                    elif current_content[-1] != \".\":\n",
    "                        current_content += \" \" + content\n",
    "                    elif current_content[-1] == \",\" or current_content[-1] == \":\":\n",
    "                        current_content += \" \" + content\n",
    "                    elif content[0].islower():\n",
    "                        current_content += \" \" + content\n",
    "                    else:\n",
    "                        new = {}\n",
    "                        new[\"section_title\"] = \"\"\n",
    "                        page = line_data[\"section_title\"].strip().split(\" \")[-1]\n",
    "                        new[\"title\"] = re.sub(r\"\\s+\", \" \", line_data[\"title\"].strip())\n",
    "                        new[\"content\"] = current_content\n",
    "                        if page in table_of_contents.values():\n",
    "                            new[\"section_title\"] = list(table_of_contents.keys())[\n",
    "                                list(table_of_contents.values()).index(page)\n",
    "                            ]\n",
    "                        else:\n",
    "                            new[\"section_title\"] = (\n",
    "                                line_data[\"section_title\"]\n",
    "                                .strip()\n",
    "                                .split(\"API Version\")[0]\n",
    "                                .strip()\n",
    "                            )\n",
    "\n",
    "                        new[\"page\"] = page\n",
    "                        if (\n",
    "                            new[\"section_title\"]\n",
    "                            and \"Table of Contents\" not in new[\"content\"]\n",
    "                        ):\n",
    "                            filtered_lines_data.append(new)\n",
    "                        current_content = content\n",
    "                else:\n",
    "                    current_content = content\n",
    "\n",
    "with open(\"output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_lines_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put data to ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/tvpwz_9d6c56fms92tltys800000gn/T/ipykernel_76373/1241684842.py:7: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(index=\"s3_guide\", body=doc)\n"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch([{\"host\": \"localhost\", \"port\": 9200, \"scheme\": \"http\"}])\n",
    "\n",
    "with open(\"output.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for doc in data:\n",
    "    es.index(index=\"s3_guide\", body=doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
